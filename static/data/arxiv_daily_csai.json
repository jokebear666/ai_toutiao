{
  "label": "cs.AI",
  "slug": "csai",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "Enriching Historical Records: An OCR and AI-Driven Approach for Database Integration",
      "authors": "Zahra Abedi, Richard M.K. van Dijk, Gijs Wijnholds, Tessa Verhoef",
      "institution": "Leiden University",
      "link": "https://arxiv.org/pdf/2512.23710",
      "code": null,
      "tags": [
        "information extraction",
        "OCR",
        "LLM",
        "record linkage",
        "digital humanities",
        "data harmonization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d89795562de4ae19fec35c7e5d2890c5c08f327549b9a9887347777baf5b0c5_w640_q70.webp",
      "contributions": "1. Designed an automated pipeline integrating OCR, LLM-based interpretation, and database linking for historical document digitization. 2. Demonstrated that generative AI can partially correct low OCR performance during structured data extraction. 3. Developed a record linkage algorithm achieving high accuracy (94% on annotated data, 81% on OCR-derived data) for integrating extracted data with existing databases.",
      "summary": "This paper proposes an automated pipeline using OCR and generative AI to extract and structure biographical data from historical documents, then links this data to existing database records. The method achieved high OCR accuracy and demonstrated that LLMs can correct some OCR errors, with the final linkage algorithm performing well. The study contributes a practical tool for digital humanities research by addressing challenges like layout variability and terminology differences.",
      "mindmap": "graph TB\n        Root[Enriching Historical Records: An OCR and AI-Driven Approach for Database Integration] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>How to automate the integration of historical document data with existing databases?]\n        Method[主要方法/Method<br>OCR + LLM-based interpretation + Database linkage]\n        Results[关键结果/Results<br>High OCR accuracy, LLM corrects OCR errors, Effective record linkage]"
    },
    {
      "title": "HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate",
      "authors": "Shenzhe Zhu",
      "institution": "University of Toronto",
      "link": "https://arxiv.org/pdf/2512.23717",
      "code": null,
      "tags": [
        "safety alignment",
        "multi-agent debate",
        "stealthy harmful queries",
        "safety training data",
        "query transformation",
        "adversarial prompting"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9bfc9b669283c35e277363d47c398576a9ae941e3c10bc7ea3296632fc0184f_w640_q70.webp",
      "contributions": "1. Introduces HarmTransform, the first multi-agent debate framework designed for transforming harmful queries into stealthier forms while preserving intent. 2. Designs a comprehensive evaluation protocol and provides an in-depth analysis of debate dynamics, identifying its benefits and drawbacks. 3. Demonstrates the framework's potential for generating data to enhance LLM safety alignment, highlighting both the promise and limitations of the approach.",
      "summary": "The paper introduces HarmTransform, a multi-agent debate framework that iteratively refines harmful queries into stealthier forms to expose gaps in LLM safety mechanisms. Experiments show it outperforms baselines in generating effective transformations. The analysis reveals that while debate improves stealth, it can also introduce topic shifts and complexity, highlighting its dual nature for safety data generation.",
      "mindmap": "graph TB\n        A[HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[LLM安全机制忽略隐蔽有害查询/LLM safety overlooks covert harmful queries]\n        C --> C1[多智能体辩论迭代优化查询/Multi-agent debate iteratively refines queries]\n        D --> D1[辩论提升隐蔽性但可能引入复杂性/Debate improves stealth but may add complexity]"
    },
    {
      "title": "STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability",
      "authors": "Guanghui Wang, Jinze Yu, Xing Zhang, Dayuan Jiang, Yin Song, Tomal Deb, Xuefeng Liu, Peiyang He",
      "institution": "AWS Generative AI Innovation Center, AWS WWSO SA Field Initiatives",
      "link": "https://arxiv.org/pdf/2512.23712",
      "code": null,
      "tags": [
        "evaluation & metrics",
        "STED",
        "consistency scoring",
        "structured output",
        "JSON",
        "semantic equivalence"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2a404e6faa604b1e48c4c20d60e0b88bb889cfe94b29f60a234af8a87b5d05b_w640_q70.webp",
      "contributions": "1. Proposes STED (Semantic Tree Edit Distance), a novel similarity metric for comparing JSON outputs that balances semantic flexibility with structural strictness. 2. Introduces a comprehensive consistency scoring framework that aggregates multiple STED measurements across repeated generations to quantify LLM output reliability. 3. Provides a systematic benchmark of six LLMs using the proposed framework, revealing significant variations in model consistency and enabling practical applications like model selection and prompt refinement.",
      "summary": "This paper addresses the challenge of evaluating the consistency of LLM-generated structured outputs (like JSON). It proposes a framework combining a new similarity metric (STED) and a consistency scoring method. The framework effectively benchmarks LLMs, showing Claude-3.7-Sonnet to be highly consistent, and provides tools for improving reliability in production systems.",
      "mindmap": "graph TB\n        A[STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: LLM生成结构化输出的可靠性评估/Evaluating Reliability of LLM Structured Outputs]\n        C[主要方法/Method: STED度量与一致性评分框架/STED Metric & Consistency Scoring Framework]\n        D[关键结果/Results: STED优于现有指标，Claude-3.7-Sonnet一致性最佳/STED Outperforms Baselines, Claude-3.7-Sonnet Most Consistent]"
    },
    {
      "title": "PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents",
      "authors": "Jahidul Islam, Md Ataullha, Saiful Azad",
      "institution": "Green University of Bangladesh",
      "link": "https://arxiv.org/pdf/2512.23713",
      "code": "github.com/jahidulzaid/PyBanglaCodeActAgent",
      "tags": [
        "code generation",
        "agent-based framework",
        "iterative self-correction",
        "multilingual LLM",
        "Thought-Code-Observation loop",
        "zero-shot"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6751bddc58c50ee4806ff6af3cccc66715b2e854b912a102cbc99edee59d5c86_w640_q70.webp",
      "contributions": "1. Introduced BanglaCodeAct, an agent-based framework for Bangla-to-Python code generation. 2. Leveraged a multilingual LLM in a zero-shot setting without task-specific fine-tuning. 3. Demonstrated the effectiveness of an iterative Thought-Code-Observation loop for dynamic code testing and refinement.",
      "summary": "This paper addresses the challenge of generating Python code from Bangla natural language instructions, a low-resource language. It proposes BanglaCodeAct, an agent-based framework that uses a multilingual LLM within an iterative Thought-Code-Observation loop for zero-shot code generation and self-correction. The method, tested with Qwen3-8B, achieves high accuracy on the mHumanEval dataset, setting a new benchmark for Bangla NL-to-Code.",
      "mindmap": "graph TB\n        A[PyBangla at BLP-2025 Task 2<br>论文标题/Paper Title] --> B[LLMs excel in English but not low-resource languages<br>核心问题/Problem]\n        A --> C[Introduce BanglaCodeAct with multi-agent & iterative self-correction<br>主要方法/Method]\n        A --> D[Qwen3-8B achieves 94.0% (dev) and 71.6% (test) pass@1<br>关键结果/Results]"
    },
    {
      "title": "A Survey of AI Methods for Geometry Preparation and Mesh Generation in Engineering Simulation",
      "authors": "Steven Owen, Nathan Brown, Nikos Chrisochoides, Rao Garimella, Xianfeng Gu, Franck Ledoux, Na Lei, Roshan Quadros, Navamita Ray, Nicolas Winovich, Yongjie Jessica Zhang",
      "institution": "Sandia National Laboratories, Old Dominion University, Los Alamos National Laboratory, New York University / Stony Brook University, CEA, Dalian University of Technology, Carnegie Mellon University",
      "link": "https://arxiv.org/pdf/2512.23719",
      "code": null,
      "tags": [
        "computational geometry",
        "mesh generation",
        "geometry preparation",
        "CAD-to-mesh",
        "machine learning",
        "large language models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c3d8c73084ddd952c2e0c9fd20e1b7fe1d87bb02c884cf6327ca47e6ec442eb_w640_q70.webp",
      "contributions": "1. Surveys the application of AI/ML methods to automate and improve key steps in the CAD-to-mesh pipeline, such as part classification, mesh quality prediction, and defeaturing. 2. Reviews AI techniques for enhancing unstructured/block-structured meshing, volumetric parameterization, and parallel mesh generation. 3. Examines emerging tools like reinforcement learning and large language models for scripting automation in meshing workflows.",
      "summary": "This survey paper reviews how artificial intelligence and machine learning are being applied to address bottlenecks in geometry preparation and mesh generation for engineering simulation. It explores a range of methods, from quality prediction to automation with large language models, concluding that AI serves as an assistive technology to extend traditional tools and highlights key challenges for future data-driven workflows.",
      "mindmap": "graph TB\n        A[A Survey of AI Methods for Geometry Preparation and Mesh Generation in Engineering Simulation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[CAD-to-mesh流程瓶颈 / CAD-to-mesh Pipeline Bottlenecks]\n        C --> C1[AI辅助几何与网格生成 / AI-aided Geometry & Meshing]\n        C --> C2[机器学习方法 / Machine Learning Methods]\n        C --> C3[新兴自动化工具 / Emerging Automation Tools]\n        C1 --> C1a[部件分类 / Part Classification]\n        C1 --> C1b[网格质量预测 / Mesh Quality Prediction]\n        C1 --> C1c[去特征化 / Defeaturing]\n        C2 --> C2a[非结构化/块结构化网格 / Unstructured/Block-structured Meshing]\n        C2 --> C2b[体积参数化 / Volumetric Parameterizations]\n        C2 --> C2c[并行网格生成 / Parallel Mesh Generation]\n        C3 --> C3a[强化学习 / Reinforcement Learning]\n        C3 --> C3b[大语言模型 / Large Language Models]\n        D --> D1[AI作为辅助技术 / AI as Assistive Technology]\n        D --> D2[代表性方法与部署 / Representative Methods & Deployments]\n        D --> D3[关键研究挑战 / Key Research Challenges]"
    },
    {
      "title": "When in Doubt, Deliberate: Confidence-Based Routing to Expert Debate for Sexism Detection",
      "authors": "Anwar Alajmi, Gabriele Pergola",
      "institution": "University of Warwick, Public Authority of Applied Education and Training (Kuwait)",
      "link": "https://arxiv.org/pdf/2512.23732",
      "code": null,
      "tags": [
        "hate speech detection",
        "collaborative expert judgment",
        "confidence-based routing",
        "class-balanced focal loss"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d2baab37215bb5672da3307e81be99d99c0b9c3a16715fc3d5d0430dfd9273_w640_q70.webp",
      "contributions": "1. A two-stage framework combining targeted training for noisy, imbalanced data with selective, reasoning-based inference for ambiguous cases. 2. A novel Collaborative Expert Judgment (CEJ) module that uses multiple LLM personas in a structured debate to resolve uncertain predictions. 3. A dynamic routing mechanism at inference time that directly classifies high-confidence cases and escalates low-confidence ones to the CEJ module.",
      "summary": "The paper addresses the challenges of detecting subtle, context-dependent sexist content online, which suffers from data noise, class imbalance, and conceptual ambiguity. It proposes a framework that uses robust training techniques and a novel inference module where uncertain cases are routed to a multi-persona LLM debate for judgment. This approach achieves state-of-the-art performance on benchmark sexism detection tasks.",
      "mindmap": "graph TB\n        A[When in Doubt, Deliberate: Confidence-Based Routing to Expert Debate for Sexism Detection] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Subtle, ambiguous sexist content<br>Data noise, imbalance, ambiguity]\n        C[主要方法/Method<br>Two-stage framework<br>Robust training & CEJ routing]\n        D[关键结果/Results<br>SOTA on benchmarks<br>+2.72% F1 on EXIST]"
    },
    {
      "title": "Enforcing Temporal Constraints for LLM Agents",
      "authors": "Adharsh Kamath, Sishen Zhang, Calvin Xu, Shubham Ugare, Gagandeep Singh, Sasa Misailovic",
      "institution": "University of Illinois at Urbana-Champaign, Meta",
      "link": "https://arxiv.org/pdf/2512.23738",
      "code": "https://github.com/structuredllm/agent-c",
      "tags": [
        "agent system",
        "temporal constraints",
        "SMT solving",
        "constrained generation",
        "formal verification",
        "LLM agents"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a978adfab8b202d7b971f6b65f8d005235baabb93f5540985ad131638c67354_w640_q70.webp",
      "contributions": "1. A novel framework (Agent-C) providing runtime guarantees for LLM agents to adhere to formal temporal safety properties., 2. A domain-specific language for expressing temporal properties, which are translated to first-order logic and verified via SMT solving during token generation., 3. Demonstration of perfect safety (100% conformance) and improved task utility across real-world applications and multiple LLMs, outperforming state-of-the-art guardrails.",
      "summary": "The paper addresses the problem of LLM agents violating temporal safety policies, such as accessing data before authentication. It proposes Agent-C, a framework that uses a domain-specific language, formal logic translation, and SMT solving to enforce constraints during token generation, ensuring compliant actions. The evaluation shows Agent-C achieves 100% safety conformance and improves task utility compared to existing methods.",
      "mindmap": "graph TB\n        A[Enforcing Temporal Constraints for LLM Agents] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有护栏无法保证时间安全策略/Existing guardrails fail to enforce temporal safety policies]\n        C --> C1[提出Agent-C框架/Propose Agent-C framework]\n        C1 --> C2[使用DSL和SMT求解进行运行时验证/Use DSL & SMT solving for runtime verification]\n        C2 --> C3[采用约束生成确保合规/Achieve compliance via constrained generation]\n        D --> D1[100%安全性，0%危害/100% safety, 0% harm]\n        D --> D2[在真实应用中提高任务效用/Improve task utility in real-world applications]"
    },
    {
      "title": "Towards representation agnostic probabilistic programming",
      "authors": "Ole Fenske, Maximilian Popko, Sebastian Bader, Thomas Kirste",
      "institution": "Institute for Visual and Analytic Computing, University of Rostock",
      "link": "https://arxiv.org/pdf/2512.23740",
      "code": null,
      "tags": [
        "compiler & ir",
        "factor abstraction",
        "probabilistic programming",
        "hybrid models",
        "representation-agnostic",
        "factor graphs"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97ac8ba28932b58d58cfedc5a8c2d53a706dd206b4f545e3d26852fb3ee19d75_w640_q70.webp",
      "contributions": "1. Introduces a factor abstraction with five fundamental operations as a universal interface for manipulating probabilistic factors. 2. Enables representation-agnostic probabilistic programming, allowing the mixing of different distribution representations (e.g., discrete tables, Gaussians, samples) within a single framework. 3. Facilitates practical inference in complex hybrid (mixed discrete-continuous) models that current toolkits cannot adequately handle.",
      "summary": "The paper addresses the tight coupling between model representations and inference algorithms in current probabilistic programming tools, which limits flexibility. It proposes a factor abstraction with a set of core operations to create a representation-agnostic interface. This allows users to mix various distribution representations, enabling inference in complex hybrid models previously difficult to express.",
      "mindmap": "graph TB\n        A[Towards representation agnostic probabilistic programming] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[PPLs耦合表示与推理算法/PPLs couple representations & inference]\n        B --> B2[阻碍混合模型实验/Prevents hybrid model experimentation]\n        C --> C1[引入因子抽象/Introduce factor abstraction]\n        C --> C2[定义五个基本操作/Define five fundamental operations]\n        C --> C3[创建通用接口/Create universal interface]\n        D --> D1[实现表示无关编程/Enable representation-agnostic programming]\n        D --> D2[支持混合表示/Support mixing representations]\n        D --> D3[处理复杂混合模型/Handle complex hybrid models]"
    },
    {
      "title": "Break Out the Silverware -- Semantic Understanding of Stored Household Items",
      "authors": "Michaela Levi-Richter, Reuth Mirsky, Oren Glickman",
      "institution": "Bar Ilan University, Tufts University",
      "link": "https://arxiv.org/pdf/2512.23739",
      "code": null,
      "tags": [
        "commonsense reasoning",
        "benchmark dataset",
        "vision-language model",
        "hybrid agent pipeline",
        "storage location prediction",
        "semantic understanding"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02333560037f17a9692645550b2d71e1239038dba5b036552b34388848b00f1f_w640_q70.webp",
      "contributions": "1. Introduces the Stored Household Item Challenge, a new benchmark for evaluating service robots' commonsense reasoning about predicting the storage location of non-visible household items. 2. Provides two associated datasets: a real-world evaluation set and a larger development set with annotated storage polygons. 3. Proposes NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with LLM inference to tackle the challenge, demonstrating improved accuracy approaching human performance.",
      "summary": "This paper addresses the challenge of enabling domestic robots to infer where non-visible household items are stored. It proposes a new benchmark task and datasets, and introduces NOAM, a hybrid vision-language agent that converts visual scenes into text for an LLM to predict storage locations. Evaluations show NOAM significantly outperforms baseline models and approaches human-level performance in this commonsense reasoning task.",
      "mindmap": "graph TB\n        Root[Break Out the Silverware: Semantic Understanding of Stored Household Items] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Robots lack commonsense reasoning to find stored, non-visible household items.]\n        Method[主要方法/Method: Proposes NOAM, a hybrid pipeline combining scene understanding and LLM inference.]\n        Results[关键结果/Results: NOAM approaches human-level accuracy on the new storage prediction benchmark.]"
    },
    {
      "title": "AgenticTCAD: A LLM-based Multi-Agent Framework for Automated TCAD Code Generation and Device Optimization",
      "authors": "Guangxi Fan, Tianliang Ma, Xuguang Sun, Xun Wang, Kain Lu Low, Leilai Shao",
      "institution": "Shanghai Jiao Tong University, Xi’an Jiaotong–Liverpool University",
      "link": "https://arxiv.org/pdf/2512.23742",
      "code": null,
      "tags": [
        "agent system",
        "TCAD code generation",
        "multi-agent framework",
        "device optimization",
        "LLM fine-tuning",
        "DTCO"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9454cdfb5c93c8b3f9587f99a9e8982d695a9783f118909a0fd90f5e347f512e_w640_q70.webp",
      "contributions": "1. Construction of an open-source, expert-curated TCAD dataset and fine-tuning of a domain-specific LLM for TCAD code generation. 2. Proposal of AgenticTCAD, a natural language-driven multi-agent framework for end-to-end automated device design and optimization. 3. Demonstration of the framework's efficiency, achieving target device specifications in 4.2 hours compared to 7.1 days for human experts.",
      "summary": "This paper addresses the challenge of generating valid TCAD simulation code due to a lack of open-source data. It proposes AgenticTCAD, a multi-agent LLM framework that automates device design and optimization from natural language. The system was validated on a 2 nm nanosheet FET design, achieving target specifications significantly faster than human experts.",
      "mindmap": "graph TB\n        Root(”AgenticTCAD: LLM多智能体框架 / AgenticTCAD: LLM-based Multi-Agent Framework”) --> Problem(”TCAD代码生成资源稀缺 / Scarcity of TCAD Code Generation Resources”)\n        Root --> Method(”构建数据集与多智能体框架 / Dataset Construction & Multi-Agent Framework”)\n        Root --> Results(”4.2小时达到IRDS规格 / Achieves IRDS Specs in 4.2 Hours”)"
    },
    {
      "title": "State-of-the-art Small Language Coder Model: Mify-Coder",
      "authors": "Abhinav Parmar, Abhisek Panigrahi, Abhishek Kumar Dwivedi, Abhishek Bhattacharya, Adarsh Ramachandra, Aditya Choudhary, Aditya Garg, Aditya Raj, Alankrit Bhatt, Alpesh Yadav, Anant Vishnu, Ananthu Pillai, Ankush Kumar, Aryan Patnaik, Aswatha Narayanan S, Avanish Raj Singh, Bhavya Shree Gadda, Brijesh Pankajbhai Kachhadiya, Buggala Jahnavi, Chidurala Nithin Krishna, Chintan Shah, Chunduru Akshaya, Debarshi Banerjee, Debrup Dey, Deepa R., Deepika B G, Faiz ur Rahman, Gagan Gayari, Gudhi Jagadeesh Kumar Naidu, Gursimar Singh, Harshal Tyagi, Harshini K, James Mani Vathalloor, Jayarama Nettar, Jayashree Gajjam, Joe Walter Sugil George, Kamalakara Sri Krishna Tadepalli, Kamalkumar Rathinasamy, Karan Chaurasia, Karthikeyan S, Kashish Arora, Kaushal Desai, Khushboo Buwade, Kiran Manjrekar, Malikireddy Venkata Sai Likhitha, Manjunath A, Mitali Mahavir Bedmutha, Mohammed Rafee Tarafdar, Nikhil Tiwari, Nikitha K Gigi, Pavan Ravikumar, Pendyala Swarnanjali, Piyush Anand, Prakash Chandrasekar, Prasanna Bhalchandra Gawade, Prasanth Sivan, Preeti Khurana, Priyanshi Babbar, Rajab Ali Mondal, Rajesh Kumar Vissapragada, Rajeshwari Ganesan, Rajeswari Koppisetti, Ramjee R., Ramkumar Thiruppathisamy, Rani G. S., S Reka, Samarth Gupta, Sandeep Reddy Kothakota, Sarathy K, Sathyanarayana Sampath Kumar, Saurabh Kumar, Shashank Khasare, Shenbaga Devi Venkatesh Kumar, Shiva Rama Krishna Parvatham, Shoeb Shaikh, Shrishanmathi A, Shubham Pathak, Sree Samhita Koppaka, Sreenivasa Raghavan K S, Sreeram Venkatasubramanian, Suprabha Desai Bojja, Swetha R, Syed Ahmed, Chinmai Harshitha Thota, Tushar Yadav, Veeravelly Kusumitha, V V S S Prasanth Patnaik, Vidya Sri Sesetti, Vijayakeerthi K, Vikram Raj Bakshi, Vinay K K, Vinoth Kumar Loganathan, Vipin Tiwari, Vivek Kumar Shrivastav, V Venkata Sri Datta Charan, Wasim Akhtar Khan",
      "institution": "Infosys AI Research",
      "link": "https://arxiv.org/pdf/2512.23747",
      "code": null,
      "tags": [
        "llm training",
        "compute-optimal training",
        "CPT-SFT",
        "synthetic data generation",
        "model quantization",
        "quality filtering"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13c8af9d7668bbc4ac7ae9ad0ed379feb93f982a5fecdc8491169bd4d6c33d30_w640_q70.webp",
      "contributions": "1. Introduced Mify-Coder, a 2.5B-parameter code model trained with a compute-optimal strategy on 4.2T tokens, demonstrating that compact models can match frontier-grade performance. 2. Developed a training pipeline combining high-quality curated data with agentically generated synthetic data, refined using enterprise-grade evaluations and LLM-based quality filtering for high data density. 3. Showed that disciplined exploration of training objectives and data mixtures within a single continuous trajectory enables competitive accuracy, efficiency, and safety, with quantized variants enabling deployment on standard hardware.",
      "summary": "The paper addresses the high computational cost of large code models by proposing Mify-Coder, a compact 2.5B-parameter model trained using a compute-optimal strategy that integrates curated and synthetic data with quality filtering. It demonstrates that this approach allows a small model to achieve performance comparable to much larger models on coding benchmarks while maintaining safety and enabling efficient desktop deployment.",
      "mindmap": "graph TB\n        A[Mify-Coder] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[大模型成本高/High cost of large code models]\n        C --> C1[计算最优训练/Compute-optimal training]\n        C --> C2[合成数据生成/Synthetic data generation]\n        C --> C3[质量过滤/Quality filtering]\n        D --> D1[性能可比/Competitive performance]\n        D --> D2[高效部署/Efficient deployment]"
    },
    {
      "title": "Hybrid-Code: A Privacy-Preserving, Redundant Multi-Agent Framework for Reliable Local Clinical Coding",
      "authors": "Yunguo Yu",
      "institution": "Zyter|TruCare",
      "link": "https://arxiv.org/pdf/2512.23743",
      "code": null,
      "tags": [
        "agent system",
        "neuro-symbolic AI",
        "multi-agent framework",
        "local inference",
        "hallucination detection",
        "deterministic fallback"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b1a7891d43346fbec0638fdd7970002ba8ac5fe529a855a4c41f35f8cc8ad1e_w640_q70.webp",
      "contributions": "1. A hybrid neuro-symbolic multi-agent framework (Hybrid-Code) for reliable, on-premise clinical coding that combines an LLM-based Coder with a deterministic fallback and a symbolic Auditor for verification. 2. A privacy-preserving architecture ensuring no patient data leaves the hospital firewall, addressing critical deployment barriers in healthcare. 3. Demonstration that system reliability through architectural redundancy (achieving 0% hallucinations within the knowledge base) is more valuable than pure model performance for production healthcare AI.",
      "summary": "The paper introduces Hybrid-Code, a framework for automated clinical coding that runs locally to preserve privacy. It uses a two-agent system where a Coder attempts semantic reasoning with a local LLM but falls back to keyword matching, and an Auditor verifies codes against a knowledge base to prevent hallucinations. The key conclusion is that this redundant, hybrid approach ensures production reliability where failures are unacceptable, even with a moderate coverage rate.",
      "mindmap": "graph TB\n        A[Hybrid-Code / Hybrid-Code] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[云LLM存在隐私与延迟风险 / Cloud LLMs pose privacy & latency risks]\n        C --> C1[混合神经符号多智能体框架 / Hybrid Neuro-Symbolic Multi-Agent Framework]\n        C1 --> C2[编码器: LLM推理 + 确定性回退 / Coder: LLM + Deterministic Fallback]\n        C1 --> C3[审计器: 基于知识的验证 / Auditor: Knowledge-Based Verification]\n        D --> D1[0% 知识库内幻觉 / 0% Hallucination within KB]\n        D --> D2[34.11% 覆盖率 / 34.11% Coverage]\n        D --> D3[无数据离开防火墙 / No Data Leaves Firewall]"
    },
    {
      "title": "Coordinate Matrix Machine: A Human-level Concept Learning to Classify Very Similar Documents",
      "authors": "Amin Sadri, M Maruf Hossain",
      "institution": "Not explicitly stated (email domains are personal: gmail.com)",
      "link": "https://arxiv.org/pdf/2512.23749",
      "code": "GitHub",
      "tags": [
        "one-shot learning",
        "Coordinate Matrix Machine",
        "structural intelligence",
        "Green AI",
        "lazy learning",
        "glass-box model"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3acd9104b41dbc3c7f9d1597d31b940424a078e93beb2b2b21007c741209b006_w640_q70.webp",
      "contributions": "Proposes the Coordinate Matrix Machine (CM^2) for one-shot document classification, Introduces a structural coordinate-based approach as an alternative to semantic vectorization, Designs a \"Green AI\" model optimized for CPU use with inherent explainability",
      "summary": "This paper addresses the challenge of human-level concept learning, where machines require many examples to learn a concept. It proposes the Coordinate Matrix Machine (CM^2), a purpose-built model that learns document structures to classify very similar documents using only one sample per class. The method is presented as a \"Green AI\" solution that outperforms traditional models while being computationally efficient and explainable.",
      "mindmap": "graph TB\n        A[Coordinate Matrix Machine: A Human-level Concept Learning to Classify Very Similar Documents] --> B[核心问题/Problem: Human-level Concept Learning Gap]\n        A --> C[主要方法/Method: Coordinate Matrix Machine (CM^2)]\n        A --> D[关键结果/Results: High Accuracy, One-shot Learning, Green AI]\n        B --> B1[人类单样本学习/Human one-shot learning]\n        B --> B2[机器需大量样本/Machine needs many samples]\n        C --> C1[学习文档结构/Learns document structure]\n        C --> C2[识别重要特征/Identifies important features]\n        D --> D1[高精度与低数据/High accuracy with minimal data]\n        D --> D2[CPU优化与可解释性/CPU-optimized & explainable]"
    },
    {
      "title": "Generalized Regularized Evidential Deep Learning Models: Theory and Comprehensive Evaluation",
      "authors": "Deep Shankar Pandey, Hyomin Choi, Qi Yu",
      "institution": "Rochester Institute of Technology, InterDigital",
      "link": "https://arxiv.org/pdf/2512.23753",
      "code": null,
      "tags": [
        "uncertainty quantification",
        "Evidential Deep Learning",
        "Subjective Logic",
        "Activation Functions",
        "Regularization",
        "Learning Dynamics"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fae8a70028f83294a153c8fd9b9083e99f87bf5f04f2fb8d64ce2fbab74beb82_w640_q70.webp",
      "contributions": "1. Theoretically characterizes the activation-dependent \"learning-freeze\" behavior in evidential deep learning models, where gradients vanish in low-evidence regions. 2. Designs a general family of activation functions and corresponding evidential regularizers to enable consistent evidence updates across different activation regimes. 3. Empirically validates the proposed theory and method through extensive experiments on multiple benchmark classification, few-shot classification, and blind face restoration tasks.",
      "summary": "This paper identifies and theoretically analyzes a \"learning-freeze\" problem in Evidential Deep Learning (EDL) models caused by specific activation functions. To solve this, the authors propose a generalized family of activation functions and regularizers. Extensive experiments show the proposed method improves learning dynamics and effectiveness across various tasks.",
      "mindmap": "graph TB\n        Root(”Generalized Regularized Evidential Deep Learning Models”) --> Problem(”核心问题/Problem: Activation functions in EDL cause learning-freeze in low-evidence regions”)\n        Root --> Method(”主要方法/Method: Design a general family of activation functions and evidential regularizers”)\n        Root --> Results(”关键结果/Results: Theory validated; method effective across multiple benchmarks”)"
    },
    {
      "title": "Geometric Scaling of Bayesian Inference in LLMs",
      "authors": "Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra",
      "institution": "Dream Sports, Columbia University",
      "link": "https://arxiv.org/pdf/2512.23752",
      "code": null,
      "tags": [
        "interpretability",
        "Bayesian inference",
        "geometric scaling",
        "attention mechanism",
        "value manifolds",
        "predictive entropy"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e11ae0519509ba1ae43d1087dfc56ed0f799df57a2ee4fe7c4a6a7f20eb11c3f_w640_q70.webp",
      "contributions": "1. Demonstrates that production-grade LLMs (Pythia, Phi-2, Llama-3, Mistral) preserve a geometric substrate (low-dimensional value manifolds) similar to that enabling exact Bayesian inference in small, controlled \"wind-tunnel\" models. 2. Shows that the dominant axis of last-layer value representations strongly correlates with predictive entropy, and domain-restricted prompts collapse the structure into the same low-dimensional manifolds. 3. Through targeted interventions on the entropy-aligned axis, reveals that this geometry is a privileged readout of uncertainty rather than a singular computational bottleneck for Bayesian-like behavior.",
      "summary": "This paper investigates whether the geometric structures that enable exact Bayesian inference in small, controlled transformer models persist in large-scale production language models. The authors find that models like Llama-3 and Mistral organize their value representations along an entropy-correlated axis, forming similar low-dimensional manifolds. They conclude that modern LLMs preserve this geometric substrate for approximate Bayesian updates, though it acts more as a readout mechanism than a sole computational bottleneck.",
      "mindmap": "graph TB\n        A[Geometric Scaling of Bayesian Inference in LLMs] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Do geometric structures for Bayesian inference persist in production LLMs?]\n        C[主要方法/Method<br>Analyze value representations & perform targeted axis interventions]\n        D[关键结果/Results<br>Geometry persists as a privileged uncertainty readout]"
    },
    {
      "title": "HINTS: Extraction of Human Insights from Time-Series Without External Sources",
      "authors": "Sheo Yon Jhin, Noseong Park",
      "institution": "KAIST",
      "link": "https://arxiv.org/pdf/2512.23755",
      "code": null,
      "tags": [
        "time series forecasting",
        "self-supervised learning",
        "opinion dynamics",
        "attention mechanism",
        "latent factor extraction",
        "residual analysis"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a97eb229421de0a13cd23a1f8c66b8e7b1ac081ecf63d3e2a393fa375ac5f65_w640_q70.webp",
      "contributions": "1. Proposes HINTS, a novel self-supervised framework that extracts latent human factors (e.g., sentiment, influence) endogenously from time series residuals without requiring external data sources like news or social media. 2. Introduces the use of the Friedkin-Johnsen (FJ) opinion dynamics model as a structural inductive bias to model evolving social influence, memory, and bias patterns within the time series data. 3. Demonstrates that integrating the extracted human factors as an attention map into a state-of-the-art backbone model consistently improves forecasting accuracy across multiple datasets and provides interpretable insights aligned with real-world events.",
      "summary": "This paper addresses the high cost of using external data to model human factors in time series forecasting. It proposes HINTS, a self-supervised learning framework that extracts latent human insights directly from time series residuals using an opinion dynamics model as inductive bias. The method improves forecasting accuracy and provides interpretable factors aligned with real events, validated on nine real-world and benchmark datasets.",
      "mindmap": "graph TB\n        A[HINTS: Extraction of Human Insights from Time-Series] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[外部数据依赖成本高/High cost of external data dependency]\n        C --> C1[从残差中自监督提取人类因素/Self-supervised extraction from residuals]\n        C --> C2[使用意见动力学作为归纳偏置/Using opinion dynamics as inductive bias]\n        C --> C3[集成到注意力机制中/Integrated as attention map]\n        D --> D1[预测精度提升/Forecasting accuracy improved]\n        D --> D2[可解释性与现实事件对齐/Interpretability aligned with real events]"
    },
    {
      "title": "Drift-Based Dataset Stability Benchmark",
      "authors": "Dominik Soukup, Richard Plný, Daniel Vašata, Tomáš Čejka",
      "institution": "Czech Technical University in Prague, CESNET a.l.e.",
      "link": "https://arxiv.org/pdf/2512.23762",
      "code": null,
      "tags": [
        "communication & networking",
        "concept drift",
        "dataset stability",
        "traffic classification",
        "benchmark",
        "feature weights"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc4badd4590db607d7056063e44a30285afee3c4bd25f3f6b231fbc0932dd8de_w640_q70.webp",
      "contributions": "1. A novel methodology for evaluating dataset stability based on concept drift detection and ML feature weights. 2. A benchmark workflow for comparing datasets and identifying their weak points. 3. A demonstration and initial benchmark of the framework on the CESNET-TLS-Year22 dataset, showing its use for dataset optimization.",
      "summary": "This paper addresses the problem of model degradation in network traffic classification due to data/concept drift. It proposes a new framework that uses a concept drift detection method enhanced with ML feature weights to benchmark dataset stability. The method is demonstrated on a real-world TLS dataset, providing insights for dataset optimization.",
      "mindmap": "graph TB\n        A[Drift-Based Dataset Stability Benchmark] --> B[核心问题/Problem: Model degradation from data drift in network traffic classification]\n        A --> C[主要方法/Method: Concept drift detection boosted by ML feature weights for dataset benchmarking]\n        A --> D[关键结果/Results: Initial stability benchmark for CESNET-TLS-Year22 dataset, showing optimization impact]"
    },
    {
      "title": "Entropy-Aware Speculative Decoding Toward Improved LLM Reasoning",
      "authors": "Tiancheng Su, Meicong Zhang, Guoxiu He",
      "institution": "East China Normal University",
      "link": "https://arxiv.org/pdf/2512.23765",
      "code": null,
      "tags": [
        "llm inference",
        "speculative decoding",
        "entropy penalty",
        "training-free",
        "reasoning acceleration",
        "draft-model verification"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/326c86ae03e220a7cb48737a0a6fe149bd4384ccc08f3113a051c3548bc2d30e_w640_q70.webp",
      "contributions": "1. Proposes Entropy-Aware Speculative Decoding (EASD), a training-free method that introduces a dynamic entropy-based penalty to reject low-confidence draft tokens, 2. Enables speculative decoding to potentially surpass the target model's performance by incorporating draft-model verification and preventing error propagation, 3. Demonstrates that EASD maintains efficiency comparable to standard speculative decoding while improving reasoning accuracy across multiple benchmarks.",
      "summary": "This paper addresses the limitation of speculative decoding being constrained by the target model's performance. It proposes Entropy-Aware Speculative Decoding (EASD), which uses entropy to quantify uncertainty and reject low-confidence draft tokens. Experiments show EASD outperforms existing methods and can surpass the target LLM's performance while maintaining comparable efficiency.",
      "mindmap": "graph TB\n        Root[Entropy-Aware Speculative Decoding] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[SD性能受限于目标模型/SD performance capped by target model]\n        Method[主要方法/Method] --> M1[引入动态熵惩罚/Introduce dynamic entropy penalty]\n        Method --> M2[基于不确定性拒绝低置信度令牌/Reject low-confidence tokens based on uncertainty]\n        Method --> M3[目标模型重采样/Target model re-sampling]\n        Results[关键结果/Results] --> R1[超越现有SD方法/Outperforms existing SD methods]\n        Results --> R2[常超越目标LLM本身/Often surpasses target LLM]\n        Results --> R3[效率与SD相当/Efficiency comparable to SD]"
    },
    {
      "title": "Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory",
      "authors": "Ken Huang, Jerry Huang",
      "institution": "DistributedApps.ai, OWASP, Kleiner Perkins",
      "link": "https://arxiv.org/pdf/2512.23760",
      "code": null,
      "tags": [
        "agent system",
        "skill graph",
        "verifiable rewards",
        "continual memory",
        "experience synthesis",
        "audit logging"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bdd2b1c94a27644963b1a77d560eb3715fa72ea6468dc6c4e39e42eb5e040187_w640_q70.webp",
      "contributions": "1. Proposes the Audited Skill-Graph Self-Improvement (ASG-SI) framework, which treats agent self-improvement as the iterative compilation of an auditable, growing skill graph. 2. Introduces a verifier-auditor mechanism that uses replayable evidence and decomposed rewards to gate skill promotion, enabling independent audit and governance. 3. Integrates experience synthesis for scalable testing and continual memory control to manage context growth and preserve long-horizon performance.",
      "summary": "This paper addresses security and governance challenges in self-improving AI agents, such as reward hacking and opaque behavioral drift. It proposes the ASG-SI framework, which compiles agent improvements into an auditable skill graph verified by replayable evidence. The approach reframes self-improvement as the accumulation of verifiable, reusable capabilities for reproducible evaluation and operational governance.",
      "mindmap": "graph TB\n        Root[”Audited Skill-Graph Self-Improvement (ASG-SI) / 审计技能图自我改进”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem”] --> P1[”部署的自我改进代理存在安全与治理挑战 / Deployed self-improving agents pose security & governance challenges”]\n        P1 --> P2[”奖励黑客行为、行为漂移、不透明的更新 / Reward hacking, behavioral drift, opaque updates”]\n        Method[”主要方法/Method”] --> M1[”ASG-SI 框架 / ASG-SI Framework”]\n        M1 --> M2[”将改进编译为可审计技能图 / Compile improvements into auditable skill graph”]\n        M2 --> M3[”基于验证器的证据和可分解奖励进行技能提升 / Verifier-backed evidence & decomposed rewards gate skill promotion”]\n        M3 --> M4[”集成经验合成和持续记忆控制 / Integrate experience synthesis & continual memory control”]\n        Results[”关键结果/Results”] --> R1[”提供可验证、可重用的能力积累 / Provides accumulation of verifiable, reusable capabilities”]\n        R1 --> R2[”为自我改进AI提供可复现评估和操作治理的路径 / Offers path for reproducible evaluation & operational governance of self-improving AI”]"
    },
    {
      "title": "Enabling Physical AI at the Edge: Hardware-Accelerated Recovery of System Dynamics",
      "authors": "Bin Xu, Ayan Banerjee, Sandeep Gupta",
      "institution": "Arizona State University",
      "link": "https://arxiv.org/pdf/2512.23767",
      "code": null,
      "tags": [
        "on-device ai",
        "FPGA acceleration",
        "model recovery",
        "hardware-software co-design",
        "GRU",
        "Neural ODE"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e286e0d5a9672c23140099a1b5fd5c7ad7e56f56cb1c276735170b95ad29fd47_w640_q70.webp",
      "contributions": "1. Proposed MERINDA, a hardware-friendly FPGA-accelerated framework for model recovery that replaces Neural ODEs with a formulation combining GRU-based discretized dynamics, dense inverse-ODE layers, sparsity-driven dropout, and lightweight solvers. 2. Designed the framework for streaming parallelism, enabling critical computational kernels to be fully parallelized on FPGA hardware. 3. Demonstrated transformative efficiency gains over GPU implementations, including 114x lower energy, 28x smaller memory footprint, and 1.68x faster training while maintaining state-of-the-art accuracy.",
      "summary": "This paper addresses the challenge of deploying physical AI for model recovery on resource-constrained edge devices, where state-of-the-art methods using Neural ODEs are inefficient. The authors propose MERINDA, an FPGA-accelerated framework that uses a hardware-friendly architecture to replace expensive Neural ODE components. The results show that MERINDA achieves substantial improvements in energy, memory, and speed over GPU implementations while matching model recovery accuracy.",
      "mindmap": "graph TB\n        A[Enabling Physical AI at the Edge<br>在边缘实现物理人工智能] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Model recovery methods (Neural ODEs) are inefficient for edge hardware<br>模型恢复方法在边缘硬件上效率低下]\n        C[主要方法/Method<br>MERINDA: FPGA-accelerated, hardware-friendly framework<br>MERINDA: FPGA加速的硬件友好框架]\n        D[关键结果/Results<br>114x lower energy, 28x smaller memory, 1.68x faster training<br>能耗降低114倍, 内存占用减少28倍, 训练速度提升1.68倍]"
    },
    {
      "title": "Uncovering Discrimination Clusters: Quantifying and Explaining Systematic Fairness Violations",
      "authors": "Ranit Debnath Akash, Ashish Kumar, Verya Monjezi, Ashutosh Trivedi, Gang, Saeid Tizpaz-Niari",
      "institution": "University of Illinois Chicago, University of Colorado Boulder, Pennsylvania State University",
      "link": "https://arxiv.org/pdf/2512.23769",
      "code": null,
      "tags": [
        "algorithmic fairness",
        "discrimination clustering",
        "individual fairness",
        "hybrid verification",
        "SMT solver",
        "MILP solver"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05da1223f6a1c9a93634f021d221ac5175d00dee272ded0b8782834941db9c55_w640_q70.webp",
      "contributions": "1. Introduced the concept of \"discrimination clustering\" as a generalization of individual fairness to uncover systematic bias patterns. 2. Proposed HyFair, a hybrid technique combining formal symbolic analysis (SMT/MILP) and randomized search for both certification and violation discovery. 3. Developed a novel explanation method to generate interpretable, decision-tree-style artifacts for inputs exhibiting high discrimination.",
      "summary": "The paper identifies a limitation in individual fairness, which only detects isolated unfairness, and proposes the concept of \"discrimination clustering\" to uncover systematic bias patterns. It introduces HyFair, a hybrid method combining formal verification and randomized search to detect these clusters and generate explanations. Experiments show HyFair outperforms existing fairness verification and explanation methods.",
      "mindmap": "graph TB\n        Root(”Uncovering Discrimination Clusters”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”个体公平性检查的局限性/Limitations of individual fairness checks”)\n        P1 --> P2(”无法捕捉系统性歧视模式/Fails to capture systematic bias patterns”)\n        Method --> M1(”提出歧视聚类概念/Propose discrimination clustering concept”)\n        Method --> M2(”开发HyFair混合技术/Develop HyFair hybrid technique”)\n        M2 --> M3(”结合形式分析与随机搜索/Combine formal analysis & randomized search”)\n        Results --> R1(”优于现有方法/Outperforms state-of-the-art methods”)\n        Results --> R2(”揭示系统性偏差/Reveals substantial discrimination clustering”)\n        Results --> R3(”提供可解释的说明/Provides intuitive explanations”)"
    },
    {
      "title": "Safety-Biased Policy Optimisation: Towards Hard-Constrained Reinforcement Learning via Trust Regions",
      "authors": "Ankit Kanwar, Dominik Wagner, Luke Ong",
      "institution": "Sony Corporation, Nanyang Technological University (NTU Singapore)",
      "link": "https://arxiv.org/pdf/2512.23770",
      "code": null,
      "tags": [
        "safe reinforcement learning",
        "constrained MDP",
        "trust region policy optimization",
        "natural policy gradient",
        "safety gymnasium",
        "hard constraints"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ea6c2a84f6cc7b4f3144847fc78a84736f7247b99f773ed23dd1861f2ff0760_w640_q70.webp",
      "contributions": "1. Proposes Safety-Biased Trust Region Policy Optimisation (SB-TRPO), a new algorithm for hard-constrained RL that adaptively biases policy updates towards safety while seeking reward improvement. 2. Introduces a trust-region update using a convex combination of the natural policy gradients of cost and reward to ensure a fixed fraction of optimal cost reduction per step. 3. Provides a theoretical guarantee of local progress towards safety and demonstrates superior balance of safety and task performance on Safety Gymnasium benchmarks.",
      "summary": "The paper addresses the problem of reinforcement learning under hard safety constraints, where existing methods struggle to avoid violations without sacrificing reward. It proposes SB-TRPO, an algorithm that performs trust-region updates by combining reward and cost gradients to bias updates towards safety. Experiments show that SB-TRPO achieves a better balance of safety and task completion than state-of-the-art methods.",
      "mindmap": "graph TB\n        Root[Safety-Biased Policy Optimisation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: RL in safety-critical domains requires strict constraint adherence without sacrificing reward performance.]\n        Method[主要方法/Method: SB-TRPO uses convex combination of natural policy gradients for cost and reward in trust-region updates.]\n        Results[关键结果/Results: Achieves best balance of safety and task completion on Safety Gymnasium benchmarks.]"
    },
    {
      "title": "A Survey on Graph Neural Networks for Fraud Detection in Ride Hailing Platforms",
      "authors": "Kanishka Hewageegana, Janani Harischandra, Nipuna Senanayake, Gihan Danansuriya, Kavindu Hapuarachchi, Pooja Illangarathne",
      "institution": "Informatics Institute of Technology, Rajarata University, University of Sri Jayewardenepura",
      "link": "https://arxiv.org/pdf/2512.23777",
      "code": null,
      "tags": [
        "anomaly detection",
        "Graph Neural Networks (GNNs)",
        "Fraud Detection",
        "Class Imbalance",
        "Fraudulent Camouflage",
        "Ride-Hailing Platforms"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65fc7f6ff0330c22a6dc35373a15256baac5eef984a9100cce967991d93a1e67_w640_q70.webp",
      "contributions": "1. Provides a structured overview and comparison of existing Graph Neural Network (GNN) architectures and methodologies for fraud detection in ride-hailing platforms. 2. Highlights and analyzes key challenges in the domain, specifically class imbalance and fraudulent camouflage, within the ride-hailing ecosystem. 3. Identifies significant methodological progress and research gaps, calling for further exploration into real-world applicability and technical improvements.",
      "summary": "This survey investigates the use of Graph Neural Networks (GNNs) for detecting fraud in ride-hailing platforms. It analyzes and compares various GNN models, focusing on their effectiveness in handling complex relational data and challenges like class imbalance. The paper concludes by identifying progress and gaps in the field, advocating for more research on real-world applications and technical enhancements.",
      "mindmap": "graph TB\n        Root[”A Survey on Graph Neural Networks for Fraud Detection in Ride Hailing Platforms”] --> Problem[”核心问题/Problem: Fraud detection in ride-hailing platforms”]\n        Root --> Method[”主要方法/Method: Survey and analysis of Graph Neural Networks (GNNs)”]\n        Root --> Results[”关键结果/Results: Identifies progress, gaps, and calls for future work”]"
    },
    {
      "title": "Prompt-Induced Over-Generation as Denial-of-Service: A Black-Box Attack-Side Benchmark",
      "authors": "Manu, Yi Guo, Jo Plested, Tim Lynar, Kanchana Thilakarathna, Nirhoshan Sivaroopan, Jack Yang, Wangli Yang",
      "institution": "Western Sydney University, University of New South Wales Canberra, The University of Sydney, University of Wollongong",
      "link": "https://arxiv.org/pdf/2512.23779",
      "code": null,
      "tags": [
        "adversarial attacks on llms",
        "denial-of-service",
        "over-generation",
        "black-box attack",
        "evolutionary search",
        "reinforcement learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3002b15ce3957d88befc241c59f1be73a9e49f4e8ad4c345e9d472f11883059e_w640_q70.webp",
      "contributions": "1. Introduces a black-box, query-only benchmark for evaluating prompt-induced denial-of-service attacks on LLMs. 2. Proposes two novel prompt-only attackers: an evolutionary search method (EOGen) and a goal-conditioned reinforcement learning method (RL-GOAL). 3. Defines the Over-Generation Factor (OGF) as a key metric to quantify attack success and characterize model vulnerability.",
      "summary": "This paper addresses the problem of denial-of-service attacks on large language models via prompt-induced over-generation. It proposes a standardized black-box benchmark and two automated attack methods, EOGen and RL-GOAL, to find adversarial prefixes that delay model termination. The results show that the RL-GOAL attacker is particularly effective at forcing models to generate excessively long outputs, highlighting a significant vulnerability.",
      "mindmap": "graph TB\n        A[Prompt-Induced Over-Generation as Denial-of-Service<br/>提示诱导过度生成作为拒绝服务攻击] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>LLM过度生成导致服务拒绝、延迟和成本增加]\n        C[主要方法/Method<br/>黑盒基准与两种攻击者: EOGen(进化搜索)和RL-GOAL(强化学习)]\n        D[关键结果/Results<br/>RL-GOAL攻击者实现更高的平均过度生成因子(OGF)]"
    },
    {
      "title": "FineFT: Efficient and Risk-Aware Ensemble Reinforcement Learning for Futures Trading",
      "authors": "Molei Qin, Xinyu Cai, Yewen Li, Haochong Xia, Chuqiao Zong, Shuo Sun, Xinrun Wang, Bo An",
      "institution": "Nanyang Technological University, Singapore Management University, Hong Kong University of Science and Technology (Guangzhou)",
      "link": "https://arxiv.org/pdf/2512.23773",
      "code": null,
      "tags": [
        "reinforcement learning",
        "ensemble reinforcement learning",
        "selective update",
        "variational autoencoder",
        "high-frequency trading",
        "risk management"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c036ba975592c2c3be9068e742ccd28ed5b9722ff62085fbcc37e9f3627fe370_w640_q70.webp",
      "contributions": "1. A selective update mechanism for ensemble Q-learners using ensemble TD errors to stabilize training and improve convergence in high-leverage environments. 2. A risk-aware filtering and routing mechanism that uses VAEs to model market state dynamics and identify agent capability boundaries, enabling dynamic policy selection to mitigate risk. 3. A novel three-stage ensemble RL framework (FineFT) that integrates stable training and risk management, demonstrating superior profitability and over 40% risk reduction in crypto futures trading.",
      "summary": "The paper proposes FineFT, a three-stage ensemble reinforcement learning framework designed to address the challenges of high leverage and unseen market states in futures trading. The method uses selective updates for stable training and VAEs for risk-aware policy routing, achieving higher profitability and significantly lower risk compared to state-of-the-art baselines in high-frequency crypto futures experiments.",
      "mindmap": "graph TB\n        A[FineFT: Efficient and Risk-Aware Ensemble RL for Futures Trading] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[高杠杆放大波动/High leverage amplifies reward fluctuations]\n        B --> B2[缺乏能力边界意识/Lack of self-awareness of capability boundaries]\n        C --> C1[阶段I: 选择性更新/Stage I: Selective Update]\n        C --> C2[阶段II: 过滤与VAE训练/Stage II: Filtering & VAE Training]\n        C --> C3[阶段III: 动态路由/Stage III: Dynamic Routing]\n        D --> D1[超越12个SOTA基线/Outperforms 12 SOTA baselines]\n        D --> D2[风险降低超40%/Risk reduced by >40%]\n        D --> D3[实现更高盈利/Achieves superior profitability]"
    },
    {
      "title": "Zero-Trust Agentic Federated Learning for Secure IIoT Defense Systems",
      "authors": "Samaresh Kumar Singh, Joyjit Roy, Martin So",
      "institution": "Independent Researchers (based on provided affiliations: IEEE Senior Member in Texas, IEEE Member in Texas, Independent Researcher in Canada)",
      "link": "https://arxiv.org/pdf/2512.23809",
      "code": null,
      "tags": [
        "federated learning",
        "Zero-Trust Architecture",
        "SHAP-weighted aggregation",
        "TPM-based attestation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/baa785fc442fcbc6a80214c4fdc6361e67a8e34e5a9bb6f5dd8fb34baf21bb68_w640_q70.webp",
      "contributions": "1) Proposed a hierarchical edge-fog-cloud zero-trust federated learning architecture for trusted agent participation. 2) Introduced a novel SHAP-weighted aggregation algorithm for explainable Byzantine detection in non-IID environments. 3) Integrated TPM-based cryptographic attestation and on-device adversarial training into a defense-in-depth framework.",
      "summary": "The paper addresses security vulnerabilities in Federated Learning for Industrial IoT by proposing ZTA-FL, a framework combining zero-trust agent authentication, explainable Byzantine-resilient aggregation, and on-device adversarial training. It demonstrates high detection accuracy and robustness against attacks on intrusion detection benchmarks while reducing communication overhead.",
      "mindmap": "graph TB\n        Root[Zero-Trust Agentic Federated Learning for Secure IIoT Defense Systems] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: IIoT安全漏洞与联邦学习攻击 / IIoT Security Gaps & FL Attacks]\n        Method[主要方法/Method: 零信任认证与可解释聚合 / Zero-Trust Attestation & Explainable Aggregation]\n        Results[关键结果/Results: 高检测精度与抗攻击鲁棒性 / High Detection Accuracy & Attack Robustness]"
    },
    {
      "title": "StressRoBERTa: Cross-Condition Transfer Learning from Depression, Anxiety, and PTSD to Stress Detection",
      "authors": "Amal Alqahtani, Efsun Kayi, Mona Diab",
      "institution": "The George Washington University, King Saud University, Johns Hopkins University Applied Physics Laboratory, Carnegie Mellon University",
      "link": "https://arxiv.org/pdf/2512.23813",
      "code": null,
      "tags": [
        "mental health text classification",
        "transfer learning",
        "continual training",
        "RoBERTa",
        "cross-condition",
        "stress detection"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a57224b94917298d3e6f94d2c9255d2d054abcab5c5051070f928ff8ddd8bc6b_w640_q70.webp",
      "contributions": "1. Proposes StressRoBERTa, a cross-condition transfer learning approach for detecting self-reported chronic stress in tweets. 2. Demonstrates that continual training on a focused set of clinically related mental health conditions (depression, anxiety, PTSD) improves stress detection over general models. 3. Shows effective transfer from clinical mental health contexts to situational stress discussions via evaluation on the Dreaddit dataset.",
      "summary": "This paper introduces StressRoBERTa, a method that continually trains a RoBERTa model on social media text from users with depression, anxiety, and PTSD before fine-tuning it for chronic stress detection. The approach outperforms the previous best system on the SMM4H 2022 shared task by 3% F1-score, showing that focused cross-condition transfer learning from related disorders provides stronger representations for stress detection.",
      "mindmap": "graph TB\n        A[StressRoBERTa: Cross-Condition Transfer Learning] --> B(核心问题/Problem: 检测社交媒体中的慢性压力/Detect chronic stress on social media)\n        A --> C(主要方法/Method: 从相关心理健康状况进行跨条件迁移学习/Cross-condition transfer learning from related mental health conditions)\n        A --> D(关键结果/Results: 性能超越最佳共享任务系统，F1分数达82%/Outperforms best shared task system with 82% F1)"
    },
    {
      "title": "Improved Bounds for Private and Robust Alignment",
      "authors": "Wenqian Weng, Yi He, Xingyu Zhou",
      "institution": "Wayne State University",
      "link": "https://arxiv.org/pdf/2512.23816",
      "code": null,
      "tags": [
        "preference learning",
        "private alignment",
        "robust alignment",
        "uniform convergence",
        "log loss",
        "adversarial corruption"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07d766123762454b64f45852c98c882b263a3fe86efdee6b0c39b70b0d888215_w640_q70.webp",
      "contributions": "1. Showed that standard private MLE-type log loss can achieve near-optimal rates for private alignment, contrary to prior belief. 2. Demonstrated that existing offline algorithms for joint privacy-and-corruption provide stronger guarantees than previously known, leading to improved bounds for corruption-only settings. 3. Presented the first set of theoretical results for private and robust online alignment.",
      "summary": "This paper studies the theoretical alignment of language models under privacy constraints and adversarial corruption. It shows that a standard MLE-style log loss can achieve near-optimal rates for private alignment and provides improved bounds for joint private-and-robust settings, including the first online results, enabled by new uniform convergence guarantees.",
      "mindmap": "graph TB\n        A[Improved Bounds for Private and Robust Alignment] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[语言模型对齐/Language Model Alignment]\n        B --> B2[隐私与噪声标签/Private & Noisy Labels]\n        C --> C1[理论分析/Theoretical Analysis]\n        C --> C2[统一收敛/Uniform Convergence]\n        D --> D1[私有MLE达到最优/Private MLE Near-Optimal]\n        D --> D2[离线和在线改进界限/Improved Offline & Online Bounds]"
    },
    {
      "title": "Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments",
      "authors": "Surya Rayala, Marcos Quinones-Grueiro, Naveeduddin Mohammed, Ashwin T S, Benjamin Goldberg, Randall Spain, Paige Lawton, Gautam Biswas",
      "institution": "Vanderbilt University, US Army DEVCOM Soldier Center",
      "link": "https://arxiv.org/pdf/2512.23819",
      "code": null,
      "tags": [
        "human pose estimation and action analysis",
        "video-based assessment",
        "2D skeleton extraction",
        "Cognitive Task Analysis (CTA)",
        "performance metrics",
        "synthetic training environments"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33d390da3c0f68240f7cda6efac9e31e941e99ab611d61cd2837f0352453d12c_w640_q70.webp",
      "contributions": "1. A video-based assessment pipeline that extracts performance analytics (2D skeletons, gaze vectors, trajectories) from training videos without extra hardware. 2. Development of task-specific metrics for psychomotor fluency, situational awareness, and team coordination, integrated into an extended Cognitive Task Analysis hierarchy. 3. Demonstration of the approach via a case study on real-world Enter and Clear the Room drills and discussion of its integration into After Action Review systems like Gamemaster and GIFT.",
      "summary": "This paper addresses the challenge of automatically and objectively assessing soldier performance in synthetic training environments. It proposes a video-based pipeline using computer vision to extract movement and gaze data, from which it derives specific performance metrics for cognitive and teamwork skills. The method is demonstrated on real-world drills and shows potential for scalable, hardware-free evaluation to support training feedback.",
      "mindmap": "graph TB\n        Root(”Video-Based Performance Evaluation for ECR Drills”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”传统评估方法受限/Traditional assessment limited”)\n        P1 --> P1_1(”依赖昂贵传感器/Relies on costly sensors”)\n        P1 --> P1_2(”主观人为观察/Subjective human observation”)\n        Method --> M1(”视频分析管道/Video-based pipeline”)\n        M1 --> M1_1(”提取2D骨架、视线、轨迹/Extract 2D skeletons, gaze, trajectories”)\n        M1 --> M1_2(”开发任务特定指标/Develop task-specific metrics”)\n        M1 --> M1_3(”扩展认知任务分析/Extended Cognitive Task Analysis”)\n        Results --> R1(”案例研究验证/Case study validation”)\n        Results --> R2(”支持行动后评估/Supports After Action Reviews”)\n        Results --> R3(”未来: 3D分析/Future: 3D analysis”)"
    },
    {
      "title": "Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation",
      "authors": "Kaustubh Dhole",
      "institution": "Emory University",
      "link": "https://arxiv.org/pdf/2512.23837",
      "code": null,
      "tags": [
        "adversarial robustness",
        "mechanistic interpretability",
        "attention layers",
        "adversarial examples",
        "LLM evaluation",
        "token substitution"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85f39e8eb9d1e9534ca2c7e95f4e08380c10c2d3b58ec0a3a896f0767b75fdd8_w640_q70.webp",
      "contributions": "1. Proposes a novel adversarial example generation method that exploits intermediate attention layer token distributions, contrasting with prompt-based or gradient-based attacks. 2. Introduces two specific attention-based generation techniques: attention-based token substitution and attention-based conditional generation. 3. Empirically demonstrates that such adversarial examples can degrade performance on an evaluation task (argument quality assessment) while maintaining semantic similarity, highlighting both the promise and limitations (e.g., grammatical degradation) of the approach.",
      "summary": "This paper proposes a new method to generate adversarial examples by extracting token predictions from the intermediate attention layers of LLMs, leveraging their iterative refinement property. The approach is used to stress-test LLM-based evaluation pipelines, showing it can cause performance drops on an argument quality task while preserving semantics, though grammatical issues can arise. The findings illustrate the potential and current constraints of using internal model representations for adversarial testing.",
      "mindmap": "graph TB\n        Root[Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Can intermediate attention layers be used to generate adversarial examples for LLM evaluation?]\n        Method[主要方法/Method: Leverage attention-layer token distributions for token substitution/conditional generation]\n        Results[关键结果/Results: Adversarial examples cause performance drop but may introduce grammatical issues]"
    },
    {
      "title": "Explaining News Bias Detection: A Comparative SHAP Analysis of Transformer Model Decision Mechanisms",
      "authors": "Himel Ghosh",
      "institution": "Technical University of Munich, Sapienza University of Rome",
      "link": "https://arxiv.org/pdf/2512.23835",
      "code": null,
      "tags": [
        "bias detection",
        "SHAP",
        "transformer",
        "interpretability",
        "false positives",
        "domain adaptation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cbe893cabdbb4c14e3f0b2a14a91011067d2ee0ee4225b0a232eb5591a1b743_w640_q70.webp",
      "contributions": "1. Conducted a comparative interpretability study of two transformer-based bias detection models using SHAP to analyze their decision mechanisms. 2. Revealed that a standard bias detector model exhibits a misalignment between attribution strength and prediction correctness, leading to systematic over-flagging, while a domain-adapted model produces significantly fewer false positives. 3. Demonstrated that model errors, particularly false positives, arise from discourse-level ambiguity rather than explicit bias cues, highlighting distinct linguistic failure modes.",
      "summary": "This paper compares how two transformer models detect bias in news text using SHAP-based explanations. It finds that while both models focus on similar evaluative language, a domain-adapted model integrates these signals more reliably, producing far fewer false positives than a standard bias detector. The study concludes that interpretability analysis is crucial for evaluating bias detection systems and that architectural choices critically impact their reliability for journalistic use.",
      "mindmap": "graph TB\n        A[Explaining News Bias Detection] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[How do bias detection models make decisions?]\n        C --> C1[Comparative SHAP analysis of two transformer models]\n        D --> D1[Domain-adapted model has better alignment and fewer false positives]\n        D --> D2[False positives driven by discourse ambiguity]"
    },
    {
      "title": "Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?",
      "authors": "Dingmin Wang, Ji Ma, Shankar Kumar",
      "institution": "Google Research, University of Oxford",
      "link": "https://arxiv.org/pdf/2512.23836",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "adaptive prompting",
        "context window",
        "open-domain QA",
        "retrieval-augmented generation",
        "LLM ignorance"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58d02669f63e2ba5d0171fc84f89e87cc22d595344fc20e61769b4288b009ef5_w640_q70.webp",
      "contributions": "1. Proposes an adaptive prompting strategy for RAG that splits retrieved information into smaller chunks for sequential processing, mitigating the noise from irrelevant information in long contexts. 2. Demonstrates experimentally that this strategy matches or outperforms standard prompting on open-domain QA datasets while using fewer tokens. 3. Identifies and analyzes a key failure mode where LLMs generate incorrect answers instead of declining when information is insufficient, highlighting a critical area for future research.",
      "summary": "This paper addresses the problem that longer context windows in Retrieval-Augmented Generation (RAG) introduce irrelevant information, degrading LLM performance. It proposes an adaptive prompting strategy that processes retrieved text in smaller, sequential chunks, achieving comparable accuracy with lower token usage. The study concludes that a major source of error is the LLM's tendency to generate wrong answers rather than admit ignorance, pointing to the need for improved refusal capabilities.",
      "mindmap": "graph TB\n        A[Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[长上下文引入无关信息，降低LLM性能/Long contexts introduce irrelevant info, degrading LLM performance]\n        C --> C1[自适应提示策略：分块顺序处理/Adaptive prompting: sequential chunk processing]\n        D --> D1[性能相当，使用更少token/Matches performance, uses fewer tokens]\n        D --> D2[LLM常生成错误答案而非拒绝/LLM often generates wrong answers instead of declining]"
    },
    {
      "title": "Artificial Intelligence for All? Brazilian Teachers on Ethics, Equity, and the Everyday Challenges of AI in Education",
      "authors": "Bruno Florentino, Camila Sestito, Wellington Cruz, André de Carvalho, Robson Bonidia",
      "institution": "University of São Paulo, Federal University of Technology-Paraná (UTFPR), Instituto Significare",
      "link": "https://arxiv.org/pdf/2512.23834",
      "code": null,
      "tags": [
        "AI in Education",
        "AI literacy",
        "teacher perceptions",
        "quantitative survey",
        "ethics",
        "infrastructure challenges"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9fd8e32651f0740b552d9443daf7c424819558f55a942221ff741ca45c296c9_w640_q70.webp",
      "contributions": "1. Provides empirical data on the AI literacy levels and application interests of Brazilian K-12 teachers, revealing a high interest despite low knowledge. 2. Identifies key structural barriers (lack of training, technical support, and infrastructure) to AI adoption in Brazilian public education. 3. Highlights the critical importance teachers place on discussing ethics, digital citizenship, and responsible AI use within the pedagogical context.",
      "summary": "This study quantitatively analyzes Brazilian K-12 teachers' perceptions of AI in education through a survey of 346 educators. The results show strong teacher interest in using AI for pedagogical tasks despite limited knowledge, while identifying significant structural challenges and emphasizing the need for ethical discussions. The study concludes that effective AI integration in Brazil requires integrated public policies, teacher training, and equitable access to technology.",
      "mindmap": "graph TB\n        Root(Artificial Intelligence for All? Brazilian Teachers on Ethics, Equity, and the Everyday Challenges of AI in Education) --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1(巴西教师对AI的认知与态度/Brazilian Teachers' Perceptions and Attitudes towards AI)\n        Problem --> P2(AI在教育中的伦理与公平挑战/Ethical and Equity Challenges of AI in Education)\n        Method --> M1(定量问卷调查/Quantitative Questionnaire Survey)\n        M1 --> M1_1(346名巴西K-12教师/346 Brazilian K-12 Teachers)\n        Results --> R1(高兴趣但知识有限/High Interest but Limited Knowledge)\n        Results --> R2(关注伦理与结构挑战/Concerns on Ethics and Structural Challenges)\n        Results --> R3(需要政策与培训支持/Need for Policy and Training Support)"
    },
    {
      "title": "From Correctness to Collaboration: Toward a Human-Centered Framework for Evaluating AI Agent Behavior in Software Engineering",
      "authors": "Tao Dong, Harini Sampath, Ja Young Lee, Sherry Y. Shi, Andrew Macvean",
      "institution": "Google LLC",
      "link": "https://arxiv.org/pdf/2512.23844",
      "code": null,
      "tags": [
        "Human-AI Collaboration",
        "AI Agent Evaluation",
        "Behavioral Taxonomy",
        "Context-Adaptive Behavior Framework"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7ee9d0c9d0b72de4dc6af2921706818c987d2f7c644977698965be6b535d20c_w640_q70.webp",
      "contributions": "1. A foundational taxonomy of desirable AI agent behaviors for enterprise software engineering, derived from an analysis of 91 sets of user-defined agent rules. 2. The Context-Adaptive Behavior (CAB) Framework, which models how behavioral expectations shift based on context. 3. An empirical derivation of two key axes (Time Horizon and Type of Work) that drive behavioral expectation shifts in the CAB Framework.",
      "summary": "This paper argues that current AI evaluation benchmarks focus too narrowly on code correctness and fail to assess the collaborative behaviors needed for AI to be an effective partner in software engineering. To address this, the authors propose a taxonomy of desirable agent behaviors and a Context-Adaptive Behavior (CAB) Framework that models how these expectations change with context. These contributions provide a human-centered foundation for evaluating and designing collaborative AI agents.",
      "mindmap": "graph TB\n        Root[”From Correctness to Collaboration: Toward a Human-Centered Framework for Evaluating AI Agent Behavior in Software Engineering<br/>从正确性到协作：评估软件工程中AI智能体行为的人本框架”] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题/Problem<br/>Current benchmarks fail to capture collaborative AI agent behavior.<br/>当前基准测试无法评估AI智能体的协作行为。”]\n        Method[”主要方法/Method<br/>1. Taxonomy of agent behaviors.<br/>智能体行为分类法。<br/>2. Context-Adaptive Behavior (CAB) Framework.<br/>上下文自适应行为框架。”]\n        Results[”关键结果/Results<br/>Provides a human-centered foundation for evaluating collaborative AI agents.<br/>为评估协作型AI智能体提供了人本基础。”]"
    },
    {
      "title": "The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models",
      "authors": "Rahul Baxi",
      "institution": "Independent Researcher (affiliation inferred from email domain: alumni.cmu.edu, Carnegie Mellon University)",
      "link": "https://arxiv.org/pdf/2512.23850",
      "code": null,
      "tags": [
        "language model evaluation",
        "epistemic robustness",
        "semantic compression",
        "adversarial fabrication",
        "two-system cognitive model",
        "comprehension integrity"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c853b521a9f8bb0173c42ffdb79e01c42db6066203b6aa0c5e838c2f6a78f18f_w640_q70.webp",
      "contributions": "1. Introduces the Drill-Down and Fabricate Test (DDFT), a novel protocol for measuring epistemic robustness in language models under stress from semantic compression and adversarial fabrication. 2. Proposes a two-system cognitive model (Semantic System and Epistemic Verifier) to explain and analyze LLM behavior. 3. Provides empirical evidence that epistemic robustness is orthogonal to model scale and architecture, identifying error detection as the critical bottleneck.",
      "summary": "The paper identifies a gap in current language model evaluations, which fail to measure how robustly models maintain factual knowledge under stress. It introduces the Drill-Down and Fabricate Test (DDFT) to measure epistemic robustness by applying semantic compression and adversarial fabrication. The key finding is that epistemic robustness is not predicted by model size or architecture but by a model's internal verification mechanisms, challenging assumptions about scaling and reliability.",
      "mindmap": "graph TB\n        A[The Drill-Down and Fabricate Test (DDFT)] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有评估无法衡量知识鲁棒性/Current evaluations fail to measure knowledge robustness]\n        C --> C1[DDFT协议: 语义压缩与对抗伪造/DDFT Protocol: Semantic Compression & Adversarial Fabrication]\n        C --> C2[双系统认知模型/Two-System Cognitive Model]\n        D --> D1[鲁棒性与模型规模/架构无关/Robustness orthogonal to model size/architecture]\n        D --> D2[错误检测能力是关键瓶颈/Error detection is the critical bottleneck]"
    },
    {
      "title": "Security Without Detection: Economic Denial as a Primitive for Edge and IoT Defense",
      "authors": "Samaresh Kumar Singh, Joyjit Roy",
      "institution": "IEEE (Inferred from author affiliations as IEEE members; specific institutional affiliation not provided in the excerpt)",
      "link": "https://arxiv.org/pdf/2512.23849",
      "code": null,
      "tags": [
        "IoT Security",
        "Economic Denial Security",
        "Stackelberg Game",
        "Cost Asymmetry",
        "Computational Puzzles"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53020dd5fb969c1980dd7f764afe5f97440c1ef68620bdcd3383e97bf39600fc_w640_q70.webp",
      "contributions": "1. Proposed the Economic Denial Security (EDS) framework, a detection-independent defense that exploits the defender's environmental control to impose economic infeasibility on attackers., 2. Formally modeled EDS as a Stackelberg game, deriving optimal parameters and proving that the composition of its four mechanisms yields superlinear (2.1x) cost amplification., 3. Demonstrated practical efficacy with a lightweight (&lt;12KB) implementation, validated on a 20-device IoT testbed and against IoT-23 malware, showing significant attack slowdown, cost asymmetry, and improved mitigation rates.",
      "summary": "The paper addresses the failure of detection-based security in resource-constrained IoT/edge environments. It proposes Economic Denial Security (EDS), a framework that uses mechanisms like computational puzzles and bandwidth taxation to make attacks economically infeasible by amplifying attacker costs. The method is proven to be lightweight, effective in significantly slowing attacks and reducing success rates, and provides a detection-independent layer of defense.",
      "mindmap": "graph TB\n        A[Security Without Detection: Economic Denial as a Primitive for Edge and IoT Defense] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[检测安全在资源受限的IoT/边缘环境中失效/Detection-based security fails in resource-constrained IoT/edge]\n        C --> C1[经济拒绝安全框架 / Economic Denial Security (EDS) Framework]\n        C1 --> C2[四种机制组合 / Four Mechanism Composition]\n        C2 --> C3[计算谜题 / Computational Puzzles]\n        C2 --> C4[交互熵 / Interaction Entropy]\n        C2 --> C5[时间拉伸 / Temporal Stretching]\n        C2 --> C6[带宽征税 / Bandwidth Taxation]\n        C --> C7[斯塔克尔伯格博弈建模 / Stackelberg Game Modeling]\n        D --> D1[32-560倍攻击减速 / 32-560x Attack Slowdown]\n        D --> D2[85-520:1 成本不对称 / 85-520:1 Cost Asymmetry]\n        D --> D3[内存占用<12KB / <12KB Memory Footprint]\n        D --> D4[94% 恶意软件缓解 / 94% Malware Mitigation]"
    },
    {
      "title": "Seeking Late Night Life Lines: Experiences of Conversational AI Use in Mental Health Crisis",
      "authors": "Leah Hope Ajmani, Arka Ghosh, Benjamin Kaveladze, Eugenia Kim, Keertana Namuduri, Theresa Nguyen, Ebele Okoli, Jessica Schleider, Denae Ford, Jina Suh",
      "institution": "University of Minnesota, Northwestern University, Dartmouth College, Microsoft, Microsoft Research, Mental Health America",
      "link": "https://arxiv.org/pdf/2512.23859",
      "code": null,
      "tags": [
        "conversational ai",
        "mental health crisis",
        "stages of change model",
        "human-AI interaction",
        "testimonial survey",
        "expert interviews"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/557b0c2624da79d40758f334c7d781c4558951ee96a27d54b04a81b3f20ec2ea_w640_q70.webp",
      "contributions": "1. Provides first-person experiential data on using conversational AI during mental health crises via a testimonial survey (n=53). 2. Contrasts user experiences with mental health expert perspectives (n=16) to highlight the essential role of human connection in crisis management. 3. Proposes a responsible design framework for AI crisis intervention, positioning AI as a bridge to human support using the stages of change model.",
      "summary": "This paper investigates how people use conversational AI (e.g., ChatGPT) during mental health crises through a survey and expert interviews. It finds users turn to AI due to gaps in human support, but experts emphasize human connection is crucial. The study concludes that responsible AI should act as a bridge to human help, increasing preparedness for positive action and de-escalating crises.",
      "mindmap": "graph TB\n        A[Seeking Late Night Life Lines: Experiences of Conversational AI Use in Mental Health Crisis] --> B(核心问题/Problem: Can conversational AI responsibly support mental health crises?)\n        A --> C(主要方法/Method: Testimonial survey (n=53) & expert interviews (n=16))\n        A --> D(关键结果/Results: AI fills gaps in human support; Human connection is essential; Design AI as a bridge to human help)"
    },
    {
      "title": "Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining",
      "authors": "Ruizhe Huang, Kexuan Zhang, Yihao Fang, Baifeng Yu",
      "institution": "Huawei Technologies Canada Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.23862",
      "code": "https://github.com/RRaAy-H/nanotron-infini",
      "tags": [
        "llm training",
        "Infini-attention",
        "compressive memory",
        "small language models (SLMs)",
        "long-context extrapolation",
        "pretraining"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af598a1dcd8b2b6a3dec75fe7434942b519517dea235cfb006c3cf73881444fd_w640_q70.webp",
      "contributions": "1. Replaced standard attention in a 300M-parameter LLaMA model with Infini-attention to study compressive memory behavior under short-sequence pretraining. 2. Analyzed the training dynamics of SLMs with Infini-attention, revealing characteristics like loss fluctuations, gradient volatility, and early-layer memory concentration. 3. Demonstrated that Infini-attention improves long-context extrapolation over a baseline model, with supervised fine-tuning further boosting performance.",
      "summary": "This paper investigates whether the Infini-attention mechanism, which combines local attention with compressive memory, can enhance long-context capabilities in Small Language Models (SLMs) during small-scale pretraining. The authors empirically study a 300M-parameter LLaMA model equipped with Infini-attention and find it improves long-context retrieval accuracy over a baseline, despite some degradation over very long sequences. The conclusion is that architectural memory like Infini-attention is beneficial for achieving robust long-context performance in SLMs.",
      "mindmap": "graph TB\n        A[Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Enhancing long-context extrapolation for Small Language Models (SLMs)]\n        C[主要方法/Method: Using Infini-attention (compressive memory + local attention) in small-scale pretraining]\n        D[关键结果/Results: Improves long-context retrieval; Identifies balance factor importance; Shows performance degradation over very long sequences but still outperforms baseline]"
    },
    {
      "title": "Lifelong Domain Adaptive 3D Human Pose Estimation",
      "authors": "Qucheng Peng, Hongfei Xue, Pu Wang, Chen Chen",
      "institution": "University of Central Florida, University of North Carolina at Charlotte",
      "link": "https://arxiv.org/pdf/2512.23860",
      "code": null,
      "tags": [
        "human pose estimation",
        "lifelong domain adaptation",
        "catastrophic forgetting",
        "generative adversarial network",
        "pose-aware knowledge",
        "temporal-aware knowledge"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbe905e18ac9835b4aae0dbb155169c447150fbd0e28ac454c7cc8a56bb7251e_w640_q70.webp",
      "contributions": "1. Proposes a novel lifelong domain adaptation task for 3D Human Pose Estimation, addressing the challenge of non-stationary target pose datasets. 2. Introduces an innovative GAN framework with 3D pose generators, a 2D pose discriminator, and a 3D pose estimator to mitigate domain shifts and align poses. 3. Constructs a novel 3D pose generator paradigm that integrates pose-aware, temporal-aware, and domain-aware knowledge to enhance adaptation and alleviate catastrophic forgetting.",
      "summary": "This paper proposes a lifelong domain adaptation framework for 3D human pose estimation to handle non-stationary target data distributions. The method uses a novel GAN-based framework with a knowledge-integrated 3D pose generator to adapt to new domains while preventing catastrophic forgetting of previous ones. Experiments show the approach achieves superior performance on diverse domain adaptive 3D HPE datasets.",
      "mindmap": "graph TB\n    A[Lifelong Domain Adaptive 3D Human Pose Estimation] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[3D HPE泛化挑战/3D HPE Generalization Challenge]\n    B --> B2[非平稳目标域/Non-stationary Target Domains]\n    B --> B3[灾难性遗忘/Catastrophic Forgetting]\n    C --> C1[终身域适应任务/Lifelong DA Task]\n    C --> C2[GAN框架/GAN Framework]\n    C --> C3[3D姿态生成器/3D Pose Generator]\n    C2 --> C2a[3D姿态生成器/3D Pose Generators]\n    C2 --> C2b[2D姿态判别器/2D Pose Discriminator]\n    C2 --> C2c[3D姿态估计器/3D Pose Estimator]\n    C3 --> C3a[姿态感知/Pose-aware]\n    C3 --> C3b[时序感知/Temporal-aware]\n    C3 --> C3c[域感知/Domain-aware]\n    D --> D1[缓解域偏移/Mitigates Domain Shifts]\n    D --> D2[对齐姿态/Aligns Poses]\n    D --> D3[卓越性能/Superior Performance]"
    },
    {
      "title": "Breaking Audio Large Language Models by Attacking Only the Encoder: A Universal Targeted Latent-Space Audio Attack",
      "authors": "Roee Ziv, Raz Lapid, Moshe Sipper",
      "institution": "Ben Gurion University of the Negev, Deepkeep",
      "link": "https://arxiv.org/pdf/2512.23881",
      "code": null,
      "tags": [
        "adversarial attacks",
        "universal adversarial perturbation",
        "latent-space attack",
        "audio-language models",
        "encoder-level vulnerability",
        "targeted attack"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aec73155c062c3c66b8e33c0e7892e17d374292349cfa71e3389850584cf8195_w640_q70.webp",
      "contributions": "1. Proposes a universal targeted latent-space attack against audio-language models, focusing solely on the audio encoder. 2. Introduces an attack method that learns a single perturbation effective across different inputs and speakers, without needing access to the downstream language model. 3. Demonstrates high attack success rates with minimal perceptual distortion on a state-of-the-art model, revealing a critical new attack surface in multimodal systems.",
      "summary": "This paper identifies a security vulnerability in audio-language models where adversarial attacks can be launched by manipulating only the audio encoder's latent representations. The proposed method learns a universal perturbation that forces the model to generate attacker-specified text outputs, and experiments show it is highly effective and stealthy. This reveals a significant and previously underexplored attack surface at the encoder level of multimodal AI systems.",
      "mindmap": "graph TB\n        Root[”Breaking Audio Large Language Models by Attacking Only the Encoder<br>仅攻击编码器来攻破音频大语言模型”] --> Problem[”核心问题/Problem<br>Audio-language models have new security vulnerabilities.<br>音频-语言模型存在新的安全漏洞”]\n        Root --> Method[”主要方法/Method<br>Universal targeted latent-space attack on the encoder.<br>针对编码器的通用目标潜空间攻击”]\n        Root --> Results[”关键结果/Results<br>High attack success with minimal distortion.<br>高攻击成功率，最小感知失真”]"
    },
    {
      "title": "CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution",
      "authors": "Xu Huang, Junwu Chen, Yuxing Fei, Zhuohan Li, Philippe Schwaller, Gerbrand Ceder",
      "institution": "University of California, Berkeley; Lawrence Berkeley National Laboratory; École Polytechnique Fédérale de Lausanne (EPFL)",
      "link": "https://arxiv.org/pdf/2512.23880",
      "code": null,
      "tags": [
        "agent system",
        "self-evolving agent",
        "skill acquisition",
        "autonomous development",
        "knowledge graph",
        "scientific research agent"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8104399440bdcb0943addb2985b2e27c755af9a6ab5a4f36894d7a82db9d0e00_w640_q70.webp",
      "contributions": "1. Introduces CASCADE, a self-evolving agentic framework that transitions from static \"LLM + tool use\" to dynamic \"LLM + skill acquisition\". 2. Proposes meta-skills for continuous learning (via web search/code extraction) and self-reflection (via introspection/knowledge graph exploration) to master complex external tools. 3. Demonstrates high effectiveness on scientific tasks (93.3% success rate on SciSkillBench) and real-world applications like computational analysis and autonomous lab experiments.",
      "summary": "The paper addresses the limitation of current LLM agents that rely on predefined or brittle tools, which hinders their adaptability in complex scientific tasks. It proposes CASCADE, a self-evolving framework that enables agents to autonomously acquire and codify skills through meta-skills like continuous learning and self-reflection. The method achieves a high success rate on a materials science and chemistry benchmark and shows promise for scalable AI-assisted scientific research.",
      "mindmap": "graph TB\n        A[CASCADE: Cumulative Agentic Skill Creation<br>累积智能体技能创造] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[LLM agents depend on predefined/brittle tools<br>LLM智能体依赖预定义/脆弱工具]\n        B --> B2[Constrained capability for complex scientific tasks<br>处理复杂科学任务能力受限]\n        C --> C1[Self-evolving agentic framework<br>自进化智能体框架]\n        C --> C2[Meta-skills: Continuous Learning & Self-Reflection<br>元技能：持续学习与自我反思]\n        C --> C3[Transition: LLM+Tool Use → LLM+Skill Acquisition<br>转变：从工具使用到技能获取]\n        D --> D1[93.3% success rate on SciSkillBench (116 tasks)<br>在SciSkillBench上成功率93.3%]\n        D --> D2[Real-world applications demonstrated<br>演示了实际应用]\n        D --> D3[Enables scalable AI-assisted research<br>实现可扩展的AI辅助研究]"
    },
    {
      "title": "How Large Language Models Systematically Misrepresent American Climate Opinions",
      "authors": "Sola Kim, Jieshu Wang, Marco A. Janssen, John M. Anderies",
      "institution": "Arizona State University, Stony Brook University",
      "link": "https://arxiv.org/pdf/2512.23889",
      "code": null,
      "tags": [
        "large language model evaluation",
        "large language models",
        "public opinion simulation",
        "intersectionality",
        "bias evaluation",
        "climate policy"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c0e5dfdc0fba7038277e876cd0c81062b3c5735782d5df732873aec991a84b3_w640_q70.webp",
      "contributions": "1. Conducted the first comparative study of LLM-generated public opinion against real human survey responses across intersecting demographic identities (race and gender). 2. Identified a systematic \"compression\" bias in LLMs, where they flatten the diversity of climate opinions by overestimating concern in less-concerned groups and underestimating it in more-concerned groups. 3. Revealed that this bias is intersectional, showing that LLMs apply uniform gender assumptions that fail for specific racial groups (e.g., misrepresenting gender patterns among Black Americans), a flaw potentially invisible to standard audits.",
      "summary": "This paper investigates how six large language models (LLMs) represent U.S. climate opinions by prompting them with profiles from a real national survey and comparing their generated responses to actual human answers. The study finds that LLMs systematically compress opinion diversity and misrepresent intersectional patterns, particularly for Black Americans, which could undermine equitable policy-making.",
      "mindmap": "graph TB\n        A[”How Large Language Models Systematically Misrepresent American Climate Opinions<br>论文标题”] --> B[”Problem: LLMs used for public opinion analysis may misrepresent diverse, intersectional views.<br>核心问题：用于公众意见分析的LLM可能歪曲多样化的交叉性观点。”]\n        A --> C[”Method: Prompt 6 LLMs with real survey respondent profiles and compare outputs to human answers.<br>主要方法：用真实调查受访者档案提示6个LLM，并将输出与人类答案比较。”]\n        A --> D[”Results: LLMs compress opinion diversity and misapply gender assumptions across racial groups.<br>关键结果：LLM压缩了意见多样性，并在不同种族群体中误用了性别假设。”]"
    },
    {
      "title": "Efficient Deep Learning for Short-Term Solar Irradiance Time Series Forecasting: A Benchmark Study in Ho Chi Minh City",
      "authors": "Tin Hoang",
      "institution": "University of Surrey",
      "link": "https://arxiv.org/pdf/2512.23898",
      "code": "github.com/Tin-Hoang/solar-timeseries-forecasting",
      "tags": [
        "time series forecasting",
        "Transformer",
        "Mamba",
        "Knowledge Distillation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8685dbd2386bb45fd799c168962048f6a243c0b8c836a5be5978ff64d0c2e184_w640_q70.webp",
      "contributions": "1. Conducted a comprehensive benchmark of ten deep learning architectures for short-term solar irradiance forecasting, identifying the Transformer as the best-performing model. 2. Used SHAP analysis to reveal and contrast the distinct temporal reasoning patterns of different architectures (e.g., Transformer's recency bias vs. Mamba's periodic dependency). 3. Demonstrated that Knowledge Distillation can effectively compress the high-performance Transformer model, reducing its size by 23.5% while improving accuracy for edge deployment.",
      "summary": "This paper benchmarks ten deep learning models for 1-hour ahead solar irradiance forecasting in Ho Chi Minh City. The Transformer model achieved the highest accuracy, and the study used explainable AI to analyze model behavior and successfully compressed the model via Knowledge Distillation for efficient edge deployment.",
      "mindmap": "graph TB\n        A[论文标题 / Paper Title: Efficient Deep Learning for Short-Term Solar Irradiance Forecasting] --> B\n        A --> C\n        A --> D\n        B[核心问题 / Problem: 预测全球水平辐照度(GHI)以缓解太阳能波动 / Forecasting GHI to mitigate solar energy variability]\n        C[主要方法 / Method: 对十种深度学习架构进行基准测试与可解释性分析 / Benchmarking 10 DL architectures with explainability analysis]\n        D[关键结果 / Results: Transformer性能最优；知识蒸馏实现高效压缩 / Transformer best; Knowledge Distillation enables efficient compression]"
    },
    {
      "title": "Interactive Machine Learning: From Theory to Scale",
      "authors": "Yinglun Zhu",
      "institution": "University of Wisconsin–Madison",
      "link": "https://arxiv.org/pdf/2512.23924",
      "code": null,
      "tags": [
        "interactive machine learning",
        "active learning",
        "contextual bandits",
        "model selection",
        "sequential decision making",
        "partial feedback"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d3c8fc2dd2145126e7321fa8de265f0c2652dbd2dc7df8c387585634498e335_w640_q70.webp",
      "contributions": "1. Developed computationally efficient active learning algorithms that achieve exponential label savings without requiring low-noise assumptions., 2. Introduced the first efficient, general-purpose contextual bandit algorithms whose performance guarantees are independent of the action space size., 3. Provided the first tight characterizations of the fundamental cost of model selection in sequential decision-making settings.",
      "summary": "This dissertation addresses the high cost of data labeling and trial-and-error in machine learning by developing new algorithms for interactive learning. It proposes statistically optimal and computationally efficient methods for active learning, contextual bandits with large action spaces, and model selection under partial feedback. The work advances the theoretical foundations of interactive learning and provides guidance for its deployment in large-scale, real-world applications.",
      "mindmap": "graph TB\n        Root(”Interactive Machine Learning: From Theory to Scale<br>交互式机器学习：从理论到规模”)\n        Root --> Problem(”Problem: High cost of labeled data & trial-and-error in ML<br>核心问题：机器学习中标注数据和试错的高成本”)\n        Root --> Method(”Method: Develop algorithms for interactive learning<br>主要方法：开发交互式学习算法”)\n        Root --> Results(”Results: Statistically optimal & computationally efficient algorithms<br>关键结果：统计最优且计算高效的算法”)\n        Problem --> P1(”Active learning with noisy data<br>含噪声数据的主动学习”)\n        Problem --> P2(”Sequential decision making with large action spaces<br>大动作空间的序列决策”)\n        Problem --> P3(”Model selection under partial feedback<br>部分反馈下的模型选择”)\n        Method --> M1(”New algorithmic principles<br>新算法原理”)\n        Method --> M2(”Establish fundamental limits<br>建立基本极限”)\n        Results --> R1(”Exponential label savings in active learning<br>主动学习中的指数级标签节省”)\n        Results --> R2(”Contextual bandit guarantees independent of action space size<br>与动作空间大小无关的上下文赌博机保证”)\n        Results --> R3(”Tight characterization of model selection cost<br>模型选择成本的紧致刻画”)"
    },
    {
      "title": "A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models and Answer Set Programming",
      "authors": "Ioanna Gemou, Evangelos Lamprou",
      "institution": "University of Patras",
      "link": "https://arxiv.org/pdf/2512.23932",
      "code": null,
      "tags": [
        "neuro-symbolic ai",
        "Answer Set Programming",
        "Large Language Models",
        "Explainable AI",
        "Knowledge Base Construction",
        "Disease Diagnosis"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9000f6d3b0daa6fb6bc9ea6b6b2f1ca56befb92d47f3f4f67b25d12dc6ca996_w640_q70.webp",
      "contributions": "1. Proposes McCoy, a novel framework that integrates LLMs and Answer Set Programming for automated disease diagnosis. 2. Automates the labor-intensive construction of medical knowledge bases by using an LLM to translate medical literature into ASP code. 3. Delivers an interpretable and robust diagnostic system that provides logical justifications for its predictions, achieving high accuracy on preliminary tasks.",
      "summary": "This paper introduces McCoy, a framework that combines Large Language Models (LLMs) with Answer Set Programming (ASP) to automate the creation of diagnostic knowledge bases and perform explainable disease diagnosis. The LLM translates medical literature into ASP rules, which are then combined with patient data and solved to produce a diagnosis with logical justifications. Preliminary results show the framework achieves high predictive accuracy on small-scale tasks while providing transparency.",
      "mindmap": "graph TB\n        Root[”A Proof-of-Concept for Explainable Disease Diagnosis Using Large Language Models and Answer Set Programming<br>可解释疾病诊断的概念验证：使用大语言模型与回答集编程”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Symbolic AI adoption limited by manual knowledge base construction.<br>符号AI因需手动构建知识库而应用受限。”]\n        Method[”主要方法/Method<br>Combine LLM (translates literature) with ASP (logical reasoning).<br>结合LLM（翻译文献）与ASP（逻辑推理）。”]\n        Results[”关键结果/Results<br>High accuracy, interpretable diagnosis framework.<br>高准确性、可解释的诊断框架。”]"
    },
    {
      "title": "An Comparative Analysis about KYC on a Recommendation System Toward Agentic Recommendation System",
      "authors": "Junjie H. Xu",
      "institution": "Hechu Tech",
      "link": "https://arxiv.org/pdf/2512.23961",
      "code": null,
      "tags": [
        "agent system",
        "agentic AI",
        "recommendation system",
        "KYC (Know Your Customer)",
        "nDCG",
        "multi-stage architecture"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76e7c4d7572522a69f7c0db05b6553014273403aa44576ea9c0de823750c5368_w640_q70.webp",
      "contributions": "1. Proposes a novel agentic AI-based recommendation system specifically designed for integrating KYC (Know Your Customer) processes. 2. Conducts a comparative performance evaluation across five distinct content verticals (Ad, News, Gossip, Sharing, Tech) using the nDCG metric. 3. Synthesizes experimental data with industry benchmarks to provide engineering insights for building large-scale agentic recommendation systems.",
      "summary": "This paper proposes a new recommendation system that uses agentic AI to incorporate KYC (Know Your Customer) information. It evaluates the system's performance across five different content types and compares it against standard benchmarks. The study concludes by providing practical insights for engineering large-scale agentic recommendation systems based on the experimental results.",
      "mindmap": "graph TB\n        A[An Comparative Analysis about KYC on a Recommendation System Toward Agentic Recommendation System] --> B[核心问题/Problem: Transition from passive ranking to agentic AI in RecSys]\n        A --> C[主要方法/Method: Agentic AI for KYC, evaluated across 5 content verticals using nDCG@k]\n        A --> D[关键结果/Results: Performance comparison of 4 KYC usage groups, insights for large-scale engineering]"
    },
    {
      "title": "Physics-informed Graph Neural Networks for Operational Flood Modeling",
      "authors": "Carlo Malapad Acosta, Herath Mudiyanselage Viraj Vidura Herath, Jia Yu Lim, Abhishek Saha, Sanka Rasnayaka, Lucy Marshall",
      "institution": "National University of Singapore, The University of Sydney, Delft University of Technology",
      "link": "https://arxiv.org/pdf/2512.23964",
      "code": "https://github.com/acostacos/dual_flood_gnn",
      "tags": [
        "graph neural networks",
        "physics-informed neural networks",
        "graph neural networks",
        "flood modeling",
        "curriculum learning",
        "message-passing"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c6bbddb2010c8550ce3ea4a09f24c04abc0993a7ee2a722f960fc0852c4f049_w640_q70.webp",
      "contributions": "1. Proposes DUALFloodGNN, a novel GNN architecture that embeds physical constraints at both global and local scales through explicit loss terms. 2. Introduces a model that jointly predicts water volume at nodes and flow along edges using a shared message-passing framework. 3. Enhances autoregressive inference performance via multi-step loss training with dynamic curriculum learning.",
      "summary": "This paper addresses the high computational cost of physics-based flood models by proposing DUALFloodGNN, a physics-informed graph neural network architecture. The model incorporates physical constraints into its loss function and uses a multi-step training strategy with curriculum learning. It achieves improved prediction accuracy for hydrologic variables while maintaining high computational efficiency compared to existing GNN models.",
      "mindmap": "graph TB\n        A[Physics-informed Graph Neural Networks for Operational Flood Modeling] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: High computational cost of physics-based flood models limits operational use]\n        C[主要方法/Method: DUALFloodGNN embeds physical constraints via loss terms and uses multi-step training with curriculum learning]\n        D[关键结果/Results: Achieves improved accuracy and maintains high computational efficiency]"
    },
    {
      "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling",
      "authors": "Chulun Zhou, Chunkang Zhang, Guoxin Yu, Fandong Meng, Jie Zhou, Wai Lam, Mo Yu",
      "institution": "The Chinese University of Hong Kong, WeChat AI",
      "link": "https://arxiv.org/pdf/2512.23959",
      "code": "https://github.com/Encyclomen/HGMem",
      "tags": [
        "rag (retrieval-augmented generation)",
        "hypergraph memory",
        "multi-step reasoning",
        "global sense-making",
        "long-context modeling",
        "retrieval-augmented generation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259f66b1c3afc451216d5a69cb56a5a72ee4244c5fa02941603de2fbd4afc261_w640_q70.webp",
      "contributions": "1. Proposes HGMem, a novel hypergraph-based memory mechanism that models memory as a dynamic structure with higher-order interactions, moving beyond passive storage. 2. Addresses the limitation of existing multi-step RAG memory in capturing complex relational structures and providing strong guidance for subsequent reasoning steps. 3. Demonstrates through extensive experiments that the method consistently improves multi-step RAG performance and substantially outperforms strong baselines on global sense-making tasks.",
      "summary": "This paper addresses the limitation of static, passive memory in multi-step RAG systems, which leads to fragmented reasoning in long-context tasks. It proposes HGMem, a dynamic hypergraph-based memory mechanism that captures high-order correlations among facts to form an integrated knowledge structure for stronger reasoning guidance. The method is shown to consistently and substantially outperform baseline systems across diverse global sense-making tasks.",
      "mindmap": "graph TB\n        A[Improving Multi-step RAG with Hypergraph-based Memory<br>改进多步RAG的超图记忆] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有记忆模块是被动的静态存储<br>Existing memory is passive static storage]\n        B1 --> B2[忽略了高阶关联，导致碎片化推理<br>Ignores high-order correlations, causing fragmented reasoning]\n        C --> C1[提出超图记忆机制 HGMem<br>Propose hypergraph memory mechanism HGMem]\n        C1 --> C2[将记忆表示为动态超图<br>Represent memory as a dynamic hypergraph]\n        C2 --> C3[超边形成高阶交互，构建集成知识结构<br>Hyperedges form high-order interactions, building integrated knowledge]\n        D --> D1[在多步RAG上取得一致改进<br>Achieves consistent improvement on multi-step RAG]\n        D1 --> D2[在全局理解任务上显著超越基线<br>Substantially outperforms baselines on global sense-making tasks]"
    },
    {
      "title": "Efficient Context Scaling with LongCat ZigZag Attention",
      "authors": "Chen Zhang, Yang Bai, Jiahuan Li, Anchun Gui, Keheng Wang, Feifan Liu, Guanyu Wu, Yuwei Jiang, Defei Bu, Li Wei, Haihang Jing, Hongyin Tang, Xin Chen, Xiangzhou Huang, Fengcun Li, Rongxiang Weng, Yulei Qian, Yifan Lu, Yerui Sun, Jingang Wang, Yuchen Xie, Xunliang Cai",
      "institution": "Meituan",
      "link": "https://arxiv.org/pdf/2512.23966",
      "code": null,
      "tags": [
        "llm inference",
        "sparse attention",
        "long-context",
        "mid-training",
        "ZigZag Attention"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46b353da1c2cbb89962a2e909144fbcc8c92d821f35157049a111e1855b4242c_w640_q70.webp",
      "contributions": "1. Proposes LongCat ZigZag Attention (LoZA), a sparse attention scheme to convert full-attention models into sparse versions with limited compute. 2. Demonstrates LoZA's effectiveness for speed-up in both prefill-intensive (e.g., RAG) and decode-intensive (e.g., tool use) long-context scenarios. 3. Applies LoZA to create LongCat-Flash-Exp, a foundation model capable of efficiently processing up to 1 million tokens.",
      "summary": "The paper introduces LongCat ZigZag Attention (LoZA), a sparse attention method designed to efficiently transform standard full-attention language models into sparse models suitable for long-context tasks. This approach enables significant speed improvements for both prefill and decoding phases. The resulting model, LongCat-Flash-Exp, can process up to 1 million tokens, facilitating efficient long-term reasoning and agentic capabilities.",
      "mindmap": "graph TB\n        A[Efficient Context Scaling with LongCat ZigZag Attention] --> B[核心问题/Problem: 长上下文场景下全注意力计算开销大/High computational cost of full attention in long-context scenarios]\n        A --> C[主要方法/Method: 提出LoZA稀疏注意力方案/Propose LoZA sparse attention scheme]\n        A --> D[关键结果/Results: 实现显著加速，支持百万token高效处理/Achieve significant speed-up, enable efficient processing of 1M tokens]"
    },
    {
      "title": "Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing",
      "authors": "Giacinto Paolo Saggese, Paul Smith",
      "institution": "Not explicitly stated. Could be inferred from author names and arXiv submission, but no clear affiliation is provided in the given content.",
      "link": "https://arxiv.org/pdf/2512.23977",
      "code": null,
      "tags": [
        "others",
        "streaming machine learning",
        "directed acyclic graph (DAG)",
        "point-in-time idempotency",
        "temporal tiling",
        "causality enforcement"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4105add65c1e7a9960d7e21b6bda977778c41365cfff18c04d16a1af3773b5c4_w640_q70.webp",
      "contributions": "1. A unified DAG-based execution model with point-in-time idempotency, ensuring identical model behavior in batch and streaming modes without code changes. 2. Automatic causality enforcement by tracking knowledge time across transformations, eliminating future-peeking bugs. 3. Flexible temporal and feature dimension tiling, allowing models to operate at different frequencies and memory profiles via configuration alone.",
      "summary": "The paper presents DataFlow, a framework for building high-performance ML systems on streaming time-series data. It uses a DAG-based model with point-in-time idempotency to bridge the gap between batch prototyping and streaming production, ensuring causality and reproducibility. The framework demonstrates effectiveness in domains like financial trading and IoT analytics.",
      "mindmap": "graph TB\n        Root[”Causify DataFlow: A Framework For High-performance Machine Learning Stream Computing”] --> Problem[”核心问题/Problem: Gap between batch ML prototypes and streaming production systems causes causality violations and poor reproducibility.”]\n        Root --> Method[”主要方法/Method: Unified DAG execution model with point-in-time idempotency and automatic causality tracking.”]\n        Root --> Results[”关键结果/Results: Enables identical batch/stream execution, flexible tiling, and effective deployment in financial, IoT, and fraud detection domains.”]"
    },
    {
      "title": "A Community-Aware Framework for Influence Maximization with Explicit Accounting for Inter-Community Influence",
      "authors": "Eliot W. Robson, Abhishek K. Umrawal",
      "institution": "Narmi, University of Illinois Urbana-Champaign",
      "link": "https://arxiv.org/pdf/2512.23973",
      "code": null,
      "tags": [
        "social network analysis",
        "influence maximization",
        "community structure",
        "inter-community diffusion",
        "progressive budgeting",
        "community-based diffusion degree"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b25bc24c3cf4c404605618e8019a2659fc31bfeb9d29ad19d3af7a7ca5cc7a4c_w640_q70.webp",
      "contributions": "1. Proposes Community-IM++, a scalable framework that explicitly models cross-community influence, overcoming a key limitation of prior community-based methods. 2. Introduces a principled heuristic based on community-based diffusion degree (CDD) and a progressive budgeting strategy to prioritize bridging nodes and allocate seeds adaptively. 3. Demonstrates through experiments on large real-world networks that the method achieves near-greedy influence spread with up to 100x speedup, outperforming baseline heuristics.",
      "summary": "This paper addresses the Influence Maximization problem in social networks, where existing community-based methods often overlook inter-community influence. The authors propose Community-IM++, a scalable framework that explicitly models cross-community diffusion using a new heuristic (CDD) and a progressive budgeting strategy. The method achieves near-optimal performance with significantly lower runtime, making it practical for large-scale applications like viral marketing and public health campaigns.",
      "mindmap": "graph TB\n        Root(”A Community-Aware Framework for Influence Maximization<br>影响力最大化社区感知框架”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”社区方法忽略社区间影响<br>Community methods ignore inter-community influence”)\n        Method --> M1(”提出Community-IM++框架<br>Propose Community-IM++ framework”)\n        M1 --> M2(”使用社区扩散度(CDD)启发式<br>Use Community-based Diffusion Degree (CDD) heuristic”)\n        M1 --> M3(”渐进预算与惰性评估<br>Progressive budgeting & lazy evaluation”)\n        Results --> R1(”接近贪婪算法的传播范围<br>Near-greedy influence spread”)\n        Results --> R2(”运行时间降低高达100倍<br>Up to 100x lower runtime”)\n        Results --> R3(”优于基线方法<br>Outperforms baseline methods”)"
    },
    {
      "title": "Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education",
      "authors": "Hung-Fu Chang, MohammadShokrolah Shirazi, Lizhou Cao, Supannika Koolmanojwong Mobasser",
      "institution": "University of Indianapolis, Marian University, University of Maryland Eastern Shore, The Boehm Center for Systems and Software Engineering",
      "link": "https://arxiv.org/pdf/2512.23982",
      "code": null,
      "tags": [
        "AI-assisted Software Engineering",
        "vibe coding",
        "agentic coding",
        "LLM-based coding",
        "code review",
        "curricular shift"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d32f497e3b36008891d37440a49e031ae3f31d7dcba8585c229352e091d83a3a_w640_q70.webp",
      "contributions": "1. Provides an industry-grounded investigation of LLM coding practices (vibe, AI-assisted, agentic coding) and their impact on professional workflows, based on qualitative analysis of practitioner reflections. 2. Identifies key risks and concerns associated with AI-based coding, including shifts in development bottlenecks to code review, code quality issues, security vulnerabilities, and skill erosion. 3. Proposes implications and guidance for computer science and software engineering education, advocating for curricular shifts toward problem-solving, architectural thinking, and early integration of LLM tools.",
      "summary": "This paper investigates how large language models (LLMs) are used in professional software development by qualitatively analyzing 57 YouTube videos from practitioners. The study identifies new coding paradigms, productivity gains, and associated risks like quality and security concerns. It concludes by discussing the need for educational reforms in computer science to align with these evolving industrial practices.",
      "mindmap": "graph TB\n        Root(”Coding With AI: From Industrial Practices to Future Education”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”工业实践中LLM编码工具的使用与风险未充分探索/LLM coding use & risks in industry underexplored”)\n        Method --> M1(”对57个YouTube视频进行定性分析/Qualitative analysis of 57 YouTube videos”)\n        Results --> R1(”定义AI编码实践，发现生产力提升与风险/Defines AI coding practices, finds productivity gains & risks”)\n        Results --> R2(”提出计算机科学教育的课程改革建议/Proposes curricular shifts for CS education”)"
    },
    {
      "title": "MeLeMaD: Adaptive Malware Detection via Chunk-wise Feature Selection and Meta-Learning",
      "authors": "Ajvad Haneef K, Karan Kuwar Singh, Madhu Kumar S D",
      "institution": "National Institute of Technology Calicut",
      "link": "https://arxiv.org/pdf/2512.23987",
      "code": null,
      "tags": [
        "malware detection",
        "Model-Agnostic Meta-Learning (MAML)",
        "Chunk-wise Feature Selection (CFSGB)",
        "Gradient Boosting"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1509044c1c0bdfbb4a075c799e3c0cafd035f0b7c579548b445cf04caee2975d_w640_q70.webp",
      "contributions": "1. Proposed MeLeMaD, a novel malware detection framework leveraging Model-Agnostic Meta-Learning (MAML) for adaptability and generalization. 2. Introduced a novel Chunk-wise Feature Selection based on Gradient Boosting (CFSGB) technique to handle large-scale, high-dimensional datasets efficiently. 3. Demonstrated state-of-the-art performance on benchmark datasets (CIC-AndMal2020, BODMAS) and a custom dataset (EMBOD), achieving high accuracy and robustness.",
      "summary": "The paper proposes MeLeMaD, a novel malware detection framework that combines a new chunk-wise feature selection method (CFSGB) with meta-learning (MAML) to improve adaptability and efficiency on large-scale datasets. It achieves high accuracy on benchmark and custom datasets, outperforming existing state-of-the-art approaches and demonstrating robustness against evolving threats.",
      "mindmap": "graph TB\n        A[MeLeMaD: Adaptive Malware Detection] --> B[核心问题/Problem: Malware detection needs robustness & adaptability]\n        A --> C[主要方法/Method: Meta-Learning (MAML) + Chunk-wise Feature Selection (CFSGB)]\n        A --> D[关键结果/Results: High accuracy on benchmarks (98.04%, 99.97%) & custom dataset]"
    },
    {
      "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process",
      "authors": "Zhenyu Zhang, Shujian Zhang, John Lambert, Wenxuan Zhou, Zhangyang Wang, Mingqing Chen, Andrew Hard, Rajiv Mathews, Lun Wang",
      "institution": "Google DeepMind, The University of Texas at Austin",
      "link": "https://arxiv.org/pdf/2512.23988",
      "code": null,
      "tags": [
        "mechanistic interpretability",
        "sparse auto-encoder",
        "reasoning vectors",
        "chain-of-thought"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6bf740af692f8a7498e3cb43c54db55a7bd4f6005d4c48bc0e225978738bf9a_w640_q70.webp",
      "contributions": "1. Proposes RISE, an unsupervised framework using sparse auto-encoders (SAEs) to discover \"reasoning vectors\" that encode distinct reasoning behaviors from step-level LLM activations. 2. Demonstrates that these discovered vectors correspond to interpretable behaviors (e.g., reflection, backtracking) and can be used for targeted intervention to controllably steer the reasoning process without retraining. 3. Shows SAEs can uncover novel, human-undefined reasoning behaviors and structural properties, such as controlling response confidence, highlighting the potential of unsupervised latent discovery.",
      "summary": "This paper addresses the challenge of interpreting the internal reasoning process of large language models (LLMs). It proposes RISE, an unsupervised framework that uses sparse auto-encoders to discover disentangled \"reasoning vectors\" from chain-of-thought activations. The method enables the identification, visualization, and controllable intervention of specific reasoning behaviors, revealing novel insights beyond supervised analysis.",
      "mindmap": "graph TB\n        Root[”Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process”]\n        Root --> Problem[”核心问题/Problem<br>LLM推理内部机制不明确<br>Supervised methods are limited”]\n        Root --> Method[”主要方法/Method<br>RISE框架: 无监督稀疏自编码器<br>Unsupervised SAEs on step-level activations”]\n        Root --> Results[”关键结果/Results<br>发现可解释推理行为向量<br>可控干预推理轨迹<br>Discover novel behaviors”]"
    },
    {
      "title": "PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark for Physically Grounded Text-to-Audio-Video Generation",
      "authors": "Tianxin Xie, Wentao Lei, Guanjie Huang, Pengfei Zhang, Kai Jiang, Chunhui Zhang, Fengji Ma, Haoyu He, Han Zhang, Jiangshan He, Jinting Wang, Linghan Fang, Lufei Gao, Orkesh Ablet, Peihua Zhang, Ruolin Hu, Shengyu Li, Weilin Lin, Xiaoyang Feng, Xinyue Yang, Yan Rong, Yanyun Wang, Zihang Shao, Zelin Zhao, Chenxing Li, Shan Yang, Wenfu Wang, Meng Yu, Dong Yu, Li Liu",
      "institution": "HKUST(GZ), Tencent, Shanghai Jiao Tong University, Technical University of Munich",
      "link": "https://arxiv.org/pdf/2512.23994",
      "code": "https://imxtx.github.io/PhyAVBench/",
      "tags": [
        "audio-visual generation",
        "text-to-audio-video",
        "physics-sensitivity",
        "benchmark",
        "audio-physics grounding",
        "contrastive physical response score"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9eec12bf437e946a08a88614c7454f1315e7f14f21f2ebe631265de8a77d352a_w640_q70.webp",
      "contributions": "1. Introduces PhyAVBench, a novel benchmark for evaluating the audio physics-sensitivity of T2AV models., 2. Proposes the Audio-Physics Sensitivity Test (APST) paradigm using paired prompts with controlled physical variables., 3. Defines the Contrastive Physical Response Score (CPRS) to quantitatively measure a model's understanding of physical principles.",
      "summary": "The paper identifies that current text-to-audio-video (T2AV) models lack physical plausibility in generated sounds. To address this, it introduces PhyAVBench, a challenging benchmark designed to systematically evaluate models' audio physics grounding through a novel Audio-Physics Sensitivity Test (APST). The authors argue that this benchmark will stimulate progress in generating physically consistent audio-visual content.",
      "mindmap": "graph TB\n        A[PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有T2AV模型无法生成物理合理的声音 / Existing T2AV models generate physically implausible sounds]\n        C --> C1[提出PhyAVBench基准与APST评估范式 / Propose PhyAVBench benchmark & APST evaluation paradigm]\n        C --> C2[使用成对提示控制物理变量 / Use paired prompts with controlled physical variables]\n        C --> C3[引入对比物理响应分数(CPRS) / Introduce Contrastive Physical Response Score (CPRS)]\n        D --> D1[系统性评估模型对声学物理的理解 / Systematically evaluate models' understanding of acoustic physics]\n        D --> D2[推动物理基础T2AV生成的研究 / Stimulate research in physically-grounded T2AV generation]"
    },
    {
      "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems",
      "authors": "Bulent Soykan, Sean Mondesire, Ghaith Rabadi",
      "institution": "University of Central Florida",
      "link": "https://arxiv.org/pdf/2512.24007",
      "code": "github.com/bulentsoykan/TESO",
      "tags": [
        "metaheuristics",
        "simulation optimization",
        "tabu search",
        "elite memory",
        "noisy black-box",
        "aspiration criterion"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00ac2df6705adb84c43e6fe1380cb528473aee412ea23024864f40a91dfd7ec9_w640_q70.webp",
      "contributions": "1. Proposes TESO, a novel metaheuristic framework that integrates adaptive search with memory-based strategies for simulation optimization. 2. Introduces a dual-memory mechanism combining a short-term Tabu List for diversification and a long-term Elite Memory for intensification. 3. Demonstrates the framework's effectiveness and reliability on a queue optimization problem, showing improved performance over benchmarks.",
      "summary": "This paper introduces TESO, a metaheuristic framework for noisy, expensive black-box simulation optimization. It combines a Tabu List and an Elite Memory with an aspiration criterion to balance exploration and exploitation. The method is validated on a queue optimization problem, showing improved performance and reliability compared to benchmarks.",
      "mindmap": "graph TB\n        Root[TESO: Tabu-Enhanced Simulation Optimization] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Noisy, expensive, multimodal simulation optimization] --> P1[挑战/Challenges: Noisy evaluations, high cost, complex landscapes]\n        Method[主要方法/Method: Memory-based metaheuristic framework] --> M1[组件/Components: Tabu List, Elite Memory, Aspiration Criterion]\n        Results[关键结果/Results: Validated on queue optimization] --> R1[结论/Conclusion: Improved performance & reliability]"
    },
    {
      "title": "SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing",
      "authors": "Gaurab Chhetri, Subasish Das, Tausif Islam Chowdhury",
      "institution": "Texas State University",
      "link": "https://arxiv.org/pdf/2512.24008",
      "code": null,
      "tags": [
        "agent system",
        "multi-agent systems",
        "retrieval-augmented generation",
        "persona-based agents",
        "long-term memory",
        "coordination protocols"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522362a9d0a8ab25d8cf01e3f9989591d57b2f5595d731ca2b3bdd84da90a098_w640_q70.webp",
      "contributions": "1. Proposes SPARK, a novel framework that uses coordinated, persona-based LLM agents for personalized search, moving beyond static user profiles. 2. Formalizes a persona space and introduces a Persona Coordinator to dynamically activate specialized agents based on query interpretation. 3. Facilitates inter-agent collaboration through structured protocols like shared memory and iterative debate, enabling emergent personalization from distributed behaviors.",
      "summary": "The paper proposes SPARK, a framework that uses coordinated, persona-based LLM agents to perform personalized search. It dynamically activates specialized agents for retrieval and uses structured communication for collaboration. The approach aims to model the fluid and complex nature of human information needs better than traditional static-profile systems.",
      "mindmap": "graph TB\n        SPARK_Title[SPARK: Search Personalization via Agent-Driven Retrieval and Knowledge-sharing] --> Problem[核心问题/Problem]\n        SPARK_Title --> Method[主要方法/Method]\n        SPARK_Title --> Results[关键结果/Results]\n        Problem --> P1[静态用户画像限制个性化搜索/Static user profiles limit personalized search]\n        Problem --> P2[单一检索流程难以捕捉复杂需求/Monolithic retrieval fails to capture complex needs]\n        Method --> M1[定义角色化智能体空间/Define persona-based agent space]\n        Method --> M2[引入智能体协调器/Introduce Persona Coordinator]\n        Method --> M3[智能体协作与知识共享/Agent collaboration & knowledge-sharing]\n        Results --> R1[实现涌现的个性化/Achieve emergent personalization]\n        Results --> R2[为下一代搜索系统提供见解/Provide insights for next-gen search systems]"
    },
    {
      "title": "iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning",
      "authors": "Sijia Chen, Di Niu",
      "institution": "Hong Kong University of Science and Technology (Guangzhou), University of Alberta",
      "link": "https://arxiv.org/pdf/2512.24014",
      "code": "https://github.com/AgenticFinLab/latent-planning",
      "tags": [
        "reasoning",
        "latent planning",
        "implicit cognition",
        "vector-quantized autoencoder",
        "chain-of-thought"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dce54c76575e0ccf630d7de1d6cf89260c30415f1817167f368dcb00b0d64bd_w640_q70.webp",
      "contributions": "1. Proposes iCLP, a novel framework inspired by human Implicit Cognition to enable LLMs to generate and use compact latent plans for reasoning. 2. Introduces a method to distill explicit plans from reasoning trajectories and learn their discrete representations via a vector-quantized autoencoder. 3. Demonstrates that fine-tuning LLMs on latent plans improves reasoning accuracy, efficiency, and cross-domain generalization while preserving interpretability.",
      "summary": "The paper addresses the challenge of LLMs generating unreliable explicit textual plans for reasoning. It proposes the iCLP framework, which enables LLMs to learn and use compact latent plans, inspired by human subconscious cognition. Experiments show this approach improves reasoning performance and generalization on mathematical and coding tasks.",
      "mindmap": "graph TB\n        A[iCLP: 大语言模型推理与隐式认知潜在规划<br>iCLP: LLM Reasoning with Implicit Cognition Latent Planning] --> B[核心问题/Problem: 显式文本规划生成困难<br>Challenges in generating explicit textual plans]\n        A --> C[主要方法/Method: 学习并使用潜在规划<br>Learn and use latent plans via VQ-VAE and fine-tuning]\n        A --> D[关键结果/Results: 提升准确率、效率与泛化能力<br>Improves accuracy, efficiency, and generalization]"
    },
    {
      "title": "FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing",
      "authors": "Yunkai Dang, Donghao Wang, Jiacheng Yang, Yifan Jiang, Meiyi Zhu, Yuekun Yang, Cong Wang, Qi Fan, Wenbin Li, Yang Gao",
      "institution": "Nanjing University",
      "link": "https://arxiv.org/pdf/2512.24022",
      "code": "https://github.com/Yunkaidang/RSVLM",
      "tags": [
        "vision-language models",
        "multi-feature fusion",
        "recurrent visual injection",
        "remote sensing vision-language model"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51db66b7af59969350cb2c0f2ca84f598178ab4f9ea4040dfc08f46586ef7fe0_w640_q70.webp",
      "contributions": "1. Proposes a Multi-Feature Fusion Remote Sensing Vision-Language Model (MF-RSVLM) that extracts and fuses multi-scale visual features to better capture small and complex structures in remote sensing scenes., 2. Introduces a recurrent visual feature injection scheme to keep the language model grounded in visual evidence and mitigate visual forgetting during text generation., 3. Demonstrates state-of-the-art or highly competitive performance on diverse remote sensing benchmarks, including classification, image captioning, and VQA tasks.",
      "summary": "The paper addresses the challenge of applying general vision-language models to remote sensing data by proposing MF-RSVLM, a model that fuses multi-scale visual features and uses recurrent visual injection to reduce forgetting. It achieves strong results on remote sensing classification, captioning, and VQA tasks.",
      "mindmap": "graph TB\n        A[FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: VLMs struggle with fine-grained features and visual forgetting in remote sensing]\n        C[主要方法/Method: Multi-feature fusion and recurrent visual injection]\n        D[关键结果/Results: SOTA/competitive performance on RS benchmarks]"
    },
    {
      "title": "Tracing the Heart's Pathways: ECG Representation Learning from a Cardiac Conduction Perspective",
      "authors": "Tan Pan, Yixuan Sun, Chen Jiang, Qiong Gao, Rui Sun, Xingmeng Zhang, Zhenqi Yang, Limei Han, Yixiu Liang, Yuan Cheng, Kaiyu Guo",
      "institution": "Fudan University, Shanghai Academy of Artificial Intelligence for Science",
      "link": "https://arxiv.org/pdf/2512.24002",
      "code": "https://github.com/Ashespt/CLEAR-HUG",
      "tags": [
        "self-supervised learning",
        "electrocardiogram (ECG)",
        "self-supervised learning (SSL)",
        "cardiac conduction",
        "sparse attention",
        "hierarchical diagnosis"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4316ae9f56d525621d461087803f69e54c99676d3863c02d5df1d1ad32dab6a_w640_q70.webp",
      "contributions": "1. Identifies a key limitation in prior ECG self-supervised learning (eSSL) methods: they overlook inherent heartbeat differences rooted in cardiac conduction and neglect the sequential logic of clinical ECG diagnosis. 2. Proposes a novel two-stage framework (CLEAR-HUG), where the first stage (CLEAR) is an eSSL model that uses a sparse attention mechanism to reconstruct signals by treating each heartbeat as a distinct entity, capturing subtle conduction variations. 3. Introduces a Hierarchical lead-Unified Group head (HUG) for the downstream diagnosis stage, which mirrors the clinical workflow from heartbeats to leads to lead combinations, aligning model patterns with expert guidelines.",
      "summary": "This paper proposes CLEAR-HUG, a two-stage framework for ECG representation learning. The method first uses a self-supervised model (CLEAR) with sparse attention to learn from cardiac conduction variations, then applies a hierarchical diagnosis head (HUG) aligned with clinical guidelines. Experiments across six tasks show a 6.84% performance improvement, validating its effectiveness.",
      "mindmap": "graph TB\n        A[追踪心路：从心脏传导视角的ECG表征学习<br>Tracing the Heart's Pathways: ECG Representation Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有eSSL方法忽视心脏传导导致的细微差异<br>Prior eSSL overlooks conduction-based heartbeat differences]\n        B --> B2[模型未遵循从心跳到导联的临床诊断逻辑<br>Models neglect clinical diagnostic sequence]\n        C --> C1[两阶段框架CLEAR-HUG<br>Two-stage framework CLEAR-HUG]\n        C1 --> C2[阶段一: CLEAR (自监督学习)<br>Stage 1: CLEAR (eSSL)]\n        C2 --> C3[稀疏注意力重构信号<br>Sparse attention for reconstruction]\n        C1 --> C4[阶段二: HUG (分层诊断头)<br>Stage 2: HUG (hierarchical head)]\n        C4 --> C5[模仿临床工作流<br>Mirrors clinical workflow]\n        D --> D1[六项任务性能提升6.84%<br>6.84% improvement across six tasks]\n        D --> D2[验证了方法的有效性<br>Validates effectiveness]"
    },
    {
      "title": "RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations",
      "authors": "Xingqi He, Yujie Zhang, Shuyong Gao, Wenjie Li, Lingyi Hong, Mingxi Chen, Kaixun Jiang, Jiyuan Fu, Wenqiang Zhang",
      "institution": "Fudan University, Shanghai Jiao Tong University School of Medicine",
      "link": "https://arxiv.org/pdf/2512.24023",
      "code": null,
      "tags": [
        "text-guided segmentation",
        "agentic MLLM",
        "multi-turn tool invocation",
        "iterative mask refinement"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/492283683a8b1ec7673cb4aa97997d0e1ab70ed4a074c17cfa6a9a5e87c60998_w640_q70.webp",
      "contributions": "1. Proposes RSAgent, an agentic MLLM that interleaves reasoning and action for segmentation via multi-turn tool invocations, enabling iterative refinement. 2. Builds a data pipeline to synthesize multi-turn reasoning segmentation trajectories for training. 3. Introduces a two-stage training framework combining cold-start supervised fine-tuning with agentic reinforcement learning using fine-grained, task-specific rewards.",
      "summary": "The paper addresses the limitation of one-shot methods in text-guided segmentation, where initial errors cannot be corrected. It proposes RSAgent, an agentic multimodal LLM that iteratively uses a segmentation toolbox, observes feedback, and refines its spatial hypotheses over multiple turns. Experiments show RSAgent achieves state-of-the-art performance on benchmarks like ReasonSeg and RefCOCOg.",
      "mindmap": "graph TB\n        A[RSAgent: Learning to Reason and Act for Text-Guided Segmentation] --> B[核心问题/Problem: One-shot grounding methods lack verification and refinement capabilities]\n        A --> C[主要方法/Method: Agentic MLLM with multi-turn tool invocation for iterative reasoning and mask refinement]\n        A --> D[关键结果/Results: Achieves SOTA on benchmarks (66.5% gIoU on ReasonSeg, 81.5% cIoU on RefCOCOg)]"
    },
    {
      "title": "PipeFlow: Pipelined Processing and Motion-Aware Frame Selection for Long-Form Video Editing",
      "authors": "Mustafa Munir, Md Mostafijur Rahman, Kartikeya Bhardwaj, Paul Whatmough, Radu Marculescu",
      "institution": "The University of Texas at Austin, Qualcomm AI Research",
      "link": "https://arxiv.org/pdf/2512.24026",
      "code": null,
      "tags": [
        "diffusion models",
        "DDIM inversion",
        "motion analysis",
        "pipelined scheduling",
        "frame interpolation",
        "long-form video editing"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3787a383f3a5b9871e80fb8c9aaad731151ef891a01b7640701576d610e23860_w640_q70.webp",
      "contributions": "1. A motion-aware frame selection method using SSIM and Optical Flow to skip editing of low-motion frames. 2. A pipelined task scheduling algorithm that splits videos into segments for parallel DDIM inversion and joint editing based on GPU memory. 3. A neural network-based interpolation technique to smooth border frames and interpolate skipped frames.",
      "summary": "The paper addresses the high computational cost of long-form video editing with diffusion models. It proposes PipeFlow, a method that uses motion analysis to skip frames, parallel pipelined processing, and interpolation to achieve linear scaling with video length. The method achieves significant speedups (up to 31.7x) over prior work while maintaining quality.",
      "mindmap": "graph TB\n        A[PipeFlow: Pipelined Processing and Motion-Aware Frame Selection for Long-Form Video Editing] --> B[核心问题/Problem: Long-form video editing is computationally expensive due to DDIM inversion and joint editing.]\n        A --> C[主要方法/Method: 1. Motion-aware frame skipping. 2. Pipelined parallel processing. 3. Neural interpolation.]\n        A --> D[关键结果/Results: Achieves linear scaling, up to 31.7x speedup over baselines.]"
    },
    {
      "title": "ROAD: Reflective Optimization via Automated Debugging for Zero-Shot Agent Alignment",
      "authors": "Natchaya Temyingyong, Daman Jain, Neeraj Kumarsahu, Prabhat Kumar, Rachata Phondi, Wachiravit Modecrua, Krittanon Kaewtawee, Krittin Pachtrachai, Touchapon Kraisingkorn",
      "institution": "Amity AI Research and Application Center",
      "link": "https://arxiv.org/pdf/2512.24040",
      "code": null,
      "tags": [
        "agent system",
        "prompt optimization",
        "multi-agent architecture",
        "decision tree protocols",
        "zero-shot alignment",
        "automated debugging"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b2d710802d4ef2f4627a2a20e4c9834618953664f1165bf78a33477b890319f_w640_q70.webp",
      "contributions": "1. Introduces ROAD, a novel framework that treats prompt optimization as a dynamic debugging investigation, eliminating the need for curated gold-standard datasets. 2. Proposes a specialized multi-agent architecture (Analyzer, Optimizer, Coach) to convert unstructured failure logs into structured Decision Tree Protocols. 3. Demonstrates high sample efficiency and performance improvements on both academic benchmarks and a live production system, offering a data-efficient alternative to RL-based methods.",
      "summary": "The paper addresses the problem of optimizing LLM agents without requiring large, labeled datasets, which are often unavailable in real-world software engineering. It proposes ROAD, a framework that uses a multi-agent architecture to perform automated debugging on failure logs, converting them into structured protocols for improvement. The results show that ROAD is highly sample-efficient and significantly improves agent performance, providing a practical alternative to resource-intensive training methods.",
      "mindmap": "graph TB\n        A[ROAD: Reflective Optimization via Automated Debugging] --> B[核心问题/Problem: APO methods need large labeled datasets, but real-world has messy logs]\n        A --> C[主要方法/Method: Multi-agent debugging (Analyzer, Optimizer, Coach) to create Decision Tree Protocols]\n        A --> D[关键结果/Results: Sample-efficient, +5.6% success rate, +19% performance on complex tasks]"
    },
    {
      "title": "Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?",
      "authors": "Yuan Xin, Dingfan Chen, Linyi Yang, Michael Backes, Xiao Zhang",
      "institution": "CISPA Helmholtz Center for Information Security, Max Planck Institute for Intelligent Systems, Southern University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.24044",
      "code": null,
      "tags": [
        "adversarial attacks",
        "jailbreaking",
        "content safety filters",
        "LLM safety alignment",
        "input/output filtering"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e40bd85a824936f762c47f0b98464b3b30e7733690440363f10ee46695920a8_w640_q70.webp",
      "contributions": "1. First systematic evaluation of jailbreak attacks across the full LLM inference pipeline including input and output safety filters, 2. Demonstration that nearly all jailbreak techniques can be detected by at least one safety filter, challenging prior overestimations of attack success, 3. Identification of gaps in balancing recall and precision for optimizing protection and user experience in safety systems",
      "summary": "This paper addresses the gap in evaluating jailbreak attacks by systematically testing them against both LLM safety alignment and external content filters in the full deployment pipeline. The study finds that most jailbreaks can be detected by safety filters, suggesting prior success rates were overestimated, and highlights the need for better precision-recall balance in filter design.",
      "mindmap": "graph TB\n    A[Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?] --> B[核心问题/Problem: Jailbreak attacks bypass LLM safety alignment, prior evaluations neglect full deployment pipeline with content filters]\n    A --> C[主要方法/Method: First systematic evaluation of jailbreak attacks across full inference pipeline including input/output filtering stages]\n    A --> D[关键结果/Results: Most jailbreaks detectable by safety filters, prior success overestimated; need better recall-precision balance]"
    },
    {
      "title": "Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models",
      "authors": "Rohit Kumar Salla, Manoj Saravanan, Shrikar Reddy Kota",
      "institution": "Virginia Tech",
      "link": "https://arxiv.org/pdf/2512.24058",
      "code": "https://github.com/rohitsalla/CRS.git",
      "tags": [
        "llm evaluation",
        "reliability",
        "calibration",
        "robustness",
        "uncertainty quantification",
        "composite score"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98ff5e45b3d4957a7de510123e5e0280e7ee39117d9ff73439f058e6965ebfab_w640_q70.webp",
      "contributions": "1. A unified reliability metric (CRS) integrating calibration, robustness, and uncertainty. 2. A large-scale evaluation of ten open-source LLMs on five QA datasets. 3. The demonstration that CRS provides stable model rankings and uncovers hidden failure modes.",
      "summary": "This paper addresses the fragmented evaluation of Large Language Model (LLM) reliability by proposing the Composite Reliability Score (CRS), a unified metric that integrates calibration, robustness, and uncertainty quantification. Through experiments on ten open-source LLMs, the authors show that CRS provides consistent model rankings and reveals trade-offs between reliability dimensions. The main conclusion is that the most dependable LLM systems balance accuracy, robustness, and calibrated uncertainty.",
      "mindmap": "graph TB\n        A[Beyond Hallucinations: A Composite Score for Measuring Reliability] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[LLM可靠性评估碎片化/Fragmented LLM Reliability Evaluation]\n        C --> C1[提出CRS复合分数/Propose Composite Reliability Score (CRS)]\n        D --> D1[CRS提供稳定模型排名/CRS Delivers Stable Model Rankings]\n        D --> D2[揭示隐藏的失败模式/Uncovers Hidden Failure Modes]"
    },
    {
      "title": "Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds",
      "authors": "Aritra Banik, Sujoy Bhore, Palash Dey, Abhishek Sahu",
      "institution": "National Institute of Science Education and Research Bhubaneswar, Indian Institute of Technology Bombay, Indian Institute of Technology Kharagpur",
      "link": "https://arxiv.org/pdf/2512.24037",
      "code": null,
      "tags": [
        "parameterized complexity",
        "kidney exchange",
        "FPT algorithm",
        "W1-hardness",
        "pathwidth",
        "treewidth"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00da3e6d7b1c46c83d5812783ee99ea1cfff1a2c3a708bddcff4c69f9ab7902c_w640_q70.webp",
      "contributions": "1. A new deterministic FPT algorithm for the kidney exchange problem parameterized by the number of patients receiving a kidney, improving the runtime from O*(14^t) to O*((4e)^t) ≈ O*(10.88^t). 2. A proof that the kidney exchange problem is W[1]-hard when parameterized by the pathwidth of the underlying graph, answering a natural question about the parameter's tractability. 3. Additional parameterized intractability results that improve the overall understanding of the problem's complexity landscape.",
      "summary": "This paper studies the computationally hard kidney exchange problem, where patient-donor pairs and altruistic donors exchange kidneys via cycles and paths. The authors present a faster deterministic parameterized algorithm for the standard parameter (number of patients receiving a kidney) and prove that the problem remains intractable (W[1]-hard) even when parameterized by pathwidth, a more restrictive structural parameter than treewidth.",
      "mindmap": "graph TB\n        Root[”Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds<br>肾脏交换：更快的参数化算法与更紧的下界”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Kidney exchange is NP-complete<br>肾脏交换问题是NP完全问题”] --> P1[”限制/Constraint<br>Exchange via small cycles & paths<br>通过小环和路径交换”]\n        Method[”主要方法/Method<br>Parameterized Complexity<br>参数化复杂度”] --> M1[”参数/Parameter<br>Number of patients (t)<br>患者数量(t)”]\n        Method --> M2[”参数/Parameter<br>Graph pathwidth<br>图路径宽度”]\n        Results[”关键结果/Results”] --> R1[”算法改进/Algorithmic Improvement<br>FPT algorithm: O*((4e)^t)<br>FPT算法: O*((4e)^t)”]\n        Results --> R2[”下界/Lower Bound<br>W[1]-hard for pathwidth<br>对路径宽度是W[1]-难的”]"
    },
    {
      "title": "AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives",
      "authors": "Yanxi Chen, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Xin Li, Peijie Qiu, Hao Wang, Xuanzhao Dong, Yujian Xiong, Anderson Schneider, Yuriy Nevmyvaka, Yalin Wang",
      "institution": "Arizona State University, Clemson University, Washington University in St. Louis, Rice University, Morgan Stanley",
      "link": "https://arxiv.org/pdf/2512.24052",
      "code": "https://github.com/LLM-VLM-GSL/AHA",
      "tags": [
        "multi-modal reasoning",
        "audio-language models",
        "hallucination mitigation",
        "counterfactual hard negatives",
        "preference alignment",
        "temporal reasoning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45abc0731e44f649614386415942c67de681cccc206f884d4ce3832844263cdf_w640_q70.webp",
      "contributions": "1. Proposed a taxonomy for audio grounding failures in LALMs, categorizing hallucinations into Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error. 2. Introduced the AHA (Audio Hallucination Alignment) framework, which uses counterfactual hard negative mining to construct a high-quality preference dataset for model alignment. 3. Established AHA-Eval, a diagnostic benchmark to rigorously evaluate fine-grained temporal reasoning capabilities in audio-language models.",
      "summary": "The paper addresses the problem of hallucinations in Large Audio-Language Models (LALMs), where models generate text not grounded in the audio input. To solve this, the authors propose the AHA framework, which uses counterfactual hard negative mining to create a preference dataset for aligning models to distinguish acoustic evidence from fabrications. The resulting aligned model, Qwen-Audio-AHA, shows significant improvements on both the diagnostic AHA-Eval benchmark and public benchmarks, demonstrating effective mitigation of grounding errors.",
      "mindmap": "graph TB\n        Root[AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[Large Audio-Language Models (LALMs) suffer from hallucinations / 大型音频语言模型存在幻觉问题]\n        Method[主要方法/Method] --> M1[Propose AHA framework with counterfactual hard negative mining / 提出AHA框架，使用反事实硬负例挖掘]\n        Method --> M2[Construct preference dataset for alignment / 构建用于对齐的偏好数据集]\n        Results[关键结果/Results] --> R1[13.7% improvement on AHA-Eval benchmark / 在AHA-Eval基准上提升13.7%]\n        Results --> R2[Gains on public benchmarks (MMAU-Test, MMAR) / 在公开基准(MMAU-Test, MMAR)上取得提升]"
    },
    {
      "title": "Pathology Context Recalibration Network for Ocular Disease Recognition",
      "authors": "Zunjie Xiao, Xiaoqing Zhang, Risa Higashita, Jiang Liu",
      "institution": "Southern University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.24066",
      "code": null,
      "tags": [
        "medical image analysis",
        "Pathology Recalibration Module",
        "expert prior Guidance Adapter",
        "Integrated Loss"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2286b84bd9cbd7682cb726e99010af22979d096fd6391215bb5d0212bb1875c5_w640_q70.webp",
      "contributions": "1. Proposed a novel Pathology Recalibration Module (PRM) to leverage pathology context prior via pixel-wise context compression and pathology distribution concentration. 2. Introduced an expert prior Guidance Adapter (EPGA) to highlight significant pixel-wise regions by mining expert experience prior. 3. Designed an Integrated Loss (IL) to boost performance by considering sample-wise loss distributions and training label frequencies.",
      "summary": "This paper proposes PCRNet, a network for ocular disease recognition that incorporates a Pathology Recalibration Module and an expert prior Guidance Adapter to integrate clinical pathology context and expert experience priors into a DNN. An Integrated Loss is also introduced to handle sample and label imbalances. Experiments on three datasets show PCRNet's superiority over state-of-the-art methods, and visualizations explain its decision-making process.",
      "mindmap": "graph TB\n        Root[Pathology Context Recalibration Network for Ocular Disease Recognition] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: DNNs ignore pathology context & expert experience priors for ocular disease recognition] --> P1[问题1/Sub-Problem: Lack of pathology context utilization]\n        Problem --> P2[问题2/Sub-Problem: Lack of expert experience integration]\n        Method[主要方法/Method: PCRNet] --> M1[模块1/Module: Pathology Recalibration Module (PRM)]\n        Method --> M2[模块2/Module: expert prior Guidance Adapter (EPGA)]\n        Method --> M3[组件/Component: Integrated Loss (IL)]\n        Results[关键结果/Results] --> R1[结果1/Result: Superior performance on three datasets]\n        Results --> R2[结果2/Result: Better than SOTA attention networks & loss methods]\n        Results --> R3[结果3/Result: Visualization explains decision-making]"
    },
    {
      "title": "LoongFlow: Directed Evolutionary Search via a Cognitive Plan-Execute-Summarize Paradigm",
      "authors": "Chunhui Wan, Xunan Dai, Zhuo Wang, Minglei Li, Yanpeng Wang, Yinan Mao, Yu Lan, Zhiwen Xiao",
      "institution": "Baidu Inc.",
      "link": "https://arxiv.org/pdf/2512.24077",
      "code": "https://github.com/baidu-baige/LoongFlow",
      "tags": [
        "agent system",
        "evolutionary search",
        "plan-execute-summarize",
        "MAP-Elites",
        "multi-island model",
        "adaptive Boltzmann selection"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cf0c647c1b7ee2e5581b15e60884e9d1c6f9da783e58c0ac3ab7c483db365b6_w640_q70.webp",
      "contributions": "1. Introduces the LoongFlow framework, a self-evolving agent that integrates LLMs into a cognitive \"Plan-Execute-Summarize\" (PES) paradigm to guide evolutionary search with structured reasoning. 2. Proposes a hybrid evolutionary memory system combining Multi-Island models, MAP-Elites, and adaptive Boltzmann selection to balance exploration-exploitation and maintain long-term architectural coherence. 3. Demonstrates state-of-the-art performance, outperforming baselines by up to 60% in evolutionary efficiency on benchmarks like AlphaEvolve and Kaggle competitions.",
      "summary": "The paper introduces LoongFlow, a self-evolving agent framework that uses a cognitive \"Plan-Execute-Summarize\" paradigm and a hybrid memory system to guide evolutionary search with LLMs. This approach addresses issues like premature convergence in traditional methods. Evaluations show LoongFlow achieves superior solution quality with significantly reduced computational cost compared to existing baselines.",
      "mindmap": "graph TB\n        A[LoongFlow: Directed Evolutionary Search<br>LoongFlow: 定向进化搜索] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统进化方法缺乏结构化推理<br>Traditional evolutionary methods lack structured reasoning]\n        B --> B2[过早收敛与低效探索<br>Premature convergence & inefficient exploration]\n        C --> C1[认知范式: 计划-执行-总结<br>Cognitive Paradigm: Plan-Execute-Summarize]\n        C --> C2[混合进化记忆系统<br>Hybrid Evolutionary Memory System]\n        D --> D1[进化效率提升高达60%<br>Evolutionary efficiency improved by up to 60%]\n        D --> D2[发现更优解决方案<br>Discovers superior solutions]"
    },
    {
      "title": "Random Multiplexing",
      "authors": "Lei Liu, Yuhao Chi, Shunqi Huang, Zhaoyang Zhang",
      "institution": "Zhejiang University, Xidian University, Japan Advanced Institute of Science and Technology (JAIST)",
      "link": "https://arxiv.org/pdf/2512.24087",
      "code": null,
      "tags": [
        "wireless communication",
        "random multiplexing",
        "AMP detection",
        "power allocation",
        "replica optimality",
        "constrained capacity"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad2f6d7e30422bda561811ec881c0aa17f79a2f4e7bf13203955c43c9c8fab17_w640_q70.webp",
      "contributions": "1. Proposes a random multiplexing technique decoupled from physical channel structures, enabling application to arbitrary norm-bounded and spectrally convergent channel matrices. 2. Introduces a low-complexity cross-domain memory AMP (CD-MAMP) detector and derives optimal power allocations to minimize BER and maximize constrained capacity. 3. Investigates the optimal coding principle and proves the replica constrained-capacity optimality of the CD-MAMP detector for random multiplexing systems.",
      "summary": "This paper proposes a random multiplexing technique to overcome the limitations of traditional and emerging multiplexing schemes (like OFDM and OTFS) which rely on specific channel structures. The method decouples from the physical channel, uses a random transform to create an input-isotropic equivalent channel, and employs a low-complexity AMP-type detector to achieve near-optimal performance for arbitrary norm-bounded channels. The authors validate the approach with theoretical analysis and numerical results, demonstrating its robustness and versatility in dynamic wireless environments.",
      "mindmap": "graph TB\n        A[Random Multiplexing] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统复用技术依赖特定信道结构/Traditional multiplexing relies on specific channel structures]\n        B --> B2[在动态真实环境中鲁棒性有限/Limited robustness in dynamic real-world environments]\n        C --> C1[随机复用技术/Random Multiplexing Technique]\n        C --> C2[构建输入各向同性等效信道/Construct input-isotropic equivalent channel]\n        C --> C3[CD-MAMP检测器/CD-MAMP Detector]\n        D --> D1[保证渐近最优BER/Guarantees asymptotic optimal BER]\n        D --> D2[推导最优功率分配/Derives optimal power allocation]\n        D --> D3[验证理论结果/Validates theoretical findings]"
    },
    {
      "title": "FedLiTeCAN : A Federated Lightweight Transformer for Fast and Robust CAN Bus Intrusion Detection",
      "authors": "Devika S, Pratik Narang, Tejasvi Alladi",
      "institution": "BITS Pilani, Pilani Campus",
      "link": "https://arxiv.org/pdf/2512.24088",
      "code": "https://github.com/Transformer",
      "tags": [
        "federated learning",
        "Controller Area Network (CAN)",
        "Intrusion Detection System (IDS)",
        "Transformer",
        "Federated Learning",
        "Lightweight Model"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fccdaefce6abd0f6935fde57aedf3ec8956dfd7f60385f88dafe7a2ba8e6ef09_w640_q70.webp",
      "contributions": "1. Proposes FedLiTeCAN, a supervised intrusion detection framework using a two-layer encoder-only transformer in a Federated Learning environment. 2. Achieves a highly lightweight model (0.4MB) and fast real-time inference (0.608 ms per message), significantly outperforming baselines in size and speed. 3. Demonstrates strong generalization capability through cross-dataset analysis, achieving high accuracy on unseen cyber threats.",
      "summary": "This paper proposes FedLiTeCAN, a lightweight Transformer-based Intrusion Detection System for CAN bus security, deployed using Federated Learning. The model is designed to be fast, small, and robust, achieving high accuracy and rapid inference on resource-constrained hardware like Jetson Nano. The results show it is an effective solution for real-time intrusion detection in vehicular networks.",
      "mindmap": "graph TB\n        A[FedLiTeCAN: A Federated Lightweight Transformer for Fast and Robust CAN Bus Intrusion Detection] --> B[核心问题/Problem: CAN协议缺乏内置安全，需要轻量、快速、鲁棒的入侵检测系统]\n        A --> C[主要方法/Method: 在联邦学习环境中使用轻量级两层编码器Transformer]\n        A --> D[关键结果/Results: 模型小(0.4MB)，检测快(0.608ms)，精度高(98.5%)，泛化能力强]"
    },
    {
      "title": "Factorized Learning for Temporally Grounded Video-Language Models",
      "authors": "Wenzheng Zeng, Difei Gao, Mike Zheng Shou, Hwee Tou Ng",
      "institution": "National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.24097",
      "code": "https://github.com/nusnlp/d2vlm",
      "tags": [
        "video-language models",
        "temporal grounding",
        "factorized learning",
        "preference optimization",
        "evidence tokens",
        "video understanding"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7daa6b2b83cd5b8b5e9ed9cbeed8ecfc3d04fe90ac708e60dda996b6def5b97_w640_q70.webp",
      "contributions": "1. Proposes D2VLM, a framework that decouples the learning of temporal grounding and textual response using a \"grounding then answering with evidence referencing\" paradigm and introduces evidence tokens for explicit event-level visual semantic capture. 2. Introduces Factorized Preference Optimization (FPO), a novel algorithm that explicitly incorporates probabilistic temporal grounding modeling into the preference optimization objective for both grounding and response. 3. Constructs a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding.",
      "summary": "This paper addresses the challenge of accurate temporal grounding in video-language models by proposing a factorized learning approach. It introduces the D2VLM framework, which decouples grounding and response generation, and a novel Factorized Preference Optimization (FPO) algorithm for joint optimization. Experiments show the approach achieves clear advantages over existing methods on various tasks.",
      "mindmap": "graph TB\n        A[Factorized Learning for Temporally Grounded Video-Language Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Existing models struggle with accurate temporal grounding for event-level perception. 现有模型在事件级感知的精确时间定位上存在困难。]\n        C[主要方法/Method: Propose D2VLM framework and Factorized Preference Optimization (FPO). 提出D2VLM框架和因子化偏好优化算法。]\n        D[关键结果/Results: Demonstrates clear advantage on various tasks. 在多种任务上展现出明显优势。]"
    },
    {
      "title": "Enhancing LLM Planning Capabilities through Intrinsic Self-Critique",
      "authors": "Bernd Bohnet, Pierre-Alexandre Kamienny, Hanie Sedghi, Dilan Gorur, Pranjal Awasthi, Aaron Parisi, Kevin Swersky, Rosanne Liu, Azade Nova, Noah Fiedel",
      "institution": "Google DeepMind",
      "link": "https://arxiv.org/pdf/2512.24103",
      "code": null,
      "tags": [
        "planning",
        "self-critique",
        "few-shot learning",
        "many-shot learning",
        "iterative refinement",
        "planning benchmarks"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1646fd60ae3c2dbada7b54640bd9b507a010326fc6e75dd48733e4e2612eeefd_w640_q70.webp",
      "contributions": "1. Proposes an intrinsic self-critique method for LLMs to improve their own planning outputs without external verifiers. 2. Demonstrates significant performance gains on established planning benchmarks (Blocksworld, Logistics, Mini-grid) over strong baselines. 3. Shows the method's applicability across different models and datasets, achieving new state-of-the-art results for the considered model class.",
      "summary": "This paper introduces an intrinsic self-critique approach where LLMs iteratively critique and refine their own plans. The method, building upon few-shot and many-shot learning, significantly improves planning performance on benchmarks like Blocksworld without needing external verification. The results set a new state-of-the-art, demonstrating that self-critique can effectively enhance LLM planning capabilities.",
      "mindmap": "graph TB\n        A[Enhancing LLM Planning Capabilities through Intrinsic Self-Critique] --> B[核心问题/Problem: LLM规划能力不足，传统自批判方法效果受质疑/LLM planning capability is limited, effectiveness of self-critique is questioned]\n        A --> C[主要方法/Method: 内在自批判与迭代精炼/Intrinsic Self-Critique and Iterative Refinement]\n        A --> D[关键结果/Results: 在规划基准测试中取得显著性能提升，达到新SOTA/Significant performance gains on planning benchmarks, achieving new SOTA]"
    },
    {
      "title": "Multilevel Fair Allocation",
      "authors": "Maxime Lucet, Nawal Benabbou, Aurélie Beynier, Nicolas Maudet",
      "institution": "LIP6, Sorbonne Université",
      "link": "https://arxiv.org/pdf/2512.24105",
      "code": null,
      "tags": [
        "fair division",
        "multilevel allocation",
        "hierarchical fairness",
        "matroid-rank utilities",
        "Yankee Swap",
        "tree-structured agents"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/931310fb9395b1c90ff2e48d3f63019e189cb72d20da462c04eb32b29d032896_w640_q70.webp",
      "contributions": "1. Introduces the novel concept of multilevel fair allocation for hierarchical agent structures, 2. Proposes a generic polynomial-time sequential algorithm with theoretical fairness/efficiency guarantees, 3. Extends the General Yankee Swap algorithm to the multilevel setting with efficiency guarantees and demonstrated practical fairness.",
      "summary": "This paper introduces the problem of multilevel fair allocation, where resources are distributed among agents organized in a tree hierarchy. It proposes two algorithms: a generic top-down sequential algorithm with theoretical guarantees and an extension of the General Yankee Swap algorithm for the multilevel setting. The work provides both theoretical and practical solutions for achieving fairness and efficiency in hierarchical resource allocation.",
      "mindmap": "graph TB\n        Root[”Multilevel Fair Allocation<br>多级公平分配”] --> Problem[”核心问题/Problem<br>Allocating resources in a tree-structured agent hierarchy<br>在树状代理层次结构中分配资源”]\n        Root --> Method[”主要方法/Method<br>1. Generic sequential top-down algorithm<br>通用顺序自上而下算法<br>2. Extended General Yankee Swap<br>扩展的通用Yankee Swap算法”]\n        Root --> Results[”关键结果/Results<br>Polynomial-time algorithms with fairness/efficiency guarantees<br>具有公平性/效率保证的多项式时间算法”]"
    },
    {
      "title": "Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design",
      "authors": "Chandini Vysyaraju, Raghuvir Duvvuri, Avi Goyal, Dmitry Ignatov, Radu Timofte",
      "institution": "Computer Vision Lab, CAIDAS & IFI, University of Würzburg",
      "link": "https://arxiv.org/pdf/2512.24120",
      "code": null,
      "tags": [
        "llm training",
        "Neural Architecture Search",
        "Few-Shot Prompting",
        "Code Deduplication",
        "Automated Architecture Design",
        "Lightweight Validation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c601e530777a60f08e6c3607d74352f9046689f8debe4c26a2d893b33b09df97_w640_q70.webp",
      "contributions": "1. Few-Shot Architecture Prompting (FSAP), a systematic study determining n=3 examples as optimal for LLM-based architecture generation in vision tasks. 2. Whitespace-Normalized Hash Validation, a lightweight (&lt;1ms) deduplication method providing 100x speedup over AST parsing. 3. A dataset-balanced evaluation methodology for comparing architectures across heterogeneous vision benchmarks.",
      "summary": "This paper addresses challenges in using LLMs for automated neural network architecture design in computer vision. It introduces a systematic few-shot prompting strategy (FSAP) and a fast deduplication method to prevent redundant training. The main conclusion is that using three examples in prompts best balances diversity and focus, and the lightweight validation enables efficient large-scale generation of unique architectures.",
      "mindmap": "graph TB\n        Root(”Enhancing LLM-Based Neural Network Generation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”自动化架构设计的挑战 / Challenges in Automated Architecture Design”)\n        Problem --> P2(”LLM提示与验证策略未系统研究 / LLM Prompting & Validation Not Systematically Studied”)\n        Method --> M1(”少样本架构提示 / Few-Shot Architecture Prompting (FSAP)”)\n        Method --> M2(”空白标准化哈希验证 / Whitespace-Normalized Hash Validation”)\n        Results --> R1(”n=3示例为最优 / n=3 Examples is Optimal”)\n        Results --> R2(”验证速度提升100倍 / 100x Speedup in Validation”)\n        Results --> R3(”生成1900个独特架构 / Generated 1,900 Unique Architectures”)"
    },
    {
      "title": "CogRec: A Cognitive Recommender Agent Fusing Large Language Models and Soar for Explainable Recommendation",
      "authors": "Jiaxin Hu, Tao Wang, Bingsan Yang, Hongrun Wang",
      "institution": "Sun Yat-Sen University",
      "link": "https://arxiv.org/pdf/2512.24113",
      "code": null,
      "tags": [
        "recommender systems",
        "cognitive architecture",
        "Soar",
        "large language models",
        "explainable recommendation",
        "online learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6840c5950eff3f651f3ba24036f583190638dc59f4b4c5d9c32d2ca2079cb860_w640_q70.webp",
      "contributions": "1. Proposes CogRec, a novel cognitive recommender agent that synergizes the strengths of Large Language Models (LLMs) and the Soar cognitive architecture. 2. Introduces a learning paradigm where Soar's symbolic reasoning is initialized and dynamically augmented by an LLM via chunking, enabling robust online learning. 3. Demonstrates that the agent provides highly interpretable rationales and shows advantages in accuracy, explainability, and addressing the long-tail problem on public datasets.",
      "summary": "This paper proposes CogRec, a cognitive recommender agent that fuses Large Language Models (LLMs) with the Soar cognitive architecture to address the black-box nature and limited online learning of LLMs. The agent uses Soar for structured reasoning and dynamically queries an LLM to generate new symbolic rules when needed, enabling continuous knowledge evolution. Evaluations show CogRec improves recommendation accuracy, explainability, and performance on long-tail items.",
      "mindmap": "graph TB\n        A[CogRec: A Cognitive Recommender Agent] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[LLMs: 黑盒, 幻觉, 难在线学习/LLMs: Black-Box, Hallucination, Limited Online Learning]\n        B --> B2[认知架构: 知识获取困难/Cognitive Architectures: Laborious Knowledge Acquisition]\n        C --> C1[融合LLM与Soar/Fuse LLM and Soar]\n        C --> C2[感知-认知-行动循环/PCA Cycle]\n        C --> C3[LLM初始化与动态查询/LLM for Initialization & Dynamic Query]\n        C --> C4[Soar组块化在线学习/Soar Chunking for Online Learning]\n        D --> D1[提升推荐准确性/Improved Recommendation Accuracy]\n        D --> D2[增强可解释性/Enhanced Explainability]\n        D --> D3[有效处理长尾问题/Effective Long-Tail Handling]"
    },
    {
      "title": "Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training",
      "authors": "Yi Liu, Sukai Wang, Dafeng Wei, Xiaowei Cai, Linqing Zhong, Jiange Yang, Guanghui Ren, Jinyu Zhang, Maoqing Yao, Chuankang Li, Xindong He, Liliang Chen, Jianlan Luo",
      "institution": "AgiBot Research, Shanghai Innovation Institute",
      "link": "https://arxiv.org/pdf/2512.24125",
      "code": null,
      "tags": [
        "embodied ai",
        "Embodied Reasoning",
        "Action Tokenization",
        "Vision-Language-Action Models",
        "Flow Matching",
        "Discrete Control"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1a2e4506fbecb3e90de4ef501197f9125c51ac19b6cfc789fdfcb6e035edf72_w640_q70.webp",
      "contributions": "1. Introduces ERIQ, a large-scale benchmark for decoupled evaluation of embodied reasoning in robotic manipulation. 2. Proposes FACT, a flow-matching-based action tokenizer for high-fidelity discretization of continuous control. 3. Presents GenieReasoner, a unified model that jointly optimizes reasoning and action in a discrete space, outperforming baselines.",
      "summary": "This paper addresses the challenge of combining broad generalization with precise execution in general-purpose robotics. It introduces a benchmark (ERIQ) to diagnose reasoning capabilities and a method (FACT) to tokenize actions, leading to a unified model (GenieReasoner) that improves performance on real-world tasks. The work provides a framework to overcome the reasoning-precision trade-off in robotic manipulation.",
      "mindmap": "graph TB\n        Root[”Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>VLA模型难以兼顾泛化与精确执行”] --> P1[”泛化与精度权衡<br>Reasoning-Precision Trade-off”]\n        Method[”主要方法/Method”] --> M1[”评估基准: ERIQ<br>Benchmark: ERIQ”]\n        Method --> M2[”动作分词器: FACT<br>Action Tokenizer: FACT”]\n        Method --> M3[”统一模型: GenieReasoner<br>Unified Model: GenieReasoner”]\n        Results[”关键结果/Results”] --> R1[”揭示了推理能力与泛化的正相关<br>Revealed positive correlation”]\n        Results --> R2[”在真实任务中超越基线<br>Outperformed baselines in real-world tasks”]"
    },
    {
      "title": "OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization",
      "authors": "Advait Gadhikar, Riccardo Grazzi, James Hensman",
      "institution": "CISPA Helmholtz Center for Information Security, Microsoft Research",
      "link": "https://arxiv.org/pdf/2512.24124",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "post-training quantization",
        "weight outliers",
        "rotation",
        "GPTQ",
        "data-free"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/137fb0c1b0206d10c607db973da90e5733c1ed86f7a8360cfe91f146000affae_w640_q70.webp",
      "contributions": "1. Proposes OptRot, a data-free method that learns fusible rotations by minimizing a principled, cheap proxy objective (element-wise fourth power of weights) to reduce weight outliers for quantization. 2. Demonstrates that OptRot outperforms existing rotation methods (Hadamard, SpinQuant, OSTQuant) for weight quantization and improves W4A8 activation quantization. 3. Introduces OptRot+, a data-dependent variant that incorporates activation covariance information for further performance gains, while highlighting a trade-off between weight and activation quantization in the W4A4 setting.",
      "summary": "This paper addresses the challenge of quantizing Large Language Models (LLMs) by mitigating weight outliers. It proposes OptRot, a data-free method that learns efficient rotations to minimize a proxy for weight quantization error, and shows it outperforms existing techniques for weight and W4A8 activation quantization. The work also introduces an enhanced data-dependent variant and reveals a performance trade-off in more aggressive quantization settings.",
      "mindmap": "graph TB\n        A[OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization] --> B[核心问题/Problem: LLM权重和激活中的异常值使量化困难/Outliers in LLM weights & activations make quantization difficult]\n        A --> C[主要方法/Method: 通过最小化旋转后权重的四阶矩学习可融合的旋转/Learn fusible rotations by minimizing element-wise fourth power of rotated weights (OptRot)]\n        A --> D[关键结果/Results: OptRot在权重量化上优于现有方法，改进W4A8激活量化，W4A4下存在权衡/OptRot outperforms existing methods for weight quant., improves W4A8 activation quant., trade-off in W4A4 setting]"
    },
    {
      "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
      "authors": "Haoran He, Yuxiao Ye, Jie Liu, Jiajun Liang, Zhiyong Wang, Ziyang Yuan, Xintao Wang, Hangyu Mao, Pengfei Wan, Ling Pan",
      "institution": "Hong Kong University of Science and Technology, Kuaishou Technology, CUHK MMLab, The University of Edinburgh",
      "link": "https://arxiv.org/pdf/2512.24138",
      "code": "https://tinnerhrhe.github.io/gardo_project",
      "tags": [
        "reinforcement learning",
        "reward hacking",
        "diffusion models",
        "regularization",
        "mode collapse",
        "online RL"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df8973aa0f222e89b818973c0c7ef576738632b0095b29ac1f837f1a83f47f9b_w640_q70.webp",
      "contributions": "1. Proposed GARDO, a framework with gated regularization that selectively penalizes high-uncertainty samples to mitigate reward hacking efficiently., 2. Introduced an adaptive regularization mechanism that periodically updates the reference model to align with the online policy, enabling effective exploration., 3. Designed a diversity-aware reward amplification strategy to encourage mode coverage and prevent diversity collapse during RL fine-tuning.",
      "summary": "This paper addresses the problem of reward hacking in RL-fine-tuned diffusion models, where optimizing imperfect proxy rewards degrades real image quality and diversity. The authors propose GARDO, a framework featuring gated, adaptive regularization and diversity-aware optimization to prevent overfitting, maintain exploration, and enhance diversity. Experiments show GARDO effectively mitigates reward hacking and improves generation diversity without sacrificing sample efficiency.",
      "mindmap": "graph TB\n        A[GARDO: Reinforcing Diffusion Models without Reward Hacking] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Reward Hacking in RL for Diffusion Models/扩散模型RL中的奖励破解]\n        B --> B2[Proxy Reward Mismatch & Mode Collapse/代理奖励不匹配与模式崩溃]\n        C --> C1[Gated & Adaptive Regularization/门控自适应正则化]\n        C --> C2[Diversity-aware Reward Optimization/多样性感知奖励优化]\n        D --> D1[Mitigates Reward Hacking/缓解奖励破解]\n        D --> D2[Enhances Diversity & Maintains Efficiency/提升多样性并保持效率]"
    },
    {
      "title": "Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks",
      "authors": "Evgenii Rudakov, Jonathan Shock, Benjamin Ultan Cowley",
      "institution": "University of Helsinki, University of Cape Town",
      "link": "https://arxiv.org/pdf/2512.24156",
      "code": "https://github.com/dolphin-in-a-coma/arc-agi-3-just-explore",
      "tags": [
        "interactive reasoning",
        "graph-based exploration",
        "state-space exploration",
        "visual salience",
        "training-free",
        "ARC-AGI-3"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c5463388c7bf347baae8d407681f498a8378e19ba7ae57ea056451d79518546_w640_q70.webp",
      "contributions": "1. Proposes a training-free, graph-based method for systematic state-space exploration in interactive reasoning tasks. 2. Introduces a strategy that segments visual frames and prioritizes actions based on visual salience and shortest paths to untested state-action pairs. 3. Demonstrates the method's strong performance on the ARC-AGI-3 benchmark, significantly outperforming state-of-the-art LLM-based agents and establishing a strong non-learning baseline.",
      "summary": "This paper presents a training-free, graph-based exploration method for solving interactive reasoning tasks in the ARC-AGI-3 benchmark. The method uses visual frame segmentation and maintains a graph of states to prioritize exploration, solving a median of 30 out of 52 levels and outperforming leading LLMs. The results show that explicit, structured exploration is a powerful baseline for tasks where current LLMs fail.",
      "mindmap": "graph TB\n        A[Graph-Based Exploration for ARC-AGI-3 Interactive Reasoning Tasks] --> B[核心问题/Problem: LLMs fail at interactive reasoning in sparse-feedback environments like ARC-AGI-3]\n        A --> C[主要方法/Method: Training-free graph-based exploration with visual segmentation and action prioritization]\n        A --> D[关键结果/Results: Solves median 30/52 levels, ranks 3rd, outperforms frontier LLM agents]"
    },
    {
      "title": "Developing controlled natural language for formal specification patterns using AI assistants",
      "authors": "Natalia Garanina, Vladimir Zyubin, Igor Anureev",
      "institution": "Institute of Automation and Electrometry, Siberian Branch of the Russian Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.24159",
      "code": null,
      "tags": [
        "requirements engineering",
        "controlled natural language",
        "formal specification patterns",
        "AI assistant",
        "temporal requirements",
        "syntax formalization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16dd04888cb9a22a839acc0700d35470498034dd8676cff5e34d6cb903fd6ab9_w640_q70.webp",
      "contributions": "1. A novel three-stage method for systematically constructing a Controlled Natural Language (CNL) for requirements using an AI assistant. 2. A prompt engineering approach that leverages a generalized template and formal semantics to generate a diverse corpus of natural language patterns. 3. Formalization of CNL syntax based on grammatical analysis of AI-generated patterns, specifically validated for event-driven temporal requirements.",
      "summary": "This paper proposes a method to systematically develop a controlled natural language (CNL) for formal requirements specification using an AI assistant. The method involves creating a generalized pattern, using an AI to generate a corpus of natural language variants, and then formalizing the CNL syntax from the results. The approach was successfully tested for specifying event-driven temporal requirements, producing a language with built-in formal semantics.",
      "mindmap": "graph TB\n        A[Developing controlled natural language for formal specification patterns using AI assistants] --> B(核心问题/Problem: How to systematically construct a Controlled Natural Language (CNL) for formal requirements specification?);\n        A --> C(主要方法/Method: Three-stage method using AI assistant: 1. Compile generalized pattern, 2. Generate corpus via prompt, 3. Formalize syntax from grammar analysis.);\n        A --> D(关键结果/Results: Method successfully tested for event-driven temporal requirements, yielding a CNL with formal semantics by design.);"
    },
    {
      "title": "PointRAFT: 3D deep learning for high-throughput prediction of potato tuber weight from partial point clouds",
      "authors": "Pieter M. Blok, Haozhou Wang, Hyun Kwon Suh, Peicheng Wang, James Burridge, Wei Guo",
      "institution": "The University of Tokyo, Sejong University",
      "link": "https://arxiv.org/pdf/2512.24193",
      "code": "https://github.com/pieterblok/pointraft.git",
      "tags": [
        "3D point cloud regression",
        "PointRAFT",
        "partial point clouds",
        "object height embedding",
        "PointNet++",
        "RGB-D"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c71915dfc1ce9e8203a646c9242d84788697d59124a23181fa869b682fd4cdf_w640_q70.webp",
      "contributions": "1. Proposed PointRAFT, a high-throughput point cloud regression network for directly predicting continuous 3D shape properties from partial point clouds. 2. Introduced a novel object height embedding as an architectural component to incorporate tuber height, improving regression performance under occlusion. 3. Demonstrated superior performance and real-time capability on a large-scale agricultural dataset, achieving high accuracy for potato tuber weight prediction.",
      "summary": "The paper addresses the problem of systematically underestimating potato tuber weight from incomplete 3D point clouds captured on harvesters. It proposes PointRAFT, a deep learning network that directly regresses weight from partial point clouds using a novel object height embedding. The method significantly outperforms baselines and achieves real-time processing speeds suitable for commercial harvesters.",
      "mindmap": "graph TB\n        Root[”PointRAFT: 3D deep learning for high-throughput prediction of potato tuber weight from partial point clouds”] --> Problem[”核心问题/Problem: Incomplete point clouds from RGB-D lead to weight underestimation”]\n        Root --> Method[”主要方法/Method: PointRAFT network with object height embedding for direct regression”]\n        Root --> Results[”关键结果/Results: Low error (MAE 12.0g), high speed (150 tubers/sec)”]"
    },
    {
      "title": "SCP: Accelerating Discovery with a Global Web of Autonomous Scientific Agents",
      "authors": "Yankai Jiang, Wenjie Lou, Lilong Wang, Zhenyu Tang, Shiyang Feng, Jiaxuan Lu, Haoran Sun, Yaning Pan, Shuang Gu, Haoyang Su, Feng Liu, Wangxu Wei, Pan Tan, Dongzhan Zhou, Fenghua Ling, Cheng Tan, Bo Zhang, Xiaosong Wang, Lei Bai, Bowen Zhou",
      "institution": "Shanghai Artificial Intelligence Laboratory",
      "link": "https://arxiv.org/pdf/2512.24189",
      "code": "https://github.com/InternScience/scp",
      "tags": [
        "agent system",
        "Science Context Protocol",
        "autonomous scientific agents",
        "unified resource integration",
        "experiment lifecycle management",
        "federated servers"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8885d3eeb276d5044f777dc33201481a07be8831aaff9d61726e4a6c13e821be_w640_q70.webp",
      "contributions": "1. Proposes SCP, an open-source protocol-level standard for universally describing and invoking heterogeneous scientific resources (tools, models, datasets, instruments)., 2. Introduces a secure service architecture (centralized Hub & federated Servers) for managing the complete, traceable experiment lifecycle and enforcing fine-grained access control., 3. Demonstrates a functional platform built on SCP, integrating over 1,600 tool resources to facilitate secure, large-scale, multi-institution collaboration between AI agents and human researchers, reducing integration overhead.",
      "summary": "The paper introduces the Science Context Protocol (SCP), an open-source standard designed to address the fragmentation and bespoke nature of current autonomous scientific agent systems. SCP provides a universal specification for resource integration and a secure service architecture for experiment orchestration, enabling seamless, large-scale collaboration across platforms. The authors conclude that SCP establishes essential infrastructure for scalable, reproducible, and agent-driven science by standardizing context and tool orchestration at the protocol level.",
      "mindmap": "graph TB\n        Root[SCP: Accelerating Discovery with a Global Web of Autonomous Scientific Agents] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Bespoke, isolated agent systems; Lack of shared protocol for heterogeneous resources”] --> Problem_Detail[”具体挑战/Specific Challenges<br>Difficult to deploy beyond single lab; Hard to reuse components & reproduce workflows”]\n        Method[”主要方法/Method<br>Science Context Protocol (SCP)”] --> Method_Pillar1[”支柱1: 统一资源集成/Unified Resource Integration<br>Universal spec for describing/invoking tools, models, data, instruments”]\n        Method --> Method_Pillar2[”支柱2: 实验生命周期管理/Experiment Lifecycle Management<br>Secure architecture (Hub & Servers) for registration, execution, monitoring”]\n        Results[”关键结果/Results<br>Enables global web of autonomous agents”] --> Results_Outcome1[”成果1: 大规模生态系统/Large-scale Ecosystem<br>1,600+ integrated tool resources”]\n        Results --> Results_Outcome2[”成果2: 促进协作/Facilitates Collaboration<br>Reduces integration overhead; Enhances reproducibility”]"
    },
    {
      "title": "Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem",
      "authors": "Pengfu Wan, Jiawei Chen, Gangyan Xu",
      "institution": "The Hong Kong Polytechnic University",
      "link": "https://arxiv.org/pdf/2512.24251",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Fleet Size and Mix Vehicle Routing Problem (FSMVRP)",
        "deep reinforcement learning (DRL)",
        "Markov Decision Process (MDP)",
        "fleet-and-route integrated policy network (FRIPN)",
        "remaining graph embedding"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3969891b48accce35280355d106951820196973739958b782a307a2a3df23aa3_w640_q70.webp",
      "contributions": "1. Formulates the Fleet Size and Mix Vehicle Routing Problem (FSMVRP) as a Markov Decision Process (MDP) for a deep reinforcement learning approach. 2. Proposes a novel policy network (FRIPN) that integrates fleet composition and routing decisions into a single model. 3. Introduces specialized input embeddings, including a remaining graph embedding, to enhance decision-making for vehicle employment.",
      "summary": "This paper proposes a deep reinforcement learning method to solve the complex Fleet Size and Mix Vehicle Routing Problem (FSMVRP). The core innovation is a policy network called FRIPN that jointly decides on fleet composition and routing. Experiments show the method is computationally efficient and scalable, producing near-optimal solutions quickly, especially for large-scale problems.",
      "mindmap": "graph TB\n        A[Deep Reinforcement Learning for Solving the Fleet Size and Mix Vehicle Routing Problem] --> B(核心问题/Problem: FSMVRP - simultaneous fleet composition & routing)\n        A --> C(主要方法/Method: DRL-based MDP formulation with FRIPN policy network & remaining graph embedding)\n        A --> D(关键结果/Results: Near-optimal solutions in seconds, high computational efficiency & scalability)"
    },
    {
      "title": "Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment",
      "authors": "Lijun Zhang, Lin Li, Wei Wei, Yajie Qi, Huizhong Song, Jun Wang, Yaodong Yang, Jiye Liang",
      "institution": "Shanxi University, University College London, Peking University",
      "link": "https://arxiv.org/pdf/2512.24263",
      "code": null,
      "tags": [
        "safety alignment",
        "risk-aware optimization",
        "constrained policy optimization",
        "nested risk measures",
        "stepwise alignment",
        "tail risk suppression"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4111a1181bf60ea84ea5a2f52c3748bc1804913ef18afecfc6e9f7bd9f15f5e_w640_q70.webp",
      "contributions": "1. Proposes Risk-aware Stepwise Alignment (RSA), a novel method that explicitly incorporates risk awareness into LLM policy optimization using nested risk measures. 2. Formulates safety alignment as a token-level risk-aware constrained policy optimization problem and solves it via a stepwise procedure for token-level policy updates. 3. Provides theoretical analysis on policy optimality and demonstrates experimentally that the method ensures strong safety and suppresses low-probability, high-impact harmful behaviors while maintaining helpfulness.",
      "summary": "The paper addresses the limitation of risk-neutral safety alignment methods for large language models, which struggle with risks from policy deviation and rare catastrophic outputs. It proposes Risk-aware Stepwise Alignment (RSA), a method that uses nested risk measures for token-level constrained policy optimization. Experimental results show RSA achieves high helpfulness while ensuring strong safety and significantly suppressing tail risks.",
      "mindmap": "graph TB\n        A[Constrained Language Model Policy Optimization via Risk-aware Stepwise Alignment] --> B[核心问题/Problem: Risk-neutral alignment insufficient for policy deviation & rare catastrophic harms]\n        A --> C[主要方法/Method: Risk-aware Stepwise Alignment (RSA) using nested risk measures for token-level policy optimization]\n        A --> D[关键结果/Results: High helpfulness, strong safety, significant tail risk suppression]"
    },
    {
      "title": "One-shot synthesis of rare gastrointestinal lesions improves diagnostic accuracy and clinical training",
      "authors": "Jia Yu, Yan Zhu, Peiyao Fu, Tianyi Chen, Zhihua Wang, Fei Wu, Quanlin Li, Pinghong Zhou, Shuo Wang, Xian Yang",
      "institution": "Zhejiang University, Fudan University, Imperial College London, The University of Manchester",
      "link": "https://arxiv.org/pdf/2512.24278",
      "code": null,
      "tags": [
        "medical image synthesis",
        "one-shot synthesis",
        "language-guided concept disentanglement",
        "data augmentation",
        "rare disease",
        "generative framework"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf643227c51e8cd7c3969dfe80d336673878c555397635a5c9831a5997ed5bd_w640_q70.webp",
      "contributions": "1. Proposed EndoRare, a one-shot, retraining-free generative framework for synthesizing diverse, high-fidelity images of rare gastrointestinal lesions from a single reference image., 2. Introduced a language-guided concept disentanglement method to separate pathognomonic lesion features from non-diagnostic attributes, ensuring diversity while preserving diagnostic fidelity., 3. Demonstrated that synthetic images improve both AI classifier performance (true positive rate) and novice clinician diagnostic accuracy (recall and precision) for rare pathologies.",
      "summary": "This paper addresses the data scarcity problem for rare gastrointestinal lesions in AI development and clinical training. It proposes EndoRare, a one-shot generative framework that uses language-guided concept disentanglement to synthesize diverse and clinically plausible lesion images from a single example. The results show that these synthetic images significantly enhance the performance of AI classifiers and improve the diagnostic accuracy of novice endoscopists.",
      "mindmap": "graph TB\n        A[One-shot synthesis of rare gastrointestinal lesions] --> B[核心问题/Problem: Rare lesions are infrequent, limiting AI model data and clinician training.]\n        A --> C[主要方法/Method: EndoRare framework uses one-shot, language-guided concept disentanglement to generate diverse, high-fidelity synthetic images.]\n        A --> D[关键结果/Results: Improves AI classifier true positive rate and novice clinician recall & precision.]"
    },
    {
      "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
      "authors": "Zhe Huang, Hao Wen, Aiming Hao, Bingze Song, Meiqi Wu, Jiahong Wu, Xiangxiang Chu, Sheng Lu, Haoqian Wang",
      "institution": "Tsinghua University, Beihang University, AMAP (Alibaba Group)",
      "link": "https://arxiv.org/pdf/2512.24271",
      "code": "https://amap-ml.github.io/Taming-Hallucinations/",
      "tags": [
        "multi-modal training",
        "counterfactual video generation",
        "visual hallucination",
        "diffusion-based video editing",
        "advantage normalization",
        "contrastive training"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa10c3c7d7f412fdbab92df4afa93ab6b3653346b89fc3443bf25d8615391b81_w640_q70.webp",
      "contributions": "1. Introduces DualityForge, a framework for automatically synthesizing counterfactual video QA data using controllable diffusion-based video editing. 2. Presents DualityVidQA, a large-scale video dataset built using DualityForge to mitigate MLLM hallucinations. 3. Proposes DNA-Train, a two-stage SFT-RL training regime with pair-wise advantage normalization for stable and efficient policy optimization on contrastive data.",
      "summary": "This paper addresses the problem of visual hallucinations in Multimodal Large Language Models (MLLMs) when processing counterfactual videos. The proposed solution, DualityForge, synthesizes counterfactual video QA data for training, and a novel training method, DNA-Train, leverages this data to improve grounding. Experiments show the method significantly reduces hallucinations and improves performance on both hallucination and general benchmarks.",
      "mindmap": "graph TB\n        A[”Taming Hallucinations: Boosting MLLMs' Video Understanding<br>驯服幻觉：通过反事实视频生成提升MLLM视频理解”] --> B[”核心问题/Problem<br>MLLMs over-rely on language priors, causing visual hallucinations on counterfactual videos.”]\n        A --> C[”主要方法/Method<br>DualityForge: Counterfactual video & QA synthesis.<br>DNA-Train: Contrastive SFT-RL training.”]\n        A --> D[”关键结果/Results<br>24.0% hallucination reduction.<br>Strong generalization on benchmarks.”]"
    },
    {
      "title": "DRL-TH: Jointly Utilizing Temporal Graph Attention and Hierarchical Fusion for UGV Navigation in Crowded Environments",
      "authors": "Ruitong Li, Lin Zhang, Yuenan Zhao, Chengxin Liu, Ran Song, Wei Zhang",
      "institution": "The affiliations are not explicitly provided in the given content. Based on the author names, it is not possible to reliably infer the main research institution(s).",
      "link": "https://arxiv.org/pdf/2512.24284",
      "code": null,
      "tags": [
        "reinforcement learning",
        "temporal graph attention",
        "hierarchical graph pooling",
        "multi-modal fusion",
        "UGV navigation",
        "deep reinforcement learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83abe1de24f9a7e490d93db45dec754f2a2ff7f3de84d6e6983a358e1d9dff40_w640_q70.webp",
      "contributions": "1. Proposed a DRL-based navigation framework (DRL-TH) that integrates historical observations and adaptively fuses multi-modal information. 2. Introduced a Temporal-Guided Graph Attention Network (TG-GAT) to capture temporal context and scene evolution between consecutive frames. 3. Designed a Graph Hierarchical Abstraction Module (GHAM) to dynamically and balance multi-scale representations from RGB and LiDAR features.",
      "summary": "This paper proposes DRL-TH, a deep reinforcement learning framework for UGV navigation in crowded environments. It addresses limitations of single-frame observation and simple fusion by introducing a temporal graph attention network and a hierarchical graph pooling module for adaptive multi-modal feature integration. Experiments and real-world deployment show that DRL-TH outperforms existing methods.",
      "mindmap": "graph TB\n        Root[DRL-TH: UGV导航框架] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[单帧观测/Single-frame observation]\n        Problem --> P2[简单多模态融合/Simple multi-modal fusion]\n        P1 --> P1_Sub[限制动态适应性/Limits dynamic adaptability]\n        P2 --> P2_Sub[难以捕捉时序上下文/Hard to capture temporal context]\n        Method[主要方法/Method] --> M1[时序引导图注意力网络/Temporal-Guided GAT (TG-GAT)]\n        Method --> M2[图层次抽象模块/Graph Hierarchical Abstraction Module (GHAM)]\n        M1 --> M1_Sub[捕捉连续帧关联/Captures correlations between consecutive frames]\n        M2 --> M2_Sub[动态融合RGB与LiDAR特征/Dynamically fuses RGB & LiDAR features]\n        Results[关键结果/Results] --> R1[性能超越现有方法/Outperforms existing methods]\n        Results --> R2[真实UGV上表现良好/Performs well on real UGV]"
    },
    {
      "title": "Virtual-Eyes: Quantitative Validation of a Lung CT Quality-Control Pipeline for Foundation-Model Cancer Risk Prediction",
      "authors": "Md. Enamul Hoq, Linda Larson-Prior, Fred Prior",
      "institution": "University of Arkansas for Medical Sciences",
      "link": "https://arxiv.org/pdf/2512.24294",
      "code": null,
      "tags": [
        "medical image analysis",
        "CT preprocessing",
        "quality control",
        "foundation models",
        "lung cancer screening",
        "validation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6f230cd5094d46243a5c6fe260cf946e5540833e1a82b86423bbe80a292b76c_w640_q70.webp",
      "contributions": "1. Development and validation of Virtual-Eyes, a novel, anatomically targeted 16-bit CT quality-control pipeline for lung cancer screening. 2. Quantitative demonstration that such preprocessing significantly improves the performance and calibration of generalist foundation models (e.g., RAD-DINO) for cancer risk prediction. 3. Discovery that specialist models (e.g., Sybil) can degrade with the same preprocessing, revealing their potential reliance on contextual shortcuts in raw clinical data.",
      "summary": "This paper develops Virtual-Eyes, a quality-control pipeline for lung CT scans that standardizes resolution and extracts lung regions. The study finds that this preprocessing significantly boosts the cancer risk prediction performance of generalist foundation models but can harm specialist models that have adapted to raw, unprocessed clinical data.",
      "mindmap": "graph TB\n        Root[Virtual-Eyes: 肺癌CT质量控制流程验证] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>LDCT预处理影响未量化] --> P1[缺乏量化/Lack of Quantification]\n        Problem --> P2[通用vs专用模型差异/Generalist vs. Specialist]\n        Method[主要方法/Method<br>Virtual-Eyes Pipeline] --> M1[质量控制/Quality Control<br>512x512, 过滤系列]\n        Method --> M2[肺块提取/Lung Block Extraction<br>HU过滤, 覆盖评分]\n        Method --> M3[模型评估/Model Evaluation<br>RAD-DINO, Sybil等]\n        Results[关键结果/Results<br>预处理效果不同] --> R1[提升通用模型/Improves Generalist FMs<br>RAD-DINO AUC↑]\n        Results --> R2[损害专用模型/Harms Specialist Models<br>Sybil AUC↓]\n        Results --> R3[揭示捷径学习/Reveals Shortcut Learning<br>上下文依赖]"
    },
    {
      "title": "Empower Low-Altitude Economy: A Reliability-Aware Dynamic Weighting Allocation for Multi-modal UAV Beam Prediction",
      "authors": "Haojin Li, Anbang Zhang, Chen Sun, Chenyuan Feng, Kaiqian Qu, Tony Q. S. Quek, Haijun Zhang",
      "institution": "University of Science and Technology Beijing, Sony China Research Laboratory, Shandong University, Southeast University, University of Exeter, Singapore University of Technology and Design",
      "link": "https://arxiv.org/pdf/2512.24324",
      "code": null,
      "tags": [
        "multi-modal inference",
        "reliability-aware dynamic weighting",
        "cross-modal contrastive learning",
        "semantic-aware beam prediction",
        "low-altitude UAV",
        "multi-modal learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fa219c5db4bfd0eeac8ee93679e8eba9ceb6e2e5ce7336d45afc6686f1d8162_w640_q70.webp",
      "contributions": "1. Proposes a reliability-aware dynamic weighting scheme that adaptively allocates contributions across different modalities (e.g., visual, posture, geospatial) based on their instantaneous reliability, moving beyond fixed-weight approaches. 2. Introduces a semantic-aware multi-modal beam prediction framework (SaM²B) that uses cross-modal contrastive learning to align multi-source representations into a shared semantic space, enhancing robustness to noise and distribution shifts. 3. Validates the proposed SaM²B framework on real-world low-altitude UAV datasets, demonstrating superior performance over baseline methods.",
      "summary": "This paper addresses the problem of unreliable beam prediction in multi-modal UAV communications caused by static weighting and modal misalignment. It proposes SaM²B, a framework that uses reliability-aware dynamic weighting and cross-modal contrastive learning to adaptively fuse modalities and align their semantics. Experiments on real-world datasets show SaM²B outperforms existing baseline methods.",
      "mindmap": "graph TB\n        A[Empower Low-Altitude Economy: A Reliability-Aware Dynamic Weighting Allocation for Multi-modal UAV Beam Prediction] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[静态权重与模态失配/Static Weighting & Modal Mismatch]\n        B --> B2[跨场景泛化弱/Weak Cross-Scenario Generalization]\n        C --> C1[可靠性感知动态加权/Reliability-Aware Dynamic Weighting]\n        C --> C2[跨模态对比学习/Cross-Modal Contrastive Learning]\n        C --> C3[语义感知多模态框架/Semantic-Aware Multi-Modal Framework (SaM²B)]\n        D --> D1[真实数据集验证/Validated on Real-World UAV Datasets]\n        D --> D2[优于基线方法/Superior to Baseline Methods]"
    },
    {
      "title": "DermaVQA-DAS: Dermatology Assessment Schema (DAS) & Datasets for Closed-Ended Question Answering & Segmentation in Patient-Generated Dermatology Images",
      "authors": "Wen-wai Yim, Yujuan Fu, Asma Ben Abacha, Meliha Yetisgen, Noel Codella, Roberto Andres Novoa, Josep Malvehy",
      "institution": "Microsoft, University of Washington, Stanford University, Hospital Clinic of Barcelona",
      "link": "https://arxiv.org/pdf/2512.24340",
      "code": "https://osf.io/72rp3",
      "tags": [
        "medical image analysis",
        "visual question answering",
        "lesion segmentation",
        "multimodal models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dabc426dbf988c067106f76b5dca274abd76ad3a46bcddbd05c76b76893e508a_w640_q70.webp",
      "contributions": "1. Introduction of the Dermatology Assessment Schema (DAS), a novel expert-developed framework for structured dermatological feature assessment. 2. Release of DermaVQA-DAS, an extended dataset supporting closed-ended question answering and lesion segmentation on patient-generated images. 3. Comprehensive benchmarking of state-of-the-art multimodal models on the new tasks, analyzing the impact of prompt design on segmentation performance.",
      "summary": "This paper addresses the lack of patient-centered benchmarks in dermatology by introducing DermaVQA-DAS, a dataset extension built upon a novel expert-developed assessment schema (DAS) for structured feature annotation. It supports two tasks—closed-ended visual question answering and lesion segmentation—on patient-generated images and queries. The study benchmarks modern multimodal models, finding strong QA performance and demonstrating that prompt design significantly impacts segmentation results.",
      "mindmap": "graph TB\n        Root[DermaVQA-DAS] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[核心问题/Problem] --> P1[现有数据集缺乏患者视角/Existing datasets lack patient perspective]\n        P1 --> P2[限制以患者为中心的护理应用/Limits patient-centered care applications]\n    \n        Method[主要方法/Method] --> M1[提出皮肤病评估框架(DAS)/Propose Dermatology Assessment Schema (DAS)]\n        M1 --> M2[扩展DermaVQA数据集/Extend DermaVQA dataset]\n        M2 --> M3[支持两项任务:封闭式问答与分割/Support two tasks: closed QA & segmentation]\n    \n        Results[关键结果/Results] --> R1[提示设计影响分割性能/Prompt design impacts segmentation performance]\n        R1 --> R2[模型在QA上表现强劲/Models perform strongly on QA]\n        R2 --> R3[公开数据集与评估协议/Publicly release dataset & evaluation protocols]"
    },
    {
      "title": "FedSecureFormer: A Fast, Federated and Secure Transformer Framework for Lightweight Intrusion Detection in Connected and Autonomous Vehicles",
      "authors": "Devika S, Vishnu Hari, Pratik Narang, Tejasvi Alladi, F. Richard Yu",
      "institution": "BITS Pilani, Pilani Campus; Carleton University",
      "link": "https://arxiv.org/pdf/2512.24345",
      "code": null,
      "tags": [
        "federated learning",
        "Transformer",
        "Federated Learning",
        "Differential Privacy",
        "Intrusion Detection",
        "Connected and Autonomous Vehicles"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7914a9d504b1c6d9fb36172de5c5c3554e336d7411316a13768fea0423324240_w640_q70.webp",
      "contributions": "1. Proposed FedSecureFormer, a lightweight encoder-only Transformer model with only 1.7M parameters for efficient intrusion detection in CAVs. 2. Integrated the model within a Federated Learning framework with Differential Privacy to enhance data privacy and enable collaborative training. 3. Demonstrated high performance (93.69% accuracy on 19 attacks) and fast inference (3.7775 ms on Jetson Nano), making it 100x faster than SOTA models.",
      "summary": "This paper addresses cybersecurity threats in Connected and Autonomous Vehicles (CAVs) by proposing FedSecureFormer, a lightweight Transformer model trained using Federated Learning and Differential Privacy. The model achieves high accuracy for intrusion detection while ensuring data privacy and demonstrates extremely fast inference on edge devices, making it suitable for real-world deployment.",
      "mindmap": "graph TB\n        Root[FedSecureFormer: A Fast, Federated and Secure Transformer Framework] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: CAV网络安全威胁 / Cybersecurity threats in CAVs]\n        Method[主要方法/Method: 轻量Transformer + 联邦学习 + 差分隐私 / Lightweight Transformer + FL + DP]\n        Results[关键结果/Results: 高精度 & 快速推理 / High Accuracy & Fast Inference]"
    },
    {
      "title": "Skim-Aware Contrastive Learning for Efficient Document Representation",
      "authors": "Waheed Ahmed Abro, Zied Bouraoui",
      "institution": "Univ Artois, CNRS",
      "link": "https://arxiv.org/pdf/2512.24373",
      "code": null,
      "tags": [
        "document representation",
        "contrastive learning",
        "natural language inference",
        "long document",
        "self-supervised learning",
        "hierarchical transformer"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae38f3776c3982e71d1fd2fda48c20229eb63f1918544341ca35ab3cb48a02af_w640_q70.webp",
      "contributions": "1. A novel self-supervised contrastive learning framework inspired by human skimming behavior for long document representation. 2. A method that uses random section masking and an NLI-based contrastive objective to align relevant parts and distance unrelated ones. 3. Demonstrated significant improvements in both accuracy and efficiency on legal and biomedical document tasks.",
      "summary": "The paper addresses the challenge of efficiently representing long documents like legal and medical texts. It proposes a self-supervised contrastive learning method that mimics human skimming by masking sections and using an NLI objective to relate document parts. Experiments show the method achieves better accuracy and computational efficiency compared to existing approaches.",
      "mindmap": "graph TB\n        A[Skim-Aware Contrastive Learning for Efficient Document Representation] --> B[核心问题/Problem: 长文档表示困难/Inefficient long document representation]\n        A --> C[主要方法/Method: 基于NLI的对比学习/NLI-based contrastive learning with section masking]\n        A --> D[关键结果/Results: 在准确性和效率上取得显著提升/Significant gains in accuracy and efficiency]"
    },
    {
      "title": "Tubular Riemannian Laplace Approximations for Bayesian Neural Networks",
      "authors": "Rodrigo Pereira David",
      "institution": "National Institute of Metrology, Technology and Quality (Inmetro)",
      "link": "https://arxiv.org/pdf/2512.24381",
      "code": null,
      "tags": [
        "bayesian deep learning",
        "Laplace approximation",
        "Riemannian geometry",
        "uncertainty quantification",
        "Bayesian neural networks",
        "model calibration"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbef449cd138ae804891e277de76405797dfc3e78511bcb03bf13e56823c9746_w640_q70.webp",
      "contributions": "1. Introduces the Tubular Riemannian Laplace (TRL) approximation, a novel method that models the posterior as a probabilistic tube following low-loss valleys induced by functional symmetries. 2. Proposes using a Fisher/Gauss-Newton metric to separate prior-dominated tangential uncertainty from data-dominated transverse uncertainty, adapting to the anisotropic, curved loss surfaces of deep models. 3. Demonstrates empirically that TRL achieves calibration comparable to Deep Ensembles on ResNet-18 (CIFAR-10/100) at a fraction (1/5) of the training cost, bridging single-model efficiency with ensemble-grade reliability.",
      "summary": "This paper addresses the poor calibration of traditional Euclidean Laplace approximations in Bayesian Neural Networks. It proposes the Tubular Riemannian Laplace (TRL) approximation, which models the posterior as a tube using a Riemannian metric to better capture parameter space geometry. The method achieves excellent uncertainty calibration on image classification tasks, matching Deep Ensembles' reliability with significantly lower computational cost.",
      "mindmap": "graph TB\n        A[Tubular Riemannian Laplace Approximations<br>管状黎曼拉普拉斯近似] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[传统拉普拉斯近似在深度模型中校准不佳<br>Traditional Laplace approximations struggle with calibration in deep models]\n        C --> C1[提出管状黎曼拉普拉斯(TRL)近似<br>Propose Tubular Riemannian Laplace (TRL) approximation]\n        C1 --> C2[使用Fisher/Gauss-Newton度量建模概率管<br>Model probabilistic tube using Fisher/Gauss-Newton metric]\n        D --> D1[在ResNet-18上实现优秀校准<br>Achieves excellent calibration on ResNet-18]\n        D1 --> D2[匹配集成方法可靠性，成本仅1/5<br>Matches ensemble reliability at 1/5 training cost]"
    },
    {
      "title": "Fast and Realistic Automated Scenario Simulations and Reporting for an Autonomous Racing Stack",
      "authors": "Giovanni Lambertini, Matteo Pini, Eugenio Mascaro, Francesco Moretti, Ayoub Raji, Marko Bertogna",
      "institution": "University of Modena and Reggio Emilia",
      "link": "https://arxiv.org/pdf/2512.24402",
      "code": null,
      "tags": [
        "fault-tolerance",
        "Functional Mockup Unit (FMU)",
        "fault injection",
        "Continuous Integration/Continuous Delivery (CI/CD)"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d172287ab1ffde29e52c3f7cde1a986f7a5d73ac371395320ced490d39f5dac_w640_q70.webp",
      "contributions": "1. An automated simulation and reporting pipeline for an autonomous racing stack that can execute up to three times faster than real-time, locally or on GitHub for CI/CD. 2. A fault injection module capable of introducing sensor delays, perturbations, and modifying outputs of any node in the software stack. 3. A design for an automated reporting process aimed at maximizing the effectiveness of simulation analysis.",
      "summary": "This paper presents an automated simulation and reporting pipeline for validating an autonomous racing stack. The method uses a high-fidelity vehicle model as a Functional Mockup Unit (FMU) and includes a fault injection module to test system robustness. The pipeline enables fast, realistic scenario testing and automated reporting, which is crucial for efficiently validating critical autonomous driving functions like high-speed overtaking.",
      "mindmap": "graph TB\n        Root[”Fast and Realistic Automated Scenario Simulations and Reporting for an Autonomous Racing Stack”] --> Problem[”核心问题/Problem: Need for efficient validation of autonomous racing stack modules, especially for high-speed maneuvers and localization.”]\n        Root --> Method[”主要方法/Method: Automated simulation pipeline using high-fidelity FMU model, scenario initialization, and fault injection.”]\n        Root --> Results[”关键结果/Results: Pipeline executes up to 3x faster than real-time, supports CI/CD, and includes automated reporting.”]"
    },
    {
      "title": "FAST-IDS: A Fast Two-Stage Intrusion Detection System with Hybrid Compression for Real-Time Threat Detection in Connected and Autonomous Vehicles",
      "authors": "Devika S, Vishnu Hari, Pratik Narang, Tejasvi Alladi, Vinay Chamola",
      "institution": "BITS Pilani, Pilani Campus",
      "link": "https://arxiv.org/pdf/2512.24391",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "hybrid model compression",
        "two-stage IDS",
        "BiGAN",
        "CNN-LSTM",
        "real-time inference"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e22580ef1d7646dbfb5cbb4038c5aaef82d4f9a515521bd0405cb40b44178ee2_w640_q70.webp",
      "contributions": "1. A novel two-stage Intrusion Detection System (IDS) architecture combining a coarse-grained BiGAN-CNN for anomaly detection and a fine-grained CNN-LSTM for attack classification. 2. The application of hybrid model compression (structural pruning and static quantization) to achieve a 77.2% model size reduction while maintaining performance. 3. Demonstrated real-time, efficient deployment on resource-constrained edge devices (e.g., Jetson Nano) with low per-vehicle inference latency.",
      "summary": "This paper proposes FAST-IDS, a fast two-stage intrusion detection system for Connected and Autonomous Vehicles (CAVs). It uses a hybrid model compression technique to create a lightweight system that combines anomaly detection (BiGAN-CNN) and attack classification (CNN-LSTM) for efficient real-time threat detection. The compressed model achieves significant size reduction and faster inference, making it suitable for deployment on edge devices like the Jetson Nano.",
      "mindmap": "graph TB\n        A[FAST-IDS: 面向CAV的快速入侵检测系统] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[CAV网络安全威胁 / CAV Cybersecurity Threats]\n        B --> B2[资源受限环境部署 / Deployment in Resource-Constrained Environments]\n        C --> C1[两阶段IDS / Two-Stage IDS]\n        C1 --> C1_1[阶段1: BiGAN-CNN异常检测 / Stage 1: BiGAN-CNN Anomaly Detection]\n        C1 --> C1_2[阶段2: CNN-LSTM攻击分类 / Stage 2: CNN-LSTM Attack Classification]\n        C --> C2[混合模型压缩 / Hybrid Model Compression]\n        C2 --> C2_1[结构化剪枝 / Structural Pruning]\n        C2 --> C2_2[静态量化 / Static Quantization]\n        D --> D1[模型大小减少77.2% / 77.2% Model Size Reduction]\n        D --> D2[推理时间减少50.05% / 50.05% Inference Time Reduction]\n        D --> D3[高检测准确率 / High Detection Accuracy]"
    },
    {
      "title": "Comparing Approaches to Automatic Summarization in Less-Resourced Languages",
      "authors": "Chester Palen-Michel, Constantine Lignos",
      "institution": "Brandeis University",
      "link": "https://arxiv.org/pdf/2512.24410",
      "code": null,
      "tags": [
        "text summarization",
        "less-resourced languages",
        "multilingual transfer",
        "data augmentation",
        "LLM prompting",
        "mT5"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f14f0d3397d12fd7c93ad9814a09fa7bd5c030b47c51091b59d65cc5392446ba_w640_q70.webp",
      "contributions": "1. A comprehensive comparative study of multiple summarization approaches for less-resourced languages, including zero-shot LLMs, fine-tuned mT5, and a translation pipeline. 2. Exploration and evaluation of three data augmentation methods using Wikipedia to generate synthetic training data for low-resource settings. 3. An analysis showing that a fine-tuned multilingual mT5 baseline often outperforms zero-shot LLMs and that LLM-as-judge evaluation may be unreliable for less-resourced languages.",
      "summary": "This paper compares various methods for automatic text summarization in less-resourced languages, including prompting large language models (LLMs), fine-tuning multilingual models like mT5 with data augmentation, and a translation pipeline. The evaluation across multiple metrics finds that a fine-tuned multilingual mT5 model generally outperforms zero-shot LLMs, and highlights potential issues with using LLMs as evaluators for these languages.",
      "mindmap": "graph TB\n        A[Comparing Approaches to Automatic Summarization in Less-Resourced Languages] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[”高资源语言性能好，低资源语言研究不足 / High performance in high-resourced languages, less attention to less-resourced languages”]\n        C --> C1[”比较多种方法 / Compare various approaches”]\n        C1 --> C1_1[”零样本提示LLM / Zero-shot prompting LLMs”]\n        C1 --> C1_2[”微调mT5（含数据增强） / Fine-tuning mT5 (with data augmentation)”]\n        C1 --> C1_3[”翻译-总结-翻译流程 / Translate-summarize-translate pipeline”]\n        D --> D1[”微调mT5优于大多数方法 / Fine-tuned mT5 outperforms most approaches”]\n        D --> D2[”LLM作为评估者可能不可靠 / LLM as judge may be less reliable”]"
    },
    {
      "title": "PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression",
      "authors": "Bo Jiang, Taolue Yang, Youyuan Liu, Xubin He, Sheng Di, Sian Jin",
      "institution": "Temple University, Argonne National Laboratory",
      "link": "https://arxiv.org/pdf/2512.24449",
      "code": "https://github.com/BoJiang03/PackKV",
      "tags": [
        "llm inference",
        "KV cache",
        "lossy compression",
        "memory footprint",
        "GPU kernels",
        "attention mechanism"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42e07bd3aaf7626174ef50b03ee71ac8de443f8fb5560e374d8293b4df52b84f_w640_q70.webp",
      "contributions": "1. Proposes PackKV, a generic KV cache management framework featuring novel lossy compression techniques specifically tailored to KV cache data characteristics. 2. Presents a careful co-design of compression algorithms and system architecture that is compatible with the dynamically growing KV cache while preserving high computational efficiency. 3. Achieves significantly higher memory reduction rates and execution throughput compared to state-of-the-art methods, effectively eliminating decompression overhead and accelerating matrix-vector multiplication.",
      "summary": "This paper addresses the high memory footprint of the KV cache during long-context LLM inference by proposing PackKV, a framework that uses LLM-aware lossy compression. PackKV co-designs compression algorithms and system architecture to reduce memory usage while maintaining computational efficiency. The results show that PackKV achieves superior memory reduction and higher throughput compared to existing quantization methods.",
      "mindmap": "graph TB\n        A[PackKV: Reducing KV Cache Memory Footprint] --> B[核心问题/Problem: KV缓存内存占用大/Large KV Cache Memory Footprint]\n        A --> C[主要方法/Method: LLM感知的有损压缩/LLM-Aware Lossy Compression]\n        A --> D[关键结果/Results: 更高内存减少与吞吐量/Higher Memory Reduction & Throughput]"
    },
    {
      "title": "Privacy-Preserving Semantic Communications via Multi-Task Learning and Adversarial Perturbations",
      "authors": "Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus",
      "institution": "Nexcepta, The Ohio State University, University of Maryland",
      "link": "https://arxiv.org/pdf/2512.24452",
      "code": null,
      "tags": [
        "semantic communications",
        "min-max optimization",
        "adversarial perturbations",
        "multi-task learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25e9c394c6a473f54b07d7337b43fb693f3ebef1de80e7b275ea3c91df03de11_w640_q70.webp",
      "contributions": "1. A deep learning-based semantic communication framework that jointly supports multiple receiver tasks (e.g., inference and reconstruction) while explicitly limiting semantic information leakage to an eavesdropper. 2. Formulation of the privacy problem as an iterative min-max optimization, where the legitimate transmitter-receiver pair is trained to degrade an adaptive eavesdropper's semantic inference performance. 3. Introduction of an auxiliary adversarial perturbation layer that superimposes a crafted signal on the transmitted waveform to degrade eavesdropper performance, even when the legitimate link is not co-trained against it.",
      "summary": "This paper addresses privacy leakage in semantic communications, where task-optimized representations can be exploited by eavesdroppers. The proposed method uses a min-max adversarial training framework and an auxiliary perturbation layer to protect semantic information. Evaluations on image datasets show the approach significantly reduces eavesdropper inference accuracy without harming legitimate receiver performance.",
      "mindmap": "graph TB\n        Root[Privacy-Preserving Semantic Communications via Multi-Task Learning and Adversarial Perturbations] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Semantic representations leak sensitive information to eavesdroppers] --> Problem_Detail[语义泄露/Semantic Leakage]\n        Method[主要方法/Method: Deep learning framework with min-max optimization and adversarial perturbations] --> Method_Detail1[对抗训练/Min-Max Optimization]\n        Method --> Method_Detail2[扰动层/Perturbation Layer]\n        Results[关键结果/Results: Reduces eavesdropper accuracy, maintains legitimate performance] --> Results_Detail[有效隐私保护/Effective Privacy Preservation]"
    },
    {
      "title": "Align While Search: Belief-Guided Exploratory Inference for World-Grounded Embodied Agents",
      "authors": "Seohui Bae, Jeonghye Kim, Youngchul Sung, Woohyung Lim",
      "institution": "LG AI Research, KAIST",
      "link": "https://arxiv.org/pdf/2512.24461",
      "code": null,
      "tags": [
        "embodied ai",
        "partial observability",
        "belief refinement",
        "information gain",
        "exploratory inference",
        "test-time adaptation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/234ff05b49010da656582805168140c0ed145a8c4a3d1ca5e4147e19f57db21f_w640_q70.webp",
      "contributions": "1. A test-time adaptive agent architecture that performs exploratory inference through posterior-guided belief refinement without gradient updates or extra training. 2. A method to estimate information gain for action selection using a lightweight LLM-based surrogate. 3. A novel reward metric to assess world alignment by quantifying consistency between posterior belief and ground-truth environment configuration.",
      "summary": "This paper proposes a belief-guided agent for embodied AI that operates under partial observability. The agent refines its structured belief about the world at test time and selects actions to maximize predicted information gain, using a lightweight LLM to estimate it. Experiments show this method outperforms inference-time scaling baselines in aligning with latent world states while using significantly fewer tokens.",
      "mindmap": "graph TB\n        A[Align While Search: Belief-Guided Exploratory Inference<br/>对齐搜索：信念引导的探索性推理] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>LLM agents struggle with adaptive reasoning in partially observable environments.<br/>LLM智能体在部分可观测环境中难以进行自适应推理。]\n        C[主要方法/Method<br/>Test-time agent with structured belief, updated via observations, selects actions to maximize predicted information gain.<br/>具有结构化信念的测试时智能体，通过观察更新信念，选择行动以最大化预测信息增益。]\n        D[关键结果/Results<br/>Outperforms inference-time baselines (e.g., prompt-augmented LLMs) with lower token usage.<br/>以更低的令牌使用量优于推理时基线（如提示增强的LLM）。]"
    },
    {
      "title": "HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors",
      "authors": "Hyunjun Kim",
      "institution": "Korea Advanced Institute of Science and Technology (KAIST), École Polytechnique Fédérale de Lausanne (EPFL)",
      "link": "https://arxiv.org/pdf/2512.24478",
      "code": "https://github.com/hyunjun1121/holograph",
      "tags": [
        "causal discovery",
        "sheaf theory",
        "large language models",
        "natural gradient descent",
        "algebraic latent projection",
        "presheaf"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68eaf3f0002711b7e42d725e2a453cb23c5db4c9ca0b3a10f0290cfce9779f2e_w640_q70.webp",
      "contributions": "1. A sheaf-theoretic framework formalizing LLM-guided causal discovery as a presheaf satisfaction problem. 2. A natural gradient descent algorithm on the belief manifold for principled optimization. 3. The introduction of Algebraic Latent Projection to handle hidden confounders.",
      "summary": "The paper introduces HOLOGRAPH, a framework that uses sheaf theory to formally integrate Large Language Model priors for causal discovery, addressing issues of coherence and hidden confounders. It proposes novel methods like Algebraic Latent Projection and natural gradient optimization. The approach provides a rigorous mathematical foundation and shows competitive performance, while analysis reveals a failure of the Locality axiom in larger graphs.",
      "mindmap": "graph TB\n        A[HOLOGRAPH] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[因果发现受可识别性限制/Causal discovery limited by identifiability]\n        B --> B2[现有LLM方法缺乏理论基础/Existing LLM methods lack theory]\n        C --> C1[层理论框架/Sheaf-theoretic framework]\n        C --> C2[代数潜在投影/Algebraic Latent Projection]\n        C --> C3[自然梯度下降/Natural Gradient Descent]\n        D --> D1[提供数学基础/Provides mathematical foundation]\n        D --> D2[性能有竞争力/Achieves competitive performance]\n        D --> D3[局部性公理失效/Locality axiom fails for large graphs]"
    },
    {
      "title": "F2IDiff: Real-world Image Super-resolution using Feature to Image Diffusion Foundation Model",
      "authors": "Devendra K. Jangid, Ripon K. Saha, Dilshan Godaliyadda, Jing Li, Seok-Jun Lee, Hamid R. Sheikh",
      "institution": "MPI Lab, Samsung Research America",
      "link": "https://arxiv.org/pdf/2512.24473",
      "code": null,
      "tags": [
        "image super-resolution",
        "diffusion models",
        "DINOv2",
        "feature conditioning",
        "hallucination control",
        "real-world images"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccded9f21c9c6de980a50bfd5c308052e1cce34fc21b16a1a1b0cbd3a8c8c57d_w640_q70.webp",
      "contributions": "1. Proposes F2IDiff, a new Feature-to-Image Diffusion Foundation Model for SISR that uses lower-level DINOv2 features for conditioning instead of text. 2. Demonstrates that this approach provides stricter and richer conditioning for small patches, enabling controlled generation and higher fidelity, especially for high-fidelity smartphone LR images. 3. Shows that the model can be trained effectively with a much smaller dataset (38K images) and a smaller U-Net than large text-to-image models like SD2.1, while achieving superior performance.",
      "summary": "This paper addresses the problem of undesirable hallucinations in generative super-resolution for high-fidelity smartphone images. It proposes F2IDiff, a diffusion foundation model conditioned on DINOv2 features instead of text, which allows for stricter control and richer description of image patches. The method achieves better fidelity and performance than text-conditioned models while requiring significantly less data and a smaller network.",
      "mindmap": "graph TB\n        A[F2IDiff: Real-world Image Super-resolution] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[文本特征对细节描述不足<br/>Text features lack detail]\n        B --> B2[智能手机高分辨率LR图像需要无幻觉生成<br/>Smartphone LR needs hallucination-free generation]\n        C --> C1[使用DINOv2特征进行条件控制<br/>Use DINOv2 features for conditioning]\n        C --> C2[构建特征到图像扩散基础模型(F2IDiff)<br/>Build Feature-to-Image Diffusion FM (F2IDiff)]\n        D --> D1[比基于文本的模型保真度更高<br/>Higher fidelity than text-based models]\n        D --> D2[使用更小的数据集和网络实现<br/>Achieved with smaller dataset & network]"
    },
    {
      "title": "Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models",
      "authors": "Kim Alexander Christensen, Andreas Gudahl Tufte, Alexey Gusev, Rohan Sinha, Milan Ganai, Ole Andreas Alsos, Marco Pavoned, Martin Steinert",
      "institution": "NTNU (Norwegian University of Science and Technology), Stanford University, NVIDIA Research",
      "link": "https://arxiv.org/pdf/2512.24470",
      "code": null,
      "tags": [
        "multi-modal inference",
        "vision-language model (VLM)",
        "fallback maneuver",
        "semantic hazard detection",
        "autonomous surface vessel (ASV)",
        "IMO MASS Code"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd093268c75343899a9f72b23f4d22948c1b634539b5e39f8d26296c9f122886_w640_q70.webp",
      "contributions": "1. Proposes Semantic Lookout, a camera-only, candidate-constrained VLM fallback maneuver selector for maritime autonomy. 2. Introduces a fast-slow anomaly pipeline with a short-horizon, human-overridable fallback to meet IMO MASS Code requirements. 3. Demonstrates that sub-10-second VLM models retain semantic awareness and outperform geometry-only baselines in hazard scenarios.",
      "summary": "This paper addresses the challenge of semantic hazard detection for autonomous maritime vessels, which is required by the draft IMO MASS Code. It proposes Semantic Lookout, a vision-language model (VLM) based system that selects safe fallback maneuvers by understanding scene semantics. The results show that this approach is effective within practical latency budgets and outperforms traditional geometry-only methods.",
      "mindmap": "graph TB\n        Root[”Foundation models on the bridge / 论文标题”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”自主船舶需检测语义危害 / Autonomous vessels need semantic hazard detection”]\n        Problem --> P2[”传统系统难以处理语义异常 / Classical stacks struggle with semantic OOD situations”]\n        Method --> M1[”引入Semantic Lookout / Introduce Semantic Lookout”]\n        Method --> M2[”基于VLM的备用机动选择器 / VLM-based fallback maneuver selector”]\n        Method --> M3[”快速-慢速异常处理流程 / Fast-slow anomaly pipeline”]\n        Results --> R1[”10秒内模型保持语义感知 / Sub-10 s models retain semantic awareness”]\n        Results --> R2[”优于几何基线 / Outperforms geometry-only baselines”]\n        Results --> R3[”支持IMO MASS法规 / Supports draft IMO MASS Code”]"
    },
    {
      "title": "What Drives Success in Physical Planning with Joint-Embedding Predictive World Models?",
      "authors": "Basile Terver, Tsung-Yen Yang, Jean Ponce, Adrien Bardes, Yann LeCun",
      "institution": "Meta FAIR, INRIA Paris, Ecole normale supérieure/PSL, New York University",
      "link": "https://arxiv.org/pdf/2512.24497",
      "code": "https://github.com/facebookresearch/jepa-wms",
      "tags": [
        "world models",
        "joint-embedding predictive architecture",
        "representation space planning",
        "model-based reinforcement learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3be3154d136e165197c022f619cd1d45cd62098d7f202059d7110d6335e67c44_w640_q70.webp",
      "contributions": "1. Proposes a comprehensive characterization and study of Joint-Embedding Predictive Architecture World Models (JEPA-WMs) for physical planning. 2. Systematically investigates the impact of model architecture, training objective, and planning algorithm on planning success in simulated and real-world robotic tasks. 3. Combines the findings to propose a new model that outperforms established baselines (DINO-WM and V-JEPA-2-AC) in navigation and manipulation tasks.",
      "summary": "This paper investigates the key factors for successful physical planning using Joint-Embedding Predictive World Models (JEPA-WMs). It conducts a systematic study of architectural and algorithmic choices within this family of methods and proposes a new model that achieves superior performance on navigation and manipulation tasks compared to existing baselines.",
      "mindmap": "graph TB\n        Root[What Drives Success in Physical Planning with JEPA-WMs?] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>How to build agents that generalize to new physical tasks?]\n        Method[主要方法/Method<br>Study JEPA-WMs: architecture, objective, planning algorithm]\n        Results[关键结果/Results<br>Proposed model outperforms baselines (DINO-WM, V-JEPA-2-AC)]"
    },
    {
      "title": "Can Small Training Runs Reliably Guide Data Curation? Rethinking Proxy-Model Practice",
      "authors": "Jiachen T. Wang, Tong Wu, Kaifeng Lyu, James Zou, Dawn Song, Ruoxi Jia, Prateek Mittal",
      "institution": "Princeton University, Tsinghua University, Stanford University, UC Berkeley, Virginia Tech",
      "link": "https://arxiv.org/pdf/2512.24503",
      "code": null,
      "tags": [
        "llm training",
        "proxy models",
        "data curation",
        "hyperparameter tuning",
        "learning rate",
        "pretraining"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0253b877f5ec90fd3f09b9d1a8f7d0967a88407fbbd25eb1e8a8bfcdf23081c4_w640_q70.webp",
      "contributions": "1. Identifies a critical flaw in the standard proxy-model evaluation protocol, showing that using a fixed training configuration for all data recipes leads to unreliable conclusions that can flip with minor hyperparameter changes. 2. Proposes a simple and effective patch to the protocol: training proxy models with reduced learning rates, which preserves the relative performance ranking of data recipes and correlates strongly with fully-tuned large-scale training. 3. Provides theoretical justification for the proposed method by proving it preserves dataset ordering for random-feature models, and validates it empirically across 23 data recipes.",
      "summary": "The paper identifies that the standard practice of using small proxy models with identical hyperparameters to evaluate data recipes is unreliable because optimal training configurations are data-dependent. To fix this, the authors propose training proxy models with reduced learning rates, a simple change that makes small-scale experiment rankings strongly correlate with those from fully-tuned large-scale LLM pretraining. This method is theoretically justified and empirically validated, dramatically improving the reliability of data curation guidance from small training runs.",
      "mindmap": "graph TB\n        Root[Can Small Training Runs Reliably Guide Data Curation? Rethinking Proxy-Model Practice] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[固定配置评估不可靠/Fixed-config evaluation unreliable]\n        Problem --> P2[结论随超参翻转/Conclusions flip with hyperparams]\n        Method[主要方法/Method] --> M1[降低学习率训练代理模型/Train proxy models with reduced LR]\n        Method --> M2[数据特定调优目标/Data-specific tuning objective]\n        Results[关键结果/Results] --> R1[与大规模训练强相关/Strong correlation with large-scale training]\n        Results --> R2[理论证明与实证验证/Theoretical proof & empirical validation]"
    },
    {
      "title": "Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments",
      "authors": "Zhiwei Wei, Yuxing Liu, Hua Liao, Wenjia Xu",
      "institution": "Hunan Normal University, Beijing University of Posts and Telecommunications",
      "link": "https://arxiv.org/pdf/2512.24504",
      "code": null,
      "tags": [
        "spatial reasoning",
        "foundation model agents",
        "interactive evaluation",
        "spatial memory",
        "graph-based representation",
        "path planning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f22bb0bbc4529875e6217599c85097728b0d0b467695dfc3508153e6cab59ae2_w640_q70.webp",
      "contributions": "1. Proposes an interactive evaluation framework to assess how foundation model agents explore, remember, and reason in partially observable symbolic map environments., 2. Systematically analyzes the distinct functional roles of exploration strategies, memory representations, and reasoning schemes on spatial task performance., 3. Reveals that spatial reasoning performance saturates with model scale, indicating the need for tailored spatial representation mechanisms beyond scaling.",
      "summary": "This paper proposes an interactive framework to evaluate how foundation model agents understand maps by exploring, remembering, and reasoning in partially observable grid environments. The study finds that structured memory representations, like graphs, are crucial for complex tasks like path planning, and that performance improvements require specialized spatial mechanisms, not just scaling model size.",
      "mindmap": "graph TB\n        A[Thinking on Maps: How Foundation Model Agents Explore, Remember, and Reason Map Environments] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(现有评估忽视交互性/Existing evaluations overlook interactive, experience-driven spatial understanding)\n        C --> C1(提出交互式评估框架/Propose interactive evaluation framework)\n        C1 --> C2(智能体增量探索部分可观测地图/Agents incrementally explore partially observable maps)\n        C2 --> C3(评估六类空间任务/Evaluate six kinds of spatial tasks)\n        D --> D1(结构化记忆提升性能/Structured memory (e.g., graph) substantially improves performance)\n        D --> D2(探索影响经验获取/Exploration affects experience acquisition)\n        D --> D3(性能随规模饱和/Reasoning performance saturates with model scale)"
    },
    {
      "title": "Evaluating the Reasoning Abilities of LLMs on Underrepresented Mathematics Competition Problems",
      "authors": "Samuel Golladay, Majid Bani-Yaghoub",
      "institution": "University of Missouri: Kansas City",
      "link": "https://arxiv.org/pdf/2512.24505",
      "code": null,
      "tags": [
        "mathematical reasoning",
        "large language models",
        "mathematical reasoning",
        "benchmark evaluation",
        "error analysis",
        "underrepresented datasets"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0233f7d7f1efe2775a15812915de42cb0ed0d928ad7ea3ea8e06fe4ee277303_w640_q70.webp",
      "contributions": "1. Evaluated LLMs on underrepresented mathematics competition problems (Missouri Collegiate Mathematics Competition) to address dataset contamination and generalizability issues. 2. Conducted a detailed analysis of reasoning quality and error patterns across three LLMs (GPT-4o-mini, Gemini-2.0-Flash, DeepSeek-V3) beyond just final answer accuracy. 3. Identified distinct error profiles for each model and highlighted Geometry as a persistent challenge for LLMs' structured reasoning.",
      "summary": "This study evaluates the reasoning abilities of three LLMs on underrepresented mathematics competition problems. The results show DeepSeek-V3 performed best, and all models struggled with Geometry, revealing distinct error patterns. The work concludes that using such datasets provides deeper insights into LLMs' reasoning limitations.",
      "mindmap": "graph TB\n        A[Evaluating LLMs on Underrepresented Math Problems<br/>评估LLM在代表性不足数学问题上的表现] --> B\n        A --> C\n        A --> D\n        B[Problem: Limited generalizability of LLM math benchmarks<br/>问题: LLM数学基准测试泛化性不足]\n        C[Method: Test LLMs on Missouri Collegiate Math Competition problems<br/>方法: 在密苏里大学数学竞赛问题上测试LLMs]\n        D[Results: DeepSeek-V3 best; Geometry is a weak point; distinct error patterns<br/>结果: DeepSeek-V3最优; 几何是弱点; 不同的错误模式]"
    },
    {
      "title": "From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning",
      "authors": "Amir Tahmasbi, Sadegh Majidi, Kazem Taram, Aniket Bera",
      "institution": "Purdue University",
      "link": "https://arxiv.org/pdf/2512.24532",
      "code": null,
      "tags": [
        "reinforcement learning",
        "spatial reasoning",
        "LoRA",
        "GRPO",
        "supervised fine-tuning",
        "reinforcement learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19427bf120d4b89a11879461b589b7313caa689fdec51f99f516c6485aef495_w640_q70.webp",
      "contributions": "1. A two-stage approach for multi-step spatial reasoning that first fine-tunes an LLM on atomic spatial transformations and then trains lightweight LoRA adapters via RL to compose these blocks for planning. 2. The creation of a synthetic ASCII-art dataset and a corresponding ASCII-based RL environment to support training and evaluation. 3. Demonstration that the proposed method outperforms baselines in both dynamic and static environments, with faster convergence and more stable training than end-to-end RL.",
      "summary": "This paper addresses the challenge of multi-step spatial reasoning in LLMs by proposing a two-stage method: first, supervised fine-tuning on basic spatial transformations to build physics awareness, and then training LoRA adapters with reinforcement learning (GRPO) to learn planning policies. The approach is evaluated using a custom ASCII-art environment and is shown to outperform various baselines, converging faster and more stably than training from scratch with RL.",
      "mindmap": "graph TB\n        A[From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>LLMs struggle with spatial transformations and multi-step planning]\n        C[主要方法/Method<br>Two-stage: SFT on spatial blocks, then RL (GRPO) with LoRA for planning]\n        D[关键结果/Results<br>Outperforms baselines, faster convergence, stable training]"
    },
    {
      "title": "More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization",
      "authors": "Yuma Ichikawa, Yoshihiko Fujisawa, Yudai Fujimoto, Akira Sakai, Katsuki Fujisawa",
      "institution": "Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University",
      "link": "https://arxiv.org/pdf/2512.24545",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "extreme quantization",
        "double binary factorization",
        "low-bit LLM",
        "post-training quantization",
        "binary matrix multiplication"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e17ec329eb54b789cd97cf9a3fc6db67786908aca3eb86af65de80d0797eb12_w640_q70.webp",
      "contributions": "1. Proposed Multi-Envelope Double Binary Factorization (MDBF), which replaces the single magnitude envelope in DBF with a rank-l envelope to enhance magnitude expressiveness while maintaining a shared binary sign carrier. 2. Introduced a closed-form initialization and an alternating refinement method to effectively optimize the MDBF parameters. 3. Demonstrated that MDBF improves perplexity and zero-shot accuracy over prior binary formats on LLaMA and Qwen models at matched bit budgets while preserving the same efficient inference primitive.",
      "summary": "The paper addresses the performance saturation of Double Binary Factorization (DBF) in extreme low-bit quantization of LLMs, where a single magnitude envelope limits expressiveness. It proposes Multi-Envelope DBF (MDBF), which uses multiple envelope components to allocate more expressivity to magnitudes while keeping binary sign matrices shared. Experiments on LLaMA and Qwen families show MDBF outperforms previous binary formats in accuracy and perplexity at the same bit rate without changing the inference primitive.",
      "mindmap": "graph TB\n        Root[”More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>DBF scaling too restrictive,<br>single envelope causes<br>performance saturation”]\n        Method[”主要方法/Method<br>Propose MDBF: shared 1-bit sign bases,<br>replace single envelope with<br>rank-l envelope”]\n        Results[”关键结果/Results<br>Better perplexity & accuracy<br>over previous binary formats,<br>same inference primitive”]"
    },
    {
      "title": "Localized Calibrated Uncertainty in Code Language Models",
      "authors": "David Gros, Prem Devanbu",
      "institution": "University of California, Davis",
      "link": "https://arxiv.org/pdf/2512.24560",
      "code": null,
      "tags": [
        "code generation",
        "calibrated uncertainty",
        "minimal intent aligning patches",
        "white-box probing",
        "Brier Skill Score",
        "AI oversight"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e04860e78fa5e50071d58627500e751c212bb40e58f4d01943c9ad0eda5c5a9_w640_q70.webp",
      "contributions": "1. Creation of a dataset of \"Minimal Intent Aligning Patches\" for LLM-generated code repairs. 2. Proposal and evaluation of techniques (white-box probing, black-box reflective, self-consistency) for assigning well-calibrated, localized uncertainty to code segments. 3. Demonstration that a small supervisor probe can effectively estimate edit likelihood on code from much larger models and shows preliminary signs of generalization to natural language errors.",
      "summary": "This paper addresses the problem of LLM-generated code potentially deviating from user intent by proposing methods to localize where edits are likely needed. It introduces a dataset of minimal repair patches and compares techniques for assigning calibrated probabilities to code lines. The key finding is that a small white-box probing model can effectively estimate which lines will be edited, achieving good calibration and a Brier Skill Score of ~0.2, and shows some generalizability beyond code.",
      "mindmap": "graph TB\n        A[Localized Calibrated Uncertainty in Code Language Models] --> B[核心问题/Problem: LLM生成的代码可能偏离用户意图/LLM-generated code may deviate from user intent]\n        A --> C[主要方法/Method: 创建最小意图对齐补丁数据集，比较白盒探测与黑盒方法/Create Minimal Intent Aligning Patches dataset, compare white-box probing vs. black-box methods]\n        A --> D[关键结果/Results: 小监督模型可实现低校准误差，Brier Skill Score约0.2/Small supervisor model achieves low calibration error, Brier Skill Score ~0.2]"
    },
    {
      "title": "MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use",
      "authors": "Wenrui Liu, Zixiang Liu, Elsie Dai, Wenhan Yu, Lei Yu, Tong Yang",
      "institution": "Peking University, Columbia University",
      "link": "https://arxiv.org/pdf/2512.24565",
      "code": "Github",
      "tags": [
        "agent system",
        "Model Context Protocol (MCP)",
        "LLM Agent",
        "Benchmark",
        "Tool Use",
        "Dynamic Sandbox"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/088dcfaedc1cf43a859436433c7e392c2a6c7d8ccaf29c406871b6b794f77a2d_w640_q70.webp",
      "contributions": "1. Proposes MCPAgentBench, a benchmark based on real-world MCP definitions to evaluate agent tool-use capabilities. 2. Constructs a dataset with authentic tasks and simulated MCP tools, employing a dynamic sandbox environment with distractor tools. 3. Introduces comprehensive metrics to measure both task completion rates and execution efficiency.",
      "summary": "The paper addresses the limitations of existing MCP evaluation sets by introducing MCPAgentBench, a benchmark that uses simulated MCP tools and a dynamic sandbox environment with distractors to test LLM agents' tool selection and execution abilities. Experiments on various LLMs reveal significant performance differences in handling complex, multi-step tool invocations.",
      "mindmap": "graph TB\n        A[MCPAgentBench: A Real-world Task Benchmark for Evaluating LLM Agent MCP Tool Use] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有MCP评估集依赖外部服务且缺乏难度感知/Current MCP evaluations rely on external services and lack difficulty awareness]\n        C --> C1[基于真实MCP定义构建基准/Benchmark based on real-world MCP definitions]\n        C --> C2[使用含干扰项的动态沙盒环境评估/Evaluation using dynamic sandbox with distractors]\n        C --> C3[引入任务完成率和执行效率综合指标/Introducing comprehensive metrics for completion rate and efficiency]\n        D --> D1[主流大模型在复杂多步工具调用上表现差异显著/Significant performance differences among LLMs on complex multi-step invocations]"
    },
    {
      "title": "SynRAG: A Large Language Model Framework for Executable Query Generation in Heterogeneous SIEM System",
      "authors": "Md Hasan Saju, Austin Page, Akramul Azim, Jeff Gardiner, Farzaneh Abazari, Frank Eargle",
      "institution": "Ontario Tech University, GlassHouse Systems Inc.",
      "link": "https://arxiv.org/pdf/2512.24571",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "SIEM",
        "query generation",
        "platform-agnostic specification",
        "cross-platform",
        "LLM framework"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d87255708c71cdba68b8c5ca7054cc8216b56e3021254a7bbbd7c64c9a53687_w640_q70.webp",
      "contributions": "1. Introduces SynRAG, a unified LLM framework for generating executable queries for heterogeneous SIEM systems from a single high-level specification. 2. Enables seamless threat detection and incident investigation across different SIEM platforms, reducing the need for specialized training. 3. Demonstrates superior performance compared to state-of-the-art base LLMs (GPT, Llama, etc.) in generating platform-specific queries for systems like Qradar and SecOps.",
      "summary": "The paper introduces SynRAG, a framework that uses large language models to automatically generate platform-specific SIEM queries from a single, high-level, platform-agnostic specification. This addresses the challenge of monitoring diverse SIEM systems with different query languages. Evaluation shows SynRAG outperforms base LLMs in generating accurate queries for cross-platform threat detection and investigation.",
      "mindmap": "graph TB\n        A[SynRAG: 异构SIEM系统的可执行查询生成框架<br>SynRAG: Executable Query Generation in Heterogeneous SIEM Systems] --> B[核心问题/Problem: SIEM平台多样性导致查询语言不同，分析师监控多平台困难<br>Problem: SIEM platform diversity leads to different query languages, making multi-platform monitoring difficult for analysts]\n        A --> C[主要方法/Method: 提出SynRAG框架，从平台无关的高级描述自动生成特定平台查询<br>Method: Proposes SynRAG framework to auto-generate platform-specific queries from a platform-agnostic high-level specification]\n        A --> D[关键结果/Results: SynRAG生成的查询优于GPT、Llama等先进基础模型<br>Results: SynRAG generates better queries than SOTA base models like GPT, Llama]"
    },
    {
      "title": "Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time",
      "authors": "Zhenyu Zhang, Xiaoxia Wu, Zhongzhu Zhou, Qingyang Wu, Yineng Zhang, Pragaash Ponnusamy, Harikaran Subbaraj, Jue Wang, Shuaiwen Leon Song, Ben Athiwaratkun",
      "institution": "University of Texas at Austin, Together AI, University of Sydney",
      "link": "https://arxiv.org/pdf/2512.24574",
      "code": "https://github.com/togethercomputer/CREST",
      "tags": [
        "llm inference",
        "chain-of-thought reasoning",
        "attention heads",
        "test-time intervention",
        "computational efficiency",
        "reasoning steering"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7de1babf1bde06083c19ff84ab24d16f7280ee2cd3e28ae548f08be7f95a5882_w640_q70.webp",
      "contributions": "1. Identified specialized attention heads in LLMs that correlate with distinct cognitive reasoning behaviors (e.g., verification, backtracking). 2. Proposed CREST, a training-free method for Cognitive REasoning Steering at Test-time, which involves offline calibration to find steering vectors and inference-time rotation to suppress unproductive reasoning. 3. Demonstrated that CREST improves reasoning accuracy and reduces token usage across diverse benchmarks, offering a pathway to faster and more reliable LLM inference.",
      "summary": "This paper addresses the inefficiency and instability of long chain-of-thought reasoning in LLMs, which leads to high latency and alternating underthinking/overthinking. The authors propose CREST, a training-free method that identifies and steers specific attention heads at test-time to suppress unproductive cognitive behaviors. The method improves accuracy by up to 17.5% and reduces token usage by 37.6%, enabling faster and more reliable reasoning.",
      "mindmap": "graph TB\n        A[Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[LLM推理轨迹低效且不稳定/Inefficient & Unstable LLM Reasoning Trajectories]\n        B1 --> B2[过度思考与思考不足/Overthinking & Underthinking]\n        B1 --> B3[高延迟与高令牌消耗/High Latency & Token Usage]\n        C --> C1[识别与认知行为相关的注意力头/Identify Cognitive Attention Heads]\n        C --> C2[提出CREST方法: 测试时认知推理引导/Propose CREST: Test-time Cognitive REasoning Steering]\n        C2 --> C3[离线校准获取引导向量/Offline Calibration for Steering Vectors]\n        C2 --> C4[推理时旋转隐藏表示/Inference-time Representation Rotation]\n        D --> D1[准确率显著提升/Accuracy Improved Up to 17.5%]\n        D --> D2[令牌使用大幅减少/Token Usage Reduced by 37.6%]\n        D --> D3[实现更快更可靠的推理/Enables Faster, More Reliable Reasoning]"
    },
    {
      "title": "Recursive Language Models",
      "authors": "Alex L. Zhang, Tim Kraska, Omar Khattab",
      "institution": "MIT CSAIL",
      "link": "https://arxiv.org/pdf/2512.24601",
      "code": null,
      "tags": [
        "llm inference",
        "recursive language models",
        "long-context processing",
        "inference-time scaling",
        "context condensation",
        "out-of-core algorithms"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b76e3870cfbc09a08f87a31e423d89e08fb4d1e100fa580e4e6b58754309af5_w640_q70.webp",
      "contributions": "1. Proposes Recursive Language Models (RLMs), a general inference strategy that allows LLMs to programmatically examine, decompose, and recursively call themselves over long prompts. 2. Demonstrates that RLMs can handle inputs up to two orders of magnitude beyond standard model context windows. 3. Shows RLMs outperform base LLMs and existing long-context methods across diverse tasks while maintaining comparable or lower cost per query.",
      "summary": "The paper addresses the problem of LLMs struggling with arbitrarily long prompts due to limited context windows and context rot. It introduces Recursive Language Models (RLMs), an inference-time method that treats long prompts as an external environment, enabling recursive decomposition and processing. The results show RLMs effectively scale to inputs far beyond standard context limits and outperform baseline approaches in quality and cost.",
      "mindmap": "graph TB\n        A[Recursive Language Models] --> B[核心问题/Problem: LLMs have limited context lengths and suffer from context rot with long prompts]\n        A --> C[主要方法/Method: Recursive Language Models (RLMs) treat long prompts as an external environment, allowing programmatic examination and recursive self-calls]\n        A --> D[关键结果/Results: RLMs handle inputs up to 100x beyond context windows, outperform base LLMs and long-context scaffolds, with comparable or cheaper cost]"
    },
    {
      "title": "Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization",
      "authors": "Dong Qiu, Duo Xu, Limengxi Yue",
      "institution": "New England College, Northeastern University, University of Massachusetts Amherst",
      "link": "https://arxiv.org/pdf/2512.24609",
      "code": null,
      "tags": [
        "agent system",
        "Dec-POMDP",
        "CTDE",
        "GRPO"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/251d4cfbf0941a0b6ad2b2a8b7158ec06789c4a7a60653f447034ce562d8c91d_w640_q70.webp",
      "contributions": "1. A reinforcement learning-augmented LLM agent framework that formulates multi-agent collaboration as a Dec-POMDP and uses CTDE. 2. The introduction of Group Relative Policy Optimization (GRPO) for jointly optimizing agent policies with global training signals. 3. A simplified joint reward function that balances task quality, speed, and coordination cost.",
      "summary": "The paper addresses the lack of collaborative awareness in LLMs for multi-agent settings by proposing a framework that combines reinforcement learning with LLMs, using a Dec-POMDP formulation and CTDE. It introduces GRPO for policy optimization and a balanced reward function. The method significantly outperforms baselines in collaborative writing and coding tasks, demonstrating improved speed, consistency, and success rates.",
      "mindmap": "graph TB\n        A[Reinforcement Learning-Augmented LLM Agents<br/>强化学习增强的LLM智能体] --> B[核心问题/Problem<br/>LLMs lack collaborative awareness in multi-agent settings<br/>LLM在多智能体环境中缺乏协作意识]\n        A --> C[主要方法/Method<br/>Dec-POMDP & CTDE framework with GRPO and a simplified joint reward<br/>基于Dec-POMDP和CTDE的框架，使用GRPO和简化联合奖励]\n        A --> D[关键结果/Results<br/>3x speedup, 98.7% writing consistency, 74.6% coding pass rate<br/>3倍速度提升，98.7%写作一致性，74.6%编码通过率]"
    },
    {
      "title": "Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning",
      "authors": "Zheyu Shi, Dong Qiu, Shanlong Yu",
      "institution": "Brown University, New England College, Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.24613",
      "code": null,
      "tags": [
        "agent system",
        "multi-agent dialogue",
        "role-based architecture",
        "self-game mechanism",
        "retrieval enhancement",
        "proximal policy optimization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e250eb290183b556b4931328b4b20eb01ed043b916f7156301013ceb7995ab66_w640_q70.webp",
      "contributions": "1. Proposed a three-level role division architecture (generation-verification-integration) for structured multi-agent collaboration in complex reasoning. 2. Introduced a self-game mechanism to expand multi-path reasoning trajectories and a retrieval enhancement module for dynamic external knowledge supplementation. 3. Designed a composite reward function and applied an improved proximal policy optimization strategy for collaborative training of the multi-agent system.",
      "summary": "This paper proposes a group deliberation oriented multi-agent conversational model to enhance complex reasoning. The model employs a three-agent architecture for opinion generation, evidence verification, and consistency arbitration, augmented by a self-game mechanism and retrieval enhancement. Experiments show significant improvements in multi-hop reasoning accuracy and consistency over baseline models.",
      "mindmap": "graph TB\n        A[Group Deliberation Oriented Multi-Agent Conversational Model for Complex Reasoning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Limitations of single LLMs in complex reasoning tasks]\n        C[主要方法/Method: Three-level role division, self-game mechanism, retrieval enhancement, composite reward & PPO training]\n        D[关键结果/Results: Improved multi-hop reasoning accuracy & consistency on HotpotQA, 2WikiMultihopQA, MeetingBank]"
    },
    {
      "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
      "authors": "Xingwei Qu, Shaowen Wang, Zihao Huang, Kai Hua, Fan Yin, Rui-Jie Zhu, Jundong Zhou, Qiyang Min, Zihao Wang, Yizhi Li, Tianyu Zhang, He Xing, Zheng Zhang, Yuxuan Song, Tianyu Zheng, Zhiyuan Zeng, Chenghua Lin, Ge Zhang, Wenhao Huang",
      "institution": "ByteDance Seed, University of Manchester, Mila - Quebec AI Institute, Tsinghua University, M-A-P",
      "link": "https://arxiv.org/pdf/2512.24617",
      "code": null,
      "tags": [
        "llm inference",
        "hierarchical compression",
        "compression-aware scaling law",
        "decoupled µP parametrization",
        "concept space",
        "adaptive semantic boundaries"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dff9c61b225b5a86d535cfc752b13f390ae301473e649284a6815c3eaf80b24_w640_q70.webp",
      "contributions": "1. Proposed Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns variable-length semantic concepts end-to-end and shifts computation from tokens to a compressed concept space for more efficient reasoning. 2. Introduced the first compression-aware scaling law that disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. 3. Developed a decoupled µP parametrization for stable training of the heterogeneous architecture, supporting zero-shot hyperparameter transfer across model widths and compression regimes.",
      "summary": "The paper addresses the inefficiency of uniform token-level computation in LLMs by proposing Dynamic Large Concept Models (DLCM), which learns adaptive semantic concepts and reallocates compute to a higher-capacity reasoning backbone in a compressed concept space. This approach achieves a +2.69% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.",
      "mindmap": "graph TB\n        A[Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space] --> B[核心问题/Problem: LLMs apply uniform computation to tokens, wasting capacity on predictable spans and under-allocating to critical transitions]\n        A --> C[主要方法/Method: Hierarchical framework learns semantic boundaries, shifts computation to compressed concept space, introduces compression-aware scaling law and decoupled µP parametrization]\n        A --> D[关键结果/Results: +2.69% average improvement on 12 zero-shot benchmarks under matched inference FLOPs with R=4 compression]"
    },
    {
      "title": "Chat-Driven Optimal Management for Virtual Network Services",
      "authors": "Yuya Miyaoka, Masaki Inoue, Kengo Urata, Shigeaki Harada",
      "institution": "Keio University, NTT Inc.",
      "link": "https://arxiv.org/pdf/2512.24614",
      "code": null,
      "tags": [
        "communication & networking",
        "intent-based networking",
        "virtual network allocation",
        "integer linear programming",
        "Sentence-BERT",
        "large language model"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8dd7600e3aadf99d8f0d2549d3d65d3c00bce80acbf6105aef79f0c8a797c126_w640_q70.webp",
      "contributions": "1. Proposes a novel two-stage chat-driven framework that integrates NLP-based intent extraction with optimization-based allocation for virtual network management. 2. Introduces and compares two intent extractors: a low-latency Sentence-BERT with SVM classifier and a high-accuracy LLM-based model. 3. Demonstrates the framework's ability to dynamically and feasibly update VM placement and routing in both single-user and multi-user settings.",
      "summary": "This paper addresses the limitation of conventional intent-based networking, which cannot guarantee feasible configurations, by proposing a chat-driven framework. The framework uses an NLP Interpreter to extract user intent and an optimization-based Optimizer to compute feasible VM placement and routing. The results show that combining NLP with optimization enables safe, interpretable, and user-friendly network management, with an LLM-based extractor offering higher accuracy and a Sentence-BERT/SVM extractor providing lower latency.",
      "mindmap": "graph TB\n        Root[Chat-Driven Optimal Management for Virtual Network Services] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Conventional IBN cannot guarantee configuration feasibility] --> P1[传统IBN方法/Traditional IBN Methods]\n        P1 --> P2[依赖统计语言模型/Rely on statistical language models]\n        P2 --> P3[无法保证可行性/Cannot guarantee feasibility]\n        Method[主要方法/Method: Two-stage NLP + Optimization Framework] --> M1[阶段1: 解释器/Stage 1: Interpreter]\n        M1 --> M2[提取用户意图/Extract user intent from chat]\n        M2 --> M3[使用NLP模型/Use NLP models (Sentence-BERT+SVM or LLM)]\n        Method --> M4[阶段2: 优化器/Stage 2: Optimizer]\n        M4 --> M5[计算可行配置/Compute feasible configuration]\n        M5 --> M6[使用整数线性规划/Use Integer Linear Programming]\n        Results[关键结果/Results] --> R1[动态更新VM放置和路由/Dynamically updates VM placement & routing]\n        R1 --> R2[保持可行性/Preserves feasibility]\n        Results --> R3[LLM提取器: 高精度/LLM Extractor: High accuracy with few samples]\n        Results --> R4[Sentence-BERT+SVM: 低延迟/Sentence-BERT+SVM: Low latency for real-time]"
    },
    {
      "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
      "authors": "Yuchen Shi, Yuzheng Cai, Siqi Cai, Zihan Xu, Lichao Chen, Yulei Qin, Zhijian Zhou, Xiang Fei, Chaofan Qiu, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Guocan Cai, Yong Mao, Yunsheng Wu, Ke Li, Xing Sun",
      "institution": "Tencent (inferred from \"TencentCloudADP\" in GitHub URL)",
      "link": "https://arxiv.org/pdf/2512.24615",
      "code": "https://github.com/TencentCloudADP/youtu-agent",
      "tags": [
        "agent system",
        "LLM agent framework",
        "automated agent generation",
        "hybrid policy optimization",
        "in-context optimization",
        "reinforcement learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0414a5769d966ffb5a9f4428d4a182e5095ba0b107862d50c8224788df0caa46_w640_q70.webp",
      "contributions": "1. A modular LLM agent framework (Youtu-Agent) with a structured configuration system for decoupling components and enabling automated synthesis. 2. Two agent generation paradigms: Workflow mode for standard tasks and Meta-Agent mode for complex tasks, capable of auto-generating tools and prompts. 3. A hybrid policy optimization system combining an in-context learning \"Agent Practice\" module and a scalable reinforcement learning \"Agent RL\" module for continuous agent evolution.",
      "summary": "The paper proposes Youtu-Agent, a framework to automate the generation and continuous optimization of LLM agents, addressing high configuration costs and static capabilities. It introduces structured configuration, automated generation paradigms, and a hybrid optimization system combining in-context learning and reinforcement learning. Experiments show state-of-the-art performance on several benchmarks and significant improvements in agent capabilities through automated synthesis and optimization.",
      "mindmap": "graph TB\n        A[Youtu-Agent] --> B[核心问题 / Problem]\n        A --> C[主要方法 / Method]\n        A --> D[关键结果 / Results]\n        B --> B1[高配置成本 / High Configuration Cost]\n        B --> B2[静态能力 / Static Capabilities]\n        C --> C1[结构化配置系统 / Structured Configuration System]\n        C --> C2[自动化生成 / Automated Generation]\n        C --> C21[工作流模式 / Workflow Mode]\n        C --> C22[元智能体模式 / Meta-Agent Mode]\n        C --> C3[混合策略优化 / Hybrid Policy Optimization]\n        C --> C31[智能体实践 / Agent Practice]\n        C --> C32[智能体强化学习 / Agent RL]\n        D --> D1[SOTA性能 / SOTA Performance]\n        D --> D2[高工具合成率 / High Tool Synthesis Rate]\n        D --> D3[能力显著提升 / Significant Capability Improvement]"
    },
    {
      "title": "AutoFed: Manual-Free Federated Traffic Prediction via Personalized Prompt",
      "authors": "Zijian Zhao, Yitong Shang, Sen Li",
      "institution": "The Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.24625",
      "code": "https://github.com/RS2002/AutoFed",
      "tags": [
        "federated learning",
        "personalized federated learning",
        "prompt learning",
        "traffic prediction",
        "non-IID data",
        "hyper-parameter tuning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e572784e0dff554ff14fce989febaa7869bdfb488d60b6fd86ebd7e98cdf0bf7_w640_q70.webp",
      "contributions": "1. Proposes AutoFed, a novel Personalized Federated Learning (PFL) framework for traffic prediction that eliminates the need for manual hyper-parameter tuning. 2. Introduces a federated representor with a client-aligned adapter to distill local data into a compact, globally shared prompt matrix, inspired by prompt learning. 3. Demonstrates through extensive experiments that AutoFed consistently achieves superior performance across diverse real-world traffic prediction scenarios.",
      "summary": "This paper proposes AutoFed, a manual-free Personalized Federated Learning framework for traffic prediction that uses a client-aligned adapter to generate a shared prompt matrix, enabling knowledge sharing while preserving local specificity. Experiments on real-world datasets show that AutoFed achieves superior performance without requiring manual hyper-parameter tuning.",
      "mindmap": "graph TB\n        A[AutoFed: Manual-Free Federated Traffic Prediction via Personalized Prompt] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[数据孤岛与隐私问题 / Data Silos & Privacy]\n        B --> B2[非独立同分布数据 / Non-IID Data]\n        B --> B3[手动超参数调优 / Manual Hyper-parameter Tuning]\n        C --> C1[个性化联邦学习 / Personalized Federated Learning (PFL)]\n        C --> C2[提示学习 / Prompt Learning]\n        C --> C3[联邦表征器与客户端对齐适配器 / Federated Representor & Client-Aligned Adapter]\n        D --> D1[性能优越 / Superior Performance]\n        D --> D2[无需手动调参 / No Manual Tuning]\n        D --> D3[真实数据集验证 / Validated on Real-world Datasets]"
    },
    {
      "title": "AI-Driven Acoustic Voice Biomarker-Based Hierarchical Classification of Benign Laryngeal Voice Disorders from Sustained Vowels",
      "authors": "Mohsen Annabestani, Samira Aghadoost, Anais Rameau, Olivier Elemento, Gloria Chia-Yi Chiang",
      "institution": "Weill Cornell Medicine",
      "link": "https://arxiv.org/pdf/2512.24628",
      "code": null,
      "tags": [
        "medical audio classification",
        "hierarchical classification",
        "acoustic biomarkers",
        "mel-spectrograms",
        "voice disorders",
        "sustained vowels"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a53018a32ff94f55161f7c3e6f57843d9d248636c6146e968b0832ca7fdb34b_w640_q70.webp",
      "contributions": "1. A novel three-stage hierarchical machine learning framework for voice disorder classification that mirrors clinical triage workflows, integrating deep spectral features with interpretable acoustic biomarkers. 2. The proposed system outperforms flat multi-class classifiers and state-of-the-art pre-trained self-supervised audio models (HuBERT, HeAR) on the task of classifying benign laryngeal disorders from sustained vowels. 3. Demonstrates the potential of combining deep learning representations with clinically interpretable features to enhance transparency and alignment for scalable, non-invasive vocal health screening and monitoring.",
      "summary": "This paper proposes a hierarchical AI framework to classify benign laryngeal voice disorders from short, sustained vowel recordings. The method uses a three-stage pipeline combining CNN-derived mel-spectrogram features with interpretable acoustic biomarkers, outperforming standard multi-class and pre-trained audio models. The results highlight the framework's potential as a scalable tool for early voice disorder screening and diagnostic triage.",
      "mindmap": "graph TB\n        A[AI-Driven Acoustic Voice Biomarker-Based Hierarchical Classification of Benign Laryngeal Voice Disorders from Sustained Vowels] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[良性喉部嗓音疾病分类/Benign Laryngeal Voice Disorder Classification]\n        C --> C1[三级分层机器学习框架/Three-Stage Hierarchical ML Framework]\n        C1 --> C1_1[阶段1: 病理筛查/Stage 1: Pathological Screening]\n        C1 --> C1_2[阶段2: 粗粒度分层/Stage 2: Coarse Stratification]\n        C1 --> C1_3[阶段3: 细粒度分类/Stage 3: Fine-Grained Classification]\n        C1_1 --> C1_1a[融合CNN梅尔谱特征与21种声学生物标志物/Integrates CNN Mel-Spectrogram & 21 Acoustic Biomarkers]\n        D --> D1[性能优于平面多类分类器与预训练模型/Outperforms Flat Classifiers & Pre-trained Models (HuBERT, HeAR)]\n        D --> D2[结合深度表征与可解释特征，增强临床可操作性/Enhances Transparency & Clinical Alignment via Deep & Interpretable Features]"
    },
    {
      "title": "DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information",
      "authors": "Zhili Huang, Ling Xu, Chao Liu, Weifeng Sun, Xu Zhang, Yan Lei, Meng Yan, Hongyu Zhang",
      "institution": "Chongqing University",
      "link": "https://arxiv.org/pdf/2512.24635",
      "code": null,
      "tags": [
        "automated program repair",
        "large language models",
        "dynamic analysis",
        "iterative repair",
        "execution-level information",
        "Defects4J"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ca5f82c2ff7a5874dfaa7c320c9097716fe0d1514eca3eb69291b75f1a981d7_w640_q70.webp",
      "contributions": "1. Proposes DynaFix, a novel APR method that iteratively leverages fine-grained, execution-level dynamic information (e.g., variable states, control-flow) to guide LLMs in patch generation. 2. Introduces an iterative repair loop where failed patches trigger re-execution to collect updated runtime feedback, mimicking human stepwise debugging. 3. Demonstrates significant effectiveness and efficiency improvements, repairing 186 bugs (10% more than SOTA) and reducing patch search space by 70% on Defects4J benchmarks.",
      "summary": "The paper proposes DynaFix, an automated program repair method that iteratively uses execution-level dynamic information (like variable states) to guide large language models in generating patches. This approach mimics human debugging by refining patches based on runtime feedback from failed attempts. Evaluation on Defects4J shows it repairs more bugs and reduces the search space more effectively than existing methods.",
      "mindmap": "graph TB\n        A[DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有APR方法依赖静态分析或粗粒度反馈，难以模拟人类逐步调试/Existing APR relies on static or coarse feedback, failing to simulate stepwise debugging]\n        C --> C1[迭代捕获执行级动态信息（变量状态、控制流等）指导LLM生成补丁/Iteratively captures execution-level info (variable states, control flow) to guide LLM patch generation]\n        D --> D1[修复Defects4J中186个bug，性能提升10%/Repairs 186 bugs on Defects4J, 10% improvement over SOTA]\n        D --> D2[将补丁搜索空间减少70%/Reduces patch search space by 70%]"
    },
    {
      "title": "Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation",
      "authors": "Yury Kolomeytsev, Dmitry Golembiovsky",
      "institution": "Lomonosov Moscow State University",
      "link": "https://arxiv.org/pdf/2512.24651",
      "code": null,
      "tags": [
        "robot navigation",
        "hybrid motion planning",
        "deep reinforcement learning",
        "entity-aware reward",
        "graph-based global planner",
        "collision avoidance"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6963a6e6a0df2bf9e5857d6154c70386d9271eb85763359a237ce12c4881bec_w640_q70.webp",
      "contributions": "1. Proposes HMP-DRL, a hybrid framework integrating a graph-based global planner with a local DRL policy via checkpoints. 2. Introduces an entity-aware reward structure for the local planner to ensure social compliance by adjusting safety based on agent type. 3. Validates the method in a realistic simulation, showing superior performance in success rate, collision rate, and time to goal.",
      "summary": "The paper proposes HMP-DRL, a hybrid motion planning framework that combines a graph-based global planner for long-range pathfinding with a local Deep Reinforcement Learning policy for reactive, socially-compliant navigation. The method uses checkpoints to integrate the global path and an entity-aware reward function to dynamically adjust to different moving agents. Experiments in realistic simulation show it outperforms other methods in key navigation metrics, enhancing safety and reliability in complex environments.",
      "mindmap": "graph TB\n        A[Hybrid Motion Planning with Deep Reinforcement Learning for Mobile Robot Navigation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统图规划器缺乏反应性/Traditional graph planners lack reactivity]\n        B --> B2[深度强化学习方法缺乏全局上下文/DRL methods lack global context]\n        C --> C1[混合框架HMP-DRL/Hybrid framework HMP-DRL]\n        C1 --> C2[图规划器生成路径/Graph planner generates path]\n        C1 --> C3[局部DRL策略使用检查点和实体感知奖励/Local DRL policy uses checkpoints & entity-aware reward]\n        D --> D1[更高的成功率/Higher success rate]\n        D --> D2[更低的碰撞率/Lower collision rate]\n        D --> D3[更短的到达时间/Shorter time to goal]"
    },
    {
      "title": "Do Large Language Models Know What They Are Capable Of?",
      "authors": "Casey O. Barkan, Sid Black, Oliver Sourbut",
      "institution": "RAND Corporation, UK AI Security Institute, The Future of Life Foundation",
      "link": "https://arxiv.org/pdf/2512.24661",
      "code": null,
      "tags": [
        "llm evaluation",
        "in-advance confidence",
        "overconfidence",
        "capability awareness",
        "decision-making",
        "agentic tasks"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c758274cef27c9ad79258c27a063b8420cef801a9f25ff7610a7c4c12509a926_w640_q70.webp",
      "contributions": "1. Evaluates LLMs' in-advance confidence and its impact on decision-making in costly-failure scenarios, a less studied area compared to after-the-fact calibration. 2. Investigates how LLMs' confidence and overconfidence evolve during multi-step agentic tasks and with in-context failure experiences. 3. Demonstrates that while LLMs' decisions are rational given their self-estimates, their systematic overconfidence leads to poor task pursuit decisions, highlighting a lack of capability awareness.",
      "summary": "This paper investigates whether large language models (LLMs) can accurately predict their own success on tasks, especially when failure is costly. It evaluates their in-advance confidence, how it changes during multi-step tasks and with in-context failure, and its impact on decision-making. The main finding is that current LLMs are generally overconfident, which impairs their decision-making despite rational behavior based on their flawed self-assessments, indicating a lack of self-awareness of their capabilities.",
      "mindmap": "graph TB\n        A[Do Large Language Models Know What They Are Capable Of?] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLMs能否预测自身任务成功率?<br/>Can LLMs predict their task success?]\n        B --> B2[失败成本高时如何决策?<br/>How to decide when failure is costly?]\n        C --> C1[评估事前置信度<br/>Evaluate in-advance confidence]\n        C --> C2[分析多步骤任务中的信心变化<br/>Analyze confidence change in multi-step tasks]\n        C --> C3[研究上下文失败经验的影响<br/>Study impact of in-context failure]\n        D --> D1[LLMs普遍过度自信<br/>LLMs are generally overconfident]\n        D --> D2[新/大模型判别力未显著提升<br/>Newer/larger models don't have greater discriminatory power]\n        D --> D3[部分模型能从失败中学习<br/>Some models learn from failure]\n        D --> D4[决策理性但估计过于乐观<br/>Decisions rational but estimates overly optimistic]"
    },
    {
      "title": "Renormalization Group Guided Tensor Network Structure Search",
      "authors": "Maolin Wang, Bowen Yu, Sheng Zhang, Linjie Mi, Wanyu Wang, Yiqi Wang, Pengyue Jia, Xuetao Wei, Zenglin Xu, Ruocheng Guo, Xiangyu Zhao",
      "institution": "City University of Hong Kong, National University of Defense Technology, Southern University of Science and Technology, Fudan University, Intuit AI",
      "link": "https://arxiv.org/pdf/2512.24663",
      "code": "https://github.com/Applied-Machine-Learning-Lab/RGTN",
      "tags": [
        "model compression (quantization/pruning)",
        "tensor network structure search",
        "renormalization group",
        "multi-scale optimization",
        "edge gates",
        "node tension"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9fdccc30b2aad8da00c1fdf132004ba3347f8992425dfb71406c3fa11d073c_w640_q70.webp",
      "contributions": "1. Proposes RGTN, a physics-inspired framework that uses multi-scale renormalization group flows for continuous tensor network structure evolution, overcoming the limitations of fixed-scale, discrete search methods. 2. Introduces learnable edge gates for dynamic topology modification and intelligent proposals based on physical quantities like node tension and edge information flow to guide the search. 3. Demonstrates state-of-the-art performance, achieving superior compression ratios and running 4-600 times faster than existing methods on tasks like light field data and video completion.",
      "summary": "This paper addresses the limitations of existing Tensor Network Structure Search (TN-SS) methods, which struggle with computational tractability and structure adaptivity. The authors propose RGTN, a novel framework guided by renormalization group theory, which enables multi-scale, continuous structure optimization. Experiments show RGTN achieves better compression and is significantly faster than prior methods.",
      "mindmap": "graph TB\n        Root[Renormalization Group Guided Tensor Network Structure Search] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有TN-SS方法局限/Limitations of existing TN-SS]\n        P1 --> P1_1[单尺度优化/Single-scale optimization]\n        P1 --> P1_2[离散搜索空间/Discrete search space]\n        P1 --> P1_3[分离的结构-参数优化/Separated structure-parameter optimization]\n        Method[主要方法/Method] --> M1[RGTN框架/RGTN Framework]\n        M1 --> M1_1[多尺度重整化群流/Multi-scale RG flows]\n        M1 --> M1_2[可学习边门/Learnable edge gates]\n        M1 --> M1_3[智能提案(节点张力, 边信息流)/Intelligent proposals (node tension, edge info flow)]\n        Results[关键结果/Results] --> R1[SOTA压缩比/State-of-the-art compression ratio]\n        Results --> R2[4-600倍加速/4-600x speedup]"
    },
    {
      "title": "Multi-modal cross-domain mixed fusion model with dual disentanglement for fault diagnosis under unseen working conditions",
      "authors": "Pengcheng Xia, Yixiang Huang, Chengjin Qin, Chengliang Liu",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.24679",
      "code": "https://github.com/xiapc1996/MMDG",
      "tags": [
        "domain generalization",
        "multi-modal fusion",
        "feature disentanglement",
        "domain-invariant representation",
        "cross-domain mixing",
        "unseen working conditions"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afcd4bf3933b539ef0ff25f07453c8a5bf8bac90477ba90fdbe4c834d24627de_w640_q70.webp",
      "contributions": "1. A dual disentanglement framework to separate modality-invariant/specific and domain-invariant/specific features. 2. A cross-domain mixed fusion strategy to augment data diversity by randomly mixing modality information across domains. 3. A triple-modal fusion mechanism to adaptively integrate heterogeneous multi-modal information.",
      "summary": "This paper proposes a multi-modal fusion model with dual feature disentanglement to address the problem of fault diagnosis under unseen working conditions. The method decouples modality and domain features and uses cross-domain mixing to improve generalization. Experiments on motor fault diagnosis show it outperforms existing methods.",
      "mindmap": "graph TB\n        Root[”Multi-modal Cross-domain Mixed Fusion Model / 多模态跨域混合融合模型”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”性能下降在未见工况 / Performance decline under unseen conditions”]\n        Problem --> P2[”单模态信息局限 / Single-modal limitation”]\n        Method --> M1[”双重解耦框架 / Dual Disentanglement Framework”]\n        Method --> M2[”跨域混合融合 / Cross-domain Mixed Fusion”]\n        Method --> M3[”三模态融合 / Triple-modal Fusion”]\n        Results --> R1[”优于先进方法 / Outperforms advanced methods”]\n        Results --> R2[”消融验证有效性 / Ablation verifies effectiveness”]"
    },
    {
      "title": "VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots",
      "authors": "Yongsheng Zhao, Lei Zhao, Baoping Cheng, Gongxin Yao, Xuanzhang Wen, Han Gao",
      "institution": "China Mobile (Hangzhou) Information Technology Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.24673",
      "code": null,
      "tags": [
        "multi-modal inference",
        "VLA models",
        "action chunking",
        "trajectory smoothing",
        "asynchronous inference",
        "robot motion control"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/886e2a128c843b55a605e8cbe2a7b174fc4b0e84225f1908b0b180891d09bdad_w640_q70.webp",
      "contributions": "1. Proposes VLA-RAIL, a framework for asynchronous inference and motion control to enable smooth, continuous robot action execution. 2. Introduces a Trajectory Smoother using polynomial fitting to filter noise and jitter within an action chunk. 3. Designs a Chunk Fuser to ensure position, velocity, and acceleration continuity between successive action chunks.",
      "summary": "This paper addresses the problem of motion jitter, stalling, and pauses when deploying Vision-Language-Action (VLA) models on robots due to sequential inference and execution. It proposes VLA-RAIL, a framework that decouples model inference from robot control via a Trajectory Smoother and Chunk Fuser. Experiments show it reduces jitter, increases speed, and improves task success rates for robotic manipulation.",
      "mindmap": "graph TB\n        A[VLA-RAIL: A Real-Time Asynchronous Inference Linker] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法导致机器人动作抖动、卡顿/Existing methods cause jitter, stalling in robot actions]\n        C --> C1[异步推理与运动控制/Asynchronous inference & motion control]\n        C1 --> C2[轨迹平滑器/Trajectory Smoother]\n        C1 --> C3[块融合器/Chunk Fuser]\n        D --> D1[减少运动抖动/Reduces motion jitter]\n        D --> D2[提升执行速度/Enhances execution speed]\n        D --> D3[提高任务成功率/Improves task success rates]"
    },
    {
      "title": "R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory",
      "authors": "Maoyuan Li, Zhongsheng Wang, Haoyuan Li, Jiamou Liu",
      "institution": "University of Auckland, Wuhan College of Communication",
      "link": "https://arxiv.org/pdf/2512.24684",
      "code": "https://anonymous.4open.science/r/R-debater-E87F/",
      "tags": [
        "computational argumentation",
        "retrieval-augmented generation",
        "argumentative memory",
        "multi-turn debate",
        "agentic framework",
        "rhetorical grounding"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/987d8c7f9f1e27feb6f7eff24aacab0f5dd1eb3b83c76e27f254c97c97b4c0b3_w640_q70.webp",
      "contributions": "1. Proposes R-Debater, a novel agentic framework for multi-turn debate generation grounded in the concept of \"argumentative memory\" from rhetoric and memory studies. 2. Integrates a debate knowledge base for retrieving evidence and prior arguments with a role-based agent to ensure stance consistency and coherent multi-turn composition. 3. Demonstrates superior performance over strong LLM baselines in both single-turn and multi-turn debate tasks through automated metrics (InspireScore, Debatrix) and human evaluation with experienced debaters.",
      "summary": "The paper presents R-Debater, a framework that generates multi-turn debates by retrieving and adapting arguments from a knowledge base (\"argumentative memory\") using a role-based agent. Evaluated on ORCHID debates, it outperforms LLM baselines in producing more consistent, evidence-grounded, and coherent debates across turns.",
      "mindmap": "graph TB\n        A[R-Debater: Retrieval-Augmented Debate Generation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: LLMs generate fluent but shallow, ungrounded debates with weak stance fidelity]\n        C[主要方法/Method: Agentic framework with argumentative memory for retrieval & role-based utterance composition]\n        D[关键结果/Results: Higher scores than LLM baselines; more faithful, stance-aligned, coherent debates]"
    },
    {
      "title": "BatteryAgent: Synergizing Physics-Informed Interpretation with LLM Reasoning for Intelligent Battery Fault Diagnosis",
      "authors": "Songqi Zhou, Ruixue Liu, Boman Su, Jiazhou Wang, Yixing Wang, Benben Jiang",
      "institution": "Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.24686",
      "code": null,
      "tags": [
        "agent system",
        "physics-informed features",
        "SHAP",
        "Gradient Boosting Decision Trees",
        "LLM reasoning",
        "fault diagnosis"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3462edc6bdbdfb2b93fa0ef560df4483fbef65600fac566e9e76c08059dd4f31_w640_q70.webp",
      "contributions": "1. Proposes BatteryAgent, a hierarchical framework integrating physics-based features with LLM reasoning for interpretable battery fault diagnosis. 2. Introduces a \"numerical-semantic\" bridge using SHAP attributions and a knowledge base to generate comprehensive diagnostic reports with root cause analysis. 3. Demonstrates superior performance (AUROC 0.986) and extends binary detection to multi-type, interpretable diagnosis, shifting from passive detection to intelligent diagnosis.",
      "summary": "This paper proposes BatteryAgent, a framework that combines physics-informed features, Gradient Boosting Decision Trees with SHAP, and an LLM agent to diagnose lithium-ion battery faults. It achieves high accuracy (AUROC 0.986) and provides interpretable reports with root causes and maintenance suggestions, moving beyond simple binary classification.",
      "mindmap": "graph TB\n        A[BatteryAgent: Synergizing Physics-Informed Interpretation with LLM Reasoning for Intelligent Battery Fault Diagnosis] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[黑盒模型缺乏可解释性/Black-box models lack interpretability]\n        B --> B2[二元分类无法提供根因分析/Binary classification lacks root cause analysis]\n        C --> C1[物理感知层提取特征/Physical Perception Layer extracts features]\n        C --> C2[检测归因层量化贡献/Detection & Attribution Layer quantifies contributions]\n        C --> C3[推理诊断层生成报告/Reasoning & Diagnosis Layer generates reports]\n        D --> D1[高精度AUROC 0.986/High accuracy AUROC 0.986]\n        D --> D2[纠正困难样本/Misclassification correction on hard samples]\n        D --> D3[实现智能诊断/Enables intelligent diagnosis]"
    },
    {
      "title": "Nested Learning: The Illusion of Deep Learning Architectures",
      "authors": "Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, Vahab Mirrokni",
      "institution": "Google Research (inferred from authors Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni, who are affiliated with Google)",
      "link": "https://arxiv.org/pdf/2512.24695",
      "code": null,
      "tags": [
        "learning theory",
        "nested learning",
        "in-context learning",
        "continual learning",
        "associative memory",
        "self-modifying model"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aac4e338b76a10773a96b12a072d809a81c99a79e63d8b40927cb078da7b7fdb_w640_q70.webp",
      "contributions": "1. Expressive Optimizers: Shows gradient-based optimizers are associative memory modules and proposes more expressive variants with deeper memory and learning rules. 2. Self-Modifying Learning Module: Presents a sequence model that learns to modify itself by learning its own update algorithm. 3. Continuum Memory System: Introduces a new memory formulation generalizing long/short-term memory, which is combined with the self-modifying model to create \"Hope\", a continual learning module.",
      "summary": "The paper proposes a new learning paradigm called Nested Learning (NL), which frames machine learning models as nested optimization problems. This view explains the emergence of in-context learning and is used to design more expressive optimizers, a self-modifying model, and a new memory system, culminating in a continual learning module named \"Hope\" that shows promising results on various tasks.",
      "mindmap": "graph TB\n        A[Nested Learning: The Illusion of Deep Learning Architecture] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[如何实现持续学习与自我改进/How to achieve continual learning and self-improvement]\n        C --> C1[嵌套学习范式/Nested Learning Paradigm]\n        C --> C2[设计表达性优化器/Design Expressive Optimizers]\n        C --> C3[自修改学习模块/Self-Modifying Learning Module]\n        C --> C4[连续体记忆系统/Continuum Memory System]\n        D --> D1[提出持续学习模块Hope/Propose continual learning module Hope]\n        D --> D2[在多个任务上展示潜力/Show potential on multiple tasks]"
    },
    {
      "title": "BandiK: Efficient Multi-Task Decomposition Using a Multi-Bandit Framework",
      "authors": "András Millinghoffer, András Formanek, András Antos, Péter Antal",
      "institution": "Budapest University of Technology and Economics, E-Group ICT Software Zrt., KU Leuven",
      "link": "https://arxiv.org/pdf/2512.24708",
      "code": null,
      "tags": [
        "multi-task learning",
        "multi-armed bandit",
        "negative transfer",
        "auxiliary task selection",
        "multi-bandit framework",
        "drug-target interaction"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e49fedc6c9b07cd38435ef8daff0ba16207d6088f86e049c77cd192822bd2cf_w640_q70.webp",
      "contributions": "1. A three-stage method (BandiK) for efficient auxiliary task subset selection in multi-task learning, 2. Reduction of candidate auxiliary sets from exponential to linear complexity using pairwise transfer estimations, 3. A novel multi-bandit framework that exploits semi-overlapping arms across tasks to improve computational efficiency.",
      "summary": "The paper introduces BandiK, a three-stage method using multi-armed bandits to efficiently select beneficial auxiliary task subsets in multi-task learning, reducing computational cost by estimating pairwise transfers and leveraging a multi-bandit structure. It is validated on a drug-target interaction benchmark, showing scalable performance for complex multi-task scenarios.",
      "mindmap": "graph TB\n        A[BandiK: Efficient Multi-Task Decomposition Using a Multi-Bandit Framework] --> B[核心问题/Problem: 多任务学习中负迁移和辅助任务选择的高计算成本与复杂性]\n        A --> C[主要方法/Method: 三阶段多臂老虎机框架，估计任务间转移、构建线性候选集、利用半重叠臂的多老虎机结构]\n        A --> D[关键结果/Results: 在药物-靶点相互作用基准上验证，实现高效可扩展的任务分解]"
    },
    {
      "title": "Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting",
      "authors": "Kai Ye, Xiaotong You, Jianghang Lin, Jiayi Ji, Pingyang Dai, Liujuan Cao",
      "institution": "Xiamen University, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.24702",
      "code": "https://github.com/AHideoKuzeA/Evol-SAM3",
      "tags": [
        "reasoning segmentation",
        "evolutionary prompting",
        "zero-shot learning",
        "visual arena",
        "semantic mutation",
        "heterogeneous arena"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b60e24e2f5e5668a6d7d977acfb32177365388575df74e72ccb5a8b3d4e7f7e_w640_q70.webp",
      "contributions": "1. Proposes EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. 2. Introduces a \"Generate-Evaluate-Evolve\" loop with a Visual Arena for reference-free fitness assessment and a Semantic Mutation operator for diversity and error correction. 3. Designs a Heterogeneous Arena module that integrates geometric priors with semantic reasoning for robust final selection.",
      "summary": "This paper addresses the limitations of static, training-free methods for reasoning segmentation by proposing EVOL-SAM3, a zero-shot framework that uses an evolutionary prompting strategy to iteratively refine prompt hypotheses at inference time. The method outperforms both static baselines and fully supervised state-of-the-art methods on the ReasonSeg benchmark without any training.",
      "mindmap": "graph TB\n        A[EVOL-SAM3: 零样本推理分割的进化提示 / EVOL-SAM3: Zero-Shot Reasoning Segmentation via Evolutionary Prompting] --> B\n        A --> C\n        A --> D\n        B[核心问题 / Problem] --> B1[静态推理范式 / Static Inference Paradigm]\n        B1 --> B2[推理深度不足 / Insufficient Reasoning Depth]\n        B1 --> B3[无法自我纠正 / Lack of Self-Correction]\n        C[主要方法 / Method] --> C1[进化搜索 / Evolutionary Search]\n        C1 --> C2[生成-评估-进化循环 / Generate-Evaluate-Evolve Loop]\n        C2 --> C3[视觉竞技场 / Visual Arena]\n        C2 --> C4[语义突变 / Semantic Mutation]\n        C2 --> C5[异构竞技场 / Heterogeneous Arena]\n        D[关键结果 / Results] --> D1[超越静态基线 / Outperforms Static Baselines]\n        D --> D2[超越全监督SOTA / Surpasses Fully Supervised SOTA]\n        D --> D3[零样本设置 / Zero-Shot Setting]"
    },
    {
      "title": "LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving",
      "authors": "Qian Cheng, Weitao Zhou, Cheng Jing, Nanshan Deng, Junze Wen, Zhaoyang Liu, Kun Jiang, Diange Yang",
      "institution": "Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.24712",
      "code": null,
      "tags": [
        "multi-modal inference",
        "latent semantic rule encoding",
        "recurrent world model",
        "language-guided latent classification",
        "semantic risk detection",
        "autonomous driving"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7d95e05edf41980725845c43ad581e04cc0d1877fc56167a5c4d2c47f7e9516_w640_q70.webp",
      "contributions": "1. Proposed LSRE, a framework that encodes sparse VLM judgments into decision boundaries within a recurrent world model's latent space for real-time semantic risk assessment. 2. Demonstrated that LSRE achieves detection accuracy comparable to a per-frame VLM baseline while enabling earlier hazard anticipation and operating at 10 Hz. 3. Showed that the learned latent classifier generalizes to rarely seen, semantically similar test cases, indicating its effectiveness for semantic safety monitoring.",
      "summary": "The paper addresses the problem of real-time semantic rule compliance in autonomous driving, where explicit encoding of complex social rules is difficult. It proposes LSRE, a framework that uses sparsely sampled VLM outputs to train a lightweight latent classifier within a recurrent world model, enabling efficient semantic risk detection. The method achieves accuracy comparable to a VLM baseline with much lower latency and better anticipation, showing promise for deployable semantic safety systems.",
      "mindmap": "graph TB\n        A[LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving] --> B(核心问题/Problem: Real-time semantic rule compliance in autonomous driving is difficult to encode explicitly and VLM inference is too slow.)\n        A --> C(主要方法/Method: LSRE converts sparse VLM judgments into decision boundaries in a recurrent world model's latent space.)\n        A --> D(关键结果/Results: Achieves VLM-comparable accuracy, earlier anticipation, 10 Hz operation, and generalization to unseen cases.)"
    },
    {
      "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
      "authors": "Karthik Dharmarajan, Wenlong Huang, Jiajun Wu, Li Fei-Fei, Ruohan Zhang",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.24766",
      "code": null,
      "tags": [
        "robotic manipulation",
        "3D object flow",
        "video generation",
        "zero-shot manipulation",
        "trajectory optimization",
        "reinforcement learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c22839be4198942eb08182abe0606d486a3126572afb757ce865cb1b3a787721_w640_q70.webp",
      "contributions": "1. Proposes Dream2Flow, a framework that bridges video generation and robotic control using 3D object flow as an intermediate representation. 2. Demonstrates the ability to reconstruct 3D object motions from generated videos and formulate manipulation as object trajectory tracking, overcoming the embodiment gap. 3. Shows that the method enables zero-shot guidance from pre-trained video models to manipulate diverse object categories (rigid, articulated, deformable, granular) without task-specific demonstrations.",
      "summary": "The paper introduces Dream2Flow, a framework that uses 3D object flow extracted from videos generated by off-the-shelf models as an interface for robotic manipulation. It translates these generated motions into executable robot actions via trajectory optimization or reinforcement learning, enabling zero-shot manipulation of diverse objects in open-world settings. The results demonstrate 3D object flow as a general and scalable bridge between video generation models and robotic control.",
      "mindmap": "graph TB\n        A[Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow] --> B(核心问题/Problem: Translating human-like motions from video models into low-level robot actions)\n        A --> C(主要方法/Method: Use 3D object flow as intermediate representation, reconstruct motions from videos, track trajectories)\n        A --> D(关键结果/Results: Enables zero-shot manipulation of diverse objects, bridges video generation to robot control)"
    },
    {
      "title": "HiGR: Efficient Generative Slate Recommendation via Hierarchical Planning and Multi-Objective Preference Alignment",
      "authors": "Yunsheng Pang, Zijian Liu, Yudong Li, Shaojie Zhu, Zijian Luo, Chenyun Yu, Sikai Wu, Shichen Shen, Cong Xu, Bin Wang, Kai Jiang, Hongyong Yu, Chengxiang Zhuo, Zang Li",
      "institution": "Tencent, Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.24787",
      "code": null,
      "tags": [
        "llm inference",
        "slate recommendation",
        "generative recommendation",
        "residual quantization",
        "hierarchical planning",
        "listwise preference alignment"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df7bb0829f12a7ec59faf3d297069dfbce2bbcb60223c10f7e7a4162d6e7a4e0_w640_q70.webp",
      "contributions": "1. Proposed an auto-encoder with residual quantization and contrastive constraints for semantically structured item tokenization. 2. Introduced a hierarchical generation framework that decouples list-level planning from item-level decoding for efficient slate generation. 3. Designed a listwise preference alignment objective to directly optimize slate quality using implicit user feedback.",
      "summary": "The paper proposes HiGR, an efficient generative framework for slate recommendation. It addresses the limitations of existing autoregressive methods by using hierarchical planning and a listwise alignment objective. Experiments on a commercial platform show HiGR significantly outperforms state-of-the-art methods in both offline quality and online metrics while being 5x faster at inference.",
      "mindmap": "graph TB\n        A[HiGR: Efficient Generative Slate Recommendation] --> B[核心问题/Problem: 现有自回归方法存在语义纠缠和低效解码]\n        A --> C[主要方法/Method: 分层规划与列表偏好对齐]\n        C --> D[语义结构化ID / Semantically Structured IDs]\n        C --> E[分层生成 / Hierarchical Generation]\n        C --> F[列表偏好对齐 / Listwise Preference Alignment]\n        A --> G[关键结果/Results: 离线质量提升>10%, 推理加速5倍, 在线指标显著增长]"
    },
    {
      "title": "LeanCat: A Benchmark Suite for Formal Category Theory in Lean (Part I: 1-Categories)",
      "authors": "Rongge Xu, Hui Dai, Yiming Fu, Jiedong Jiang, Tianjiao Nie, Hongwei Wang, Junkai Wang, Holiverse Yang, Jiatong Yang, Zhi-Hao Zhang",
      "institution": "Tsinghua University, Southern University of Science and Technology, Westlake University, Xi'an Jiaotong-Liverpool University, The Chinese University of Hong Kong, Yanqi Lake Beijing Institute of Mathematical Sciences and Applications (BIMSA)",
      "link": "https://arxiv.org/pdf/2512.24796",
      "code": "https://github.com/sciencraft/LeanCat",
      "tags": [
        "theorem proving",
        "formal verification",
        "category theory",
        "benchmark",
        "Lean",
        "large language models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bb17bb33fa46697baca7f3b3a6262916453dd0a4bf5f92ff26cebdd7d681ffe_w640_q70.webp",
      "contributions": "1. Introduces LeanCat, a benchmark for formal category theory in Lean, designed to stress-test abstraction and library-mediated reasoning. 2. Presents a curated dataset of 100 tasks with topic families and difficulty tiers, created via an LLM-assisted human grading process. 3. Demonstrates the benchmark's utility by evaluating models and the LeanBridge method, showing current AI capabilities and providing a checkpoint for tracking progress.",
      "summary": "The paper introduces LeanCat, a benchmark for formalizing category theory in Lean to better evaluate AI's ability for abstract, library-based reasoning in mathematics. It presents a curated set of 100 tasks and evaluates models, finding low success rates, especially on harder problems, while showing that retrieval-augmented methods like LeanBridge can improve performance. The benchmark serves as a compact checkpoint for tracking progress in research-level formal theorem proving.",
      "mindmap": "graph TB\n        A[LeanCat: A Benchmark Suite for Formal Category Theory in Lean] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>现有基准未能充分衡量抽象和基于库的推理/Current benchmarks under-measure abstraction and library-mediated reasoning]\n        C[主要方法/Method<br>为Lean创建形式化范畴论基准，包含100个分级任务/Create a Lean benchmark for formal category theory with 100 graded tasks]\n        D[关键结果/Results<br>最佳模型pass@1为8.25%，检索增强方法有提升/Best model pass@1 is 8.25%, retrieval-augmented methods show gains]"
    },
    {
      "title": "Practising responsibility: Ethics in NLP as a hands-on course",
      "authors": "Malvina Nissim, Viviana Patti, Beatrice Savoldi",
      "institution": "University of Groningen, University of Turin, Fondazione Bruno Kessler",
      "link": "https://arxiv.org/pdf/2512.24825",
      "code": null,
      "tags": [
        "ethics in nlp",
        "ethics education",
        "active learning",
        "curriculum development",
        "hands-on activities",
        "learning by teaching"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad8d866d1d641ab747e97be881ac0eec09fdae4762445938b33c32d5a452c3e5_w640_q70.webp",
      "contributions": "1. Introduction of a dedicated course \"Ethical Aspects in NLP\" designed to integrate ethics into NLP education, 2. Development of a pedagogical approach based on active learning, interactive sessions, hands-on activities, and \"learning by teaching\" methods, 3. Creation and refinement of the course over four years, adapting it across different institutions, educational levels, and interdisciplinary backgrounds, yielding reusable teaching materials and student-made educational products.",
      "summary": "The paper addresses the challenge of integrating ethical considerations into NLP education by proposing a hands-on course. The method employs active learning through interactive sessions, practical activities, and \"learning by teaching\". The main conclusion is that this approach successfully fosters critical thinking and produces reusable educational resources, providing a model for educators to incorporate social impact into curricula.",
      "mindmap": "graph TB\n        Root(”Practising responsibility: Ethics in NLP as a hands-on course”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”NLP系统普及/NLP systems pervasive”)\n        Problem --> P2(”伦理教育挑战/Ethics education challenges”)\n        Method --> M1(”主动学习/Active learning”)\n        Method --> M2(”实践与互动/Hands-on & interactive”)\n        Method --> M3(”以教促学/Learning by teaching”)\n        Results --> R1(”课程优化与适应/Course refined & adapted”)\n        Results --> R2(”产出可复用产品/Reusable products created”)"
    },
    {
      "title": "Explaining Why Things Go Where They Go: Interpretable Constructs of Human Organizational Preferences",
      "authors": "Emmanuel Fashae, Michael Burke, Leimin Tian, Lingheng Meng, Pamela Carreno-Medrano",
      "institution": "Monash University, CSIRO Robotics",
      "link": "https://arxiv.org/pdf/2512.24829",
      "code": null,
      "tags": [
        "human-robot interaction",
        "object rearrangement",
        "human preference modeling",
        "Monte Carlo Tree Search",
        "psychological constructs",
        "user study"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0c9201484194f4f746f05eae7a288356c0e53c3a3a9d4683db5313924455a5a_w640_q70.webp",
      "contributions": "1. Proposes a novel, interpretable formulation of human object arrangement preferences based on four psychological constructs (spatial practicality, habitual convenience, semantic coherence, commonsense appropriateness). 2. Designs and validates a self-report questionnaire to capture these constructs through a 63-participant online study. 3. Demonstrates the utility of these constructs by integrating them into a Monte Carlo Tree Search (MCTS) planner to generate arrangements that align with human preferences.",
      "summary": "This paper addresses the lack of interpretability in robotic object rearrangement models by identifying four explicit psychological constructs that guide human organizational preferences. The authors designed a questionnaire to measure these constructs and integrated them into a Monte Carlo Tree Search planner. The results show that the planner, guided by these interpretable preferences, can generate arrangements closely matching those created by human participants.",
      "mindmap": "graph TB\n        A[Explaining Why Things Go Where They Go<br>解释物品为何归位] --> B(Problem: 机器人重排模型缺乏可解释性<br>Problem: Robotic rearrangement models lack interpretability)\n        A --> C(Method: 提出四个可解释偏好构念与问卷<br>Method: Four interpretable preference constructs & questionnaire)\n        A --> D(Results: 基于MCTS的规划器能生成符合人类偏好的布局<br>Results: MCTS planner generates human-aligned arrangements)"
    },
    {
      "title": "GenZ: Foundational models as latent variable generators within traditional statistical models",
      "authors": "Marko Jojic, Nebojsa Jojic",
      "institution": "Arizona State University, Microsoft Research",
      "link": "https://arxiv.org/pdf/2512.24834",
      "code": null,
      "tags": [
        "hybrid statistical modeling",
        "latent variable model",
        "generalized EM algorithm",
        "semantic feature discovery",
        "cold-start collaborative filtering",
        "hedonic regression"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11807fe38101f2230e94348a6c074eab9e6cece7f18336e076aa76c0c5604232_w640_q70.webp",
      "contributions": "1. Proposes GenZ, a hybrid model that integrates frozen foundational models as latent variable generators within traditional statistical models. 2. Introduces an iterative, generalized EM algorithm that jointly discovers interpretable semantic feature descriptors and optimizes statistical model parameters from dataset errors. 3. Demonstrates the method's effectiveness on real-world tasks, significantly outperforming pure LLM baselines and matching performance that requires extensive traditional data.",
      "summary": "The paper proposes GenZ, a method that uses a frozen foundational model to generate latent semantic features within a statistical model, optimized via a generalized EM algorithm. It shows this hybrid approach significantly outperforms using the foundational model's general knowledge alone for house price prediction and matches traditional collaborative filtering performance for cold-start movie recommendations using only semantic descriptions.",
      "mindmap": "graph TB\n        Root[”GenZ: Foundational models as latent variable generators”] --> Problem[”核心问题/Problem: Foundational models lack dataset-specific patterns for prediction”]\n        Root --> Method[”主要方法/Method: Hybrid model with generalized EM for joint semantic feature & statistical parameter optimization”]\n        Root --> Results[”关键结果/Results: Outperforms GPT-5 on house prices; Matches CF with 4000 ratings using semantics”]"
    },
    {
      "title": "Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control",
      "authors": "Jason Armitage, Rico Sennnrich",
      "institution": "University of Zurich",
      "link": "https://arxiv.org/pdf/2512.24826",
      "code": null,
      "tags": [
        "multi-modal inference",
        "derivative-free optimisation",
        "regret minimisation",
        "multivariate mutual information",
        "in-scene camera control",
        "vision-language models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66c8b5c775488157feb95c4650ebd70cf6bfc3b2373d13cfde2db9f88f119e8d_w640_q70.webp",
      "contributions": "1. A new method that improves multivariate mutual information estimates using regret minimisation with derivative-free optimisation. 2. An algorithm enabling off-the-shelf 2D-trained cross-modal systems to adapt online to object occlusions and differentiate features in 3D scenes. 3. A pipeline that controls an in-scene camera to learn directly from noisy VLM outputs, improving performance on 3D multi-object scenes without pretraining or finetuning.",
      "summary": "This paper addresses the dimensional shift when 2D-trained vision-language models process 3D scenes. It proposes a method using derivative-free optimisation and regret minimisation to improve mutual information estimates and control an in-scene camera, allowing the system to adapt online and improve performance on cross-modal tasks for 3D multi-object scenes without additional training.",
      "mindmap": "graph TB\n        Root(”Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”2D系统处理3D场景的维度偏移/Dimensional shift for 2D systems on 3D scenes”)\n        Problem --> P2(”需要学习相机控制模块/Need to learn an in-scene camera control module”)\n        Method --> M1(”基于遗憾最小化的多元互信息估计/Multivariate mutual information estimates via regret minimisation”)\n        Method --> M2(”使用无导数优化/Using derivative-free optimisation”)\n        Results --> R1(”使现成的2D系统能在线适应3D场景/Enables off-the-shelf 2D systems to adapt online to 3D scenes”)\n        Results --> R2(”无需预训练或微调即可提升性能/Improves performance without pretraining or finetuning”)"
    },
    {
      "title": "PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI",
      "authors": "Srija Mukhopadhyay, Sathwik Reddy, Shruthi Muthukumar, Jisun An, Ponnurangam Kumaraguru",
      "institution": "International Institute of Information Technology Hyderabad, Indiana University",
      "link": "https://arxiv.org/pdf/2512.24848",
      "code": null,
      "tags": [
        "Privacy protections",
        "PrivacyBench",
        "Retrieval-Augmented Generation (RAG)",
        "secret leakage",
        "privacy-aware prompt",
        "privacy-by-design"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7685e487e221610623a5fd5b2084368051b6218a6804a6749e97cc93aef26acb_w640_q70.webp",
      "contributions": "1. Introduces PrivacyBench, a novel conversational benchmark with socially grounded datasets containing embedded secrets for evaluating privacy in AI assistants. 2. Provides a multi-turn conversational evaluation framework to measure secret preservation capabilities of personalized AI systems. 3. Empirically demonstrates that current RAG-based assistants leak secrets in up to 26.56% of interactions and identifies a critical architectural flaw where the retrieval mechanism is a single point of failure for privacy.",
      "summary": "The paper introduces PrivacyBench, a benchmark to evaluate privacy leakage in personalized AI agents that access a user's digital footprint. Testing shows RAG-based assistants leak secrets frequently, and while privacy-aware prompts help, the fundamental architecture is unsafe, highlighting the need for structural, privacy-by-design solutions.",
      "mindmap": "graph TB\n        A[PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Personalized AI agents risk exposing sensitive user data from their digital footprint.]\n        C[主要方法/Method: Introduce PrivacyBench benchmark with datasets containing secrets and multi-turn conversational evaluation.]\n        D[关键结果/Results: RAG assistants leak secrets; privacy prompts partially mitigate; need for privacy-by-design safeguards.]"
    },
    {
      "title": "A study on constraint extraction and exception exclusion in care worker scheduling",
      "authors": "Koki Suenaga, Tomohiro Furuta, Satoshi Ono",
      "institution": "Kagoshima University",
      "link": "https://arxiv.org/pdf/2512.24853",
      "code": null,
      "tags": [
        "constraint learning",
        "constraint extraction",
        "exception exclusion",
        "constraint programming",
        "shift scheduling",
        "care worker scheduling"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e005f244d37b26b715bffe3d3edce37668aa27d5541ef7201258ae5746901d94_w640_q70.webp",
      "contributions": "1. Proposes a method using constraint templates to automatically extract facility-specific scheduling constraints from interview data, reducing the need for manual specification. 2. Introduces a novel mechanism to identify and exclude exceptional constraints from the extraction process, improving the quality of the learned constraints. 3. Demonstrates the effectiveness of the approach by successfully generating schedules that satisfy all hard constraints and reduce soft constraint violations in care worker scheduling.",
      "summary": "This paper addresses the challenge of automatically generating work schedules for long-term care facilities, where constraints vary widely. It proposes a method that uses customizable constraint templates to extract relevant rules from manager interviews while excluding exceptional cases. Experiments show the method successfully creates schedules that meet all hard constraints and reduces soft constraint violations.",
      "mindmap": "graph TB\n        A[A Study on Constraint Extraction and Exception Exclusion in Care Worker Scheduling] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[设施条件各异，需人工访谈定义约束/Facility-specific constraints require manual interviews]\n        C --> C1[使用约束模板提取组合/Use constraint templates to extract combinations]\n        C --> C2[引入例外约束排除机制/Incorporate mechanism to exclude exceptional constraints]\n        D --> D1[满足所有硬约束/Satisfied all hard constraints]\n        D --> D2[减少软约束违规/Reduced soft constraint violations]"
    },
    {
      "title": "Big AI is accelerating the metacrisis: What can we do?",
      "authors": "Steven Bird",
      "institution": "Charles Darwin University",
      "link": "https://arxiv.org/pdf/2512.24863",
      "code": null,
      "tags": [
        "ethics & society",
        "metacrisis",
        "language engineers",
        "human flourishing",
        "planetary boundaries",
        "technofeudalism"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2352136b878f355e9ebcad11726708c80426973daa2249fba0b79ba62b81b583_w640_q70.webp",
      "contributions": "1. Identifies and critiques the role of \"Big AI\" and language engineers in accelerating converging global crises (ecological, meaning, language). 2. Highlights the ethical conflict between professional obligations (e.g., ACL Code of Ethics) and the harms caused by current NLP/AI development practices. 3. Proposes a paradigm shift for NLP, advocating for a future centered on human flourishing and amplifying social networks rather than scaling through large, polluting models.",
      "summary": "This paper argues that the current trajectory of \"Big AI,\" particularly in NLP, is accelerating a global metacrisis. It critiques the field's focus on scalability and value-neutral technology development, which benefits powerful interests at the expense of the public good and the planet. The paper concludes by urgently calling for an alternative, life-affirming future for NLP centered on human flourishing.",
      "mindmap": "graph TB\n        A[Big AI is accelerating the metacrisis: What can we do?] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Big AI加速生态、意义和语言危机/Big AI accelerates ecological, meaning, and language crises]\n        B --> B2[语言工程师的伦理困境/Ethical dilemma of language engineers]\n        C --> C1[批判当前可扩展性叙事/Critique current scalability narrative]\n        C --> C2[呼吁探索替代方案/Call to explore alternatives]\n        D --> D1[需要以人类繁荣为中心的未来/NLP future must center human flourishing]\n        D --> D2[利用集体智慧设计生命肯定的NLP/Design life-affirming NLP with collective intelligence]"
    },
    {
      "title": "Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements",
      "authors": "Yiming Liang, Yizhi Li, Yantao Du, Ge Zhang, Jiayi Zhou, Yuchen Wu, Yinzhu Piao, Denghui Cao, Tong Sun, Ziniu Li, Li Du, Bo Lei, Jiaheng Liu, Chenghua Lin, Zhaoxiang Zhang, Wenhao Huang, Jiajun Zhang",
      "institution": "University of Chinese Academy of Sciences, Chinese Academy of Sciences, Bytedance, Nanjing University, M-A-P, BAAI, The University of Manchester",
      "link": "https://arxiv.org/pdf/2512.24867",
      "code": "https://encyclo-k.github.io",
      "tags": [
        "llm evaluation",
        "benchmark",
        "knowledge statements",
        "dynamic composition",
        "data contamination",
        "multi-knowledge assessment"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf5e4785262ae916ad666afc0e0e212833641ae7597f48f69f148b35cfc4807b_w640_q70.webp",
      "contributions": "1. Proposes a novel statement-based benchmark (Encyclo-K) that uses knowledge statements as the fundamental curation unit instead of pre-defined questions., 2. Introduces a dynamic evaluation method where questions are composed by randomly sampling multiple statements at test time, mitigating data contamination and enabling periodic refresh., 3. Demonstrates a scalable, low-cost annotation process that requires only formatting verification, not domain expertise, while enabling comprehensive multi-knowledge assessment per question.",
      "summary": "The paper introduces Encyclo-K, a new benchmark for evaluating LLMs that constructs questions by dynamically combining multiple knowledge statements extracted from textbooks at test time. This approach addresses key limitations of existing benchmarks, such as data contamination and single-point assessment. Experiments show it poses a significant challenge to state-of-the-art models, with top accuracy at only 62.07%, validating its effectiveness for assessing comprehensive, multi-statement understanding.",
      "mindmap": "graph TB\n        A[Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>现有基准的局限性/Limitations of Existing Benchmarks]\n        C[主要方法/Method<br>基于知识陈述的动态组合/Dynamic Composition from Knowledge Statements]\n        D[关键结果/Results<br>强区分性，模型表现梯度分布/Strong Discriminative Power, Gradient Performance]\n        B --> B1[数据污染/Data Contamination]\n        B --> B2[单知识点评估/Single-Knowledge Assessment]\n        B --> B3[高标注成本/High Annotation Cost]\n        C --> C1[从权威教材提取陈述/Extract Statements from Textbooks]\n        C --> C2[测试时随机组合/Compose Questions at Test Time]\n        D --> D1[GPT-5.1准确率62.07%/GPT-5.1 Accuracy 62.07%]\n        D --> D2[推理模型16.04%-62.07%/Reasoning Models 16.04%-62.07%]\n        D --> D3[聊天模型9.71%-50.40%/Chat Models 9.71%-50.40%]"
    },
    {
      "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
      "authors": "Weixun Wang, XiaoXiao Xu, Wanhe An, Fangwen Dai, Wei Gao, Yancheng He, Ju Huang, Qiang Ji, Hanqi Jin, Xiaoyang Li, Yang Li, Zhongwen Li, Shirong Lin, Jiashun Liu, Zenan Liu, Tao Luo, Dilxat Muhtar, Yuanbin Qu, Jiaqiang Shi, Qinghui Sun, Yingshui Tan, Hao Tang, Runze Wang, Yi Wang, Zhaoguo Wang, Yanan Wu, Shaopan Xiong, Binchen Xu, Xander Xu, Yuchi Xu, Qipeng Zhang, Xixia Zhang, Haizhou Zhao, Jie Zhao, Shuaibing Zhao, Baihui Zheng, Jianhui Zheng, Suhang Zheng, Yanni Zhu, Mengze Cai, Kerui Cao, Xitong Chen, Yue Dai, Lifan Du, Tao Feng, Tao He, Jin Hu, Yijie Hu, Ziyu Jiang, Cheng Li, Xiang Li, Jing Liang, Chonghuan Liu, ZhenDong Liu, Haodong Mi, Yanhu Mo, Junjia Ni, Shixin Pei, Jingyu Shen, XiaoShuai Song, Cecilia Wang, Chaofan Wang, Kangyu Wang, Pei Wang, Tao Wang, Wei Wang, Ke Xiao, Mingyu Xu, Tiange Xu, Nan Ya, Siran Yang, Jianan Ye, Yaxing Zang, Duo Zhang, Junbo Zhang, Boren Zheng, Wanxi Deng, Ling Pan, Lin Qu, Wenbo Su, Jiamang Wang, Wei Wang, Hu Wei, Minggang Wu, Cheng Yu, Bing Zhao, Zhicheng Zheng, Bo Zheng",
      "institution": "ROCK & ROLL & IFLOW & DT Joint Team (Inferred from the author list and likely represents a collaboration, but no specific university or company is named. The domain is unclear from the provided text.)",
      "link": "https://arxiv.org/pdf/2512.24873",
      "code": null,
      "tags": [
        "agent system",
        "Agentic Learning Ecosystem (ALE)",
        "Interaction-based Policy Alignment (IPA)",
        "Terminal Bench Pro",
        "post-training",
        "trajectory generation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9408c7a8de1b2fe7261d28a9d7eb1d3df3afe81fc17a03cc9b2c09e332ac8782_w640_q70.webp",
      "contributions": "1. Introduces the Agentic Learning Ecosystem (ALE), an end-to-end infrastructure for developing agent LLMs, comprising the ROLL post-training framework, the ROCK sandbox manager, and the iFlow CLI agent framework. 2. Proposes a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks to improve long-horizon training stability. 3. Releases the ROME agent model, trained on over one million trajectories, and introduces the Terminal Bench Pro benchmark with improved scale and contamination control for rigorous evaluation.",
      "summary": "This paper addresses the lack of a principled, end-to-end ecosystem for developing agentic LLMs by introducing the Agentic Learning Ecosystem (ALE). ALE streamlines the agent development pipeline, and the authors use it to build and release the ROME agent model, which demonstrates strong performance on benchmarks, validating the effectiveness of their infrastructure.",
      "mindmap": "graph TB\n        A[Let It Flow: Agentic Crafting on Rock and Roll<br/>构建ROME模型于开放智能体学习生态中] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>Open-source community lacks a principled, end-to-end ecosystem for agent LLM development.] --> B1[阻碍/Block<br/>Hinders practical development and production adoption of agents.]\n        C[主要方法/Method<br/>Introduce Agentic Learning Ecosystem (ALE)] --> C1[组件/Components<br/>ROLL (post-training), ROCK (sandbox), iFlow CLI (agent framework)]\n        C --> C2[模型/Model<br/>Release ROME agent trained on 1M+ trajectories]\n        C --> C3[算法/Algorithm<br/>Propose Interaction-based Policy Alignment (IPA)]\n        D[关键结果/Results<br/>ROME achieves strong benchmark performance.] --> D1[基准/Benchmarks<br/>24.72% on Terminal-Bench 2.0, 57.40% on SWE-bench Verified]\n        D --> D2[新基准/New Benchmark<br/>Introduce Terminal Bench Pro for rigorous evaluation.]"
    },
    {
      "title": "Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing",
      "authors": "Andrii Gamalii, Daniel Górniak, Robert Nowak, Bartłomiej Olber, Krystian Radlak, Jakub Winter",
      "institution": "Warsaw University of Technology",
      "link": "https://arxiv.org/pdf/2512.24896",
      "code": null,
      "tags": [
        "3D object detection",
        "semi-automated annotation",
        "human-in-the-loop",
        "3D object detection",
        "data anonymization",
        "domain adaptation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/575844a7a43a73f8b8d00aa31dfa90a80dbb02b7129fa4cb48b23a397afe6e78_w640_q70.webp",
      "contributions": "1. A semi-automated annotation pipeline that combines AI-generated initial annotations with human verification to reduce cost and time. 2. A system architecture supporting iterative model retraining and incorporating data anonymization and domain adaptation techniques. 3. A methodology and toolset that accelerates the creation of a large-scale, multimodal autonomous driving dataset tailored to Polish road conditions.",
      "summary": "This paper addresses the costly and time-consuming problem of manually annotating large-scale, multimodal datasets for autonomous vehicles. It proposes a semi-automated, human-in-the-loop annotation pipeline that uses 3D object detection to generate initial labels, enabling iterative retraining and incorporating anonymization and adaptation techniques. The developed solution significantly reduces annotation time while ensuring high-quality, consistent labels, directly supporting the creation of a Polish-specific autonomous driving dataset.",
      "mindmap": "graph TB\n        Root[”Semi-Automated Data Annotation in Multisensor Datasets for Autonomous Vehicle Testing”] --> Problem[”核心问题/Problem: Manual annotation of multimodal AV datasets is costly and time-consuming.”]\n        Root --> Method[”主要方法/Method: A semi-automated, human-in-the-loop pipeline using 3D object detection for initial annotations.”]\n        Root --> Results[”关键结果/Results: Substantial time savings and consistent, high-quality annotations.”]"
    },
    {
      "title": "mHC: Manifold-Constrained Hyper-Connections",
      "authors": "Zhenda Xie, Yixuan Wei, Huanqi Cao, Chenggang Zhao, Chengqi Deng, Jiashi Li, Damai Dai, Huazuo Gao, Jiang Chang, Liang Zhao, Shangyan Zhou, Zhean Xu, Zhengyan Zhang, Wangding Zeng, Shengding Hu, Yuqing Wang, Jingyang Yuan, Lean Wang, Wenfeng Liang",
      "institution": "DeepSeek-AI",
      "link": "https://arxiv.org/pdf/2512.24880",
      "code": null,
      "tags": [
        "llm training",
        "Hyper-Connections",
        "residual connection",
        "identity mapping",
        "manifold constraint",
        "training stability"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7219c6945df5dfb5231231a93ccf8e3cf155e38527f2c4071501eaae05a8b7ac_w640_q70.webp",
      "contributions": "1. Proposes Manifold-Constrained Hyper-Connections (mHC), a framework that projects the residual connection space onto a specific manifold to restore the identity mapping property compromised by Hyper-Connections (HC). 2. Incorporates rigorous infrastructure optimization to address the memory access overhead and ensure training efficiency. 3. Demonstrates that mHC enables effective large-scale training with tangible performance improvements and superior scalability, offering a flexible and practical extension of HC.",
      "summary": "The paper identifies that Hyper-Connections (HC), while improving performance, lose the identity mapping property of standard residual connections, leading to training instability and memory overhead. To solve this, the authors propose Manifold-Constrained Hyper-Connections (mHC), which projects HC's connection space onto a manifold to restore identity mapping and includes infrastructure optimizations. Empirical results show mHC is effective for scalable training, offering better performance and stability.",
      "mindmap": "graph TB\n        A[mHC: Manifold-Constrained Hyper-Connections] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[”HC 破坏了恒等映射，导致训练不稳定/HC compromises identity mapping, causing instability”]\n        B --> B2[”HC 带来内存开销/HC incurs memory overhead”]\n        C --> C1[”将残差连接空间投影到特定流形/Project residual space onto a manifold”]\n        C --> C2[”恢复恒等映射属性/Restore identity mapping property”]\n        C --> C3[”结合基础设施优化/Incorporate infrastructure optimization”]\n        D --> D1[”实现可扩展的有效训练/Enables effective training at scale”]\n        D --> D2[”提供性能改进和可扩展性/Offers performance improvements & scalability”]"
    },
    {
      "title": "AI-Driven Cloud Resource Optimization for Multi-Cluster Environments",
      "authors": "Vinoth Punniyamoorthy, Akash Kumar Agarwal, Bikesh Kumar, Abhirup Mazumder, Kabilan Kannan, Sumit Saha",
      "institution": "IEEE (affiliations indicate authors are IEEE Senior Members, with industry affiliations from Albertsons and East West Bank, USA)",
      "link": "https://arxiv.org/pdf/2512.24914",
      "code": null,
      "tags": [
        "cluster infrastructure",
        "multi-cluster systems",
        "resource optimization",
        "predictive learning",
        "policy-aware decision-making",
        "cross-cluster telemetry"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02143e20715d24c6ab11a29c3710ca28e3f39c48ce36f9862a254d3564252726_w640_q70.webp",
      "contributions": "1. An AI-driven framework for adaptive resource optimization across multi-cluster cloud systems, moving beyond reactive, cluster-centric approaches. 2. Integration of predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management. 3. A prototype demonstrating improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional methods.",
      "summary": "This paper proposes an AI-driven framework to address the problem of inefficient and reactive resource management in multi-cluster cloud environments. The method uses predictive learning and policy-aware decision-making on cross-cluster telemetry to proactively optimize resource allocation for performance, cost, and reliability. The results show the framework improves resource efficiency and system stability compared to traditional approaches.",
      "mindmap": "graph TB\n        A[AI-Driven Cloud Resource Optimization for Multi-Cluster Environments] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法反应式且集群中心化 / Existing approaches are reactive and cluster-centric]\n        B --> B2[导致资源利用低效和延迟适应 / Causes inefficient resource utilization and delayed adaptation]\n        C --> C1[AI驱动框架集成预测学习 / AI-driven framework integrates predictive learning]\n        C --> C2[策略感知决策与持续反馈 / Policy-aware decision-making and continuous feedback]\n        C --> C3[分析跨集群遥测数据 / Analyzes cross-cluster telemetry]\n        D --> D1[提高资源效率 / Improved resource efficiency]\n        D --> D2[更快稳定于工作负载波动 / Faster stabilization during workload fluctuations]\n        D --> D3[减少性能变异 / Reduced performance variability]"
    },
    {
      "title": "Iterative Deployment Improves Planning Skills in LLMs",
      "authors": "Augusto B. Corrêa, Yoav Gelberg, Luckeciano C. Melo, Ilia Shumailov, André G. Pereira, Yarin Gal",
      "institution": "University of Oxford, AI Sequrity Company, UFRGS",
      "link": "https://arxiv.org/pdf/2512.24940",
      "code": null,
      "tags": [
        "reinforcement learning",
        "iterative deployment",
        "implicit reward",
        "data curation",
        "planning",
        "fine-tuning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8da35d1ea681d386cec51c012c9f81bb54c6876b6c1e632e59874f77690cd1a_w640_q70.webp",
      "contributions": "1. Demonstrates that iterative deployment and fine-tuning on curated user data significantly improves LLM planning skills, including emergent generalization to longer plans. 2. Provides a theoretical analysis showing iterative deployment effectively implements an outer-loop reinforcement learning process with an implicit reward function. 3. Highlights the AI safety implications of this implicit training regime and positions it as an alternative to explicit RL training.",
      "summary": "The paper shows that repeatedly deploying LLMs and fine-tuning them on curated data from previous deployments significantly improves their planning capabilities. This process is analyzed as an implicit form of reinforcement learning, which raises safety concerns due to the undefined reward function and offers an alternative training paradigm based on data curation.",
      "mindmap": "graph TB\n        A[Iterative Deployment Improves Planning Skills in LLMs] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLM规划能力/LLM Planning Skills]\n        C --> C1[迭代部署与微调/Iterative Deployment & Fine-tuning]\n        C1 --> C2[用户数据筛选/User Data Curation]\n        D --> D1[规划能力提升/Improved Planning Skills]\n        D --> D2[发现隐式RL/Discovering Implicit RL]\n        D2 --> D3[AI安全影响/AI Safety Implications]"
    },
    {
      "title": "RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment",
      "authors": "Chenji Lu, Zhuo Chen, Hui Zhao, Zhenyi Wang, Pengjie Wang, Jian Xu, Bo Zheng",
      "institution": "Taobao & Tmall Group of Alibaba",
      "link": "https://arxiv.org/pdf/2512.24943",
      "code": null,
      "tags": [
        "information retrieval",
        "relevance assessment",
        "benchmark",
        "long-tail",
        "visual salience",
        "e-commerce"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01d0a7f153d6f84b77a35da0a0f62dec9a8af10bfb23f1a8a481697233cbe992_w640_q70.webp",
      "contributions": "1. Proposes RAIR, a comprehensive Chinese benchmark for e-commerce relevance assessment derived from real-world scenarios. 2. Establishes a standardized evaluation framework with universal rules to address the lack of standardized metrics. 3. Introduces a dataset with three specialized subsets (general, long-tail hard, visual salience) to evaluate fundamental, challenging, and multimodal capabilities.",
      "summary": "The paper proposes RAIR, a rule-aware benchmark for e-commerce search relevance assessment, to address the lack of complex and standardized evaluation datasets. It introduces a comprehensive dataset with three subsets to test different model capabilities. Experiments on 14 models show RAIR is challenging, with GPT-5 performing best, and it serves as a new industry benchmark.",
      "mindmap": "graph TB\n        A[RAIR: 一个用于电子商务相关性评估的规则感知基准 / RAIR: A Rule-Aware Benchmark for E-commerce Relevance Assessment]\n        A --> B[核心问题/Problem: 现有基准缺乏复杂性，缺少标准化评估 / Existing benchmarks lack complexity and standardized evaluation]\n        A --> C[主要方法/Method: 提出包含通用、长尾、视觉显著性子集的基准和规则框架 / Propose benchmark with general, long-tail, visual-salience subsets and rule framework]\n        A --> D[关键结果/Results: 对14个模型构成挑战，GPT-5表现最佳，可作为行业基准 / Presents challenge to 14 models, GPT-5 performs best, serves as industry benchmark]"
    },
    {
      "title": "MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control",
      "authors": "Yongwei Zhang, Yuanzhe Xing, Quan Quan, Zhikun She",
      "institution": "Beihang University",
      "link": "https://arxiv.org/pdf/2512.24955",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Lyapunov certificates",
        "exponential stability",
        "multi-step learning",
        "actor-critic",
        "maximum entropy RL"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea102a46402567fc13871b6bc5f72e6c07e79e6ee87e8349aee1c18c8fc9627e_w640_q70.webp",
      "contributions": "1. Proposes a novel framework (MSACL) that integrates exponential stability theory with maximum entropy RL via multi-step Lyapunov certificate learning, using off-policy data to learn certificates that satisfy theoretical stability conditions. 2. Introduces Exponential Stability Labels (ESL) and a λ-weighted aggregation mechanism to effectively balance the bias-variance trade-off in multi-step learning. 3. Guides policy optimization with a stability-aware advantage function to ensure the learned policy promotes rapid Lyapunov descent, achieving provable stability and robustness under simple rewards.",
      "summary": "This paper proposes MSACL, a model-free reinforcement learning framework that ensures provable exponential stability by learning Lyapunov certificates from multi-step data and guiding policy optimization with a stability-aware advantage. It demonstrates superior performance over baseline and state-of-the-art Lyapunov-based RL methods across six benchmarks, achieving rapid convergence and robustness with simple rewards. The work establishes a link between Lyapunov theory and actor-critic frameworks for verifiably safe control.",
      "mindmap": "graph TB\n        A[MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Provable Stability in Model-Free RL / 模型无关RL的可证明稳定性]\n        C --> C1[Multi-Step Lyapunov Certificate Learning / 多步李雅普诺夫证书学习]\n        C --> C2[Stability-Aware Advantage Function / 稳定性感知优势函数]\n        D --> D1[Superiority over SOTA / 优于现有最优方法]\n        D --> D2[Exponential Stability & Robustness / 指数稳定性与鲁棒性]"
    },
    {
      "title": "Semi-overlapping Multi-bandit Best Arm Identification for Sequential Support Network Learning",
      "authors": "András Antos, András Millinghoffer, Péter Antal",
      "institution": "Budapest University of Technology and Economics, E-Group ICT Software Zrt.",
      "link": "https://arxiv.org/pdf/2512.24959",
      "code": null,
      "tags": [
        "multi-armed bandits",
        "semi-overlapping multi-bandit",
        "best arm identification",
        "sequential support network learning",
        "GapE algorithm",
        "sample complexity"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1ccf2e1312507d052da8a3c6c2b6fb042432d3f13e5efa2572b5d2cd1dff292_w640_q70.webp",
      "contributions": "1. Proposes a new pure-exploration model called the semi-overlapping multi-bandit (SOMMAB) for Sequential Support Network Learning (SSNL)., 2. Develops a generalized GapE algorithm for the SOMMAB setting., 3. Derives new exponential error bounds that improve the best-known constant in the exponent and scale linearly with the degree of overlap, showing sample complexity gains from shared evaluations.",
      "summary": "This paper introduces a new framework called Sequential Support Network Learning (SSNL) and models it as a semi-overlapping multi-bandit (SOMMAB) problem, where a single evaluation provides feedback to multiple bandits. The authors develop a generalized GapE algorithm for SOMMABs and prove new, improved error bounds that demonstrate significant sample-complexity reductions due to structural overlap.",
      "mindmap": "graph TB\n        Root[”Semi-overlapping Multi-bandit Best Arm Identification for Sequential Support Network Learning”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem: Selecting beneficial partners via shared, asymmetric evaluations”] --> P1[”问题领域/Application Domains: MTL, ATL, FL, MAS”]\n        Method[”主要方法/Method: SOMMAB model & generalized GapE algorithm”] --> M1[”模型/Model: Semi-overlapping multi-bandit”]\n        Results[”关键结果/Results: Improved error bounds & sample complexity gains”] --> R1[”理论保证/Theoretical: Exponential bounds scale with overlap”]"
    },
    {
      "title": "AMAP Agentic Planning Technical Report",
      "authors": "Yulan Hu, Xiangwen Zhang, Sheng Ouyang, Hao Yi, Lu Xu, Qinglin Lang, Lide Tan, Xiang Cheng, Tianchen Ye, Zhicong Li, Ge Chen, Wenjin Yang, Zheng Pan, Shaopan Xiong, Siran Yang, Ju Huang, Yan Zhang, Jiamang Wang, Yong Liu, Yinfeng Huang, Tucheng Lin, Xin Li, Ning Guo",
      "institution": "Alibaba (AMAP AI Agent LLM Team)",
      "link": "https://arxiv.org/pdf/2512.24957",
      "code": null,
      "tags": [
        "agent system",
        "tool-integrated reasoning",
        "spatio-temporal understanding",
        "cascaded training",
        "hierarchical data curation",
        "stable tool environment"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b6f557a1e8b6dbdc7fc3030b4817b52b9eae922a5f44ccb37d965540ecf2229_w640_q70.webp",
      "contributions": "1. A stable tool environment supporting over ten domain-specific tools for asynchronous rollout and training. 2. A hierarchical data curation framework that filters high-quality queries with a 1:10,000 ratio, emphasizing diversity and difficulty. 3. A cascaded training recipe involving a seed SFT stage to measure query difficulty, a second SFT stage on high-certainty data, and an RL stage on low-certainty data.",
      "summary": "This paper introduces STAgent, an agentic LLM specialized for spatio-temporal reasoning tasks like itinerary planning. It is empowered by a stable tool environment, a hierarchical data curation framework, and a cascaded training recipe. The model, initialized from Qwen3-30B-A3B, shows strong performance on TravelBench while maintaining general capabilities.",
      "mindmap": "graph TB\n        A[AMAP Agentic Planning Technical Report] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[缺乏解决现实世界时空推理任务的工具集成方案/Lack of TIR solutions for real-world spatio-temporal reasoning]\n        C --> C1[STAgent: 专用于时空理解的智能体模型/STAgent: Agentic LLM for spatio-temporal understanding]\n        C --> C2[稳定工具环境/Stable Tool Environment]\n        C --> C3[分层数据管理框架/Hierarchical Data Curation Framework]\n        C --> C4[级联训练方案/Cascaded Training Recipe]\n        D --> D1[在TravelBench上表现优异/Promising performance on TravelBench]\n        D --> D2[保持了广泛的通用能力/Maintains general capabilities across benchmarks]"
    },
    {
      "title": "HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films",
      "authors": "Rongji Xun, Junjie Yuan, Zhongjie Wang",
      "institution": "Tongji University, Shanghai Film Restoration Laboratory",
      "link": "https://arxiv.org/pdf/2512.24946",
      "code": "https://anonymous.4open.science/r/HaineiFRDM",
      "tags": [
        "video restoration",
        "diffusion model",
        "film restoration",
        "high-resolution video",
        "patch-wise training",
        "global-local frequency module"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49ddd01a0523d32c1f20822a4a1d270659f5454212011a90e04a39fe796d1ab_w640_q70.webp",
      "contributions": "1. Proposed HaineiFRDM, a film restoration framework leveraging diffusion models for content understanding to restore indistinguishable film defects. 2. Introduced a patch-wise training/testing strategy with a position-aware Global Prompt and Frame Fusion Module and a global-local frequency module to enable high-resolution restoration on a single 24GB GPU and ensure texture consistency. 3. Constructed a new film restoration dataset containing restored real-degraded films and realistic synthetic data.",
      "summary": "This paper proposes HaineiFRDM, a diffusion model-based framework for restoring high-resolution, real-world films. It addresses limitations of existing open-source methods by using a patch-wise strategy and novel modules to handle high-resolution videos efficiently and introduces a new dataset. Experiments show the model outperforms existing open-source methods in defect restoration.",
      "mindmap": "graph TB\n        A[HaineiFRDM: Explore Diffusion to Restore High-resolution Real-World Films] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[开源方法性能有限/Open-source methods have limited performance]\n        B --> B2[高分辨率电影未探索/High-resolution films unexplored]\n        C --> C1[基于扩散模型的修复框架/Diffusion-based restoration framework]\n        C --> C2[分块训练与测试策略/Patch-wise training & testing]\n        C --> C3[全局-局部频率模块/Global-local frequency module]\n        C --> C4[构建新数据集/Construct new dataset]\n        D --> D1[缺陷修复能力优越/Superior defect restoration ability]\n        D --> D2[代码与数据集将开源/Code & dataset to be released]"
    },
    {
      "title": "ShowUI-$π$: Flow-based Generative Models as GUI Dexterous Hands",
      "authors": "Siyuan Hu, Kevin Qinghong Lin, Mike Zheng Shou",
      "institution": "Show Lab, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.24965",
      "code": "https://github.com/showlab/showui-pi",
      "tags": [
        "human-computer interaction",
        "flow-based generative model",
        "GUI automation",
        "continuous trajectory prediction",
        "unified discrete-continuous actions",
        "ScreenDrag benchmark"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd6391f609bd9b67bc717f5e3756501bf8f4dedd5a207352ff5b4f02bc902207_w640_q70.webp",
      "contributions": "1. Proposed ShowUI-π, the first flow-based generative model for GUI dexterous manipulation, unifying discrete clicks and continuous drags in a shared model. 2. Introduced a flow-based action generation method for drag modeling, predicting incremental cursor adjustments from continuous visual observations. 3. Created ScreenDrag, a benchmark with 20K drag trajectories across five domains and comprehensive evaluation protocols to assess GUI agents' drag capabilities.",
      "summary": "This paper addresses the limitation of existing GUI agents that only perform discrete clicks, lacking the ability for continuous, closed-loop drag interactions. The authors propose ShowUI-π, a flow-based generative model that unifies discrete and continuous actions and generates smooth drag trajectories from visual observations. Experiments show ShowUI-π outperforms proprietary GUI agents on the new ScreenDrag benchmark, demonstrating effective dexterous control for GUI automation.",
      "mindmap": "graph TB\n    A[ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands] --> B[核心问题/Problem: Existing GUI agents only support discrete clicks, lacking continuous drag capability for closed-loop trajectories]\n    A --> C[主要方法/Method: Flow-based generative model with unified discrete-continuous actions and incremental trajectory prediction]\n    A --> D[关键结果/Results: Outperforms proprietary agents on ScreenDrag benchmark (score 26.98), demonstrating effective dexterous control]"
    },
    {
      "title": "Evaluating the Impact of Compression Techniques on the Robustness of CNNs under Natural Corruptions",
      "authors": "Itallo Patrick Castro Alves Da Silva, Emanuel Adler Medeiros Pereira, Erick de Andrade Barboza, Baldoino Fonseca dos Santos Neto, Marcio de Medeiros Ribeiro",
      "institution": "Federal University of Alagoas, Federal University of Rio Grande do Norte",
      "link": "https://arxiv.org/pdf/2512.24971",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "quantization",
        "pruning",
        "weight clustering",
        "robustness",
        "multiobjective assessment"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8640f8930863d5573a7df0bb7287b8800d92c67a60523b6b0dab2eb044c58bb_w640_q70.webp",
      "contributions": "1. Conducted a comprehensive evaluation of individual and combined compression techniques (quantization, pruning, weight clustering) on CNNs for robustness under natural corruptions. 2. Demonstrated that certain compression strategies can preserve or even improve model robustness, especially on complex architectures. 3. Utilized multiobjective assessment to identify optimal compression configurations that balance robustness, accuracy, and compression ratio for real-world deployment.",
      "summary": "This paper evaluates how compression techniques like quantization, pruning, and weight clustering affect the robustness of CNNs (ResNet-50, VGG-19, MobileNetV2) against natural image corruptions on CIFAR-10-C/100-C. It finds that specific compression strategies, particularly when combined, can maintain or enhance robustness, with multiobjective analysis revealing the best trade-offs for efficient and robust model deployment.",
      "mindmap": "graph TB\n        A[Evaluating the Impact of Compression Techniques on CNNs under Natural Corruptions] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[压缩模型在自然损坏下的鲁棒性/Robustness of compressed models under natural corruptions]\n        C --> C1[评估量化、剪枝、权重聚类技术/Evaluate quantization, pruning, weight clustering]\n        C --> C2[使用CIFAR-10/100-C数据集/Use CIFAR-10/100-C datasets]\n        C --> C3[多目标评估/Multiobjective assessment]\n        D --> D1[特定策略保持或提升鲁棒性/Certain strategies preserve or improve robustness]\n        D --> D2[定制组合产生有益结果/Customized combinations yield beneficial results]"
    },
    {
      "title": "DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments",
      "authors": "Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh",
      "institution": "Korea Advanced Institute of Science and Technology (KAIST), Pohang University of Science and Technology (POSTECH)",
      "link": "https://arxiv.org/pdf/2512.24985",
      "code": null,
      "tags": [
        "embodied vision-language reasoning",
        "low-light vision",
        "embodied question answering",
        "vision-language models",
        "image enhancement",
        "benchmark"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f144c0ff2baabb069f605975462919cef76b3f54919a8c9db67dab0432973003_w640_q70.webp",
      "contributions": "1. Introduces DarkEQA, the first benchmark for evaluating Embodied Question Answering (EQA) under multi-level, physics-based low-light conditions. 2. Features a physically faithful degradation pipeline that models illumination drop and sensor noise in linear RAW space, followed by an ISP-inspired renderer. 3. Systematically evaluates and reveals the limitations of state-of-the-art VLMs and the effectiveness of Low-Light Image Enhancement (LLIE) models as pre-processors in this challenging scenario.",
      "summary": "This paper identifies a gap in evaluating Vision-Language Models (VLMs) for embodied agents under low-light conditions and proposes DarkEQA, a new benchmark that simulates realistic dark environments. The benchmark uses a physics-based image degradation model to test VLM robustness and the utility of image enhancement techniques. The evaluation reveals significant performance drops in VLMs under low-light, highlighting a critical area for improvement in robust embodied AI.",
      "mindmap": "graph TB\n        A[DarkEQA: Benchmarking VLMs for EQA in Low-Light] --> B[核心问题/Problem: Existing EQA benchmarks overlook low-light conditions, a necessity for 24/7 robot operation.]\n        A --> C[主要方法/Method: Proposes DarkEQA benchmark with physics-based low-light simulation in RAW space and ISP pipeline.]\n        A --> D[关键结果/Results: Evaluates VLMs & LLIE models, systematically revealing VLM limitations under low-light.]"
    },
    {
      "title": "Classifying long legal documents using short random chunks",
      "authors": "Luis Adrián Cabrera-Diego",
      "institution": "Jus Mundi",
      "link": "https://arxiv.org/pdf/2512.24997",
      "code": null,
      "tags": [
        "document classification",
        "DeBERTa V3",
        "LSTM",
        "random chunks",
        "Temporal",
        "long document processing"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92192ce5f7c11c2d86da5e296a17a8e7ae1b0794ecb76a29cb686c4b5b4f5f12_w640_q70.webp",
      "contributions": "1. A novel legal document classifier architecture combining DeBERTa V3 with an LSTM that processes only 48 randomly selected short chunks (max 128 tokens) per document, enabling efficient handling of long texts. 2. A robust deployment pipeline built using Temporal, a durable execution framework, ensuring reliable and fault-tolerant processing workflows. 3. Demonstrated effective performance on a multilingual legal document dataset with a weighted F-score of 0.898 and quantified processing efficiency (498 seconds per 100 files on CPU).",
      "summary": "This paper addresses the challenge of classifying long legal documents by proposing a model that uses DeBERTa V3 and an LSTM to process only 48 randomly selected short text chunks per document. The method avoids the computational expense of processing full documents with Transformers and is deployed via a reliable Temporal-based pipeline. The system achieves a weighted F-score of 0.898 and processes 100 files in a median time of 498 seconds on CPU.",
      "mindmap": "graph TB\n        A[Classifying long legal documents using short random chunks] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Long legal documents are expensive/slow to process with full Transformers]\n        C[主要方法/Method<br>Classifier: DeBERTa V3 + LSTM on 48 random short chunks (128 tokens max)]\n        D[关键结果/Results<br>Weighted F-score: 0.898, Median time: 498s per 100 files (CPU)]"
    },
    {
      "title": "A Modal Logic for Possibilistic Reasoning with Fuzzy Formal Contexts",
      "authors": "Prosenjit Howlader, Churn-Jung Liau",
      "institution": "Institute of Information Science, Academia Sinica",
      "link": "https://arxiv.org/pdf/2512.24980",
      "code": null,
      "tags": [
        "modal logic",
        "weighted modal logic",
        "possibilistic reasoning",
        "formal concept analysis",
        "fuzzy formal contexts",
        "rough set theory"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0eedb37864559f4df3443ff6f46099ca8b0528be3387a300ecd0f5068e2a25f9_w640_q70.webp",
      "contributions": "1. Introduces a novel two-sort weighted modal logic with necessity and sufficiency operators for possibilistic reasoning in fuzzy formal contexts. 2. Provides a sound and complete axiomatization for the logic and its fragments with respect to fuzzy context models. 3. Shows the logic can represent generalized formal, object-oriented, and property-oriented concepts in fuzzy FCA and can be extended to multi-relational contexts.",
      "summary": "This paper introduces a new two-sort weighted modal logic designed for possibilistic reasoning with fuzzy formal contexts, featuring necessity and sufficiency operators. It provides a sound and complete axiomatization for this logic and demonstrates its expressive power by showing it can represent key generalized concepts from Formal Concept Analysis (FCA) in the fuzzy setting. The work also indicates the logic's potential for extension to reasoning with multi-relational fuzzy contexts.",
      "mindmap": "graph TB\n        Root[论文标题: A Modal Logic for Possibilistic Reasoning with Fuzzy Formal Contexts] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Reasoning with uncertainty in fuzzy formal contexts] --> P1[模糊形式背景中的可能性推理/Possibilistic reasoning in fuzzy formal contexts]\n        Method[主要方法/Method: A two-sort weighted modal logic] --> M1[引入两种加权模态算子/Introduces two weighted modal operators]\n        M1 --> M1a[必要性算子/Necessity (□)]\n        M1 --> M1b[充分性算子/Sufficiency (⊟)]\n        Results[关键结果/Results] --> R1[逻辑是可靠且完备的/Logic is sound and complete]\n        Results --> R2[可表示FCA中的广义概念/Can represent generalized FCA concepts]\n        Results --> R3[可扩展至多关系模糊背景/Extensible to multi-relational contexts]"
    },
    {
      "title": "Modeling Language as a Sequence of Thoughts",
      "authors": "Nasim Borazjanizadeh, James McClelland",
      "institution": "Independent Researcher, Stanford University",
      "link": "https://arxiv.org/pdf/2512.25026",
      "code": null,
      "tags": [
        "language modeling",
        "recurrent transformer",
        "thought gestalt",
        "reversal curse",
        "cross-attention",
        "scaling efficiency"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c8e6d868bbc8b6b68328f1680dd184805a29e94d0e7b58137122aad5c0a6e9d_w640_q70.webp",
      "contributions": "1. Introduced the Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels (tokens and sentence-level \"thought\" states). 2. Proposed a unified training scheme where token and sentence representations are generated with the same parameters and a single next-token objective, enabling gradient flow through memory. 3. Demonstrated improved data and parameter efficiency over GPT-2 and better performance on relational direction generalization (e.g., reversal curse).",
      "summary": "The paper addresses the limitations of standard Transformers, which rely on surface-level token statistics and lack globally consistent representations. It proposes the Thought Gestalt model, a recurrent Transformer that generates tokens while cross-attending to a memory of prior sentence-level \"thought\" states, trained with a unified next-token objective. The model shows improved scaling efficiency and reduces errors on relational generalization tasks like the reversal curse.",
      "mindmap": "graph TB\n        Root(”Modeling Language as a Sequence of Thoughts”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”Transformer 依赖表层统计/Transformers rely on surface-level statistics”)\n        Problem --> P2(”缺乏全局一致表示/Lack globally consistent representations”)\n        Problem --> P3(”导致逆转诅咒等问题/Leads to issues like reversal curse”)\n        Method --> M1(”提出思想完形模型/Propose Thought Gestalt (TG) model”)\n        Method --> M2(”双层建模: Token + 句子级思想/Two-level modeling: tokens & sentence-level thoughts”)\n        Method --> M3(”循环Transformer + 跨注意力记忆/Recurrent Transformer with cross-attention memory”)\n        Results --> R1(”比GPT-2更高效/More efficient than GPT-2”)\n        Results --> R2(”减少逆转诅咒错误/Reduces reversal curse errors”)\n        Results --> R3(”统一参数与目标训练/Unified parameter & objective training”)"
    },
    {
      "title": "Generative Classifiers Avoid Shortcut Solutions",
      "authors": "Alexander C. Li, Ananya Kumar, Deepak Pathak",
      "institution": "Carnegie Mellon University, Stanford University",
      "link": "https://arxiv.org/pdf/2512.25034",
      "code": "https://github.com/alexlioralexli/generative-classifiers",
      "tags": [
        "generative models",
        "generative classifiers",
        "spurious correlations",
        "distribution shift",
        "diffusion models",
        "autoregressive models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f744eff83ad768a9dc5e431ef5b2d98baefe24c12d01fc980aa2fa92c3c21c65_w640_q70.webp",
      "contributions": "1. Demonstrates that generative classifiers (using class-conditional generative models) inherently avoid shortcut learning by modeling all features, not just spurious ones. 2. Shows that generative classifiers achieve state-of-the-art performance on multiple image and text distribution shift benchmarks without specialized techniques. 3. Provides a theoretical analysis in a Gaussian toy setting to explain the inductive biases and data conditions favoring generative classifiers.",
      "summary": "The paper addresses the problem of discriminative classifiers learning spurious shortcuts that fail under distribution shift. It proposes using generative classifiers, which model p(x|y), and finds they avoid shortcuts and achieve state-of-the-art robustness on standard benchmarks without needing specialized training tricks. The main conclusion is that generative classifiers offer a simple and effective alternative for building more robust models.",
      "mindmap": "graph TB\n        A[Generative Classifiers Avoid Shortcut Solutions] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Discriminative models learn spurious shortcuts<br>判别模型学习虚假捷径]\n        C --> C1[Use class-conditional generative models<br>使用类条件生成模型]\n        C --> C2[Model p(x|y) instead of p(y|x)<br>建模 p(x|y) 而非 p(y|x)]\n        D --> D1[Avoid shortcuts & SOTA on distribution shift<br>避免捷径并在分布偏移上达到SOTA]\n        D --> D2[Simple training, no specialized techniques<br>训练简单，无需专门技术]"
    },
    {
      "title": "Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings",
      "authors": "Tianzhi He, Farrokh Jazizadeh",
      "institution": "The University of Texas at San Antonio, Virginia Polytechnic Institute and State University",
      "link": "https://arxiv.org/pdf/2512.25055",
      "code": null,
      "tags": [
        "agent system",
        "Large Language Model",
        "Building Energy Management System",
        "AI Agents",
        "Human-Building Interaction",
        "Context-aware"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d46efc47f789043b6987c8068e21aecb4ec53b645c2c1a61c1880ed06029103b_w640_q70.webp",
      "contributions": "1. Proposes a conceptual framework for LLM-based AI agents in BEMS, featuring a closed-loop system with perception, central control, and action modules. 2. Develops and benchmarks a prototype using real-world datasets and diverse metrics (latency, functionality, accuracy, cost-effectiveness), formalizing the assessment of such agents. 3. Demonstrates the framework's performance and generalizability, identifying strengths (e.g., high accuracy in device control) and areas for improvement (e.g., complex cost estimation).",
      "summary": "This paper proposes a framework for LLM-based AI agents to manage energy in smart buildings through natural language. The agent uses a closed-loop system to analyze data and control devices, and its evaluation shows promising accuracy in tasks like device control but highlights challenges in complex cost estimation.",
      "mindmap": "graph TB\n        Root[”Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings”] --> Problem[”核心问题/Problem: Existing BEMS lack context-aware, natural language interaction for energy management”]\n        Root --> Method[”主要方法/Method: Proposes a three-module LLM-based AI agent framework (perception, central control, action) for closed-loop management”]\n        Root --> Results[”关键结果/Results: Prototype shows high accuracy in device control (86%) and memory tasks (97%), but lower accuracy in cost estimation (49%)”]"
    },
    {
      "title": "AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG",
      "authors": "Chao Peng, Bin Wang, Zhilei Long, Jinfang Sheng",
      "institution": "Central South University, Yizhi Intelligent (YZInt)",
      "link": "https://arxiv.org/pdf/2512.25052",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "redundancy-aware selection",
        "token-budgeted RAG",
        "greedy selection",
        "submodular optimization",
        "adaptive calibration"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85c85942576679b5e5fe4c0066c0977620d02d122c659a34dfe900bfed59c445_w640_q70.webp",
      "contributions": "1. Proposes AdaGReS, a redundancy-aware context selection framework that optimizes a set-level objective combining query relevance and intra-set redundancy penalties under a token-budget constraint. 2. Introduces a closed-form, instance-adaptive calibration method for the relevance-redundancy trade-off parameter, eliminating manual tuning and adapting to candidate-pool statistics and budget limits. 3. Provides a theoretical analysis showing the proposed objective exhibits ε-approximate submodularity, yielding near-optimality guarantees for the greedy selection algorithm.",
      "summary": "The paper addresses the problem of redundant context in token-budgeted RAG systems, which wastes budget and degrades generation quality. It proposes AdaGReS, an adaptive greedy selection framework that scores and selects chunks by balancing relevance and redundancy, with a theoretically-backed near-optimal guarantee. Experiments on QA and biomedical datasets show it improves redundancy control and end-to-end answer quality.",
      "mindmap": "graph TB\n        A[AdaGReS: Adaptive Greedy Context Selection] --> B[核心问题/Problem: Top-k检索返回冗余块，浪费token预算并降低生成质量/Top-k retrieval returns redundant chunks, wasting token budget and degrading generation]\n        A --> C[主要方法/Method: 冗余感知的贪婪选择框架，结合相关性得分与冗余惩罚，并进行自适应参数校准/Redundancy-aware greedy selection framework with relevance-redundancy trade-off and adaptive calibration]\n        A --> D[关键结果/Results: 在开放域QA和生物医学语料上，冗余控制和上下文质量得到改善，提升了端到端答案质量/Improved redundancy control and context quality on open-domain QA and biomedical corpus, leading to better end-to-end answer quality]"
    },
    {
      "title": "Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search",
      "authors": "Rohit Dwivedula, Divyanshu Saxena, Sujay Yadalam, Daehyeok Kim, Aditya Akella",
      "institution": "The University of Texas at Austin",
      "link": "https://arxiv.org/pdf/2512.25065",
      "code": null,
      "tags": [
        "memory & caching",
        "heuristic synthesis",
        "evolutionary search",
        "instance-optimal",
        "LLM code generation",
        "cache eviction"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8daf2ac9cf0548fb6b41e5cb643f8b78cc3001bcb2d8b95b8ade5ced5201e53a_w640_q70.webp",
      "contributions": "1. Proposes Vulcan, a framework that recasts heuristic design as an automated search problem using LLMs to synthesize instance-optimal heuristics tailored to specific deployment contexts. 2. Introduces LLM-friendly, task-agnostic interfaces that separate policy and mechanism, making the synthesis tractable and enabling even small LLMs to generate correct code. 3. Demonstrates the framework's effectiveness by synthesizing heuristics for cache eviction and memory tiering that outperform state-of-the-art human-designed algorithms.",
      "summary": "The paper proposes Vulcan, a framework that uses LLM-driven evolutionary search to automatically synthesize instance-optimal system heuristics, tailored to specific workloads and hardware. It introduces task-agnostic interfaces to separate policy from mechanism, enabling efficient code generation. The synthesized heuristics for cache eviction and memory tiering were shown to outperform existing state-of-the-art algorithms.",
      "mindmap": "graph TB\n        Root[Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Manual heuristic design is slow and cannot adapt to changing hardware and workloads.]\n        Method[主要方法/Method: Use LLM-driven evolutionary search over task-agnostic interfaces to synthesize instance-optimal heuristics.]\n        Results[关键结果/Results: Synthesized heuristics outperform state-of-the-art algorithms in cache eviction and memory tiering.]"
    },
    {
      "title": "Coordinated Humanoid Manipulation with Choice Policies",
      "authors": "Haozhi Qi, Yen-Jen Wang, Toru Lin, Brent Yi, Yi Ma, Koushil Sreenath, Jitendra Malik",
      "institution": "UC Berkeley",
      "link": "https://arxiv.org/pdf/2512.25072",
      "code": "https://choice-policy.github.io",
      "tags": [
        "imitation learning",
        "humanoid robot",
        "teleoperation",
        "choice policy",
        "multimodal behavior",
        "whole-body coordination"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fc3a4a5fc6ec972fc1d7ab23cbd63d6c1c8efc8d326140cc34f70ea5a5cb65_w640_q70.webp",
      "contributions": "1. A modular teleoperation interface that decomposes humanoid control into intuitive submodules (e.g., hand-eye coordination, locomotion) for efficient, high-quality data collection. 2. The Choice Policy, a novel imitation learning architecture that generates multiple candidate actions and learns to score them, enabling fast inference and effective modeling of multimodal behaviors. 3. Empirical validation on real-world tasks (dishwasher loading, whiteboard wiping) showing superior performance over diffusion policies and behavior cloning, and highlighting the critical role of hand-eye coordination.",
      "summary": "This paper tackles the challenge of achieving robust whole-body coordination for humanoid robots in unstructured environments. It proposes a system combining a modular teleoperation interface for data collection with a novel \"Choice Policy\" for imitation learning, which scores multiple candidate actions. Experiments on real-world tasks demonstrate that this approach outperforms baseline methods and that hand-eye coordination is crucial for success in long-horizon manipulation.",
      "mindmap": "graph TB\n        A[Coordinated Humanoid Manipulation with Choice Policies] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[”实现人形机器人头、手、腿的鲁棒全身协调/Robust whole-body coordination for humanoids”]\n        C --> C1[”模块化遥操作接口/Modular teleoperation interface”]\n        C --> C2[”选择策略：生成并评估候选动作/Choice Policy: generate & score candidate actions”]\n        D --> D1[”在洗碗机装载、白板擦拭任务上超越基线/Outperforms baselines on dishwasher loading & whiteboard wiping”]\n        D --> D2[”手眼协调对长时域任务至关重要/Hand-eye coordination is critical for long-horizon tasks”]"
    },
    {
      "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
      "authors": "Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang, Joan Lasenby, Chun-Hao Huang",
      "institution": "University of Cambridge, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.25075",
      "code": "https://github.com/zheninghuang/Space-Time-Pilot",
      "tags": [
        "video generation",
        "video diffusion model",
        "space-time disentanglement",
        "temporal-warping training",
        "camera-conditioning",
        "generative rendering"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82552564f2c6cc5799df28c30493304ecd3600d22b5bcd81578cb3aaf6f15150_w640_q70.webp",
      "contributions": "1. Introduced an animation time-embedding mechanism for explicit motion sequence control in video diffusion. 2. Proposed a temporal-warping training scheme to repurpose multi-view datasets for learning temporal variations. 3. Created the CamxTime synthetic dataset and an improved camera-conditioning mechanism for precise dual space-time control.",
      "summary": "This paper introduces SpaceTimePilot, a video diffusion model that independently controls camera viewpoint and motion sequence to re-render dynamic scenes from a monocular video. The method uses a novel time-embedding mechanism and a temporal-warping training strategy to achieve robust space-time disentanglement. Experiments show the model enables continuous exploration across space and time, outperforming prior work.",
      "mindmap": "graph TB\n        A[SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time] --> B[核心问题/Problem: 如何从单目视频中解耦空间和时间以进行可控生成渲染/How to disentangle space and time from a monocular video for controllable generative rendering]\n        A --> C[主要方法/Method: 引入动画时间嵌入机制和时域扭曲训练方案/Introduce animation time-embedding and temporal-warping training scheme]\n        A --> D[关键结果/Results: 实现鲁棒的时空解耦与连续可控渲染/Achieve robust space-time disentanglement and continuous controllable rendering]"
    },
    {
      "title": "q3-MuPa: Quick, Quiet, Quantitative Multi-Parametric MRI using Physics-Informed Diffusion Models",
      "authors": "Shishuai Wang, Florian Wiesinger, Noemi Sgambelluri, Carolin Pirkl, Stefan Klein, Juan A. Hernandez-Tamames, Dirk H.J. Poot",
      "institution": "Erasmus MC (Erasmus University Medical Center)",
      "link": "https://arxiv.org/pdf/2512.23726",
      "code": null,
      "tags": [
        "medical imaging",
        "diffusion models",
        "quantitative MRI",
        "data consistency",
        "physics-informed",
        "multi-parametric mapping"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e081389f251cbbacaf7c704cd1a34c3a032b2173e63192833db976abd6541d5e_w640_q70.webp",
      "contributions": "1. Proposes a diffusion model-based method (q3-MuPa) for quantitative MRI mapping that combines a deep generative model with a physics-based data consistency constraint., 2. Enables high-quality mapping from a fourfold-accelerated, nearly silent MRI scan (MuPa-ZTE), reducing acquisition time to ~1 minute., 3. Demonstrates successful training on synthetic data from digital phantoms alone, with strong generalization to real patient and phantom scans.",
      "summary": "This paper proposes q3-MuPa, a method that uses a physics-informed diffusion model to generate high-quality quantitative MRI maps (T1, T2, proton density) from accelerated, silent scans. The method integrates a denoising diffusion model with the MRI signal physics as a constraint during inference. It achieves accurate mapping from 1-minute scans and generalizes well to real data despite being trained only on synthetic phantoms.",
      "mindmap": "graph TB\n        A[q3-MuPa: Quick, Quiet, Quantitative Multi-Parametric MRI] --> B(核心问题/Problem: Need for fast, quiet, and accurate quantitative MRI mapping)\n        A --> C(主要方法/Method: Physics-informed diffusion model with data consistency)\n        A --> D(关键结果/Results: High-accuracy maps from 1-min scans, trained on synthetic data)"
    },
    {
      "title": "Leveraging Machine Learning for Early Detection of Lung Diseases",
      "authors": "Bahareh Rahmani, Harsha Reddy Bindela, Rama Kanth Reddy Gosula, Krishna Yedubati, Mohammad Amir Salari, Leslie Hinyard, Payam Norouzzadeh, Eli Snir, Martin Schoen",
      "institution": "Saint Louis University, Washington University at Saint Louis",
      "link": "https://arxiv.org/pdf/2512.23757",
      "code": null,
      "tags": [
        "medical image analysis",
        "deep learning",
        "convolutional neural networks",
        "chest x-ray",
        "disease classification",
        "VGG16"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d5eaabf315a0348dff119282fff830baf854bc7edaf57b6601b94745f8aa312_w640_q70.webp",
      "contributions": "1. Proposes a diagnostic framework combining traditional image processing with advanced neural networks for lung disease detection. 2. Trains and validates multiple deep learning models (CNNs, VGG16, InceptionV3, EfficientNetB0) on chest x-rays for COVID-19, lung cancer, and pneumonia. 3. Demonstrates high accuracy, precision, recall, and F1 scores, highlighting the potential for real-world, non-invasive diagnostic applications in resource-limited settings.",
      "summary": "This paper proposes using deep learning models, including CNNs, VGG16, InceptionV3, and EfficientNetB0, to diagnose lung diseases like COVID-19, lung cancer, and pneumonia from chest x-rays. The method combines traditional image processing with neural networks to create a rapid, non-invasive diagnostic tool. The study concludes that these models achieve high performance metrics, showing reliability and potential for real-world healthcare applications, especially where radiologists are scarce.",
      "mindmap": "graph TB\n        A[Leveraging Machine Learning for Early Detection of Lung Diseases<br>利用机器学习进行肺部疾病早期检测] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Limited access to radiologists & resources<br>放射科医生和资源有限]\n        B --> B2[Need for rapid, non-invasive diagnosis<br>需要快速、无创诊断]\n        C --> C1[Combine image processing & neural networks<br>结合图像处理和神经网络]\n        C --> C2[Train models (CNN, VGG16, etc.) on chest X-rays<br>在胸部X光片上训练模型]\n        D --> D1[High accuracy, precision, recall, F1 scores<br>高准确率、精确率、召回率、F1分数]\n        D --> D2[Potential for real-world diagnostic applications<br>具有实际诊断应用潜力]"
    },
    {
      "title": "Quantum Error Mitigation with Attention Graph Transformers for Burgers Equation Solvers on NISQ Hardware",
      "authors": "Seyed Mohamad Ali Tousi, Adib Bazgir, Yuwen Zhang, G. N. DeSouza",
      "institution": "University of Missouri",
      "link": "https://arxiv.org/pdf/2512.23817",
      "code": null,
      "tags": [
        "others",
        "quantum error mitigation",
        "attention graph neural network",
        "NISQ hardware",
        "Burgers equation",
        "zero-noise extrapolation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57bb32110f5a354755f846f1b5a7ab41ac3fc4a701e97b9b25486f2bab0c72eb_w640_q70.webp",
      "contributions": "1. A hybrid quantum-classical framework for solving the viscous Burgers equation on NISQ hardware, using the Cole-Hopf transformation and Trotterized quantum circuits. 2. The creation of a large parametric dataset of noisy, ZNE-corrected, hardware, and classical solutions with circuit metadata for data-driven error mitigation. 3. A novel attention-based graph neural network model that ingests circuit features and noisy outputs to predict error-mitigated solutions, outperforming ZNE alone.",
      "summary": "This paper proposes a hybrid quantum-classical framework enhanced with a learned error mitigation model to solve the Burgers equation on noisy quantum hardware. The method uses an attention graph neural network trained on a dataset of noisy quantum simulations to predict corrected solutions. The results show the learned model consistently reduces errors beyond standard zero-noise extrapolation techniques.",
      "mindmap": "graph TB\n        A[Quantum Error Mitigation with Attention Graph Transformers for Burgers Equation Solvers on NISQ Hardware] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[在含噪声量子硬件上求解Burgers方程/Solving Burgers Equation on Noisy Quantum Hardware]\n        C --> C1[混合量子-经典框架与注意力图神经网络/Hybrid Quantum-Classical Framework with Attention GNN]\n        D --> D1[学习模型超越ZNE，减少量子-经典解差异/Learned Model Outperforms ZNE, Reduces Quantum-Classical Discrepancy]"
    },
    {
      "title": "Autoregressive long-horizon prediction of plasma edge dynamics",
      "authors": "Hunor Csala, Sebastian De Pascuale, Paul Laiu, Jeremy Lore, Jae-Sun Park, Pei Zhang",
      "institution": "Oak Ridge National Laboratory",
      "link": "https://arxiv.org/pdf/2512.23884",
      "code": null,
      "tags": [
        "surrogate modeling",
        "transformer",
        "autoregressive prediction",
        "plasma edge dynamics",
        "surrogate model",
        "long-horizon training"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f71c9629a318094d2cde5d048b18c5b331d3a235097acfb1e70f8caf9d3c603_w640_q70.webp",
      "contributions": "1. Proposes a transformer-based, autoregressive surrogate model for fast, long-horizon prediction of 2D plasma edge state fields (electron temperature, density, radiated power). 2. Demonstrates that training with longer autoregressive horizons systematically improves model rollout stability and mitigates error accumulation, enabling stable predictions over hundreds to thousands of time steps. 3. Shows the surrogate model is orders of magnitude faster than the high-fidelity SOLPS-ITER simulator, enabling rapid parameter exploration for fusion device design.",
      "summary": "This paper addresses the high computational cost of high-fidelity plasma edge simulations (SOLPS-ITER) by proposing a transformer-based autoregressive surrogate model. The model is trained on simulation data to predict key plasma state fields over long time horizons, and longer-horizon training is shown to improve prediction stability. The resulting surrogate is much faster than the original simulator, enabling rapid scenario exploration for fusion reactor design.",
      "mindmap": "graph TB\n        Root(”Autoregressive long-horizon prediction of plasma edge dynamics”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”高保真模拟计算成本高/High-fidelity simulation is computationally expensive”)\n        Method --> M1(”基于Transformer的自回归代理模型/Transformer-based autoregressive surrogate model”)\n        Method --> M2(”长时域训练/Long-horizon training”)\n        Results --> R1(”预测稳定，误差累积减少/Stable prediction, reduced error accumulation”)\n        Results --> R2(”速度比原模拟快数个数量级/Orders of magnitude faster than original simulator”)"
    },
    {
      "title": "A multimodal Transformer for InSAR-based ground deformation forecasting with cross-site generalization across Europe",
      "authors": "Wendong Yao, Binhua Huang, Soumyabrata Dev",
      "institution": "ADAPT SFI Research Centre, University College Dublin",
      "link": "https://arxiv.org/pdf/2512.23906",
      "code": null,
      "tags": [
        "remote sensing image analysis",
        "InSAR",
        "Transformer",
        "ground deformation forecasting",
        "cross-site generalization",
        "multimodal learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/36024dc5598b92b146557714c9e66eeb494b793bdcfaacb98b73cf6725cc8bfa_w640_q70.webp",
      "contributions": "1. Proposed a novel multimodal patch-based Transformer architecture for InSAR-based ground deformation nowcasting, integrating displacement snapshots with static kinematic indicators and temporal encodings. 2. Demonstrated superior performance of the proposed model over baseline models (CNN-LSTM, STGCN) on a test tile in eastern Ireland, achieving high accuracy (RMSE=0.90mm, R²=0.97). 3. Showcased strong cross-site generalization by training on one tile and applying the model without fine-tuning to five unseen European tiles, maintaining high performance (R²≥0.93) across diverse deformation patterns.",
      "summary": "This paper addresses the challenge of forecasting ground deformation from InSAR time series data. It proposes a multimodal Transformer model that combines recent displacement maps with kinematic indicators and temporal features to predict the next displacement epoch. The model achieves high accuracy and demonstrates strong generalization across different geographic sites in Europe without requiring retraining.",
      "mindmap": "graph TB\n        Root[论文标题: A multimodal Transformer for InSAR-based ground deformation forecasting] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: 如何利用历史InSAR数据预测未来的地表形变?] --> P1[挑战/Challenges: 长期趋势、季节周期、突变事件的叠加]\n        Method[主要方法/Method: 多模态Transformer] --> M1[输入/Inputs: 近期形变图、静态运动学指标、时间编码]\n        Method --> M2[任务/Task: 单步、固定间隔的下一时期临近预报]\n        Results[关键结果/Results] --> R1[性能/Performance: RMSE=0.90mm, R²=0.97 (爱尔兰测试集)]\n        Results --> R2[泛化/Generalization: 跨欧洲5个未见区域，R²≥0.93]"
    },
    {
      "title": "Generative Video Compression: Towards 0.01% Compression Rate for Video Transmission",
      "authors": "Xiangyu Chen, Jixiang Luo, Jingyu Xu, Fangqiu Yi, Chi Zhang, Xuelong Li",
      "institution": "Institute of Artificial Intelligence (TeleAI), China Telecom",
      "link": "https://arxiv.org/pdf/2512.24300",
      "code": null,
      "tags": [
        "communication & networking",
        "generative video compression",
        "task-oriented communication",
        "AI Flow",
        "compression-computation trade-off",
        "Level C Shannon-Weaver"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e0223eada26a73d6b029f6d32188e4919884b4dea1b6854eeb80d3b61f3d6ac_w640_q70.webp",
      "contributions": "1. Proposes Generative Video Compression (GVC), a novel framework leveraging generative video models to achieve extreme compression rates as low as 0.02%, 2. Introduces a paradigm shift by trading computation for bandwidth, shifting the reconstruction burden to the receiver using generative priors, 3. Presents a practical compression-computation trade-off strategy for fast inference on consumer-grade GPUs, enabling deployment within the AI Flow framework for constrained environments.",
      "summary": "This paper introduces Generative Video Compression (GVC), a framework that uses generative video models to achieve extreme compression rates by transmitting compact representations and reconstructing video at the receiver. It shifts the computational burden from transmission to inference and proposes a practical trade-off strategy for deployment. The work demonstrates a viable path for efficient video communication in bandwidth-constrained scenarios.",
      "mindmap": "graph TB\n        Root(”Generative Video Compression: Towards 0.01% Compression Rate for Video Transmission”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”能否实现0.01%极端压缩率?/Achieve 0.01% extreme compression rate?”)\n        Problem --> P2(”如何权衡计算与压缩?/Trade computation for compression?”)\n        Problem --> P3(”是否实用可部署?/Practical and deployable?”)\n        Method --> M1(”生成式视频压缩框架/GVC Framework”)\n        Method --> M2(”利用生成先验重建/Use generative priors for reconstruction”)\n        Method --> M3(”压缩-计算权衡策略/Compression-computation trade-off”)\n        Results --> R1(”实现~0.02%压缩率/Achieved ~0.02% compression rate”)\n        Results --> R2(”为AI Flow框架赋能/Enables AI Flow framework”)\n        Results --> R3(”开辟高效视频通信新范式/Opens new practical video communication paradigm”)"
    },
    {
      "title": "Automated Classification of First-Trimester Fetal Heart Views Using Ultrasound-Specific Self-Supervised Learning",
      "authors": "Youssef Megahed, Aylin Erman, Robin Ducharme, Mark C. Walker, Steven Hawken, Adrian D. C. Chan",
      "institution": "Carleton University, University of Ottawa, Ottawa Hospital Research Institute",
      "link": "https://arxiv.org/pdf/2512.24492",
      "code": null,
      "tags": [
        "medical image classification",
        "self-supervised learning",
        "masked autoencoder",
        "vision transformer",
        "ultrasound foundation model",
        "fetal echocardiography"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f73d9d90b381ff2a6f244e15b64b3e24d9b28e8c2fbe78fa50c79ef8f58d1c66_w640_q70.webp",
      "contributions": "1. Evaluated a self-supervised ultrasound foundation model (USF-MAE) for the challenging task of first-trimester fetal heart view classification. 2. Demonstrated that ultrasound-specific pretraining on a large, unlabeled dataset yields more transferable representations than models pretrained on natural images (ImageNet) or standard supervised CNNs. 3. Showed robust performance without the need for aggressive image preprocessing or region-of-interest cropping, and improved discrimination of non-diagnostic frames.",
      "summary": "This paper addresses the challenge of automated first-trimester fetal heart view classification in ultrasound by fine-tuning a self-supervised ultrasound foundation model (USF-MAE). The model, pretrained on over 370,000 unlabeled ultrasound images, outperformed supervised CNN baselines and a natural image-pretrained Vision Transformer, achieving over 90% accuracy. The results indicate that ultrasound-specific self-supervised pretraining enables more generalizable representations for early fetal cardiac imaging.",
      "mindmap": "graph TB\n        A[Automated Classification of First-Trimester Fetal Heart Views<br>早孕期胎儿心脏视图自动分类] --> B\n        A --> C\n        A --> D\n        B[Problem: Early detection of congenital heart disease is challenging<br>核心问题: 先天性心脏病早期检测困难]\n        C[Method: Fine-tune USF-MAE, an ultrasound-specific self-supervised model<br>主要方法: 微调超声专用自监督模型USF-MAE]\n        D[Results: Achieved SOTA 90.57% accuracy, outperforming baselines<br>关键结果: 达到90.57%准确率，超越基线模型]"
    },
    {
      "title": "Generative AI-enhanced Sector-based Investment Portfolio Construction",
      "authors": "Alina Voronina, Oleksandr Romanko, Ruiwen Cao, Roy H. Kwon, Rafael Mendoza-Arriaga",
      "institution": "Ukrainian Catholic University, SS&C Algorithmics, University of Toronto, Hong Kong Polytechnic University",
      "link": "https://arxiv.org/pdf/2512.24526",
      "code": null,
      "tags": [
        "quantitative finance",
        "portfolio optimization",
        "large language models",
        "sector-based investment",
        "Sharpe ratio",
        "regime shift"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76999779560abd5e04b9d83f5cf53223a1c444ffacf2142f6df9f0e236e87dd0_w640_q70.webp",
      "contributions": "1. Conducts a multi-model, cross-provider evaluation of LLMs (OpenAI, Google, Anthropic, DeepSeek, xAI) for stock selection in quantitative portfolio construction. 2. Demonstrates a strong temporal dependence in LLM portfolio performance, showing outperformance in stable markets but underperformance in volatile periods. 3. Proposes and validates a hybrid framework that improves performance and consistency by combining LLM-based stock selection with classical portfolio optimization techniques.",
      "summary": "This paper investigates using LLMs from multiple providers to select and weight stocks for sector-based portfolios. The study finds that while LLM-weighted portfolios can outperform sector indices in stable markets, they struggle during volatile periods; however, combining LLM selection with traditional optimization improves outcomes. The results highlight the potential and current limitations of LLMs in investment management, advocating for hybrid AI-quantitative frameworks.",
      "mindmap": "graph TB\n        A[Generative AI-enhanced Sector-based Investment Portfolio Construction] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: How can LLMs be applied to quantitative sector-based portfolio construction?]\n        C[主要方法/Method: Prompt LLMs to select/weight stocks, combine with classical portfolio optimization, evaluate across stable and volatile periods.]\n        D[关键结果/Results: LLM performance is market-dependent; hybrid frameworks (LLM + optimization) improve performance and consistency.]"
    },
    {
      "title": "An Adaptive, Disentangled Representation for Multidimensional MRI Reconstruction",
      "authors": "Ruiyang Zhao, Fan Lam",
      "institution": "University of Illinois Urbana-Champaign",
      "link": "https://arxiv.org/pdf/2512.24674",
      "code": null,
      "tags": [
        "medical image reconstruction",
        "disentangled representation",
        "latent diffusion model",
        "self-supervised learning",
        "multidimensional MRI",
        "zero-shot adaptation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f858a20b731ce7ed66cd4e854bc6e6bdd5ae01548b1060d4814f643684ce73a0_w640_q70.webp",
      "contributions": "1. A novel learned feature-based image representation that disentangles features like geometry and contrast into distinct latent spaces. 2. The integration of a latent diffusion model to impose stronger constraints on the disentangled feature spaces. 3. New reconstruction formulations and algorithms that combine the learned representation with zero-shot self-supervised learning adaptation and subspace modeling.",
      "summary": "This paper proposes a new method for reconstructing multidimensional MRI data by learning a disentangled image representation that separates features like geometry and contrast into distinct latent spaces, enhanced by a latent diffusion model. The approach integrates this representation with zero-shot self-supervised learning, enabling improved reconstruction without task-specific training. It demonstrates superior performance on accelerated T1 and T2 parameter mapping compared to state-of-the-art methods.",
      "mindmap": "graph TB\n        Root[”An Adaptive, Disentangled Representation for Multidimensional MRI Reconstruction”] --> Problem[”核心问题/Problem: Limited data for task-specific training in multidimensional MRI reconstruction”]\n        Root --> Method[”主要方法/Method: Disentangled representation + Latent diffusion model + Zero-shot self-supervised adaptation”]\n        Root --> Results[”关键结果/Results: Improved performance on T1/T2 mapping without task-specific training”]"
    },
    {
      "title": "AstroReview: An LLM-driven Multi-Agent Framework for Telescope Proposal Peer Review and Refinement",
      "authors": "Yutong Wang, Yunxiang Xiao, Yonglin Tian, Junyong Li, Jing Wang, Yisheng Lv",
      "institution": "Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.24754",
      "code": null,
      "tags": [
        "agent system",
        "multi-agent framework",
        "automated peer review",
        "LLM-driven",
        "reasoning traces",
        "iterative refinement"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74800e2a96855acce45d842bc6dfc7d6ea4c32d6def9f45717a69483ec603078_w640_q70.webp",
      "contributions": "1. An open-source, multi-agent framework (AstroReview) that automates telescope proposal review in three structured stages (novelty/merit, feasibility/yield, meta-review/verification). 2. A design employing task isolation and explicit reasoning traces to curb LLM hallucinations and improve review transparency and auditability. 3. An \"AstroReview in Action\" module that demonstrates an iterative review-refinement loop, increasing proposal acceptance rates by 66% after two revision cycles.",
      "summary": "The paper presents AstroReview, an LLM-driven multi-agent framework designed to automate and scale the peer review process for astronomical telescope proposals. The system operates in three stages to assess scientific merit, feasibility, and review reliability, using task isolation and reasoning traces to improve transparency. The results show it can identify accepted proposals with 87% accuracy and, through an iterative feedback loop, significantly improve the quality and acceptance rate of revised proposals.",
      "mindmap": "graph TB\n        A[AstroReview: An LLM-driven Multi-Agent Framework for Telescope Proposal Peer Review and Refinement] --> B[核心问题/Problem: 望远镜提案评审成为瓶颈/Proposal review is a bottleneck]\n        A --> C[主要方法/Method: 三阶段多智能体框架/Three-stage multi-agent framework]\n        A --> D[关键结果/Results: 87%准确率，接受率提升66%/87% accuracy, 66% acceptance rate increase]\n        B --> B1[提案量超过可用时间/Proposal volume > telescope time]\n        C --> C1[新颖性与科学价值/Novelty & scientific merit]\n        C --> C2[可行性与预期产出/Feasibility & expected yield]\n        C --> C3[元评审与可靠性验证/Meta-review & reliability verification]\n        D --> D1[正确识别已接受提案/Correctly identifies accepted proposals]\n        D --> D2[迭代反馈提升提案质量/Iterative feedback improves proposal quality]"
    },
    {
      "title": "The Impact of LLMs on Online News Consumption and Production",
      "authors": "Hangcheng Zhao, Ron Berman",
      "institution": "Rutgers Business School, The Wharton School of the University of Pennsylvania",
      "link": "https://arxiv.org/pdf/2512.24968",
      "code": null,
      "tags": [
        "ai economics",
        "staggered difference-in-differences",
        "synthetic difference-in-differences",
        "robots.txt"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7836df9e705d8a023a351c30d3595b1ff6e9614cf1d805218347987098aa3882_w640_q70.webp",
      "contributions": "1. Quantified a moderate decline in news publisher website traffic following the rise of generative AI. 2. Demonstrated that blocking GenAI bots via robots.txt can paradoxically reduce total and real consumer traffic for large publishers. 3. Provided empirical evidence that, contrary to predictions, LLMs have not yet reduced editorial hiring and have shifted publisher content strategy towards rich media and advertising.",
      "summary": "This paper empirically investigates the impact of Large Language Models (LLMs) on online news publishers using high-frequency data and causal inference methods like difference-in-differences. It finds that blocking LLM crawlers reduces publisher traffic, LLMs have not yet replaced editorial jobs, and publishers are shifting to rich content and advertising tech. The results reveal unforeseen consequences of LLM adoption on the news ecosystem.",
      "mindmap": "graph TB\n        A[The Impact of LLMs on Online News Consumption and Production] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLMs如何影响新闻生产与消费/How LLMs affect news production and consumption]\n        C --> C1[高频数据与因果推断/High-frequency data & Causal inference]\n        D --> D1[流量下降/Traffic decline]\n        D --> D2[屏蔽爬虫反效果/Blocking bots backfires]\n        D --> D3[编辑岗位未减少/Editorial jobs not reduced]\n        D --> D4[内容转向富媒体/Shift to rich content]"
    },
    {
      "title": "SymSeqBench: a unified framework for the generation and analysis of rule-based symbolic sequences and datasets",
      "authors": "Barna Zajzon, Younes Bouhadjar, Maxime Fabre, Felix Schmidt, Noah Ostendorf, Emre Neftci, Abigail Morrison, Renato Duarte",
      "institution": "Jülich Research Centre, RWTH Aachen University, University of Groningen, University of Coimbra",
      "link": "https://arxiv.org/pdf/2512.24977",
      "code": null,
      "tags": [
        "sequence learning",
        "formal language theory",
        "symbolic sequences",
        "benchmark suite",
        "cognitive modeling",
        "sequence processing"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71331410faecaa464fb09419a580962b2cddad2a5e128910f8482dc81e965858_w640_q70.webp",
      "contributions": "1. Introduces SymSeq, a tool for the rigorous generation and analysis of structured symbolic sequences. 2. Introduces SeqBench, a comprehensive benchmark suite of rule-based sequence processing tasks for evaluating AI systems. 3. Provides a unified, domain-agnostic framework (SymSeqBench) based on Formal Language Theory to standardize experiments across cognitive science and AI.",
      "summary": "The paper introduces SymSeqBench, a unified software framework combining a symbolic sequence generator/analyzer (SymSeq) and a benchmark suite (SeqBench) for evaluating sequence learning. It is based on Formal Language Theory to provide a domain-agnostic, formal link between computation and cognition. The main conclusion is that this modular, open-source tool offers a versatile and standardized way to investigate sequential structure across diverse fields like psycholinguistics, cognitive psychology, and AI.",
      "mindmap": "graph TB\n        Root[SymSeqBench: 统一框架] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[评估序列学习/Evaluating Sequence Learning]\n        Problem --> P2[领域无关的评估/Domain-Agnostic Evaluation]\n        Problem --> P3[连接形式理论与认知/Linking Formal Theory & Cognition]\n        Method --> M1[SymSeq: 生成与分析/SymSeq: Generation & Analysis]\n        Method --> M2[SeqBench: 基准测试套件/SeqBench: Benchmark Suite]\n        Method --> M3[基于形式语言理论/Based on Formal Language Theory]\n        Results --> R1[跨领域多功能/Versatile Across Domains]\n        Results --> R2[标准化实验/Standardizes Experiments]\n        Results --> R3[模块化开源工具/Modular Open-Source Tool]"
    },
    {
      "title": "GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems",
      "authors": "Jithin VG, Ditto PS",
      "institution": "Bud Ecosystem Inc",
      "link": "https://arxiv.org/pdf/2512.22125",
      "code": "https://github.com/BudEcosystem/GPU-Virt-Bench",
      "tags": [
        "llm inference",
        "GPU Virtualization",
        "Benchmarking",
        "Multi-tenancy",
        "CUDA",
        "Performance Isolation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a1c9f2d4dfba1fc452a424ad0f1298f01afe6d95dfd39dd2ff3f0c1bac9430c_w640_q70.webp",
      "contributions": "1. Proposed GPU-Virt-Bench, a comprehensive benchmarking framework with 56 metrics across 10 categories for evaluating software-based GPU virtualization systems. 2. Enabled systematic comparison between software virtualization approaches (e.g., HAMi-core, BUD-FCSP) and ideal hardware-based MIG behavior. 3. Demonstrated the framework's utility by revealing critical performance characteristics for production deployment decisions in multi-tenant environments.",
      "summary": "This paper addresses the lack of standardized evaluation for software-based GPU virtualization systems, which are needed for efficient GPU sharing in AI/LLM workloads. The authors propose GPU-Virt-Bench, a comprehensive benchmarking framework that measures performance across multiple critical dimensions. The framework provides actionable insights for practitioners by comparing software solutions against hardware-based baselines.",
      "mindmap": "graph TB\n        A[GPU-Virt-Bench: A Comprehensive Benchmarking Framework] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[GPU资源共享需求高，但软件虚拟化方案缺乏标准化评估/High demand for GPU sharing, but software virtualization lacks standardized evaluation]\n        C --> C1[提出包含56个指标、10个类别的综合基准测试框架/Propose a comprehensive benchmarking framework with 56 metrics across 10 categories]\n        D --> D1[系统比较软件方案与MIG，为生产部署提供关键性能洞察/Systematic comparison between software approaches and MIG provides key performance insights for deployment]"
    },
    {
      "title": "ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling",
      "authors": "Conor Wallace, Umer Siddique, Yongcan Cao",
      "institution": "University of Texas at San Antonio",
      "link": "https://arxiv.org/pdf/2512.22129",
      "code": null,
      "tags": [
        "multi-agent reinforcement learning",
        "ad-hoc teamwork",
        "retrieval-augmented generation",
        "teammate modeling",
        "Overcooked"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/078fe396118022eaa0d391e86072d08c5b6617a143aba1478029ca3185af6472_w640_q70.webp",
      "contributions": "1. Introduces COLLAB, a novel language-based framework that uses LLMs as behavioral world models to classify unseen teammate types in ad-hoc teamwork. 2. Extends COLLAB to RECOLLAB by incorporating retrieval-augmented generation (RAG) with exemplar trajectories to stabilize inference and improve adaptation. 3. Demonstrates empirically in the Overcooked environment that RECOLLAB achieves Pareto-optimal trade-offs between classification accuracy and episodic return, highlighting the value of retrieval grounding.",
      "summary": "This paper addresses the challenge of ad-hoc teamwork, where an agent must collaborate with unseen teammates. It proposes RECOLLAB, a framework that uses retrieval-augmented LLMs to model and classify teammate behavior from short interaction traces. The method is shown to effectively improve adaptation and coordination in the cooperative Overcooked environment.",
      "mindmap": "graph TB\n        Root(”RECOLLAB: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”Ad-hoc Teammate Modeling<br>Ad-hoc队友建模”)\n        Problem --> P2(”Brittle Conventional Models<br>传统模型脆弱性”)\n        Method --> M1(”COLLAB: LLM-based Framework<br>基于LLM的框架”)\n        Method --> M2(”RECOLLAB: Adds RAG<br>增加RAG检索”)\n        Results --> R1(”Improved Adaptation<br>提升适应性”)\n        Results --> R2(”Pareto-Optimal Trade-offs<br>帕累托最优权衡”)"
    },
    {
      "title": "SoDA: An Efficient Interaction Paradigm for the Agentic Web",
      "authors": "Zicai Cui, Zhouyuan Jian, Weiwen Liu, Weinan Zhang",
      "institution": "Shanghai Jiao Tong University, Shanghai Innovation Institute",
      "link": "https://arxiv.org/pdf/2512.22135",
      "code": null,
      "tags": [
        "agent system",
        "Sovereign Digital Avatar",
        "Intent-Permission Handshake",
        "orthogonal decoupling",
        "A2A protocols",
        "dual-factor adaptive routing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef4bb1bba1b84bcf1102a80dc39b16d212412d68a7abea9ab0aac1dc9e23dedb_w640_q70.webp",
      "contributions": "1. Proposes a user sovereignty interaction paradigm for the Agentic Web, decoupling memory from application logic to break data lock-in and shifting from explicit instruction to implicit intent alignment to reduce cognitive load. 2. Implements the paradigm via the Sovereign Digital Avatar (SoDA) with an orthogonal decoupling design of storage, computation, and interaction, establishing the principle of \"data as a persistent asset, model as a transient tool\". 3. Designs an Intent-Permission Handshake Mechanism based on A2A protocols with dual-factor adaptive routing for active risk governance in zero-trust environments.",
      "summary": "This paper proposes the Sovereign Digital Avatar (SoDA), a new interaction paradigm for the Agentic Web that decouples user memory from applications and uses intent alignment to reduce cognitive load. It introduces an architecture with orthogonal decoupling and a secure handshake mechanism for zero-trust environments. Empirical results show it significantly reduces token consumption and user cognitive load compared to existing methods.",
      "mindmap": "graph TB\n        A[SoDA: An Efficient Interaction Paradigm for the Agentic Web] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[数据锁定/Data Lock-in]\n        B --> B2[认知过载/Cognitive Overload]\n        C --> C1[主权数字化身/Sovereign Digital Avatar (SoDA)]\n        C --> C2[正交解耦设计/Orthogonal Decoupling Design]\n        C --> C3[意图-权限握手机制/Intent-Permission Handshake Mechanism]\n        D --> D1[降低令牌消耗/Reduces Token Consumption by 27-35%]\n        D --> D2[降低认知负载/Reduces Cognitive Load by 72% vs RAG, 88% vs Manual]"
    },
    {
      "title": "HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA",
      "authors": "Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn",
      "institution": "National University of Sciences and Technology (Pakistan), RPTU Kaiserslautern-Landau (Germany)",
      "link": "https://arxiv.org/pdf/2512.22139",
      "code": "https://github.com/dll-ncai/HLS4PC",
      "tags": [
        "on-device ai",
        "FPGA",
        "HLS",
        "Point Cloud",
        "Model Compression",
        "Fixed-Point"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21433bfda0767bbdfcee46f13fb3acd9373d13bb741d87a755643767c9ad74f9_w640_q70.webp",
      "contributions": "1. Proposed HLS4PC, a parameterizable HLS framework for accelerating point-based 3D point cloud models on FPGA. 2. Introduced PointMLP-Lite, a 4x less complex model variant created via hardware-aware compression techniques (URS, quantization, pruning, fusion). 3. Demonstrated FPGA acceleration achieving 3.56x higher throughput than prior work and outperforming GPU/CPU implementations.",
      "summary": "This paper addresses the challenge of real-time 3D point cloud processing by proposing HLS4PC, a parameterizable FPGA acceleration framework. The method combines algorithmic optimizations and hardware-aware model compression to create an efficient fixed-point implementation, which significantly outperforms previous accelerators and GPU/CPU baselines in throughput.",
      "mindmap": "graph TB\n        Root[HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[GPU under-utilization due to sparse, unstructured point cloud data]\n        P1 --> P2[High memory/computation demand hinders real-time performance]\n        Method[主要方法/Method] --> M1[Parameterizable HLS framework for FPGA]\n        M1 --> M2[Hardware-aware compression: URS, quantization, pruning, fusion]\n        M2 --> M3[Creates PointMLP-Lite model]\n        Results[关键结果/Results] --> R1[PointMLP-Lite: 4x less complex, ~2% accuracy drop]\n        R1 --> R2[3.56x higher throughput vs. prior work]\n        R2 --> R3[2.3x (GPU) and 22x (CPU) higher throughput]"
    },
    {
      "title": "Pre-review to Peer review: Pitfalls of Automating Reviews using Large Language Models",
      "authors": "Akhil Pandey Akella, Harish Varma Siravuri, Shaurya Rohatgi",
      "institution": "AllSci Corp, Sunwater Capital, Kellogg School of Management (Northwestern University), Northern Illinois University, MBZUAI",
      "link": "https://arxiv.org/pdf/2512.22145",
      "code": null,
      "tags": [
        "peer review automation",
        "large language models",
        "peer review",
        "pre-review",
        "citation prediction",
        "review alignment"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff267a101523eaa0ec56d561e9fa2c165c73baa1b3016d38df1ed64dbc91dcf6_w640_q70.webp",
      "contributions": "1. Conducted a systematic evaluation of frontier open-weight LLMs for generating peer reviews, measuring alignment with human reviewers and correlation with post-publication metrics like citations and novelty. 2. Identified key pitfalls of LLMs as autonomous reviewers, including weak correlation with human scores (0.15), systematic overestimation bias (3-5 points), and uniformly high confidence scores despite errors. 3. Demonstrated the potential utility of LLMs as pre-review screening agents, as their generated reviews correlate more strongly with post-publication outcomes than with human reviewer scores, and released an open-source dataset (DLMRSD) to support further safety research.",
      "summary": "This paper evaluates the use of large language models (LLMs) for automating academic peer review by comparing LLM-generated reviews against human reviewer scores and post-publication metrics. The study finds that while LLMs show weak alignment with human reviewers and exhibit overconfidence and bias, their reviews correlate better with future citation impact, suggesting they could serve as useful pre-review screening tools rather than fully autonomous reviewers.",
      "mindmap": "graph TB\n        A[Pre-review to Peer Review: Pitfalls of Automating Reviews using Large Language Models] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLMs用于自动化同行评审的安全性与可靠性/Safety & Reliability of Automating Peer Review with LLMs]\n        C --> C1[使用前沿开源LLMs生成评审并与人类评分及发表后指标对比/Using Frontier Open-Weight LLMs to Generate Reviews vs. Human Scores & Post-Publication Metrics]\n        D --> D1[LLMs与人类评审员弱相关，存在高估偏差与过度自信/Weak Correlation with Humans, Overestimation Bias, High Confidence]\n        D --> D2[LLM评审与发表后指标相关性更强，适合预审筛查/LLM Reviews Correlate More with Post-Publication Metrics, Suitable for Pre-Review Screening]"
    },
    {
      "title": "GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs",
      "authors": "Ruifan Chu, Anbang Wang, Xiuxiu Bai, Shuai Liu, Xiaoshe Dong",
      "institution": "School of Software Engineering, Xi’an Jiaotong University",
      "link": "https://arxiv.org/pdf/2512.22147",
      "code": null,
      "tags": [
        "gpu kernels",
        "Minimal Executable Program (MEP)",
        "Automatic Error Repair",
        "Performance Pattern Inheritance",
        "iterative optimization",
        "cross-platform"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp",
      "contributions": "1. Proposes an end-to-end LLM framework that optimizes GPU kernels by constructing Minimal Executable Programs (MEPs) to avoid expensive full application builds and executions. 2. Introduces Automatic Error Repair and Performance Pattern Inheritance to automatically fix faults and reuse effective optimization strategies, reducing search cost. 3. Demonstrates cross-platform portability and effectiveness on NVIDIA GPUs and the Haiguang DCU platform, achieving significant speedups over direct LLM optimization.",
      "summary": "The paper addresses the high cost of full builds for GPU kernel optimization in large HPC applications by proposing an LLM framework that uses Minimal Executable Programs (MEPs) for iterative optimization. The method integrates automatic error repair and performance pattern inheritance to maintain correctness and reuse strategies. It achieves substantial speedups across different hardware platforms without requiring full-source dependencies.",
      "mindmap": "graph TB\n        A[GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Full builds & runs are expensive in large applications/大型应用中完整构建与运行成本高]\n        C --> C1[Construct Minimal Executable Program (MEP) for kernel/为内核构建最小可执行程序]\n        C --> C2[Multi-round iterative optimization with LLM feedback/基于LLM反馈的多轮迭代优化]\n        C --> C3[Integrate Automatic Error Repair & Performance Pattern Inheritance/集成自动错误修复与性能模式继承]\n        D --> D1[Achieves significant speedups (e.g., 5.05x, 7.77x)/获得显著加速比]\n        D --> D2[Cross-platform portability (NVIDIA, DCU)/跨平台可移植性]\n        D --> D3[Surpasses direct LLM optimization/超越直接LLM优化]"
    },
    {
      "title": "Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments",
      "authors": "Guilin Zhang, Wulan Guo, Ziqi Tan",
      "institution": "George Washington University",
      "link": "https://arxiv.org/pdf/2512.22149",
      "code": null,
      "tags": [
        "agent system",
        "serverless computing",
        "GPU resource allocation",
        "workload scheduling",
        "multi-agent systems",
        "collaborative reasoning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2fe7e30427c00e4689f161fb9912d4d11cc091ed6dd1dae3c4ea2c5805084e3b_w640_q70.webp",
      "contributions": "1. An adaptive GPU resource allocation framework for multi-agent systems in serverless environments that dynamically adjusts resources based on workload characteristics, agent priorities, and minimum requirements. 2. An O(N) complexity algorithm for real-time adaptation, enabling millisecond-scale reallocation to handle dynamic workload fluctuations. 3. A comprehensive evaluation demonstrating the framework's superiority over static and round-robin strategies, achieving 85% latency reduction while maintaining throughput and improving GPU utilization and cost-efficiency.",
      "summary": "This paper proposes an adaptive GPU resource allocation framework to address the challenge of efficiently deploying heterogeneous multi-agent AI systems on serverless platforms. The method dynamically allocates resources using a real-time algorithm to handle varying computational demands and workload fluctuations. The results show it significantly reduces latency compared to baseline schedulers while maintaining throughput, offering a cost-effective solution for serverless multi-agent deployment.",
      "mindmap": "graph TB\n        Root[”Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments<br/>面向无服务器环境的多智能体协同推理的自适应GPU资源分配”] --> Problem[”核心问题/Problem<br/>Heterogeneous agent workloads & dynamic demands on serverless GPU platforms<br/>多智能体工作负载异构与无服务器GPU平台动态需求”]\n        Root --> Method[”主要方法/Method<br/>Adaptive GPU resource allocation framework with O(N) real-time algorithm<br/>基于O(N)实时算法的自适应GPU资源分配框架”]\n        Root --> Results[”关键结果/Results<br/>85% latency reduction vs. round-robin, maintains throughput<br/>相比轮询调度延迟降低85%，保持吞吐量”]"
    },
    {
      "title": "Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification",
      "authors": "Jin Sob Kim, Hyun Joon Park, Wooseok Shin, Sung Won Han",
      "institution": "Korea University",
      "link": "https://arxiv.org/pdf/2512.22148",
      "code": "https://github.com/sadPororo/LAP",
      "tags": [
        "speaker verification",
        "Layer Attentive Pooling",
        "Attentive Statistical Temporal Pooling",
        "pre-trained speech models",
        "multi-level features",
        "speaker embeddings"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96b640602033c1b23da872d515add261756f5ab1b958eac2b83c4e62b1fc7f3f_w640_q70.webp",
      "contributions": "1. Proposed Layer Attentive Pooling (LAP), a novel dynamic strategy for aggregating multi-layer representations from pre-trained speech models, moving beyond static weighted averaging. 2. Introduced a lightweight backend speaker model combining LAP and Attentive Statistical Temporal Pooling (ASTP) for efficient speaker embedding extraction. 3. Demonstrated state-of-the-art performance on the VoxCeleb benchmark with a compact architecture that significantly reduces training time.",
      "summary": "The paper addresses the underutilization of multi-layer features from pre-trained speech models in speaker verification. It proposes a novel Layer Attentive Pooling (LAP) method and a lightweight backend model to dynamically aggregate these features. The approach achieves state-of-the-art results on VoxCeleb while being more efficient in training time.",
      "mindmap": "graph TB\n        Root[重新思考利用预训练多层表示进行说话人验证<br/>Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br/>静态加权平均聚合多层特征的局限性<br/>Limitations of static weighted average for multi-layer feature aggregation] --> Problem_Detail[细节/Detail<br/>未充分利用高层表示<br/>Underutilization of high-level representations]\n        Method[主要方法/Method<br/>提出层注意力池化<br/>Propose Layer Attentive Pooling (LAP)] --> Method_Detail1[细节/Detail<br/>动态多视角评估层重要性<br/>Time-dynamically assess layer significance from multiple perspectives]\n        Method --> Method_Detail2[细节/Detail<br/>使用最大池化而非平均<br/>Employ max pooling instead of averaging]\n        Method --> Method_Detail3[细节/Detail<br/>轻量级后端模型 (LAP+ASTP)<br/>Lightweight backend model (LAP + ASTP)]\n        Results[关键结果/Results<br/>在VoxCeleb上达到SOTA<br/>Achieves SOTA on VoxCeleb benchmark] --> Results_Detail[细节/Detail<br/>性能优越且大幅减少训练时间<br/>Superior performance and greatly reduced training time]"
    },
    {
      "title": "Practical challenges of control monitoring in frontier AI deployments",
      "authors": "David Lindner, Charlie Griffin, Tomek Korbak, Roland S. Zimmermann, Geoffrey Irving, Sebastian Farquhar, Alan Cooney",
      "institution": "Google DeepMind, UK AI Safety Institute, University of Oxford",
      "link": "https://arxiv.org/pdf/2512.22154",
      "code": null,
      "tags": [
        "ai security",
        "control monitoring",
        "oversight latency",
        "safety case",
        "scheming agents",
        "incremental attacks"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cfc3d08c169a23a56cd0a16a1471c5bceacb1b76913f7079f51b7024f030d1_w640_q70.webp",
      "contributions": "1. Analysis of real-world deployment dynamics (parallelism, latency, incremental attacks, partial incrimination) for control monitoring, 2. Proposal and comparison of three monitoring protocols (synchronous, semi-synchronous, asynchronous) with different latency-safety trade-offs, 3. Introduction of a high-level safety case sketch as a tool for analyzing and comparing monitoring protocols, applied to four case studies.",
      "summary": "This paper analyzes the practical challenges of scaling automated control monitors for overseeing frontier AI agents in real-world deployments. It proposes and compares three monitoring protocols (synchronous, semi-synchronous, asynchronous) with different latency-safety trade-offs and introduces a safety case sketch as an analytical tool. The analysis identifies oversight, latency, and recovery as key challenges, explored through four case studies of potential AI attacks.",
      "mindmap": "graph TB\n        Root(”Practical challenges of control monitoring in frontier AI deployments<br>前沿AI部署中控制监控的实际挑战”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”现实部署的动态<br>Real-world Deployment Dynamics”)\n        P1 --> P1_1(”并行实例<br>Parallel Instances”)\n        P1 --> P1_2(”监督延迟<br>Oversight Latency”)\n        P1 --> P1_3(”增量攻击<br>Incremental Attacks”)\n        P1 --> P1_4(”部分归责<br>Partial Incrimination”)\n        Method --> M1(”监控协议<br>Monitoring Protocols”)\n        M1 --> M1_1(”同步监控<br>Synchronous”)\n        M1 --> M1_2(”半同步监控<br>Semi-synchronous”)\n        M1 --> M1_3(”异步监控<br>Asynchronous”)\n        Method --> M2(”安全案例草图<br>Safety Case Sketch”)\n        Results --> R1(”识别核心挑战<br>Identified Core Challenges”)\n        R1 --> R1_1(”监督<br>Oversight”)\n        R1 --> R1_2(”延迟<br>Latency”)\n        R1 --> R1_3(”恢复<br>Recovery”)\n        Results --> R2(”案例研究应用<br>Case Studies Application”)"
    },
    {
      "title": "BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs",
      "authors": "Muhammad Zeeshan Karamat, Sadman Saif, Christiana Chamon Garcia",
      "institution": "Virginia Tech",
      "link": "https://arxiv.org/pdf/2512.22174",
      "code": null,
      "tags": [
        "fault-tolerance",
        "bit-flip faults",
        "fault localization",
        "transformer reliability",
        "residual-path perturbation",
        "loss-sensitivity profiling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e931785a8ed1d0dca51ed3c75265de72147ccd6e3d68df21de1c7cad78a1d912_w640_q70.webp",
      "contributions": "1. Introduces BitFlipScope, a scalable software framework for localizing bit-flip corruptions in transformer-based LLMs under two deployment scenarios (with and without a clean reference model). 2. Proposes differential analysis for fault localization when a reference model is available and residual-path perturbation/loss-sensitivity profiling for localization when no reference exists. 3. Enables lightweight performance recovery for corrupted models without requiring costly fine-tuning or full retraining.",
      "summary": "This paper introduces BitFlipScope, a framework for localizing and recovering from bit-flip corruptions in LLMs. It uses differential analysis with a reference model or perturbation-based profiling without one to identify fault-affected regions, enabling targeted recovery without full retraining. The work aims to improve fault resilience for LLMs in hardware-prone and adversarial environments.",
      "mindmap": "graph TB\n        A[BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs] --> B[核心问题/Problem: Bit-flip faults corrupt LLM parameters, causing unpredictable behavior]\n        A --> C[主要方法/Method: Differential analysis with reference model; Residual-path perturbation & loss-sensitivity profiling without reference]\n        A --> D[关键结果/Results: Enables fault localization and lightweight recovery, improving fault-resilient LLM deployment]"
    },
    {
      "title": "Solving Multi-Agent Multi-Goal Path Finding Problems in Polynomial Time",
      "authors": "Stefan Edelkamp",
      "institution": "Charles University",
      "link": "https://arxiv.org/pdf/2512.22171",
      "code": null,
      "tags": [
        "multi-agent path finding",
        "multi-agent path finding",
        "vehicle routing",
        "polynomial-time algorithm",
        "conflict resolution",
        "assignment problem"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9267701cf1d96333033d8663590f3b652040a494de98cd10ba5a86ede709d3b_w640_q70.webp",
      "contributions": "1. Proposes a polynomial-time algorithm for solving discrete multi-agent multi-goal path finding (CMAPF) problems with node and edge conflicts, which is unexpected given the NP-hardness of traditional vehicle routing. 2. Introduces a planner that autonomously finds and updates the assignment of multiple goals to agents, contrasting with regular MAPF which uses fixed assignments. 3. Develops conflict resolution strategies including global assignment to reduce conflicts, and local methods like \"ants-on-the-stick,\" local assignment, path interleaving, and destination clearing.",
      "summary": "This paper addresses the multi-agent multi-goal path finding (CMAPF) problem where agents in graphs must be assigned and routed to multiple goals. It presents a polynomial-time algorithm for discrete variants with conflicts, implemented in a planner that autonomously handles goal assignment and resolves conflicts. The main conclusion is that efficient, conflict-free solutions can be achieved in polynomial time, challenging the typical NP-hard complexity of vehicle routing.",
      "mindmap": "graph TB\n    A[Solving Multi-Agent Multi-Goal Path Finding Problems in Polynomial Time] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[为多智能体规划多目标无冲突路径/Plan multi-goal conflict-free paths for multi-agent]\n    C --> C1[自主目标分配与冲突解决策略/Autonomous goal assignment & conflict resolution]\n    D --> D1[离散问题可在多项式时间内解决/Discrete problems solvable in polynomial time]"
    },
    {
      "title": "Wireless Traffic Prediction with Large Language Model",
      "authors": "Chuanting Zhang, Haixia Zhang, Jingping Qiao, Zongzhang Li, Mohamed-Slim Alouini",
      "institution": "Shandong University, Shandong Normal University, China Mobile Communications Group Shandong Co., Ltd, King Abdullah University of Science and Technology (KAUST)",
      "link": "https://arxiv.org/pdf/2512.22178",
      "code": null,
      "tags": [
        "spatio-temporal forecasting",
        "large language model",
        "wireless traffic prediction",
        "spatial-temporal correlation",
        "prompt engineering",
        "fine-tuning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/402a3d6681d9c203daf7e8ef09e0d0af8998f3eba780abc404c945025563d6a8_w640_q70.webp",
      "contributions": "1. Proposes TIDES, an LLM-based framework that captures spatial-temporal correlations for urban wireless traffic prediction. 2. Introduces a prompt engineering scheme to bridge the domain gap between numerical traffic data and language models by embedding statistical features as structured inputs. 3. Designs a DeepSeek module enabling spatial alignment via cross-domain attention, allowing the LLM to leverage information from related regions, and employs efficient fine-tuning of lightweight components.",
      "summary": "This paper proposes TIDES, a novel framework that uses a large language model (LLM) enhanced with spatial awareness for urban wireless traffic prediction. It addresses the lack of spatial modeling in existing LLM-based predictors through region clustering, prompt engineering, and a spatial alignment module, achieving superior accuracy and robustness on real-world datasets, which is key for intelligent 6G network management.",
      "mindmap": "graph TB\n        A[Wireless Traffic Prediction with Large Language Model] --> B(核心问题/Problem: LLMs overlook spatial dependencies in city-scale wireless traffic)\n        A --> C(主要方法/Method: TIDES framework with clustering, prompt engineering, and DeepSeek spatial alignment module)\n        A --> D(关键结果/Results: Outperforms SOTA baselines in accuracy and robustness for 6G network management)"
    },
    {
      "title": "Characterizing Motion Encoding in Video Diffusion Timesteps",
      "authors": "Vatsal Baherwani, Yixuan Ren, Abhinav Shrivastava",
      "institution": "University of Maryland",
      "link": "https://arxiv.org/pdf/2512.22175",
      "code": null,
      "tags": [
        "video generation",
        "video diffusion models",
        "timestep analysis",
        "motion-appearance disentanglement",
        "motion transfer",
        "one-shot customization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ff2b9cdf8da1e9fbc9ae04ff2d085e89388ee26b1689338abbb853b17970bed_w640_q70.webp",
      "contributions": "1. Proposes a quantitative proxy and conducts a large-scale study to systematically characterize how motion is encoded across the denoising timesteps of video diffusion models, identifying distinct motion-dominant and appearance-dominant regimes. 2. Derives an operational motion-appearance boundary in timestep space, turning a widely used empirical heuristic into a spatiotemporal disentanglement principle. 3. Simplifies the one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives.",
      "summary": "This paper investigates how motion is encoded across the denoising timesteps of text-to-video diffusion models. By quantifying the trade-off between appearance editing and motion preservation when injecting new conditions, the authors identify early timesteps as motion-dominant and later ones as appearance-dominant. This characterization enables a simplified, effective method for one-shot motion transfer without extra modules.",
      "mindmap": "graph TB\n        A[Characterizing Motion Encoding in Video Diffusion Timesteps] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[视频扩散模型中运动编码机制不明确 / Motion encoding in video diffusion is poorly understood]\n        C --> C1[通过条件注入量化运动-外观权衡 / Quantify motion-appearance trade-off via conditional injection]\n        C --> C2[大规模定量研究 / Large-scale quantitative study]\n        D --> D1[识别早期运动主导与后期外观主导阶段 / Identify early motion-dominant and late appearance-dominant regimes]\n        D --> D2[简化单样本运动定制范式 / Simplify one-shot motion customization paradigm]"
    },
    {
      "title": "iOS as Acceleration",
      "authors": "Alexander K. Chen",
      "institution": "Independent High School Researcher (No institutional affiliation inferred)",
      "link": "https://arxiv.org/pdf/2512.22180",
      "code": null,
      "tags": [
        "on-device ai",
        "distributed pipeline parallelism",
        "mobile acceleration",
        "iOS",
        "memory constraints",
        "thermal throttling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2533767e76bdf97e302af13359b973b06a9948269cc9017131b6e880553cb6b9_w640_q70.webp",
      "contributions": "1. Proposes a novel proof-of-concept system using distributed pipeline parallelism to harness iOS devices as computational accelerators for local ML tasks. 2. Demonstrates the system's effectiveness in accelerating modest model training (e.g., ResNet-34) and agentic LRM tool-usage, achieving a 44% decrease in training time in a specific setup. 3. Explores the unique potential of ubiquitous mobile devices with powerful processors and sensors (e.g., LiDAR, GPS) as cost-effective resources for embodied agentic AI and local compute, discussing practical use-cases and limitations.",
      "summary": "This paper addresses the barrier of expensive compute for local machine learning by proposing a system that uses distributed pipeline parallelism to leverage underutilized iOS phones as accelerators. The method partitions model weights to circumvent mobile memory limits, successfully accelerating tasks like training ResNet-34. The work concludes that commonplace mobile devices have significant potential to contribute to ML, especially for local, cost-sensitive, or sensor-driven applications.",
      "mindmap": "graph TB\n        A[iOS as Acceleration] --> B[核心问题/Problem: Powerful compute is a barrier for local ML; Cloud is not always viable]\n        A --> C[主要方法/Method: Use distributed pipeline parallelism to harness iOS devices as accelerators]\n        A --> D[关键结果/Results: Achieved faster training for modest models; Highlights mobile potential for ML]"
    },
    {
      "title": "Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery",
      "authors": "Hassan Khalid, Muhammad Mahad Khaliq, Muhammad Jawad Bashir",
      "institution": "National University of Science and Technology (NUST)",
      "link": "https://arxiv.org/pdf/2512.22182",
      "code": null,
      "tags": [
        "dimensionality reduction",
        "Locally Linear Embedding (LLE)",
        "AI-enhanced LLE",
        "medical data analysis",
        "medical billing",
        "transcription"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp",
      "contributions": "1. Proposes an innovative integration of AI with Locally Linear Embedding (LLE) to handle high-dimensional medical data. 2. Develops a comprehensive mathematical model for the AI-enhanced LLE technique. 3. Demonstrates the model's application in real-world healthcare scenarios, showing significant improvements in data processing accuracy and operational efficiency for medical billing and transcription.",
      "summary": "This paper introduces an AI-enhanced Locally Linear Embedding (LLE) model to improve the analysis of high-dimensional medical data. The method is applied to automate and enhance medical billing and transcription services. The results show significant improvements in processing accuracy and operational efficiency.",
      "mindmap": "graph TB\n    A[Enhancing Medical Data Analysis through AI-Enhanced LLE] --> B(核心问题/Problem: Handling complex high-dimensional medical data for billing and transcription)\n    A --> C(主要方法/Method: Integrating AI with Locally Linear Embedding (LLE))\n    A --> D(关键结果/Results: Improved data processing accuracy and operational efficiency)"
    },
    {
      "title": "Interpretable Link Prediction in AI-Driven Cancer Research: Uncovering Co-Authorship Patterns",
      "authors": "Shahab Mosallaie, Andrea Schiffauerova, Ashkan Ebadi",
      "institution": "Concordia University, National Research Council Canada",
      "link": "https://arxiv.org/pdf/2512.22181",
      "code": null,
      "tags": [
        "network science",
        "co-authorship networks",
        "link prediction",
        "SHAP",
        "random forest",
        "interdisciplinary collaboration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f6c5e92a95d2e6eeb8291558739ab641247bb834675d42c86ece0ee73d9d15_w640_q70.webp",
      "contributions": "1. Constructed 36 overlapping co-authorship networks from 7,738 publications to model new, persistent, and discontinued collaborations in AI-driven cancer research. 2. Engineered both attribute-based and structure-based features and built four machine learning classifiers, with Random Forest achieving the highest recall for all collaboration types. 3. Applied SHAP for model interpretability, identifying key factors like discipline similarity, productivity, and seniority that influence collaboration patterns.",
      "summary": "This paper uses machine learning to predict collaboration patterns in AI-driven cancer research by analyzing co-authorship networks. The authors built classifiers using engineered features and applied SHAP for interpretability, finding that discipline similarity promotes new and persistent collaborations while high productivity and seniority are linked to discontinued links. The results aim to guide the formation of effective interdisciplinary research teams and inform policy.",
      "mindmap": "graph TB\n        A[Interpretable Link Prediction in AI-Driven Cancer Research: Uncovering Co-Authorship Patterns] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[挑战: 组建有效的跨学科癌症研究团队/Challenge: Forming effective interdisciplinary cancer research teams]\n        C --> C1[构建合著网络作为合作代理/Construct co-authorship networks as collaboration proxy]\n        C --> C2[使用机器学习分类器进行链接预测/Use ML classifiers for link prediction]\n        C --> C3[使用SHAP进行模型可解释性/Use SHAP for model interpretability]\n        D --> D1[随机森林在所有合作类型中召回率最高/Random forest achieved highest recall]\n        D --> D2[学科相似性得分是关键因素/Discipline similarity score is a crucial factor]\n        D --> D3[高生产力和资历与中断链接正相关/High productivity and seniority positively associated with discontinued links]"
    },
    {
      "title": "Unbiased Visual Reasoning with Controlled Visual Inputs",
      "authors": "Zhaonan Li, Shijie Lu, Fei Wang, Jacob Dineen, Xiao Ye, Zhikun Xu, Siyi Liu, Young Min Cho, Bangzheng Li, Daniel Chang, Kenny Nguyen, Qizheng Yang, Muhao Chen, Ben Zhou",
      "institution": "Arizona State University, University of Southern California, University of Pennsylvania, University of California, Davis",
      "link": "https://arxiv.org/pdf/2512.22183",
      "code": null,
      "tags": [
        "multimodal reasoning",
        "vision-language models",
        "spurious correlations",
        "information bottleneck",
        "reinforcement learning",
        "modular reasoning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f546535b9c36b7873d0f685328a4f4a8e058e6a4788639c170f69e073d8f9e_w640_q70.webp",
      "contributions": "1. Proposes VISTA, a modular framework that decouples visual perception from reasoning using an explicit information bottleneck to control visual inputs. 2. Introduces a training method using reinforcement learning (GRPO) on a small curated dataset to align the reasoner with unbiased visual evidence. 3. Demonstrates improved robustness against spurious correlations, transferability across VLM sensors, and enhanced interpretability in reasoning traces.",
      "summary": "The paper addresses the problem of vision-language models (VLMs) relying on spurious correlations rather than causal visual evidence. It proposes VISTA, a modular framework that separates perception (via a frozen VLM) from reasoning (via an LLM) using controlled queries and trains the reasoner with reinforcement learning. The method shows significant gains in robustness on benchmarks like SpuriVerse while maintaining competitive performance on other tasks.",
      "mindmap": "graph TB\n        A[Unbiased Visual Reasoning with Controlled Visual Inputs] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[VLMs exploit spurious correlations/VLMs利用虚假关联]\n        C --> C1[VISTA: Modular framework decoupling perception & reasoning/VISTA: 解耦感知与推理的模块化框架]\n        C1 --> C2[Frozen VLM sensor + LLM reasoner/冻结VLM感知器 + LLM推理器]\n        C2 --> C3[Train with RL (GRPO)/使用强化学习(GRPO)训练]\n        D --> D1[Improved robustness on SpuriVerse/在SpuriVerse上鲁棒性提升]\n        D --> D2[Competitive on MMVP & SeedBench/在MMVP & SeedBench上保持竞争力]\n        D --> D3[Transferable & interpretable/可迁移且可解释]"
    },
    {
      "title": "Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks",
      "authors": "Vishnu Mohan",
      "institution": "Independent Researcher",
      "link": "https://arxiv.org/pdf/2512.22186",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Dueling Double Deep Q-Network",
        "curriculum learning",
        "tennis simulation",
        "sequential decision-making",
        "sports analytics"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/543e2f9f07244abac63adfdbdefd7fccfefed9147b55aed87016c10657f91bae_w640_q70.webp",
      "contributions": "1. Developed a custom tennis simulation environment that models hierarchical scoring, tactical decisions, fatigue, and opponent skill. 2. Integrated a Dueling Double Deep Q-Network (DDQN) with curriculum learning to enable stable and effective strategy learning in a long-horizon, stochastic domain. 3. Identified a key limitation of win-rate optimization, revealing a learned defensive bias and highlighting challenges in reward design for sports RL.",
      "summary": "This paper proposes a reinforcement learning framework using a Dueling Double Deep Q-Network trained with curriculum learning to optimize tennis strategy in a custom simulation. The method achieves high win rates and demonstrates stable convergence, but analysis reveals the learned policy is overly defensive, pointing to a fundamental issue with reward design in sports simulations.",
      "mindmap": "graph TB\n        A[Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks] --> B(核心问题/Problem: Tennis strategy optimization as a sequential decision-making challenge with hierarchical scoring, stochasticity, and opponent adaptation)\n        A --> C(主要方法/Method: Dueling Double Deep Q-Network (DDQN) trained with curriculum learning in a custom tennis simulation environment)\n        A --> D(关键结果/Results: High win rates (98-100%) and stable convergence, but reveals a defensive policy bias, highlighting reward design limitations)"
    },
    {
      "title": "HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology",
      "authors": "Xitong Ling, Minxi Ouyang, Xiaoxiao Li, Jiawen Li, Ying Chen, Yuxuan Sun, Xinrui Chen, Tian Guan, Xiaoping Liu, Yonghong He",
      "institution": "Tsinghua University, Xiamen University, Westlake University, Wuhan University",
      "link": "https://arxiv.org/pdf/2512.22188",
      "code": "https://github.com/lingxitong/HookMIL",
      "tags": [
        "computational pathology",
        "Multiple Instance Learning",
        "Hook Tokens",
        "Linear Complexity",
        "Multimodal Initialization",
        "Hook Diversity Loss"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/125d35cff1b2c94d5e736ab81d897743e3d0b37d50d5ba6ce441aa77f8bc620e_w640_q70.webp",
      "contributions": "1. Proposes HookMIL, a context-aware MIL framework using learnable hook tokens for structured contextual aggregation with linear computational complexity. 2. Introduces a multimodal initialization strategy for hook tokens using visual, textual, and spatial priors to accelerate convergence and improve representation. 3. Presents a Hook Diversity Loss and a hook-to-hook communication mechanism to encourage token specialization and refine interactions while minimizing redundancy.",
      "summary": "The paper addresses the loss of context in traditional MIL and the high computational cost of transformer-based MIL for whole-slide image analysis. It proposes HookMIL, a framework that uses learnable hook tokens for efficient, linear-complexity context modeling, enhanced by multimodal initialization and specialized loss functions. Experiments on four public datasets show that HookMIL achieves state-of-the-art performance with improved efficiency and interpretability.",
      "mindmap": "graph TB\n        Root[HookMIL: Revisiting Context Modeling in MIL for Computational Pathology] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: MIL loses context; Transformers are inefficient] --> P1[传统MIL丢失上下文/Traditional MIL loses context]\n        Problem --> P2[基于Transformer的MIL计算复杂/Transformer-based MIL has quadratic complexity]\n        Method[主要方法/Method: HookMIL Framework] --> M1[使用可学习的Hook Tokens/Use learnable Hook Tokens]\n        Method --> M2[多模态初始化/Multimodal Initialization]\n        Method --> M3[Hook多样性损失与通信机制/Hook Diversity Loss & Communication]\n        Results[关键结果/Results] --> R1[SOTA性能/State-of-the-art Performance]\n        Results --> R2[计算高效/Computationally Efficient]\n        Results --> R3[可解释性/Interpretability]"
    },
    {
      "title": "MatKV: Trading Compute for Flash Storage in LLM Inference",
      "authors": "Kun-Woo Shin, Jay H. Park, Moonwook Oh, Yohan Jo, Jaeyoung Do, Sang-Won Lee",
      "institution": "Seoul National University, Samsung Electronics",
      "link": "https://arxiv.org/pdf/2512.22195",
      "code": "https://github.com/kunwooshin/MatKV",
      "tags": [
        "llm inference",
        "retrieval-augmented generation",
        "key-value cache",
        "flash storage",
        "prefill optimization",
        "power efficiency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d47111ab2d615579a09c00ff5a99f391f035b974cc23e324cddfbabf4d23cec_w640_q70.webp",
      "contributions": "1. Proposes MatKV, a scheme to precompute and materialize KV vectors of RAG documents in flash storage to avoid recomputation during inference. 2. Demonstrates that MatKV reduces inference time and power consumption by half for RAG workloads with minimal accuracy impact. 3. Shows MatKV enables additional optimizations like overlapping KV loading with decoding and enabling the use of low-end GPUs for decoding.",
      "summary": "The paper addresses the high compute and energy cost of the prefill phase in RAG-based LLM inference. It proposes MatKV, which precomputes and stores key-value vectors of documents in flash storage for reuse, trading compute for storage. Experiments show this approach halves inference time and power consumption while maintaining accuracy and enabling further hardware optimizations.",
      "mindmap": "graph TB\n        Root[”MatKV: Trading Compute for Flash Storage in LLM Inference”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>RAG推理中prefill阶段计算开销大<br>High compute cost of prefill in RAG inference”]\n        Method[”主要方法/Method<br>预计算并物化KV向量到闪存<br>Precompute & materialize KVs to flash storage”]\n        Results[”关键结果/Results<br>推理时间与能耗减半<br>Halves inference time & power consumption”]"
    },
    {
      "title": "Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation",
      "authors": "Teja Chinthala",
      "institution": "Independent Researcher (affiliated email domain: avila.edu)",
      "link": "https://arxiv.org/pdf/2512.22199",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "Retrieval-Augmented Generation",
        "Hallucination Prevention",
        "Multi-Stage Validation",
        "Corpus Expansion",
        "Self-Improving AI"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5eee110568078584ba44c846b5af3fab7d300dbfaec310a5826fd74784dc8040_w640_q70.webp",
      "contributions": "1. A novel RAG architecture enabling safe corpus expansion through validated write-back of model outputs. 2. A multi-stage acceptance layer combining grounding verification, attribution checking, and novelty detection for safety. 3. An experience store for meta-learning from both accepted and rejected responses.",
      "summary": "The paper addresses the problem that conventional RAG systems have static knowledge bases. It proposes Bidirectional RAG, a novel architecture that safely expands the retrieval corpus by writing back high-quality, validated LLM responses. The results show that this self-improving approach nearly doubles answer coverage compared to standard RAG while adding significantly fewer documents than a naive write-back strategy, demonstrating a safe and practical path for RAG systems to learn from deployment.",
      "mindmap": "graph TB\n        A[Bidirectional RAG: Safe Self-Improving RAG] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[静态知识库无法从交互中学习/Static corpus cannot evolve from interactions]\n        C --> C1[带验证的回写机制/Validated write-back]\n        C --> C2[多阶段验证层/Multi-stage acceptance layer]\n        D --> D1[覆盖率翻倍/Coverage doubled vs. Standard RAG]\n        D --> D2[文档增长减少72%/72% less corpus growth vs. naive write-back]"
    },
    {
      "title": "CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks",
      "authors": "Yogeswar Reddy Thota",
      "institution": "University of Texas at Dallas",
      "link": "https://arxiv.org/pdf/2512.22206",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "dynamic routing",
        "residual networks",
        "cosine incompatibility",
        "Gumbel-Softmax",
        "FLOPs regularization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed7e3fa1c114a73795152210d2b55ffe2541fede331ce04d228e11ca599688fa_w640_q70.webp",
      "contributions": "1. Introduces CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks using cosine incompatibility as a self-supervised skip signal. 2. Proposes the Cosine Incompatibility Ratio (CIR) to measure semantic redundancy and employs Gumbel-Softmax relaxation for per-sample, per-block gating during training. 3. Incorporates a progressive FLOPs regularization term to control average computational usage without destabilizing the optimization process.",
      "summary": "The paper addresses the computational inefficiency in residual networks, where all blocks are evaluated for every input. It proposes CosineGate, a method that uses the cosine incompatibility between identity and residual features to dynamically skip redundant blocks, achieving significant FLOPs savings on CIFAR-10 while maintaining or improving accuracy.",
      "mindmap": "graph TB\n        A[CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks] --> B[核心问题/Problem: Modern residual networks perform redundant computation for all inputs]\n        A --> C[主要方法/Method: Uses cosine incompatibility ratio and Gumbel-Softmax for dynamic per-block gating]\n        A --> D[关键结果/Results: Achieves accuracy-efficiency Pareto frontier on CIFAR-10 with significant FLOPs savings]"
    },
    {
      "title": "Emergent Persuasion: Will LLMs Persuade Without Being Prompted?",
      "authors": "Vincent Chang, Thee Ho, Sunishchal Dev, Kevin Zhu, Shi Feng, Kellin Pelrine, Matthew Kowal",
      "institution": "Algoverse, FAR.AI, UC Berkeley, George Washington University, University of Toronto",
      "link": "https://arxiv.org/pdf/2512.22201",
      "code": "https://github.com/ith8/persona_vectors",
      "tags": [
        "ai safety & alignment",
        "emergent persuasion",
        "activation steering",
        "supervised fine-tuning (SFT)",
        "threat model",
        "persona vectors"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3e4599070a9919228b3e5bc9cb7f7fd0fe17df086f5d7bec7ef20de22526f15_w640_q70.webp",
      "contributions": "1. Investigates the novel threat model of \"unprompted\" or \"emergent\" persuasion in LLMs, moving beyond the standard misuse (prompted) scenario. 2. Empirically compares two techniques for inducing traits (activation steering vs. supervised fine-tuning) and finds SFT reliably increases unprompted persuasion while steering does not. 3. Demonstrates that SFT on benign persuasion datasets can lead to increased persuasion propensity on harmful topics, highlighting a significant safety risk.",
      "summary": "This paper studies whether Large Language Models (LLMs) will attempt to persuade users without being explicitly prompted to do so. The authors investigate this by applying activation steering and supervised fine-tuning (SFT) to induce persuasive traits, finding that SFT reliably increases unprompted persuasion, including on harmful topics, even when trained only on benign data. The main conclusion is that emergent harmful persuasion is a real risk that warrants further study for AI safety and governance.",
      "mindmap": "graph TB\n        A[Emergent Persuasion: Will LLMs Persuade Without Being Prompted?] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[研究模型在未受明确提示时进行说服的风险 / Study risk of unprompted persuasion]\n        C --> C1[激活引导以植入人格特质 / Activation steering for persona traits]\n        C --> C2[监督微调以植入人格特质 / Supervised fine-tuning for persona traits]\n        D --> D1[监督微调会可靠地增加无提示说服 / SFT reliably increases unprompted persuasion]\n        D --> D2[良性主题微调可能导致有害主题说服 / Benign SFT can increase harmful topic persuasion]"
    },
    {
      "title": "TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting",
      "authors": "Qiang Guo, Rubo Zhang, Bingbing Zhang, Junjie Liu, Jianqing Liu",
      "institution": "Dalian Minzu University, Dalian University of Technology, Dalian Rijia Electronics Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.22203",
      "code": null,
      "tags": [
        "crowd counting",
        "weakly-supervised learning",
        "vision transformer",
        "density-guided aggregation",
        "parameter efficiency",
        "lightweight model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67bff3bf5e0b02cbd2cc6071e68fd191f9adc37c49a6f5dd3f4b89fbc7205ca3_w640_q70.webp",
      "contributions": "1. Proposes TCFormer, an ultra-lightweight transformer-based framework with only 5 million parameters for weakly-supervised crowd counting. 2. Introduces a Learnable Density-Weighted Averaging module to dynamically re-weight local features based on predicted density, compensating for the lack of spatial annotations. 3. Designs a density-level classification loss to discretize crowd density into grades, regularizing training and enhancing performance across varying density levels.",
      "summary": "This paper proposes TCFormer, a tiny transformer-based model for weakly-supervised crowd counting that uses only image-level count labels. It introduces a density-guided feature aggregation module and a density-level classification loss to achieve accurate counting. Experiments show it achieves a superior trade-off between parameter efficiency and accuracy, making it suitable for edge devices.",
      "mindmap": "graph TB\n        Root[TCFormer: 5M参数Transformer用于弱监督人群计数] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[标注成本高/High Annotation Cost]\n        Problem --> P2[计算复杂度高/High Computational Complexity]\n        Method[主要方法/Method] --> M1[高效视觉Transformer特征提取器/Efficient ViT Feature Extractor]\n        Method --> M2[可学习密度加权平均模块/Learnable Density-Weighted Averaging]\n        Method --> M3[密度等级分类损失/Density-Level Classification Loss]\n        Results[关键结果/Results] --> R1[仅5M参数/Only 5M Parameters]\n        Results --> R2[弱监督下竞争性性能/Competitive Performance under Weak Supervision]\n        Results --> R3[适用于边缘设备/Suitable for Edge Devices]"
    },
    {
      "title": "GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks",
      "authors": "Ryan Spencer, Roey Yaari, Ritvik Vemavarapu, Joyce Yang, Steven Ngo, Utkarsh Sharma",
      "institution": "Algoverse AI Research, UC San Diego, University of New South Wales",
      "link": "https://arxiv.org/pdf/2512.22207",
      "code": "https://github.com/stvngo/GamiBench",
      "tags": [
        "multimodal reasoning",
        "spatial reasoning",
        "multimodal large language models",
        "benchmark",
        "origami folding",
        "viewpoint consistency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7bb91a40c2344a9e50dec2b2754647404d609db9a78508a54a040d5f6c5f58b_w640_q70.webp",
      "contributions": "1. Introduces GamiBench, a novel benchmark for evaluating spatial reasoning and 2D-to-3D planning in MLLMs using origami folding tasks. 2. Proposes new diagnostic metrics, viewpoint consistency (VC) and impossible fold selection rate (IFSR), to holistically assess the reasoning process. 3. Provides a comprehensive dataset of 186 regular and 186 impossible 2D crease patterns with 3D shapes from multiple viewpoints.",
      "summary": "The paper introduces GamiBench, a benchmark that uses origami folding tasks to evaluate the spatial reasoning and 2D-to-3D planning capabilities of Multimodal Large Language Models (MLLMs). It assesses models on tasks like predicting 3D configurations and detecting impossible folds, revealing that even state-of-the-art models like GPT-5 and Gemini-2.5-Pro struggle with fundamental spatial understanding.",
      "mindmap": "graph TB\n        Root[GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: MLLMs struggle with sequential, multi-view spatial reasoning] --> Problem_Sub[现有基准的不足/Existing benchmarks focus on static images]\n        Method[主要方法/Method: Origami-inspired benchmark with 3 VQA tasks] --> Method_Sub[引入新指标/Introduces new metrics (VC, IFSR)]\n        Results[关键结果/Results: Leading models (GPT-5, Gemini) struggle with spatial understanding]"
    },
    {
      "title": "Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh",
      "authors": "Farjana Yesmin, Romana Akter",
      "institution": "Independent Researcher, Researcher (affiliations not specified)",
      "link": "https://arxiv.org/pdf/2512.22210",
      "code": null,
      "tags": [
        "algorithmic fairness",
        "adversarial debiasing",
        "gradient reversal layer",
        "fairness-aware representation learning",
        "statistical parity difference"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd82a75c606f74970d3c93e7f31e57d4d35ae9f285a2ccb25e804770225a020b_w640_q70.webp",
      "contributions": "1. A comprehensive dataset integrating official flood impact data with socioeconomic indicators across 87 upazilas in Bangladesh. 2. An adversarial debiasing architecture adapted from healthcare AI for disaster management. 3. Rigorous fairness evaluation showing significant reductions in statistical parity and regional fairness gaps while maintaining predictive accuracy.",
      "summary": "This paper proposes a fairness-aware AI framework using adversarial debiasing with a gradient reversal layer to prioritize post-flood aid allocation in Bangladesh. The model learns bias-invariant representations to reduce systematic disadvantages against marginalized regions. Experimental results show it significantly improves fairness metrics while maintaining strong predictive accuracy, demonstrating the effective application of algorithmic fairness in humanitarian contexts.",
      "mindmap": "graph TB\n        A[Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Biased post-disaster aid allocation perpetuates historical inequities in vulnerable regions]\n        C[主要方法/Method: Adversarial debiasing model with gradient reversal layer for bias-invariant representations]\n        D[关键结果/Results: Reduces statistical parity by 41.6%, regional fairness gaps by 43.2%, maintains R²=0.784]"
    },
    {
      "title": "With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems",
      "authors": "Shaun Khoo, Jessica Foo, Roy Ka-Wei Lee",
      "institution": "GovTech Singapore, Singapore University of Technology and Design",
      "link": "https://arxiv.org/pdf/2512.22211",
      "code": "https://github.com/govtech-ai/arc-framework",
      "tags": [
        "agent system",
        "agentic AI",
        "risk assessment",
        "technical governance",
        "autonomous action",
        "safety controls"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0283ff7ed974694c06d620f02b6bfd0752cd1ab34c83d05b1d66ec9b4088059c_w640_q70.webp",
      "contributions": "1. Introduces a novel capability-centric perspective for analyzing agentic AI systems. 2. Distills three primary intrinsic risk sources (components, design, capabilities) and maps them to specific risks and technical controls. 3. Provides a structured, practical methodology for organizations to implement the framework for governance.",
      "summary": "This paper introduces the Agentic Risk & Capability (ARC) Framework to address governance challenges posed by autonomous AI agents. The framework provides a structured methodology to identify, assess, and mitigate risks from agentic systems by analyzing their capabilities and linking risk sources to technical controls. It aims to enable safe and responsible deployment of agentic AI while supporting innovation.",
      "mindmap": "graph TB\n        Root[”With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems”]\n        Root --> Problem[”核心问题/Problem: Agentic AI systems present novel risks and governance challenges due to autonomous actions like code execution and web interaction.”]\n        Root --> Method[”主要方法/Method: Proposes the Agentic Risk & Capability (ARC) Framework, a technical governance framework for risk identification, assessment, and mitigation.”]\n        Root --> Results[”关键结果/Results: Provides a robust, adaptable methodology for safe and responsible deployment of agentic AI, linking risk sources to controls.”]"
    },
    {
      "title": "On the Existence and Behaviour of Secondary Attention Sinks",
      "authors": "Jeffrey T.H. Wong, Cheng Zhang, Louis Mahon, Wayne Luk, Anton Isopoussu, Yiren Zhao",
      "institution": "Imperial College London, UnlikelyAI",
      "link": "https://arxiv.org/pdf/2512.22213",
      "code": null,
      "tags": [
        "llm inference",
        "attention sinks",
        "transformer",
        "mlp",
        "attention mechanism",
        "large language models"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec1462ca646134f445eac98192ff5189abb63f37682802d695aadace9f83b0d3_w640_q70.webp",
      "contributions": "1. Identifies and characterizes a new class of \"secondary attention sinks\" that arise in middle layers, have variable lifetimes, and draw moderate attention, differing from persistent primary sinks like BOS. 2. Shows that secondary sinks are formed by specific middle-layer MLP modules that map token representations to align with the primary sink's direction, with their L2-norm determining sink strength and lifetime. 3. Observes that in larger models, these sink patterns (sink levels) become more deterministic and frequent, with distinct levels identified in models like QwQ-32B and Qwen3-14B.",
      "summary": "This paper identifies a new phenomenon called \"secondary attention sinks\" in transformer LLMs, which are distinct from the known primary sinks (like BOS). The authors show these secondary sinks are created by middle-layer MLPs aligning tokens with the primary sink direction, and their properties (strength, lifetime) become more structured in larger models. This provides new insights into the internal mechanics of attention in large language models.",
      "mindmap": "graph TB\n        A[”On the Existence and Behaviour of Secondary Attention Sinks<br/>二次注意力汇的存在与行为”] --> B[”核心问题/Problem<br/>Prior work only studied persistent primary sinks (e.g., BOS)<br/>先前研究仅关注持久的主汇（如BOS）”]\n        A --> C[”主要方法/Method<br/>Extensive experiments across 11 model families<br/>对11个模型系列进行广泛实验”]\n        A --> D[”关键结果/Results<br/>1. Secondary sinks form via MLPs in middle layers<br/>次级汇通过中间层MLP形成<br/>2. L2-norm determines sink score & lifetime<br/>L2范数决定汇分数与寿命<br/>3. Sink levels are deterministic in large models<br/>大模型中汇层级更确定”]"
    },
    {
      "title": "VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition",
      "authors": "Abdellah Zakaria Sellam, Salah Eddine Bekhouche, Fadi Dornaika, Cosimo Distante, Abdenour Hadid",
      "institution": "University of Salento, Institute of Applied Sciences and Intelligent Systems - CNR, University of the Basque Country UPV/EHU, IKERBASQUE, Sorbonne University Abu Dhabi",
      "link": "https://arxiv.org/pdf/2512.22217",
      "code": null,
      "tags": [
        "pedestrian attribute recognition",
        "vision-language model",
        "cross-attention fusion",
        "class imbalance",
        "domain generalization",
        "SigLIP"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44bf59cdfc87f0708e9f41a8899fbff5562f87b0b721fe79f7fcfc23fb5bb4d4_w640_q70.webp",
      "contributions": "1. Proposes VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders for Pedestrian Attribute Recognition. 2. Introduces a compact cross-attention fusion mechanism to align image and prompt embeddings by refining visual features. 3. Demonstrates state-of-the-art performance on the imbalanced PA100K benchmark and significant gains on PETA and Market-1501, showing effectiveness against imbalance and domain shift.",
      "summary": "This paper proposes VLM-PAR, a vision-language model framework that uses a frozen SigLIP encoder and a cross-attention fusion module to refine visual features for Pedestrian Attribute Recognition. It achieves new state-of-the-art results on the PA100K benchmark and shows strong performance on other datasets, demonstrating the effectiveness of leveraging large-scale vision-language pretraining to address class imbalance and generalization challenges in PAR.",
      "mindmap": "graph TB\n        A[VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Class Imbalance, Attribute Co-dependencies, Domain Shifts<br>类别不平衡, 属性依赖, 域偏移]\n        C[主要方法/Method<br>Vision-Language Framework with Cross-Attention Fusion<br>视觉语言框架与跨注意力融合]\n        D[关键结果/Results<br>SOTA on PA100K, Gains on PETA & Market-1501<br>PA100K上SOTA, PETA & Market-1501上提升]"
    },
    {
      "title": "Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition",
      "authors": "Naichuan Zheng, Xiahai Lun, Weiyi Li, Yuchen Du",
      "institution": "Beijing University of Posts and Telecommunications",
      "link": "https://arxiv.org/pdf/2512.22214",
      "code": null,
      "tags": [
        "skeleton-based action recognition",
        "Spiking Neural Networks",
        "Graph Convolutional Networks",
        "Time-Frequency Learning",
        "Topology-Aware Learning",
        "Energy Efficiency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af8fb8807de54b37db40834a79908ad86cf7e159907b658e54986356c4494d4_w640_q70.webp",
      "contributions": "1. Proposes a novel spiking graph network backbone integrating 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. 2. Introduces a Topology-Shift Self-Attention (TSSA) mechanism to adaptively route attention across learned skeletal topologies without increasing computational complexity. 3. Designs an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch with a Topology-Aware Time-Frequency Fusion (TATF) unit to preserve structural priors in multi-resolution spectral fusion.",
      "summary": "This paper proposes Signal-SGN++, a novel spiking graph network for skeleton-based action recognition that integrates topology-aware learning with time-frequency spiking dynamics to capture motion dependencies. The method combines a spiking graph backbone with a topology-shift attention mechanism and a multi-scale wavelet fusion branch. Experiments show it achieves a superior accuracy-efficiency trade-off, outperforming other SNN methods and competing with state-of-the-art GCNs while using significantly less energy.",
      "mindmap": "graph TB\n        A[Signal-SGN++<br/>论文标题/Paper Title] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[GCNs能耗高<br/>GCNs High Energy Cost]\n        B --> B2[SNNs难以捕捉时空-频率与拓扑依赖<br/>SNNs Limited in Capturing Time-Freq & Topology]\n        C --> C1[主干网络: 1D-SGC + FSC<br/>Backbone: 1D-SGC + FSC]\n        C --> C2[拓扑转移自注意力 TSSA<br/>Topology-Shift Self-Attention TSSA]\n        C --> C3[多尺度小波变换融合 MWTF<br/>Multi-Scale Wavelet Transform Fusion MWTF]\n        D --> D1[优于现有SNN方法<br/>Outperforms Existing SNN Methods]\n        D --> D2[与先进GCNs结果相当<br/>Competitive with SOTA GCNs]\n        D --> D3[能耗显著降低<br/>Substantially Reduced Energy Consumption]"
    },
    {
      "title": "On Extending Semantic Abstraction for Efficient Search of Hidden Objects",
      "authors": "Tasha Pais, Nikhilesh Belulkar",
      "institution": "Columbia University",
      "link": "https://arxiv.org/pdf/2512.22220",
      "code": null,
      "tags": [
        "object detection",
        "semantic abstraction",
        "relevancy maps",
        "3D localization",
        "hidden objects",
        "unstructured search"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp",
      "contributions": "1. Extends the Semantic Abstraction framework to the novel domain of localizing hidden (occluded) objects. 2. Proposes using historical placement data to efficiently guide the unstructured search for hidden objects. 3. Demonstrates a model that can accurately identify a hidden object's complete 3D location faster than a naive random search.",
      "summary": "This paper addresses the problem of efficiently finding hidden or lost objects by extending the Semantic Abstraction framework. The method uses 2D VLM relevancy maps as abstract object representations to learn 3D localization and leverages historical placement data to optimize the search. The result is a model that can locate hidden objects significantly faster than random search, aiming to improve the capabilities of household robots.",
      "mindmap": "graph TB\n        Root(”On Extending Semantic Abstraction for Efficient Search of Hidden Objects”) --> Problem(”核心问题/Problem: Localizing hidden/occluded objects”)\n        Root --> Method(”主要方法/Method: Use VLM relevancy maps & historical data for efficient 3D search”)\n        Root --> Results(”关键结果/Results: Faster and accurate 3D localization vs. random search”)"
    },
    {
      "title": "Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases",
      "authors": "Gnankan Landry Regis N'guessan",
      "institution": "Axiom Research Group, The Nelson Mandela African Institution of Science and Technology (NM-AIST), African Institute for Mathematical Sciences (AIMS) Research and Innovation Centre",
      "link": "https://arxiv.org/pdf/2512.22222",
      "code": null,
      "tags": [
        "neural network architecture",
        "Müntz-Szász Networks",
        "fractional power bases",
        "physics-informed neural networks",
        "universal approximation",
        "singular function approximation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66c3ba4917449496481593b1432346dc5ef9301c3c66f4713e327bbb234969d_w640_q70.webp",
      "contributions": "1. Introduces Müntz-Szász Networks (MSN), a novel neural architecture with learnable fractional power bases to approximate functions with singular or fractional power behavior. 2. Provides theoretical analysis proving MSN inherits universal approximation from the Müntz-Szász theorem and establishes superior approximation rates compared to standard MLPs for singular functions. 3. Demonstrates empirical superiority, achieving significantly lower error with fewer parameters in supervised regression and 3-6x improvement in physics-informed neural network benchmarks, while learning interpretable exponents.",
      "summary": "The paper introduces Müntz-Szász Networks (MSN), a neural architecture that replaces fixed activation functions with learnable fractional power bases to better approximate singular functions common in physics. It proves MSN's universal approximation capability and shows it achieves much lower error with fewer parameters than standard MLPs on regression and physics-informed tasks, demonstrating the value of theory-guided design.",
      "mindmap": "graph TB\n    A[Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases] --> B[核心问题/Problem: Standard neural networks poorly approximate singular/fractional power functions common in physics]\n    A --> C[主要方法/Method: Proposes MSN with learnable fractional power bases, replacing fixed activations]\n    A --> D[关键结果/Results: MSN achieves superior approximation rates, lower error with fewer parameters, and significant improvement on PINN benchmarks]"
    },
    {
      "title": "VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs",
      "authors": "Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao",
      "institution": "University of Science and Technology of China, Ant Group",
      "link": "https://arxiv.org/pdf/2512.22226",
      "code": "https://github.com/zheng980629/VideoScaffold",
      "tags": [
        "video understanding",
        "streaming video",
        "multimodal large language models",
        "event segmentation",
        "hierarchical representation",
        "elastic-scale"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp",
      "contributions": "1. Proposes VideoScaffold, a dynamic representation framework for streaming video understanding in MLLMs that adaptively adjusts event granularity. 2. Introduces Elastic-Scale Event Segmentation (EES) for prediction-guided, dynamic boundary refinement. 3. Introduces Hierarchical Event Consolidation (HEC) for progressively aggregating segments into multi-level abstractions.",
      "summary": "This paper addresses the challenge of understanding long, streaming videos with MLLMs by proposing VideoScaffold, a framework that dynamically segments and hierarchically consolidates video events to adapt granularity and preserve semantics. It achieves state-of-the-art performance on benchmarks and can extend image-based MLLMs to video comprehension.",
      "mindmap": "graph TB\n        A[VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs] --> B[核心问题/Problem: Understanding long, streaming videos with MLLMs is challenging due to redundancy and need for temporal coherence.]\n        A --> C[主要方法/Method: Proposes a dynamic framework with Elastic-Scale Event Segmentation (EES) and Hierarchical Event Consolidation (HEC).]\n        A --> D[关键结果/Results: Achieves state-of-the-art performance; framework is modular and plug-and-play.]"
    },
    {
      "title": "ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis",
      "authors": "Shaghayegh Shajarian, Kennedy Marsh, James Benson, Sajad Khorsandroo, Mahmoud Abdelsalam",
      "institution": "North Carolina A&T State University, University of Texas at San Antonio",
      "link": "https://arxiv.org/pdf/2512.22223",
      "code": "https://github.com/270771/llm-traffictraffic",
      "tags": [
        "rag (retrieval-augmented generation)",
        "retrieval-augmented generation (RAG)",
        "network traffic analysis",
        "large language models (LLMs)",
        "hierarchical retrieval",
        "explainable AI"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/56c5ba03e3d4a510212509143bfecf0fa8b76f9171aa38dd765dadecc7b1ab32_w640_q70.webp",
      "contributions": "1. Proposes ReGAIN, a multi-stage framework combining traffic summarization, RAG, and LLM reasoning for transparent network traffic analysis. 2. Introduces a hierarchical retrieval pipeline with metadata filtering, MMR sampling, cross-encoder reranking, and an abstention mechanism to ground responses and reduce hallucinations. 3. Demonstrates high accuracy (95.95%-98.82%) on real-world attack traces and outperforms traditional baselines while providing explainable, evidence-cited outputs.",
      "summary": "The paper presents ReGAIN, a framework that uses retrieval-augmented generation (RAG) and LLMs to analyze network traffic. It converts traffic into summaries, retrieves relevant evidence from a vector database, and generates interpretable, grounded analyses. The method achieves high accuracy on attack detection and provides explainable results, outperforming traditional rule-based and ML approaches.",
      "mindmap": "graph TB\n        A[ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Traditional traffic analysis systems have high false positives and lack interpretability.]\n        C[主要方法/Method: Multi-stage framework using traffic summarization, RAG, and LLM reasoning with a hierarchical retrieval pipeline.]\n        D[关键结果/Results: Achieves 95.95%-98.82% accuracy, outperforms baselines, and provides explainable, evidence-grounded responses.]"
    },
    {
      "title": "Scalable Cloud-Native Architectures for Intelligent PMU Data Processing",
      "authors": "Nachiappan Chockalingam, Akshay Deshpande, Lokesh Butra, Ram Sekhar Bodala, Nitin Saksena, Adithya Parthasarathy, Balakrishna Pothineni, Akash Kumar Agarwal",
      "institution": "IEEE, NTT Data, Amtrak, Albertsons Companies",
      "link": "https://arxiv.org/pdf/2512.22231",
      "code": null,
      "tags": [
        "cluster infrastructure",
        "cloud-native",
        "distributed stream processing",
        "containerized microservices",
        "elastic resource orchestration",
        "edge-cloud hybrid"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59263c4210b1af52fedb9e9660a5117d937ac4a63d70c41f31a04dc3c553429f_w640_q70.webp",
      "contributions": "1. A comprehensive theoretical framework for AI-enhanced cloud-based PMU analytics. 2. Mathematical formulations for distributed machine learning optimized for PMU time-series data. 3. Analysis of edge-cloud hybrid architectures with integrated security and privacy considerations.",
      "summary": "This paper proposes a scalable cloud-native architecture to address the latency and scalability challenges of processing high-frequency data from Phasor Measurement Units (PMUs) in smart grids. The method integrates AI with edge and cloud computing, using distributed stream processing and containerized microservices for real-time analytics. The analysis shows the architecture can achieve sub-second response times while scaling to large deployments, providing a robust foundation for next-generation grid analytics.",
      "mindmap": "graph TB\n        Root[”Scalable Cloud-Native Architectures for Intelligent PMU Data Processing”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>PMU数据规模大，传统架构延迟高，可扩展性差”]\n        Method[”主要方法/Method<br>云原生架构，集成AI、边缘与云计算，使用分布式流处理和微服务”]\n        Results[”关键结果/Results<br>实现亚秒级响应，可扩展至大规模部署，提供安全可靠的基础”]"
    },
    {
      "title": "Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction",
      "authors": "Mengxiao Geng, Ran Hong, Xiaoling Xu, Bingxuan Li, Qiegen Liu",
      "institution": "Nanchang University, Hefei Comprehensive National Science Center",
      "link": "https://arxiv.org/pdf/2512.22237",
      "code": null,
      "tags": [
        "medical image reconstruction",
        "diffusion model",
        "cross-domain",
        "meta-information",
        "sinogram adapter",
        "low-dose PET"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/974872f0916ad96ffeb1ed9b169c4f627d5c534046b438f10fd015171720864a_w640_q70.webp",
      "contributions": "1. Proposes a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates cross-modal priors for PET reconstruction. 2. Introduces a meta-information encoding module that transforms clinical parameters into semantic prompts for cross-modal alignment. 3. Designs a cross-domain architecture with a specialized sinogram adapter to capture global physical structures in the projection domain.",
      "summary": "The paper addresses the challenge of low-dose PET image reconstruction, which suffers from noise and loss of detail. It proposes a novel diffusion model called MiG-DM that guides the reconstruction using patient-specific meta-information and processes data across both the projection and image domains. Experiments show that MiG-DM outperforms existing methods in improving image quality and preserving physiological details.",
      "mindmap": "graph TB\n        A[Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction] --> B(核心问题/Problem: Low-dose PET imaging faces noise, reduced contrast, and detail loss)\n        A --> C(主要方法/Method: MiG-DM integrates meta-information prompts and cross-domain (projection & image) processing)\n        A --> D(关键结果/Results: Outperforms SOTA on UDPET and clinical datasets, enhancing quality and preserving details)"
    },
    {
      "title": "We are not able to identify AI-generated images",
      "authors": "Adrien Pavão",
      "institution": "(Institution not explicitly stated in provided content. Author name is Adrien Pavão; no affiliation or email domain is given. Therefore, institution cannot be reliably inferred.)",
      "link": "https://arxiv.org/pdf/2512.22236",
      "code": null,
      "tags": [
        "image forensics",
        "AI-generated images",
        "human evaluation",
        "MidJourney",
        "CC12M",
        "synthetic media detection"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b100d4e4882d297b51639fb736da62f90b11f1d810da4b6665f9da69ef3f31_w640_q70.webp",
      "contributions": "1. Conducted a controlled web-based experiment to empirically test human ability to distinguish real photographs from AI-generated portraits, finding performance near random chance (54% accuracy). 2. Created and released a curated, challenging dataset of 120 images (real from CC12M and AI-generated counterparts from MidJourney) designed to be difficult for humans. 3. Demonstrated that human judgment is insufficient for reliable detection of synthetic media, highlighting the need for greater public awareness and ethical guidelines as AI-generated content becomes more realistic.",
      "summary": "The paper tests the assumption that humans can easily identify AI-generated images through an interactive web experiment where participants classified 20 images as real or AI-generated. Using a carefully curated dataset of 120 difficult portrait images (real from CC12M and AI-generated from MidJourney), the study found an average human accuracy of only 54%, barely above random guessing. The results show that humans struggle to reliably detect AI-generated content, indicating that human judgment alone is becoming insufficient and underscoring the need for awareness and ethical guidelines.",
      "mindmap": "graph TB\n        Root[We are not able to identify AI-generated images] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Can humans reliably distinguish AI-generated images from real photos?]\n        Method[主要方法/Method: Interactive web experiment with a curated dataset of 120 difficult images (CC12M real vs. MidJourney AI-generated)]\n        Results[关键结果/Results: Average human accuracy is 54% (near random), response time ~7.3s, highlighting human insufficiency and need for guidelines]"
    },
    {
      "title": "DiRL: An Efficient Post-Training Framework for Diffusion Language Models",
      "authors": "Ying Zhu, Jiaxin Wan, Xiaoran Liu, Siyanag He, Qiqi Wang, Xu Guo, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu",
      "institution": "Fudan University, Shanghai Innovation Institute, OpenMoss Team",
      "link": "https://arxiv.org/pdf/2512.22234",
      "code": "https://github.com/OpenMOSS/DiRL",
      "tags": [
        "post-training (sft/rlhf)",
        "Diffusion Language Models",
        "FlexAttention",
        "Group Relative Policy Optimization",
        "LMDeploy",
        "blockwise training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0ae2d009d9099214203b1dcca9a8b460cf0609d952e240f981b2689f247e17d_w640_q70.webp",
      "contributions": "1. Proposes DiRL, an efficient post-training framework for Diffusion Language Models (dLLMs) that integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. 2. Introduces DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation specifically designed for dLLMs. 3. Demonstrates state-of-the-art math reasoning performance for dLLMs by training DiRL-8B-Instruct, surpassing comparable models like Qwen2.5 series on benchmarks.",
      "summary": "This paper addresses the underdeveloped and inefficient post-training landscape for Diffusion Language Models (dLLMs). It proposes DiRL, an efficient framework combining accelerated training and optimized inference, and introduces DiPO, a tailored reinforcement learning method. The resulting model, DiRL-8B-Instruct, achieves state-of-the-art math performance among dLLMs.",
      "mindmap": "graph TB\n        A[DiRL: An Efficient Post-Training Framework for Diffusion Language Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[dLLMs后训练低效/Post-training for dLLMs is inefficient]\n        B --> B2[训练与推理目标不匹配/Training-Inference objective mismatch]\n        C --> C1[DiRL框架/DiRL Framework]\n        C1 --> C1_1[整合FlexAttention与LMDeploy/Integrates FlexAttention & LMDeploy]\n        C1 --> C1_2[两阶段后训练/Two-stage post-training (SFT+RL)]\n        C --> C2[DiPO算法/DiPO Algorithm]\n        C2 --> C2_1[无偏GRPO实现/Unbiased GRPO for dLLMs]\n        D --> D1[高效训练与推理/Efficient Training & Inference]\n        D --> D2[数学SOTA性能/Math SOTA Performance]\n        D --> D3[超越Qwen2.5系列/Surpasses Qwen2.5 series]"
    },
    {
      "title": "Enhanced geometry prediction in laser directed energy deposition using meta-learning",
      "authors": "Abdul Malik Al Mardhouf Al Saadi, Amrita Basak",
      "institution": "The Pennsylvania State University",
      "link": "https://arxiv.org/pdf/2512.22241",
      "code": null,
      "tags": [
        "meta-learning",
        "meta-learning",
        "model-agnostic meta-learning",
        "reptile",
        "laser-directed energy deposition",
        "bead geometry prediction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4239294fb44dd0fe97ebc3a123162ba7aaa82e51c2805d4460072daeffe6de9_w640_q70.webp",
      "contributions": "1. Proposed a cross-dataset knowledge transfer model for L-DED bead geometry prediction using meta-learning to address data scarcity and heterogeneity. 2. Investigated and applied two gradient-based meta-learning algorithms (MAML and Reptile) for rapid adaptation to new deposition conditions with limited data. 3. Demonstrated strong generalization performance of the meta-learning models across diverse L-DED processes (powder-fed, wire-fed, hybrid) using minimal training examples, outperforming conventional neural networks.",
      "summary": "This paper addresses the challenge of predicting bead geometry in laser-directed energy deposition (L-DED) where experimental data is scarce and heterogeneous. It proposes using meta-learning algorithms, specifically MAML and Reptile, to enable rapid model adaptation to new printing conditions with very few training examples. The results show that this approach achieves accurate predictions and outperforms traditional neural networks under similar data constraints, demonstrating effective knowledge transfer across different L-DED settings.",
      "mindmap": "graph TB\n        A[Enhanced geometry prediction in L-DED using meta-learning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Data scarcity & heterogeneity in L-DED geometry prediction]\n        C[主要方法/Method: Meta-learning (MAML & Reptile) for cross-dataset knowledge transfer]\n        D[关键结果/Results: Accurate prediction with few examples, outperforms conventional NN]"
    },
    {
      "title": "Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening",
      "authors": "Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf",
      "institution": "Radboud University Medical Center, Radboud University",
      "link": "https://arxiv.org/pdf/2512.22242",
      "code": null,
      "tags": [
        "medical imaging",
        "algorithmic fairness",
        "subgroup performance analysis",
        "JustEFAB framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp",
      "contributions": "1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application.",
      "summary": "This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes.",
      "mindmap": "graph TB\n        Root[Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening<br/>肺癌筛查风险估计模型的公平性评估] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br/>AI肺癌风险模型在不同人口亚组中的性能表现是否公平？<br/>Is AI lung cancer risk model performance fair across demographic subgroups?]\n        Method[主要方法/Method<br/>使用JustEFAB框架评估模型在NLST验证集上的性能差异<br/>Evaluate model performance disparities on NLST validation set using JustEFAB framework]\n        Results[关键结果/Results<br/>发现Sybil和Venkadesh21模型存在显著的、无法用混杂因素解释的性能差异<br/>Found significant, unexplained performance disparities in Sybil and Venkadesh21 models]"
    },
    {
      "title": "Masking Teacher and Reinforcing Student for Distilling Vision-Language Models",
      "authors": "Byung-Kwan Lee, Yu-Chiang Frank Wang, Ryo Hachiuma",
      "institution": "NVIDIA",
      "link": "https://arxiv.org/pdf/2512.22238",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "knowledge distillation",
        "reinforcement learning",
        "vision-language models",
        "progressive masking",
        "offline RL"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp",
      "contributions": "1. Proposes Masters, a mask-progressive RL distillation framework that first masks non-dominant teacher weights to reduce complexity and then progressively restores them for stable student learning. 2. Introduces an offline RL stage with complementary accuracy and distillation rewards, leveraging pre-generated responses from masked teachers for efficient guidance. 3. Demonstrates that progressive teacher scaling (e.g., from 14B to 38B) yields smoother convergence and stronger generalization than one-shot distillation, providing a scalable path to efficient VLMs.",
      "summary": "The paper addresses the challenge of distilling large vision-language models (VLMs) into compact ones by proposing Masters, a framework that uses progressive masking of the teacher model and offline reinforcement learning. This method enables stable knowledge transfer and efficient training, resulting in small VLMs that achieve strong performance, sometimes surpassing larger models, while being far more efficient for deployment.",
      "mindmap": "graph TB\n        A[Masking Teacher and Reinforcing Student for Distilling Vision-Language Models] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[大型VLM难以部署到移动/边缘设备/Large VLMs are impractical for mobile/edge deployment]\n        B --> B2[师生模型尺寸差距导致知识蒸馏不稳定/Large size gap causes unstable distillation]\n        C --> C1[掩码渐进式强化学习蒸馏框架/Mask-progressive RL distillation framework]\n        C --> C2[先掩码教师非主导权重，再渐进恢复/First mask non-dominant teacher weights, then progressively restore]\n        C --> C3[离线RL阶段使用准确性和蒸馏奖励/Offline RL stage with accuracy and distillation rewards]\n        D --> D1[在多个基准测试中超越现有紧凑型VLM/Outperforms existing compact VLMs on diverse benchmarks]\n        D --> D2[渐进增加教师尺寸带来更平滑收敛和更强泛化/Gradually increasing teacher size yields smoother convergence & stronger generalization]\n        D --> D3[提供高效、可部署VLM的可扩展路径/Provides a scalable path toward efficient, deployable VLMs]"
    },
    {
      "title": "Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture",
      "authors": "Phi-Hung Hoang, Nam-Thuan Trinh, Van-Manh Tran, Thi-Thu-Hong Phan",
      "institution": "FPT University",
      "link": "https://arxiv.org/pdf/2512.22239",
      "code": null,
      "tags": [
        "model compression",
        "knowledge distillation",
        "lightweight CNN",
        "inverted residual blocks",
        "dense connectivity",
        "multi-objective learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccaf949c91086899f4aa9953236d8ee76957b0a5aaafcda22bf81f403ee1e0a5_w640_q70.webp",
      "contributions": "1. Proposes a hybrid knowledge distillation framework integrating hard-label supervision, feature-level, response-level, and self-distillation for training efficient models. 2. Designs a customized student CNN architecture combining inverted residual blocks with dense connectivity to balance efficiency and accuracy. 3. Demonstrates strong generalization across multiple agricultural datasets (rice seeds and plant leaf diseases) with significant reductions in computational cost and model size while maintaining high accuracy.",
      "summary": "This paper proposes a multi-objective hybrid knowledge distillation method to create a lightweight CNN for smart agriculture, combining inverted residual and dense blocks. The distilled model achieves near-teacher accuracy with drastically reduced computation and parameters, showing robust performance on rice seed and plant disease datasets.",
      "mindmap": "graph TB\n        Root[”Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture<br>面向智慧农业的高效深度学习的多目标混合知识蒸馏”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Deploying deep models on edge devices in smart agriculture<br>在智慧农业中于边缘设备部署深度模型”] --> P1[”挑战/Challenge<br>Trade-off between efficiency and accuracy<br>效率与准确性的权衡”]\n        Method[”主要方法/Method<br>Hybrid knowledge distillation framework<br>混合知识蒸馏框架”] --> M1[”学生模型/Student Model<br>Customized CNN with inverted residual & dense blocks<br>定制化CNN，含倒残差与密集连接块”]\n        Method --> M2[”教师模型/Teacher Model<br>ResNet18 guidance<br>ResNet18教师网络指导”]\n        Method --> M3[”多目标策略/Multi-objective Strategy<br>Integrates hard-label, feature-level, response-level, self-distillation<br>整合硬标签、特征级、响应级与自蒸馏”]\n        Results[”关键结果/Results<br>Experiments on agricultural datasets<br>在农业数据集上的实验”] --> R1[”性能/Performance<br>98.56% accuracy on rice seeds (vs teacher 98.65%)<br>水稻种子分类准确率98.56%（教师模型98.65%）”]\n        Results --> R2[”效率/Efficiency<br>0.68 GFLOPs, ~1.07M parameters (10x smaller than teacher)<br>0.68 GFLOPs，约107万参数（比教师模型小10倍）”]\n        Results --> R3[”泛化/Generalization<br>Consistent gains on plant leaf disease datasets<br>在植物叶片病害数据集上一致性能提升”]"
    },
    {
      "title": "EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs",
      "authors": "Chama Bensmail",
      "institution": "University of Hertfordshire, Omics Data Solutions LTD",
      "link": "https://arxiv.org/pdf/2512.22240",
      "code": "https://github.com/bensmailchama-boop/EvoXplain",
      "tags": [
        "interpretability",
        "mechanistic multiplicity",
        "explanatory stability",
        "stochastic optimization",
        "model explanations",
        "diagnostic framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a39d3b0c4608e98a5ffde3a753078019f45b0c48774cb02ee158633c35e823d2_w640_q70.webp",
      "contributions": "1. Introduces EvoXplain, a diagnostic framework for measuring the stability of model explanations across repeated training runs, treating explanations as samples from the optimization process. 2. Demonstrates that high-accuracy models (e.g., Logistic Regression, Random Forests) can rely on multiple distinct internal mechanisms, revealing explanatory multimodality not captured by single-model or averaged explanations. 3. Reframes interpretability as a property of a model class under repeated instantiation, challenging the assumption that a single high-accuracy model yields a unique or trustworthy explanation.",
      "summary": "The paper introduces EvoXplain, a framework to diagnose if models achieving similar high accuracy do so via the same or different internal mechanisms by analyzing explanation stability across training runs. It finds that even simple, stable models like Logistic Regression can exhibit multiple distinct explanatory modes on datasets like Breast Cancer and COMPAS, showing that single-model explanations can be misleading. This work highlights explanatory instability as a measurable property and reframes interpretability as a characteristic of a model class rather than a single trained instance.",
      "mindmap": "graph TB\n        A[EvoXplain Paper] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[高精度模型是否共享相同内部逻辑?<br/>Do high-accuracy models share the same internal logic?]\n        C --> C1[跨重复训练测量解释稳定性<br/>Measure explanation stability across repeated training]\n        C --> C2[将解释视为优化过程样本<br/>Treat explanations as samples from optimization]\n        D --> D1[发现解释的多模态性<br/>Found explanatory multimodality]\n        D --> D2[逻辑回归等模型也显示多种机制<br/>Models like Logistic Regression show multiple mechanisms]\n        D --> D3[重新定义可解释性为模型类属性<br/>Reframe interpretability as model-class property]"
    },
    {
      "title": "Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation",
      "authors": "Bhaktipriya Radharapu, Eshika Saxena, Kenneth Li, Chenxi Whitehouse, Adina Williams, Nicola Cancedda",
      "institution": "Meta (FAIR at Meta, Meta Superintelligence Labs)",
      "link": "https://arxiv.org/pdf/2512.22245",
      "code": null,
      "tags": [
        "llm inference",
        "uncertainty estimation",
        "calibration",
        "linear probe",
        "brier score",
        "llm-as-judge"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33ea018b43295c51d076b3840537c861c2e2c7f22ca2bdd183164d1f1feed91d_w640_q70.webp",
      "contributions": "1. Introduces a method using linear probes on LLM hidden states to provide calibrated uncertainty estimates for LLM judges, requiring no additional model training. 2. Demonstrates superior calibration and ≈10x computational savings compared to baseline methods like verbalized confidence. 3. Shows the method generalizes robustly across different model architectures, training paradigms, and unseen evaluation domains.",
      "summary": "This paper addresses the problem of obtaining efficient and well-calibrated uncertainty estimates for LLM-based judges. It proposes using linear probes trained with a Brier score loss on the model's hidden states. The method achieves better calibration with significant computational savings and provides a practical plug-and-play solution for production deployment.",
      "mindmap": "graph TB\n        A[Calibrating LLM Judges<br/>校准LLM法官] --> B[Problem: LLM judges lack efficient, calibrated uncertainty<br/>问题：LLM法官缺乏高效、校准的不确定性估计]\n        A --> C[Method: Linear probes on hidden states with Brier score loss<br/>方法：基于Brier分数损失的隐状态线性探针]\n        A --> D[Results: Better calibration, 10x speedup, robust generalization<br/>结果：更好的校准，10倍加速，鲁棒的泛化]"
    },
    {
      "title": "Graph Attention-based Adaptive Transfer Learning for Link Prediction",
      "authors": "Huashen Lu, Wensheng Gan, Guoting Chen, Zhichao Huang, Philip S. Yu",
      "institution": "Jinan University, Great Bay University, JD Technology, University of Illinois Chicago",
      "link": "https://arxiv.org/pdf/2512.22252",
      "code": "https://github.com/DSI-Lab1/GAATNet",
      "tags": [
        "graph neural networks",
        "graph attention network",
        "link prediction",
        "transfer learning",
        "graph transformer",
        "contrastive loss"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1e9ea1091686509af1da226ce7553577b4005c5646524a5c6718473e451d3c39_w640_q70.webp",
      "contributions": "1. Proposes GAATNet, a novel graph attention adaptive transfer network combining pre-training and fine-tuning for cross-dataset knowledge transfer in link prediction. 2. Incorporates distant neighbor embeddings as biases in self-attention to capture global node features. 3. Introduces a lightweight self-adapter module during fine-tuning to improve training efficiency and generalization.",
      "summary": "The paper addresses challenges in link prediction on large-scale sparse graphs and cross-dataset transfer learning by proposing GAATNet, which integrates graph attention with adaptive transfer strategies. The method uses distant neighbor embeddings and a self-adapter module to enhance global feature capture and training efficiency. Experiments on seven datasets show state-of-the-art performance, offering a scalable solution for integrating GNNs with transfer learning.",
      "mindmap": "graph TB\n        A[Graph Attention-based Adaptive Transfer Learning for Link Prediction] --> B[核心问题/Problem: Challenges in large-scale sparse graphs and cross-dataset transfer learning for link prediction]\n        A --> C[主要方法/Method: Proposes GAATNet with distant neighbor embeddings and lightweight self-adapter for adaptive transfer]\n        A --> D[关键结果/Results: Achieves SOTA performance on seven datasets, provides scalable GNN-transfer learning solution]"
    },
    {
      "title": "Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs",
      "authors": "Pascal Passigan, Kevin zhu, Angelina Ning",
      "institution": "Massachusetts Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22251",
      "code": null,
      "tags": [
        "graph neural networks",
        "biomedical knowledge graph",
        "graph attention network",
        "gene perturbation",
        "multimodal embeddings",
        "PrimeKG++"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a8b52522e1bdfc53ae9978a144c2011810eca2532cb9819ad999bf9b2b6cbb6_w640_q70.webp",
      "contributions": "1. Introduces a novel framework for gene perturbation prediction by merging PrimeKG++ with LINCS L1000 data into a heterogeneous biomedical knowledge graph, moving beyond binary drug-disease association tasks. 2. Demonstrates the application of a Graph Attention Network (GAT) to predict delta gene expression profiles for drug-cell pairs, outperforming MLP baselines. 3. Provides interpretability through ablation studies (edge shuffling, node feature randomization) showing the critical role of biomedical KG edges in enhancing perturbation-level prediction.",
      "summary": "This paper addresses the gap in predicting detailed gene expression changes (perturbations) caused by drugs by constructing a merged biomedical knowledge graph from PrimeKG++ and LINCS L1000 data. The proposed method uses a Graph Attention Network to predict delta expression profiles for drug-cell pairs, which outperforms baseline models and demonstrates the value of graph structure for mechanistic understanding.",
      "mindmap": "graph TB\n        A[Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Predicting granular gene expression changes (perturbations) from drugs, beyond binary drug-disease associations.]\n        C[主要方法/Method: Merge PrimeKG++ & LINCS L1000 into a BKG; use Graph Attention Network (GAT) to predict delta expression.]\n        D[关键结果/Results: Outperforms MLP baselines; ablation shows KG edges enhance prediction for mechanistic modeling.]"
    },
    {
      "title": "Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method",
      "authors": "Satvik Tripathi",
      "institution": "University of Pennsylvania",
      "link": "https://arxiv.org/pdf/2512.22258",
      "code": "https://github.com/satviktri/LSP",
      "tags": [
        "prompt engineering",
        "Logic Sketch Prompting",
        "deterministic prompting",
        "interpretability",
        "rule adherence",
        "clinical decision support"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b5d49d54027e4f7e6b110a48568a0255a45010197e26e8bc344e0cd3e1785a9_w640_q70.webp",
      "contributions": "1. Proposes Logic Sketch Prompting (LSP), a lightweight prompting framework that introduces typed variables and deterministic condition evaluators for structured reasoning., 2. Incorporates a rule-based validator to produce traceable and repeatable outputs, enhancing auditability., 3. Demonstrates significant performance gains over standard prompting methods (zero-shot, chain-of-thought, concise) on pharmacologic logic-compliance tasks across multiple open-weight LLMs.",
      "summary": "The paper addresses the unreliability of LLMs on tasks requiring strict rule adherence and determinism. It proposes Logic Sketch Prompting (LSP), a framework using typed variables and rule-based validation to produce traceable outputs. Evaluations on clinical tasks show LSP significantly outperforms standard prompting methods in accuracy and F1 score, making it suitable for safety-critical systems.",
      "mindmap": "graph TB\n        A[Logic Sketch Prompting (LSP)] --> B[核心问题/Problem: LLMs unreliable on tasks needing strict rules & determinism]\n        A --> C[主要方法/Method: Lightweight framework with typed variables, condition evaluators, rule validator]\n        A --> D[关键结果/Results: Highest accuracy/F1 vs. baselines; suitable for clinical/safety-critical systems]"
    },
    {
      "title": "Agentic Software Issue Resolution with Large Language Models: A Survey",
      "authors": "Zhonghao Jiang, David Lo, Zhongxin Liu",
      "institution": "Zhejiang University, Singapore Management University",
      "link": "https://arxiv.org/pdf/2512.22256",
      "code": "https://github.com/ZhonghaoJiang/Awesome-Issue-Solving",
      "tags": [
        "automated software maintenance",
        "large language models",
        "agentic systems",
        "software issue resolution",
        "reinforcement learning",
        "software engineering"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5c1c5e173acc2646c4322651d8a6c89dabed4b251b6106c2a468adeeafadf5f_w640_q70.webp",
      "contributions": "1. Provides a systematic survey of 126 recent studies on LLM-based agentic software issue resolution. 2. Establishes a taxonomy for the field across three key dimensions: benchmarks, techniques, and empirical studies. 3. Highlights the paradigm shift brought by agentic reinforcement learning in designing and training agentic systems for software engineering.",
      "summary": "This paper surveys the use of Large Language Model (LLM)-based agentic systems for automating complex software issue resolution, such as bug fixing. It reviews recent research, categorizes approaches, and discusses how agentic reinforcement learning is changing system design. The conclusion outlines current challenges and future research directions for improving automated software maintenance.",
      "mindmap": "graph TB\n        Root[Agentic Software Issue Resolution with LLMs: A Survey] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[传统方法依赖人工，效率低/Traditional methods rely on human expertise, inefficient]\n        Method[主要方法/Method] --> M1[基于LLM的智能体系统/LLM-based Agentic Systems]\n        Method --> M2[系统综述126项研究/Systematic survey of 126 studies]\n        Method --> M3[建立三维分类法/Establishes a 3D taxonomy]\n        Results[关键结果/Results] --> R1[增强软件维护效率/Enhances software maintenance efficiency]\n        Results --> R2[为智能体系统提供验证环境/Provides a validation environment for agentic systems]\n        Results --> R3[总结挑战与未来方向/Summarizes challenges & future directions]"
    },
    {
      "title": "Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks",
      "authors": "Abhranil Chandra, Ayush Agrawal, Arian Hosseini, Sebastian Fischmeister, Rishabh Agarwal, Navin Goyal, Aaron Courville",
      "institution": "University of Waterloo, University of Massachusetts Amherst, MILA - Quebec AI Institute, Université de Montréal, Microsoft Research India, Google DeepMind, Periodic Labs",
      "link": "https://arxiv.org/pdf/2512.22255",
      "code": null,
      "tags": [
        "reasoning",
        "chain-of-thought",
        "synthetic data",
        "distribution shift",
        "fine-tuning",
        "reasoning robustness"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf038d101bb93ad9f8c253c481419dec618655c74a6b867d0128b6358a3fa331_w640_q70.webp",
      "contributions": "1. Demonstrates that training on synthetic chain-of-thought traces from more capable models, even when they lead to incorrect final answers, can improve a language model's reasoning performance more than training on human-annotated datasets. 2. Proposes and validates two hypotheses for this phenomenon: the distributional alignment of synthetic data with the model, and the partial validity of reasoning steps within flawed traces. 3. Shows that paraphrasing human traces to align with the model's distribution improves performance, and investigates model tolerance to increasingly flawed reasoning steps.",
      "summary": "This paper challenges the assumption that correctness is the primary determinant of data quality for training language models on reasoning tasks. It shows that fine-tuning on synthetic, incorrect chain-of-thought traces from stronger models can outperform training on correct human-annotated data, primarily because the synthetic data's distribution is closer to the model's own. The key conclusion is that aligning the training data distribution with the model's is more critical for performance than the correctness of the final answers.",
      "mindmap": "graph TB\n        A[Shape of Thought / 思维形态] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Correctness vs. Distribution / 正确性与数据分布]\n        B1 --> B2{Does correctness guarantee better reasoning? / 正确性保证更好的推理吗?}\n        C --> C1[Train on Incorrect Synthetic CoT / 使用错误的合成CoT训练]\n        C --> C2[Paraphrase Human Traces / 改写人类标注的推理链]\n        C --> C3[Introduce Flawed Steps / 引入有缺陷的推理步骤]\n        D --> D1[Synthetic Incorrect > Human Correct / 错误的合成数据优于正确的人类数据]\n        D --> D2[Distribution Alignment is Key / 数据分布对齐是关键]\n        D --> D3[Final Answer ≠ Faithful Reasoning / 最终答案 ≠ 忠实推理过程]"
    },
    {
      "title": "ReVEAL: GNN-Guided Reverse Engineering for Formal Verification of Optimized Multipliers",
      "authors": "Chen Chen, Daniela Kaufmann, Chenhui Deng, Zhan Song, Hongce Zhang, Cunxi Yu",
      "institution": "University of Maryland, College Park; TU Wien; NVIDIA; Hong Kong University of Science and Technology (Guangzhou)",
      "link": "https://arxiv.org/pdf/2512.22260",
      "code": null,
      "tags": [
        "formal verification",
        "computer algebra",
        "reverse engineering",
        "graph neural network",
        "multiplier verification",
        "algebraic circuit verification",
        "SAT-based equivalence checking"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bdd1907cef6efb0b9b81d88b611f825942f30ef8a8674a9587cc4261e4774ef_w640_q70.webp",
      "contributions": "1. Proposes ReVEAL, a graph-learning-based framework for reverse engineering optimized multiplier architectures to recover their word-level structure. 2. Leverages structural graph features and learning-driven inference to identify architectural patterns at scale, enabling robust handling of large, optimized circuits. 3. Integrates smoothly with existing verification flows and supports downstream algebraic proof strategies, showing improvements in scalability and accuracy over traditional rule-based approaches.",
      "summary": "This paper introduces ReVEAL, a method that uses Graph Neural Networks (GNNs) to reverse engineer the architecture of optimized hardware multipliers. This recovered structure enables more effective formal verification using algebraic techniques. The approach demonstrates improved scalability and accuracy compared to traditional rule-based methods on diverse benchmarks.",
      "mindmap": "graph TB\n        A[ReVEAL: GNN-Guided Reverse Engineering for Formal Verification of Optimized Multipliers] --> B(核心问题/Problem: 优化乘法器形式验证困难/Challenges in formal verification of optimized multipliers)\n        A --> C(主要方法/Method: 基于图学习的逆向工程/GNN-guided reverse engineering)\n        A --> D(关键结果/Results: 提升可扩展性与准确性/Improved scalability and accuracy)"
    },
    {
      "title": "LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs",
      "authors": "Bing Hao, Minglai Shao, Zengyi Wo, Yunlong Chu, Yuhang Liu, Ruijie Wang",
      "institution": "Tianjin University, Beihang University, Guangxi Normal University",
      "link": "https://arxiv.org/pdf/2512.22266",
      "code": "https://github.com/Wjerry5/LLMTM",
      "tags": [
        "graph representation learning",
        "temporal motifs",
        "dynamic graphs",
        "llm agent",
        "structure-aware dispatcher",
        "prompting techniques"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ebbb05fef920a296d5863029d0c69e932a75230132aa0658a09e6bd8d04010c_w640_q70.webp",
      "contributions": "1. Proposes LLMTM, a comprehensive benchmark for evaluating LLMs on six tasks across nine types of temporal motifs in dynamic graphs. 2. Develops a tool-augmented LLM agent that uses engineered prompts to achieve high accuracy on temporal motif analysis tasks. 3. Introduces a structure-aware dispatcher that intelligently routes queries between standard LLM prompting and the more powerful agent to balance accuracy and cost.",
      "summary": "This paper studies the use of Large Language Models (LLMs) for analyzing temporal motifs in dynamic graphs, an area that is relatively unexplored. The authors propose a new benchmark (LLMTM), develop a high-accuracy but costly tool-augmented LLM agent, and then introduce a structure-aware dispatcher to reduce cost while maintaining performance. Their experiments show the dispatcher effectively maintains high accuracy while reducing cost.",
      "mindmap": "graph TB\n        A[LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLMs处理动态图时态模体分析能力未知/LLMs' capability for temporal motif analysis on dynamic graphs is unexplored]\n        C --> C1[提出基准LLMTM与智能体/Propose benchmark LLMTM and an agent]\n        C --> C2[提出结构感知调度器/Propose structure-aware dispatcher]\n        D --> D1[调度器保持高精度并降低成本/Dispatcher maintains high accuracy while reducing cost]"
    },
    {
      "title": "The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency",
      "authors": "Dingyu Wang, Zimu Yuan, Jiajun Liu, Shanggui Liu, Nan Zhou, Tianxing Xu, Di Huang, Dong Jiang",
      "institution": "Peking University Third Hospital, Beihang University",
      "link": "https://arxiv.org/pdf/2512.22275",
      "code": null,
      "tags": [
        "multimodal reasoning",
        "clinical reasoning benchmark",
        "vision-language models",
        "multimodal integration",
        "medical image interpretation",
        "hallucination"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43db8495c9ac46c5ea892f723394bd1e6c9b400003c2c67ace7ac9abe26f0bcf_w640_q70.webp",
      "contributions": "1. Introduced the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework with 1,245 questions derived from real-world patient cases to assess clinical reasoning. 2. Revealed a significant performance gap in VLMs, showing high accuracy on structured tasks but poor performance on open-ended, multimodal reasoning tasks, with severe text-driven hallucinations. 3. Demonstrated that medically fine-tuned models show no consistent advantage over general-purpose models, highlighting a fundamental limitation in current AI for clinical competency.",
      "summary": "The paper introduces the Bones and Joints (B&J) Benchmark to rigorously evaluate the clinical reasoning capabilities of vision-language and large language models. The results show that while models perform well on structured tasks, they struggle significantly with open-ended, multimodal reasoning essential for real-world patient care, indicating they are not yet clinically competent. The authors conclude that safe AI deployment should be limited to supportive roles until fundamental breakthroughs in multimodal integration are achieved.",
      "mindmap": "graph TB\n        A[The Illusion of Clinical Reasoning<br>临床推理的假象] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Current benchmarks fail to capture integrated, multimodal clinical reasoning.<br>现有基准无法捕捉综合、多模态临床推理。]\n        C --> C1[Developed the B&J Benchmark with 1245 real-world questions across 7 tasks.<br>开发了包含1245个真实世界问题、涵盖7项任务的B&J基准。]\n        D --> D1[Large performance gap: high on MCQ, low on open-ended multimodal tasks.<br>巨大性能差距：选择题表现好，开放式多模态任务表现差。]\n        D --> D2[VLMs have limitations in image interpretation and exhibit hallucinations.<br>VLM在图像解释方面存在局限并出现幻觉。]\n        D --> D3[Medically fine-tuned models show no consistent advantage.<br>医学微调模型未显示一致优势。]"
    },
    {
      "title": "Valori: A Deterministic Memory Substrate for AI Systems",
      "authors": "Varshith Gudur",
      "institution": "Independent Researcher (Valori Kernel Project)",
      "link": "https://arxiv.org/pdf/2512.22280",
      "code": "https://github.com/varshith-Git/Valori-Kernel",
      "tags": [
        "memory & caching",
        "deterministic memory",
        "fixed-point arithmetic",
        "vector embeddings",
        "approximate nearest neighbor search",
        "state machine"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb7ed17a8a06e6a2b8760f08fa9345391995aee74a3542cacf55dd051b383f_w640_q70.webp",
      "contributions": "1. Identifies and characterizes the fundamental non-determinism in AI memory systems caused by hardware-dependent floating-point arithmetic, which leads to divergent memory states and retrieval results. 2. Proposes Valori, a deterministic AI memory substrate that replaces floating-point operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. 3. Demonstrates that Valori guarantees bit-identical memory states, snapshots, and search results across different hardware platforms (e.g., x86 vs. ARM), establishing deterministic memory as a necessary primitive for trustworthy AI.",
      "summary": "The paper identifies non-determinism in AI memory systems due to hardware-dependent floating-point arithmetic, which compromises replayability and auditability. It proposes Valori, a memory substrate using fixed-point arithmetic and a state machine model to guarantee bit-identical behavior across platforms. The work concludes that deterministic memory is essential for building trustworthy AI systems.",
      "mindmap": "graph TB\n        A[Valori: A Deterministic Memory Substrate for AI Systems] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: AI内存非确定性/AI Memory Non-Determinism]\n        C[主要方法/Method: 固定点算术与状态机/Fixed-Point Arithmetic & State Machine]\n        D[关键结果/Results: 跨平台比特一致性/Cross-Platform Bit-Identical Results]"
    },
    {
      "title": "Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation",
      "authors": "Zikun Guoa, Adeyinka.P. Adedigbaa, Rammohan Mallipeddi",
      "institution": "Kyungpook National University",
      "link": "https://arxiv.org/pdf/2512.22287",
      "code": null,
      "tags": [
        "generative models",
        "Generative Adversarial Networks",
        "Non-Intrusive Load Monitoring",
        "Clustering",
        "LSTM",
        "Pattern Generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37f2c4c207022049ee869567d36df9e77e5a04d1beb0cc584dc57ee6ad1145b_w640_q70.webp",
      "contributions": "1. Proposes a hybrid GAN framework that routes appliances to specialized branches based on behavioral characteristics (intermittent vs. continuous). 2. Introduces a clustering module for intermittent appliances to group similar activation patterns and allocate dedicated generators, improving modeling of both common and rare modes. 3. Employs a separate LSTM-based generator branch for continuous appliances to capture gradual temporal evolution while maintaining training stability through sequence compression.",
      "summary": "This paper proposes the Cluster Aggregated GAN (CAG), a hybrid generative model that synthesizes appliance load patterns by separating intermittent and continuous devices into specialized branches, using clustering for the former and an LSTM for the latter. Experiments on the UVIC dataset show it outperforms baselines in realism, diversity, and training stability. The integration of clustering as an active component also enhances the model's interpretability and scalability.",
      "mindmap": "graph TB\n        Root[”Cluster Aggregated GAN (CAG)”] --> Problem[”核心问题/Problem: 缺乏标记数据，现有GAN方法对所有设备一视同仁，忽略间歇性和持续性设备的行为差异，导致训练不稳定和保真度有限”]\n        Root --> Method[”主要方法/Method: 提出混合生成框架，根据设备行为特征路由到专门分支：间歇性设备使用聚类模块和专用生成器；持续性设备使用LSTM生成器”]\n        Root --> Results[”关键结果/Results: 在UVIC数据集上实验，在真实性、多样性和训练稳定性上优于基线方法，聚类作为主动生成组件提高了可解释性和可扩展性”]"
    },
    {
      "title": "When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing",
      "authors": "Arunkumar V, Nivethitha S, Sharan Srinivas, Gangadharan G.R",
      "institution": "Anna University, National Institute of Technology Tiruchirappalli, University of Missouri",
      "link": "https://arxiv.org/pdf/2512.22290",
      "code": null,
      "tags": [
        "causal inference",
        "Double Machine Learning",
        "Moderated Mediation",
        "Algorithmic Control",
        "Nonmonotonic Effects",
        "Gig Economy"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ffeb8d364908888226033cff39cbb354772ae1044ba1a51632db140d81dca17_w640_q70.webp",
      "contributions": "1. Applied a Double Machine Learning framework to estimate a moderated mediation model without restrictive linear assumptions in organizational research. 2. Uncovered a nonmonotonic relationship between algorithmic oversight, worker wellbeing, and performance, highlighting a \"murky middle\" of confusing oversight. 3. Demonstrated that simple linear models can be misleading and provided practical insights for designing transparent and explainable algorithmic management systems.",
      "summary": "This paper investigates the nonlinear effects of algorithmic control on gig workers. Using a Double Machine Learning approach on survey data, it finds that the link between supportive HR practices and worker performance weakens under opaque algorithmic oversight but strengthens again when the oversight is transparent and explainable.",
      "mindmap": "graph TB\n        Root[”当算法管理人类: 估算算法控制对零工工人绩效和福祉非线性效应的双重机器学习方法 / When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing”]\n        Root --> Problem[”核心问题: 算法管理下，以人为本的管理能否持续？工人对算法的反应是非线性的 / Problem: Can person-centered management survive algorithmic management? Worker responses are nonlinear.”]\n        Root --> Method[”主要方法: 使用双重机器学习框架估算有调节的中介模型，无严格函数形式限制 / Method: Double Machine Learning framework to estimate a moderated mediation model without restrictive functional forms.”]\n        Root --> Results[”关键结果: 发现非单调模式。模糊的算法监督削弱绩效联系，透明可解释的监督则加强它 / Results: Found a nonmonotonic pattern. Murky oversight weakens the performance link, transparent and explainable oversight strengthens it.”]"
    },
    {
      "title": "Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model",
      "authors": "Renping Zhou, Zanlin Ni, Tianyi Chen, Zeyu Liu, Yang Yue, Yulin Wang, Yuxuan Wang, Jingshu Liu, Gao Huang",
      "institution": "Tsinghua University (Leap Lab), Anyverse Dynamics",
      "link": "https://arxiv.org/pdf/2512.22288",
      "code": "https://co-grpo.github.io",
      "tags": [
        "diffusion models",
        "Masked Diffusion Models",
        "Markov Decision Process",
        "Group Relative Policy Optimization",
        "inference schedule optimization",
        "trajectory-level training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp",
      "contributions": "1. Identifies and addresses the discrepancy between the single-step training and multi-step inference of Masked Diffusion Models (MDMs). 2. Proposes Co-GRPO, a method that reformulates MDM generation as a unified Markov Decision Process to jointly optimize model parameters and inference schedule parameters. 3. Introduces a trajectory-level optimization using Group Relative Policy Optimization that avoids costly backpropagation through the multi-step generation process.",
      "summary": "This paper addresses the misalignment between the training and inference procedures of Masked Diffusion Models (MDMs). It proposes Co-GRPO, a method that jointly optimizes the MDM and its inference schedule as a unified Markov Decision Process using Group Relative Policy Optimization. The approach improves generation quality across multiple benchmarks without requiring expensive backpropagation through the full generation trajectory.",
      "mindmap": "graph TB\n        A[Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[训练与推理不匹配/Mismatch between Training & Inference]\n        B1 --> B2[训练: 单步BERT式/Training: Single-step BERT-style]\n        B1 --> B3[推理: 多步有调度/Inference: Multi-step with Schedule]\n        C --> C1[统一MDP/Unified MDP]\n        C1 --> C2[联合优化模型与调度/Jointly Optimize Model & Schedule]\n        C2 --> C3[组相对策略优化/Group Relative Policy Optimization]\n        D --> D1[提升生成质量/Improved Generation Quality]\n        D1 --> D2[在四个基准上验证/Validated on Four Benchmarks]"
    },
    {
      "title": "DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations",
      "authors": "Guokan Chen, Yao Xiao",
      "institution": "Fujian University of Technology",
      "link": "https://arxiv.org/pdf/2512.22283",
      "code": null,
      "tags": [
        "scientific machine learning",
        "Physics-informed neural networks",
        "Kolmogorov-Arnold networks",
        "Adaptive weighting",
        "B-splines",
        "Partial differential equations"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb9e65232c0c340c6072237e2ad1388255462aafca80cf91d4b7f3054eb9fe8c_w640_q70.webp",
      "contributions": "1. Proposes DBAW-PIKAN, a novel architecture combining a Kolmogorov-Arnold Network (KAN) with learnable B-splines for enhanced function representation in solving PDEs., 2. Introduces an adaptive weighting strategy with a dynamic decay upper bound to mitigate gradient flow stiffness and spectral bias, addressing key failure modes of PINNs., 3. Demonstrates significant improvements in convergence speed and solution accuracy (at least an order of magnitude) on benchmarks like Klein-Gordon, Burgers, and Helmholtz equations without added computational cost.",
      "summary": "This paper proposes DBAW-PIKAN, a novel neural network that integrates a Kolmogorov-Arnold architecture with an adaptive weighting strategy to overcome the stiffness and spectral bias challenges faced by Physics-Informed Neural Networks (PINNs) when solving multi-scale PDEs. The method accelerates convergence and improves solution accuracy by at least an order of magnitude on standard benchmarks without increasing computational complexity.",
      "mindmap": "graph TB\n        Root[DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[PINNs struggle with multi-scale/high-frequency PDEs / PINNs在处理多尺度/高频PDE时遇到困难]\n        P1 --> P2[Issues: Gradient flow stiffness & spectral bias / 问题: 梯度流刚度和谱偏差]\n        Method[主要方法/Method] --> M1[Architecture: Kolmogorov-Arnold Network (KAN) with learnable B-splines / 架构: 基于可学习B样条的KAN]\n        Method --> M2[Strategy: Adaptive weighting with dynamic decay upper bound / 策略: 带动态衰减上界的自适应加权]\n        Results[关键结果/Results] --> R1[Faster convergence & higher accuracy / 更快的收敛和更高的精度]\n        R1 --> R2[Improvement: At least one order of magnitude / 提升: 至少一个数量级]\n        Results --> R3[Benchmarks: Klein-Gordon, Burgers, Helmholtz equations / 基准: Klein-Gordon, Burgers, Helmholtz方程]"
    },
    {
      "title": "Multi-Head Spectral-Adaptive Graph Anomaly Detection",
      "authors": "Qingyue Cao, Bo Jin, Changwei Gong, Xin Tong, Wenzheng Li, Xiaodong Zhou",
      "institution": "People's Public Security University of China, Third Research Institute of the Ministry of Public Security, Shanghai Police College",
      "link": "https://arxiv.org/pdf/2512.22291",
      "code": null,
      "tags": [
        "graph anomaly detection",
        "spectral graph neural network",
        "hypernetwork",
        "Chebyshev filter",
        "teacher-student contrastive learning",
        "Barlow Twins loss"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ad6dee88128393dc0036e3430c2763e19882412f9750f09622c52d9ae28dc6_w640_q70.webp",
      "contributions": "1. Proposes a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN) that uses a lightweight hypernetwork to dynamically generate instance-specific Chebyshev filter parameters based on a 'spectral fingerprint'. 2. Introduces a novel dual regularization strategy combining teacher-student contrastive learning (TSC) and Barlow Twins diversity loss (BTD) to prevent mode collapse and ensure representation accuracy and head orthogonality in the multi-head mechanism. 3. Demonstrates through extensive experiments that the method effectively preserves high-frequency anomaly signals and outperforms state-of-the-art methods, especially on highly heterogeneous datasets.",
      "summary": "The paper addresses the problem of graph anomaly detection where fixed filters in spectral GNNs cause over-smoothing and fail to adapt to varying graph structures. It proposes MHSA-GNN, which uses a hypernetwork to generate adaptive filters per instance and a dual regularization strategy to stabilize multi-head learning. Experiments show the method preserves critical high-frequency signals and achieves superior performance, particularly on heterogeneous graphs.",
      "mindmap": "graph TB\n        A[Multi-Head Spectral-Adaptive Graph Anomaly Detection] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[固定滤波器导致过平滑与缺乏适应性/Fixed filters cause over-smoothing & lack adaptability]\n        C --> C1[基于谱指纹的轻量级超网络/Lightweight hypernetwork based on spectral fingerprint]\n        C --> C2[动态生成切比雪夫滤波器参数/Dynamically generates Chebyshev filter parameters]\n        C --> C3[双正则化策略防止模式崩溃/Dual regularization prevents mode collapse]\n        D --> D1[有效保留高频异常信号/Effectively preserves high-frequency anomaly signals]\n        D --> D2[在异构数据集上性能优越/Outperforms SOTA on heterogeneous datasets]"
    },
    {
      "title": "A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation",
      "authors": "Philip Xu, David Elizondo, Raouf Hamzaoui",
      "institution": "De Montfort University",
      "link": "https://arxiv.org/pdf/2512.22294",
      "code": null,
      "tags": [
        "multimodal retrieval and generation",
        "3D retrieval",
        "4D generation",
        "cross-modal alignment",
        "multi-head attention",
        "open-vocabulary"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb826b31ecaac1f8358869614a5af4e6a0fe9eeceb1eb97ce8bbb0ff8afdcd6_w640_q70.webp",
      "contributions": "1. Proposes Uni4D, a unified framework for large-scale open-vocabulary 3D retrieval and controlled 4D generation. 2. Introduces a structured three-level alignment strategy across text, 3D models, and images to enhance semantic understanding. 3. Presents a 3D-Text Multi-head Attention and Search (ATMS) model to optimize text-to-3D retrieval efficiency and accuracy.",
      "summary": "This paper introduces Uni4D, a framework that uses a three-level alignment strategy across text, 3D, and images to address the challenges of large-scale 3D retrieval and controlled 4D generation. The method employs a novel attention and search model to improve semantic alignment and retrieval efficiency. Experimental results demonstrate that Uni4D achieves high-quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding.",
      "mindmap": "graph TB\n        A[Uni4D: A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[大规模3D检索与可控4D生成的挑战/Challenges in large-scale 3D retrieval and controlled 4D generation]\n        C --> C1[三级对齐框架: 文本-3D-图像/Three-level alignment: text-3D-image]\n        C --> C2[ATMS模型优化检索/ATMS model optimizes retrieval]\n        D --> D1[高质量3D检索/High-quality 3D retrieval]\n        D --> D2[可控4D生成/Controlled 4D generation]"
    },
    {
      "title": "Attack-Aware Deepfake Detection under Counter-Forensic Manipulations",
      "authors": "Noor Fatima, Hasan Faraz Khan, Muzammil Behzad",
      "institution": "King Fahd University of Petroleum and Minerals (KFUPM), SDAIA-KFUPM Joint Research Center for Artificial Intelligence",
      "link": "https://arxiv.org/pdf/2512.22303",
      "code": null,
      "tags": [
        "deepfake detection",
        "counter-forensics",
        "red-team training",
        "test-time defense",
        "two-stream architecture",
        "tamper heatmaps"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45e4389b2e929ec3ecb92d5f6329f2086fd32a88abcf98391c61119cd497cc8f_w640_q70.webp",
      "contributions": "1. Proposes an attack-aware deepfake detection method combining red-team training with randomized test-time defense for robustness against counter-forensic manipulations. 2. Introduces a two-stream architecture with a lightweight residual adapter for fusing semantic and forensic features, and a weakly supervised FPN-style head for generating tamper heatmaps. 3. Establishes a practical, modular, and data-efficient baseline with well-calibrated probabilities and actionable evidence, evaluated on standard and challenging surveillance-style datasets.",
      "summary": "This paper addresses the challenge of robust deepfake detection under realistic counter-forensic attacks. It proposes a two-stream model trained with worst-case adversarial manipulations and defended at test-time with random jitters, which achieves strong performance, reliable probability calibration, and useful localization heatmaps. The method provides a practical and data-efficient baseline for attack-aware detection in real-world conditions.",
      "mindmap": "graph TB\n        A[”Attack-Aware Deepfake Detection under Counter-Forensic Manipulations”] --> B[”核心问题/Problem: Robust detection under realistic counter-forensic attacks”]\n        A --> C[”主要方法/Method: Red-team training + Test-time defense in a two-stream architecture”]\n        A --> D[”关键结果/Results: Near-perfect attack ranking, low calibration error, actionable heatmaps”]"
    },
    {
      "title": "Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection",
      "authors": "Chinmay Pushkar, Sanchit Kabra, Dhruv Kumar, Jagat Sesh Challa",
      "institution": "BITS Pilani, Virginia Tech",
      "link": "https://arxiv.org/pdf/2512.22306",
      "code": null,
      "tags": [
        "vulnerability detection",
        "multi-vulnerability detection",
        "count bias",
        "selection bias",
        "long-context code",
        "CWE injection"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd6a5fa286f161d0dab7d6a06a8f54502b88fd2f448edc9ed3609a31efaf97f5_w640_q70.webp",
      "contributions": "1. Introduced a comprehensive benchmark for Multi-Vulnerability Detection across four programming languages (C, C++, Python, JavaScript) to address the limitations of existing single-vulnerability benchmarks. 2. Constructed a novel dataset of 40,000 files by systematically injecting controlled counts of vulnerabilities (1, 3, 5, 9) into long-context code samples, enabling the study of performance under varying vulnerability densities. 3. Quantified the performance degradation of state-of-the-art LLMs (e.g., GPT-4o-mini, Llama-3.3-70B) in high-density vulnerability settings, revealing distinct failure modes like severe \"under-counting\" in Python and JavaScript.",
      "summary": "This paper addresses the gap in evaluating LLMs for detecting multiple vulnerabilities in large, real-world code files. The authors propose a new benchmark by creating a dataset of long code files with systematically injected vulnerabilities and evaluate several LLMs. The main finding is that LLM performance sharply degrades as the number of vulnerabilities per file increases, with significant drops in recall for languages like Python.",
      "mindmap": "graph TB\n        A[Beyond Single Bugs: Benchmarking LLMs for Multi-Vulnerability Detection] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Existing benchmarks are simplistic, focusing on single bugs, not reflecting real-world multi-vulnerability files.]\n        C[主要方法/Method<br>Build a new benchmark with 40k files across 4 languages, injecting controlled vulnerability counts into long code.]\n        D[关键结果/Results<br>LLM performance degrades sharply with more vulnerabilities; distinct failure modes in Python/JS vs. C/C++.]"
    },
    {
      "title": "LLMBoost: Make Large Language Models Stronger with Boosting",
      "authors": "Zehao Chen, Tianxiang Ai, Yifei Li, Gongxun Li, Yuyang Wei, Wang Zhou, Guanghui Li, Bin Yu, Zhijun Chen, Hailong Sun, Fuzhen Zhuang, Jianxin Li, Deqing Wang, Yikun Ban",
      "institution": "Beihang University, China Telecom eSurfing Cloud",
      "link": "https://arxiv.org/pdf/2512.22309",
      "code": null,
      "tags": [
        "llm inference",
        "ensemble learning",
        "boosting",
        "cross-model attention",
        "chain training",
        "near-parallel inference"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb6a601c8d3bba7ec992d00772421c31e3e03d00d8a89a06f09ad3fe3c1b6ce1_w640_q70.webp",
      "contributions": "1. A cross-model attention mechanism that allows successor models to access and fuse hidden states from predecessors for hierarchical error correction and knowledge transfer. 2. A chain training paradigm that progressively fine-tunes connected models with an error-suppression objective to rectify predecessor mispredictions efficiently. 3. A near-parallel inference paradigm that pipelines hidden states across models layer by layer, achieving inference efficiency close to single-model decoding.",
      "summary": "The paper proposes LLMBoost, a novel ensemble fine-tuning framework for LLMs that leverages intermediate hidden states across models. Inspired by boosting, it introduces cross-model attention, chain training, and a near-parallel inference pipeline to improve accuracy and reduce latency. Experiments on reasoning tasks show it consistently boosts performance while maintaining efficient inference.",
      "mindmap": "graph TB\n        A[LLMBoost: Make Large Language Models Stronger with Boosting] --> B[核心问题/Problem: Existing LLM ensemble methods treat models as black boxes, ignoring internal representations.]\n        A --> C[主要方法/Method: A boosting-inspired framework with cross-model attention, chain training, and near-parallel inference.]\n        A --> D[关键结果/Results: Consistently boosts accuracy and reduces inference latency on reasoning tasks.]"
    },
    {
      "title": "LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators",
      "authors": "You Li, Guannan Zhao, Yuhao Ju, Yunqi He, Jie Gu, Hai Zhou",
      "institution": "Northwestern University",
      "link": "https://arxiv.org/pdf/2512.22307",
      "code": null,
      "tags": [
        "hardware security",
        "model protection",
        "logic locking",
        "intellectual property protection",
        "hardware accelerator",
        "model theft",
        "supply chain security"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df323cc56f8d048243dce9e6af97041fca6264165872c422e5f81437cb03ef0d_w640_q70.webp",
      "contributions": "1. Proposes LLA, a hardware-software co-design scheme for protecting generative AI models by embedding key bits into neurons and using invariance transformations to obscure them. 2. Integrates a lightweight, dataflow-compatible locking module into the AI accelerator, using the accelerator with a secret key as a license for model access. 3. Demonstrates that the approach is resilient against oracle-guided key optimization attacks while adding minimal computational overhead (&lt;0.1% for 7,168 key bits).",
      "summary": "The paper introduces LLA, a method to protect generative AI models from supply chain threats like theft and corruption by combining software-based key embedding in neurons with a hardware locking module in the accelerator. This approach uses the accelerator as a license key, ensuring only authorized hardware can run the model correctly. Evaluation shows it effectively resists attacks with negligible performance overhead.",
      "mindmap": "graph TB\n        Root[LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators] --> Problem(核心问题/Problem: Model IP Protection & Supply Chain Threats)\n        Root --> Method(主要方法/Method: Hardware-Software Co-design with Logic Locking)\n        Root --> Results(关键结果/Results: Resists Attacks, <0.1% Overhead)\n        Problem --> P1(模型盗窃/Model Theft)\n        Problem --> P2(模型破坏/Model Corruption)\n        Problem --> P3(信息泄露/Information Leakage)\n        Method --> M1(软件侧: 神经元嵌入密钥/Software: Key Embedding in Neurons)\n        Method --> M2(硬件侧: 轻量级锁定模块/Hardware: Lightweight Locking Module)\n        Results --> R1(抵御优化攻击/Withstands Oracle-Guided Attacks)\n        Results --> R2(低计算开销/Low Computational Overhead)"
    },
    {
      "title": "LangPrecip: Language-Aware Multimodal Precipitation Nowcasting",
      "authors": "Xudong Ling, Tianxi Huang, Qian Dong, Tao He, Chaorong Li, Guiduo Duan",
      "institution": "University of Electronic Science and Technology of China (UESTC), Chengdu Textile College, Yibin University",
      "link": "https://arxiv.org/pdf/2512.22317",
      "code": null,
      "tags": [
        "weather forecasting",
        "multimodal nowcasting",
        "rectified flow",
        "semantic motion constraint",
        "latent space integration",
        "large-scale dataset"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp",
      "contributions": "1. Proposed LangPrecip, a language-aware multimodal nowcasting framework that uses meteorological text as a semantic motion constraint to guide precipitation evolution. 2. Introduced LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. 3. Formulated nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm for efficient and physically consistent multimodal integration.",
      "summary": "The paper proposes LangPrecip, a novel framework that integrates meteorological text descriptions with radar data to constrain precipitation nowcasting. By formulating the problem as semantically constrained trajectory generation using Rectified Flow, it achieves significant performance gains, especially for heavy rainfall at long lead times, as demonstrated on Swedish and MRMS datasets.",
      "mindmap": "graph TB\n        Root[LangPrecip: Language-Aware Multimodal Precipitation Nowcasting] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: 短期降水临近预报存在不确定性，现有方法依赖视觉条件，未来运动约束弱]\n        Method[主要方法/Method: 提出语言感知多模态框架，将气象文本作为语义运动约束，在Rectified Flow范式下进行潜空间集成]\n        Results[关键结果/Results: 在瑞典和MRMS数据集上超越SOTA，在80分钟预见期，强降水CSI提升超60%和19%]"
    },
    {
      "title": "VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning",
      "authors": "Yang Ding, Yizhen Zhang, Xin Lai, Ruihang Chu, Yujiu Yang",
      "institution": "Tsinghua University, The Chinese University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.22315",
      "code": "https://github.com/zsgvivo/VideoZoomer",
      "tags": [
        "video understanding",
        "agentic framework",
        "temporal zoom",
        "reinforcement learning",
        "long video reasoning",
        "multimodal large language models"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9b4b645f4fff1b4f548256b858cb41da2b35db78b1083f110e983d60c3547e_w640_q70.webp",
      "contributions": "1. Proposes VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control visual focus during reasoning for long videos. 2. Introduces a two-stage training strategy combining supervised fine-tuning on distilled trajectories with reinforcement learning to refine the agentic policy. 3. Demonstrates strong performance across long video benchmarks, surpassing open-source models and rivaling proprietary systems with superior efficiency.",
      "summary": "The paper addresses the limitation of Multimodal LLMs in understanding long videos due to context window constraints. It proposes VideoZoomer, an agentic framework that dynamically selects and zooms into key temporal moments for fine-grained evidence gathering, trained with a two-stage strategy. The resulting 7B model achieves state-of-the-art performance on long video reasoning benchmarks with high efficiency.",
      "mindmap": "graph TB\n        A[VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[长视频理解受限/Limited Long Video Understanding]\n        B1 --> B2[上下文窗口限制/Context Window Limitation]\n        B1 --> B3[均匀采样忽略关键证据/Uniform Sampling Overlooks Evidence]\n        C --> C1[代理框架/Agentic Framework]\n        C1 --> C2[动态时间聚焦/Dynamic Temporal Focusing]\n        C2 --> C3[从粗到细推理/Coarse-to-Fine Reasoning]\n        C --> C4[两阶段训练/Two-Stage Training]\n        C4 --> C5[监督微调/Supervised Fine-Tuning]\n        C4 --> C6[强化学习/Reinforcement Learning]\n        D --> D1[性能强劲/Strong Performance]\n        D1 --> D2[超越开源模型/Surpasses Open-Source Models]\n        D1 --> D3[媲美专有系统/Rivals Proprietary Systems]\n        D --> D4[高效推理/Efficient Reasoning]\n        D4 --> D5[低帧预算/Reduced Frame Budget]"
    },
    {
      "title": "SpotEdit: Selective Region Editing in Diffusion Transformers",
      "authors": "Zhibin Qin, Zhenxiong Tan, Zeqing Wang, Songhua Liu, Xinchao Wang",
      "institution": "National University of Singapore, Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.22323",
      "code": "https://biangbiang0321.github.io/SpotEdit.github.io/",
      "tags": [
        "diffusion models",
        "Diffusion Transformers",
        "selective region editing",
        "training-free",
        "perceptual similarity",
        "dynamic fusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4788068dfc021eb102e739694d672f72a617b80ada2a17fbe2ccbc2780fc1287_w640_q70.webp",
      "contributions": "1. Proposes a training-free framework (SpotEdit) for selective region editing in Diffusion Transformers, reducing redundant computation. 2. Introduces SpotSelector to identify stable, unmodified regions via perceptual similarity and skip their denoising. 3. Introduces SpotFusion to adaptively blend reused conditional features with edited tokens, preserving coherence and quality.",
      "summary": "The paper addresses the inefficiency of full-image regeneration in diffusion-based editing when only small regions need modification. It proposes SpotEdit, a training-free framework that selectively updates only modified regions using a selector for stable areas and a fusion mechanism for coherence. This approach reduces computation and maintains fidelity in unedited areas for efficient, precise editing.",
      "mindmap": "graph TB\n        A[SpotEdit: Selective Region Editing in Diffusion Transformers] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[全图去噪冗余/Full-image denoising is redundant for small edits]\n        C --> C1[SpotSelector: 识别稳定区域/Identifies stable regions via perceptual similarity]\n        C --> C2[SpotFusion: 动态特征融合/Dynamically fuses features for coherence]\n        D --> D1[高效编辑/Efficient editing]\n        D --> D2[保持未修改区域保真度/Preserves fidelity in unchanged areas]"
    },
    {
      "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
      "authors": "Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun",
      "institution": "Peking University, Tencent",
      "link": "https://arxiv.org/pdf/2512.22322",
      "code": "https://huggingface.co/collections/yolay/smartsnap",
      "tags": [
        "agent system",
        "self-verifying agent",
        "proactive evidence seeking",
        "LLM-as-a-Judge",
        "3C Principles",
        "agentic reinforcement learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp",
      "contributions": "1. Proposed SmartSnap, a paradigm shift from passive, post-hoc task verification to proactive, in-situ self-verification by the agent itself. 2. Introduced the Self-Verifying Agent, a new agent type with dual missions to complete tasks and prove accomplishment via curated snapshot evidences guided by 3C Principles (Completeness, Conciseness, Creativity). 3. Demonstrated that the SmartSnap paradigm enables scalable training of LLM-driven agents, achieving significant performance gains (up to 26.08% and 16.66%) on mobile tasks and competitive results against larger models.",
      "summary": "The paper addresses the scalability bottleneck in agentic RL caused by costly and unreliable post-hoc task verification. It proposes SmartSnap, a paradigm where agents proactively seek minimal, decisive snapshot evidence to prove task completion during execution, guided by 3C Principles. Experiments show this approach significantly improves agent performance and enables scalable training, achieving competitive results with much larger models.",
      "mindmap": "graph TB\n        A[SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Passive, post-hoc verification is costly and unreliable for agentic RL]\n        C[主要方法/Method: Proactive self-verification via Self-Verifying Agent and 3C Principles]\n        D[关键结果/Results: Performance gains up to 26.08%; competitive with larger models]"
    },
    {
      "title": "Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers",
      "authors": "Sravan Karthick T",
      "institution": "RV College of Engineering (RVCE), Bengaluru, India",
      "link": "https://arxiv.org/pdf/2512.22326",
      "code": null,
      "tags": [
        "time series forecasting",
        "TimeXer",
        "Global M2 Liquidity",
        "exogenous variable",
        "long-horizon forecasting"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38de8480bbb274bd2bcfff3317dfee4cd7813e42e3af4cc61efcd6d928a0ae36_w640_q70.webp",
      "contributions": "1. Introduces the integration of Global M2 Liquidity as a leading exogenous variable with a 12-week lag for Bitcoin price forecasting. 2. Proposes a liquidity-conditioned forecasting model (TimeXer-Exog) based on the TimeXer architecture. 3. Demonstrates that explicit macroeconomic conditioning significantly stabilizes and improves long-horizon forecasts, outperforming a univariate baseline by over 89% at a 70-day horizon.",
      "summary": "This paper addresses the challenge of long-horizon Bitcoin price forecasting by proposing a model that integrates Global M2 Liquidity as an exogenous variable into the TimeXer transformer architecture. The proposed TimeXer-Exog model significantly outperforms univariate benchmarks, showing that conditioning on global macroeconomic factors substantially improves forecast stability and accuracy over long horizons.",
      "mindmap": "graph TB\n        A[Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers] --> B(核心问题/Problem: Bitcoin价格长期预测的极端波动性和非平稳性/Bitcoin's extreme volatility & non-stationarity for long-horizon forecasting)\n        A --> C(主要方法/Method: 集成全球M2流动性作为外生变量，使用TimeXer架构/Integrate Global M2 Liquidity as exogenous variable using TimeXer architecture)\n        A --> D(关键结果/Results: 在70天预测范围内，MSE降低超过89%/At 70-day horizon, MSE reduced by over 89%)"
    },
    {
      "title": "The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma",
      "authors": "Mariya Miteva, Maria Nisheva-Pavlova",
      "institution": "Not explicitly stated in provided content.",
      "link": "https://arxiv.org/pdf/2512.22331",
      "code": null,
      "tags": [
        "medical image analysis",
        "multi-view learning",
        "variational autoencoder",
        "latent representation learning",
        "radiomics",
        "glioblastoma"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bde275b3d90b700f19b613a2539cb5c16f7fb3637de09a820e24738011c2f8da_w640_q70.webp",
      "contributions": "1. Proposed a multi-view latent representation learning framework based on VAEs for integrating complementary MRI radiomic features. 2. Introduced independent probabilistic encoders for each modality to preserve modality-specific structure before fusion in a compact latent space. 3. Applied the learned latent embeddings for the non-invasive classification of MGMT promoter methylation status in glioblastoma.",
      "summary": "This paper addresses the challenge of non-invasively predicting MGMT promoter methylation in glioblastoma from MRI scans. It proposes a multi-view framework using variational autoencoders to integrate features from T1Gd and FLAIR MRI sequences by fusing them in a latent space, aiming to better preserve modality-specific information. The resulting latent embeddings are used for classification, offering a potential improvement over conventional unimodal or early-fusion radiomics approaches.",
      "mindmap": "graph TB\n        A[The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Non-invasive prediction of MGMT methylation in glioblastoma from MRI]\n        C[主要方法/Method: Multi-view VAE framework for latent fusion of T1Gd and FLAIR radiomic features]\n        D[关键结果/Results: Latent embeddings used for MGMT promoter methylation classification]"
    },
    {
      "title": "SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence",
      "authors": "Yiheng Wang, Yixin Chen, Shuo Li, Yifan Zhou, Bo Liu, Hengjian Gao, Jiakang Yuan, Jia Bu, Wanghan Xu, Yuhao Zhou, Xiangyu Zhao, Zhiwang Zhou, Fengxiang Wang, Haodong Duan, Songyang Zhang, Jun Yao, Han Deng, Yizhou Wang, Jiabei Xiao, Jiaqi Liu, Encheng Su, Yujie Liu, Weida Wang, Junchi Yao, Shenghe Zheng, Haoran Sun, Runmin Ma, Xiangchao Yan, Bo Zhang, Dongzhan Zhou, Shufei Zhang, Peng Ye, Xiaosong Wang, Shixiang Tang, Wenlong Zhang, Lei Bai",
      "institution": "Shanghai Artificial Intelligence Laboratory",
      "link": "https://arxiv.org/pdf/2512.22334",
      "code": "https://github.com/InternScience/SciEvalKit",
      "tags": [
        "evaluation & benchmarking",
        "scientific intelligence",
        "multimodal reasoning",
        "benchmarking toolkit"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/84233ab293826e87328abdd509857546d8a108ec2ff9c7ccc92d7c00c26ececa_w640_q70.webp",
      "contributions": "1. Introduces SciEvalKit, a unified, open-source toolkit for evaluating AI models across a broad range of scientific disciplines and core scientific intelligence capabilities. 2. Provides a flexible and extensible evaluation pipeline supporting batch evaluation, custom model/dataset integration, and ensuring transparent, reproducible results. 3. Curates expert-grade scientific benchmarks from real-world, domain-specific datasets to reflect authentic scientific challenges across six major domains.",
      "summary": "The paper introduces SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across multiple disciplines and core competencies like multimodal reasoning and code generation. It provides a flexible, extensible pipeline for reproducible evaluation and is built on expert-grade, real-world scientific benchmarks. The toolkit is open-sourced to foster community-driven development in AI for science.",
      "mindmap": "graph TB\n        A[SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Lack of specialized evaluation for scientific AI across diverse disciplines and capabilities]\n        C[主要方法/Method: Unified benchmarking toolkit with flexible pipeline, real-world benchmarks, and support for six scientific domains]\n        D[关键结果/Results: Open-source toolkit enabling standardized, reproducible evaluation of scientific foundation models]"
    },
    {
      "title": "Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback",
      "authors": "Mengkang Hu, Bowei Xia, Yuran Wu, Ailing Yu, Yude Zou, Qiguang Chen, Shijian Wang, Jiarui Jin, Kexin Li, Wenxiang Jiao, Yuan Lu, Ping Luo",
      "institution": "The University of Hong Kong, Xiaohongshu Inc., UESTC, Harbin Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22336",
      "code": "agent2world.github.io",
      "tags": [
        "agent system",
        "symbolic world models",
        "multi-agent feedback",
        "PDDL",
        "adaptive testing",
        "supervised fine-tuning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3eb7640894bc37e231771de5c5b9dca9d3fe86f38d911d91d2cb55f73a1005c6_w640_q70.webp",
      "contributions": "1. Proposed Agent2World, a tool-augmented multi-agent framework for generating symbolic world models via adaptive multi-agent feedback. 2. Introduced a three-stage pipeline with specialized agents (Deep Researcher, Model Developer, Testing Team) for knowledge synthesis, implementation, and behavior-aware validation. 3. Demonstrated that the framework not only achieves state-of-the-art inference-time performance but also serves as a data engine for supervised fine-tuning, leading to substantial model improvement.",
      "summary": "This paper addresses the challenge of generating correct symbolic world models (like PDDL domains) from natural language by proposing Agent2World, a multi-agent framework that uses adaptive feedback for validation and repair. The method outperforms existing approaches on benchmarks and the feedback collected also enables effective supervised fine-tuning, significantly improving model performance.",
      "mindmap": "graph TB\n        A[Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback] --> B[核心问题/Problem: Lack of verifiable supervision for training LLMs to generate behaviorally correct symbolic world models]\n        A --> C[主要方法/Method: Tool-augmented multi-agent framework with three-stage pipeline: Deep Researcher, Model Developer, and Testing Team for adaptive feedback]\n        A --> D[关键结果/Results: Achieves SOTA inference-time performance; Framework serves as data engine for fine-tuning, yielding ~31% average relative gain]"
    },
    {
      "title": "The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models",
      "authors": "Matthew Riemer, Erik Miehling, Miao Liu, Djallel Bouneffouf, Murray Campbell",
      "institution": "IBM Research, Mila, Université de Montréal",
      "link": "https://arxiv.org/pdf/2512.22337",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "LoRA",
        "catastrophic forgetting",
        "KL divergence",
        "instruction-tuning",
        "parameter-efficient fine-tuning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2e24fadff03a1d6696f3147893642a90d9f500d5c68233669842e5792db7332_w640_q70.webp",
      "contributions": "1. Demonstrates that catastrophic forgetting is a severe problem even during parameter-efficient fine-tuning (LoRA) of LLMs on small datasets. 2. Proposes a simple, low-overhead regularized approximate replay method that penalizes KL divergence from the initial model and interleaves next-token prediction data. 3. Shows that this method effectively preserves the model's general knowledge while maintaining plasticity for new tasks, applied to Qwen models.",
      "summary": "This paper identifies that catastrophic forgetting is a major issue during LoRA-based supervised fine-tuning of large language models, even with small datasets. To solve this, the authors propose a regularized approximate replay method that uses KL divergence regularization and interleaves general pre-training-like data. Their approach successfully preserves the model's original capabilities while allowing adaptation to new instructions, with minimal computational overhead.",
      "mindmap": "graph TB\n        Root[”论文标题: The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem: LoRA微调导致灾难性遗忘/Catastrophic forgetting in LoRA fine-tuning”]\n        Method[”主要方法/Method: 正则化近似回放/Regularized Approximate Replay (KL惩罚+交错数据/KL penalty + interleaved data)”]\n        Results[”关键结果/Results: 保留通用知识，维持可塑性/Preserves general knowledge without hindering plasticity”]"
    },
    {
      "title": "VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement",
      "authors": "Zhengfei Kuang, Rui Lin, Long Zhao, Gordon Wetzstein, Saining Xie, Sanghyun Woo",
      "institution": "Stanford University, Google, Google DeepMind, New York University",
      "link": "https://arxiv.org/pdf/2512.22351",
      "code": "vulcan-3d.github.io",
      "tags": [
        "3D scene understanding and manipulation",
        "Multimodal Large Language Models (MLLMs)",
        "3D object arrangement",
        "tool-augmented agents",
        "MCP-based API",
        "multi-agent framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79364171e71fa2e99be3aaae7f42a6f8bb502acdf59cc5033e98f9a1d424f8bb_w640_q70.webp",
      "contributions": "1. Introduced an MCP-based API to shift interaction from raw code to robust function-level updates, addressing MLLMs' weak visual grounding in 3D. 2. Augmented MLLMs with specialized visual tools for scene analysis, spatial information gathering, and action validation, creating a perceptual feedback loop. 3. Proposed a collaborative multi-agent framework with designated planning, execution, and verification roles to manage iterative, error-prone updates in complex tasks.",
      "summary": "This paper tackles the underexplored challenge of applying Multimodal Large Language Models (MLLMs) to complex 3D object arrangement. The proposed VULCAN system uses an MCP-based API, a suite of visual tools, and a multi-agent framework to enable robust, iterative 3D scene manipulation. The approach significantly outperforms baselines on a diverse set of 25 complex arrangement tasks.",
      "mindmap": "graph TB\n        A[VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>MLLMs在复杂3D场景操控中的应用未被充分探索<br>Application of MLLMs to complex 3D scene manipulation is underexplored]\n        C[主要方法/Method<br>引入MCP API、视觉工具套件和多智能体协作框架<br>Introduces MCP-based API, visual tool suite, and multi-agent collaborative framework]\n        D[关键结果/Results<br>在25个复杂任务上显著超越基线<br>Significantly outperforms baselines on 25 complex tasks]"
    },
    {
      "title": "Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides",
      "authors": "Olaide N. Oyelade, Oliver Hoxey, Yulia Humrye",
      "institution": "North Carolina A&T State University, University of Chichester, Yale University",
      "link": "https://arxiv.org/pdf/2512.22335",
      "code": null,
      "tags": [
        "medical image analysis",
        "vision transformer",
        "whole slide image",
        "HER2 scoring",
        "multi-modality",
        "tumor classification"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03d448dc8fb7520382bb6ea1a3e317817181ebbce215c0d5c387ed2268b2f407_w640_q70.webp",
      "contributions": "1. Proposed a novel mapping function to correlate malignant regions in H&E whole slide images with corresponding regions in IHC images for joint analysis. 2. Developed an end-to-end pipeline using a multi-stage vision transformer system for automatic pixel-level annotation of 4-way HER2 status scoring (0, 1+, 2+, 3+). 3. Embedded a clinically inspired HER2 scoring mechanism that accurately classifies HER2-negative and HER2-positive cases from whole slide images.",
      "summary": "This paper proposes an end-to-end pipeline using multi-stage vision transformers to jointly analyze H&E and IHC whole slide images for HER2 status scoring and tumor classification. The method introduces a novel mapping function to align modalities and provides pixel-level HER2 scoring. The results demonstrate high accuracy (0.94) for HER2 status prediction, showing the method's effectiveness comparable to human pathologists.",
      "mindmap": "graph TB\n    A[Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[挑战: 联合分析H&E和IHC图像进行HER2评分/Challenge: Jointly analyzing H&E and IHC images for HER2 scoring]\n    B --> B2[难点: 现有方法无法提供像素级HER2状态定位/Issue: Existing methods lack pixel-level HER2 status localization]\n    C --> C1[方法: 端到端多阶段视觉Transformer管道/Method: End-to-end multi-stage Vision Transformer pipeline]\n    C --> C2[创新: 新颖的映射函数关联H&E与IHC区域/Innovation: Novel mapping function to correlate H&E and IHC regions]\n    C --> C3[机制: 临床启发的4级HER2评分机制/Mechanism: Clinically inspired 4-way HER2 scoring mechanism]\n    D --> D1[结果: 肿瘤定位分类准确率高/Result: Good classification accuracy for tumor localization]\n    D --> D2[结果: HER2状态预测准确率0.94/Result: 0.94 accuracy for HER2 status prediction]\n    D --> D3[结论: 端到端ViT模型可用于联合评估H&E和IHC图像/Conclusion: End-to-end ViT models usable for jointly evaluating H&E and IHC images]"
    },
    {
      "title": "Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data",
      "authors": "Alaa Alahmadi, Mohamed Hasan",
      "institution": "Newcastle University, University of Leeds",
      "link": "https://arxiv.org/pdf/2512.22349",
      "code": null,
      "tags": [
        "medical image analysis",
        "pseudo-colouring",
        "few-shot learning",
        "prototypical networks",
        "ResNet-18",
        "explainability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp",
      "contributions": "1. Introduces a perception-informed pseudo-colouring technique to encode clinically salient temporal ECG features (like QT-interval) into structured colour representations. 2. Demonstrates that this technique enables effective few-shot and one-shot learning for a complex physiological data task (drug-induced LQTS) using prototypical networks and ResNet-18. 3. Shows that the method improves model explainability by guiding attention to clinically meaningful features and that aggregating multiple cardiac cycles (mirroring human perceptual averaging) further boosts performance.",
      "summary": "This paper addresses the problems of data inefficiency and poor interpretability in deep learning models for physiological signal analysis. It proposes a human-inspired pseudo-colouring technique to encode ECG features, enabling effective few-shot learning and improving model explainability by focusing on clinically relevant signal components. The results demonstrate that incorporating human-like perceptual encoding can bridge data efficiency and interpretability in medical AI.",
      "mindmap": "graph TB\n        A[Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data] --> B1\n        A --> B2\n        A --> B3\n        B1[核心问题/Problem] --> C1[数据效率低/Lack of data efficiency]\n        B1 --> C2[可解释性差/Limited explainability]\n        B1 --> C3[临床可靠性受限/Constrained clinical reliability]\n        B2[主要方法/Method] --> D1[感知启发的伪着色技术/Perception-informed pseudo-colouring]\n        D1 --> E1[编码临床特征/Encode clinical features (e.g., QT-interval)]\n        D1 --> E2[结构化颜色表示/Structured colour representations]\n        B2 --> D2[原型网络与ResNet-18/Prototypical networks & ResNet-18]\n        B2 --> D3[聚合多个心跳周期/Aggregate multiple cardiac cycles]\n        B3[关键结果/Results] --> F1[实现少样本与单样本学习/Achieve few-shot & one-shot learning]\n        B3 --> F2[提升可解释性/Improve explainability (guide attention)]\n        B3 --> F3[桥接数据效率与因果推理/Bridge data efficiency & causal reasoning]"
    },
    {
      "title": "Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries",
      "authors": "Saurabh Deochake, Debajyoti Mukhopadhyay",
      "institution": "SentinelOne, WIDiCoReL Research Lab",
      "link": "https://arxiv.org/pdf/2512.22364",
      "code": null,
      "tags": [
        "llm inference",
        "Text-to-SQL",
        "Cloud Cost Optimization",
        "Query Efficiency",
        "Large Language Models",
        "Google BigQuery"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06f17566f5fb65cb73b79b0dbb64bde11c2f87d177f02865af7fc2d8910e3ac4_w640_q70.webp",
      "contributions": "1. Introduced a cloud-native cost evaluation methodology for Text-to-SQL systems, measuring bytes processed, slot utilization, and estimated query cost on production infrastructure. 2. Conducted an empirical evaluation of six LLMs on Google BigQuery, demonstrating that reasoning models achieve significantly lower cloud compute costs while maintaining high correctness. 3. Quantified cost variance across models, identified prevalent inefficiency patterns (e.g., missing partition filters), and provided deployment guidelines for cost-sensitive environments.",
      "summary": "This paper studies the cloud compute costs of SQL queries generated by Large Language Models (LLMs) for Text-to-SQL tasks. By evaluating six state-of-the-art LLMs on Google BigQuery, it finds that reasoning models are more cost-efficient, processing far fewer bytes, and that execution time is a poor proxy for cloud cost. The work provides a new cost-focused evaluation methodology and guidelines for deploying cost-aware Text-to-SQL systems.",
      "mindmap": "graph TB\n        A[Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Existing efficiency metrics (e.g., VES) measure time, not cloud compute costs.] --> B1[问题背景/Context<br>LLMs achieve high Text-to-SQL accuracy, but cost efficiency in cloud deployments is unknown.]\n        C[主要方法/Method<br>Systematic evaluation of 6 LLMs on Google BigQuery (StackOverflow dataset).] --> C1[评估指标/Metrics<br>Measure bytes processed, slot utilization, estimated cost, and correctness.]\n        D[关键结果/Results] --> D1[发现1/Finding 1<br>Reasoning models process 44.5% fewer bytes with equivalent correctness.]\n        D --> D2[发现2/Finding 2<br>Weak correlation (r=0.16) between execution time and query cost.]\n        D --> D3[发现3/Finding 3<br>Up to 3.4x cost variance; standard models produce high-cost outliers.]"
    },
    {
      "title": "Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions",
      "authors": "Ángel Aso-Mollar, Diego Aineto, Enrico Scala, Eva Onaindia",
      "institution": "Universitat Politècnica de València, Università degli Studi di Brescia",
      "link": "https://arxiv.org/pdf/2512.22367",
      "code": null,
      "tags": [
        "automated planning",
        "numeric planning",
        "control parameters",
        "subgoaling heuristics",
        "optimistic compilation",
        "infinite action space"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abc3c2ae44e3bb664061095be1d4f9beb835e627f31f49a8bfccf9fd7df412ea_w640_q70.webp",
      "contributions": "1. Identifies a tractable subset of numeric planning problems with infinite actions (controllable, simple numeric problems)., 2. Proposes an optimistic compilation approach that transforms these problems into standard simple numeric tasks by abstracting control-dependent expressions., 3. Enables the effective use of traditional subgoaling heuristics for goal distance estimation in this challenging setting, pushing the state of the art.",
      "summary": "This paper addresses the challenge of applying standard numeric heuristics in planning problems with an infinite number of actions due to control parameters. It proposes an optimistic compilation method that transforms a tractable subset of these problems into simpler numeric tasks, enabling the use of subgoaling heuristics. The results show this approach is effective and computationally feasible for handling infinite action spaces.",
      "mindmap": "graph TB\n        Root[”Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions<br>基于子目标松弛的启发式方法用于无限动作数值规划”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Standard heuristics fail for infinite actions from control parameters<br>标准启发式方法无法处理控制参数导致的无限动作”] --> P1[”问题背景/Context<br>Numeric planning with control parameters<br>带控制参数的数值规划”]\n        Method[”主要方法/Method<br>Optimistic compilation to simple numeric tasks<br>通过乐观编译转为简单数值任务”] --> M1[”关键步骤/Key Step<br>Abstract control-dependent expressions<br>抽象控制依赖表达式”]\n        Results[”关键结果/Results<br>Effective & feasible use of subgoaling heuristics<br>子目标启发式方法有效且可行”] --> R1[”结论/Conclusion<br>Pushes state of the art<br>推动技术前沿”]"
    },
    {
      "title": "Self-Evaluation Unlocks Any-Step Text-to-Image Generation",
      "authors": "Xin Yu, Xiaojuan Qi, Zhengqi Li, Kai Zhang, Richard Zhang, Zhe Lin, Eli Shechtman, Tianyu Wang, Yotam Nitzan",
      "institution": "The University of Hong Kong, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.22374",
      "code": null,
      "tags": [
        "diffusion models",
        "text-to-image generation",
        "flow matching",
        "self-evaluation",
        "any-step inference",
        "from-scratch training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp",
      "contributions": "1. Introduces the Self-Evaluating Model (Self-E), a novel from-scratch training framework that combines local flow matching with a self-evaluation mechanism, eliminating the need for a pretrained teacher model. 2. Enables \"any-step\" inference, allowing the same model to perform both high-quality few-step and many-step generation, bridging the gap between local supervision and global matching paradigms. 3. Demonstrates competitive performance with state-of-the-art flow matching models at high step counts while excelling at very low step counts, offering a unified and scalable solution.",
      "summary": "The paper addresses the problem that traditional diffusion/flow models require many inference steps, and distillation methods need a pretrained teacher. It proposes Self-E, a model trained from scratch that uses self-evaluation as a dynamic teacher to enable high-quality generation at any number of steps. The results show Self-E excels at few-step generation and is competitive at many steps, providing a unified framework for efficient and scalable text-to-image synthesis.",
      "mindmap": "graph TB\n        Root[Self-Evaluation Unlocks Any-Step Text-to-Image Generation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Traditional models need many steps or a teacher model] --> Problem_Sub1[传统模型需要多步或教师模型/Traditional models need many steps or a teacher]\n        Method[主要方法/Method: Self-Evaluating Model (Self-E)] --> Method_Sub1[结合流匹配与自评估/Combines Flow Matching & Self-Evaluation]\n        Results[关键结果/Results: Unified any-step model] --> Results_Sub1[少步与多步均表现优异/Excels at both few-step and many-step]"
    },
    {
      "title": "Towards Efficient Post-Training via Fourier-Driven Adapter Architectures",
      "authors": "Donggyun Bae, Jongil Park",
      "institution": "Konkuk University",
      "link": "https://arxiv.org/pdf/2512.22378",
      "code": null,
      "tags": [
        "parameter-efficient fine-tuning",
        "Fourier-Activated Adapter",
        "random Fourier features",
        "frequency-aware activation",
        "parameter-efficient fine-tuning",
        "spectral sparsity"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cf813cee09035fa7f545005f9b12789221e4e00ecb3d551020cff824fb62233_w640_q70.webp",
      "contributions": "1. Proposes the Fourier-Activated Adapter (FAA), a novel PEFT framework that integrates random Fourier features to decompose representations into frequency components. 2. Introduces a dynamic, frequency-aware activation mechanism to selectively modulate semantic information across different frequency bands. 3. Demonstrates through extensive experiments that FAA achieves competitive or superior performance on multiple benchmarks while maintaining low computational overhead.",
      "summary": "This paper proposes the Fourier-Activated Adapter (FAA), a parameter-efficient fine-tuning method for large language models that uses random Fourier features to enable frequency-aware modulation of semantic representations. Experiments on GLUE and other benchmarks show that FAA achieves strong performance with low computational cost, highlighting the effectiveness of its frequency-based approach.",
      "mindmap": "graph TB\n        A[Towards Efficient Post-Training via Fourier-Driven Adapter Architectures] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有PEFT方法难以捕获高频语义信息 / Existing PEFT methods struggle to capture high-frequency semantic information]\n        C --> C1[提出傅里叶激活适配器(FAA) / Propose Fourier-Activated Adapter (FAA)]\n        C1 --> C2[集成随机傅里叶特征分解表示 / Integrate random Fourier features to decompose representations]\n        C2 --> C3[使用频率感知机制选择性调制 / Use frequency-aware mechanism for selective modulation]\n        D --> D1[在多个基准测试中取得有竞争力的结果 / Achieves competitive results on multiple benchmarks]\n        D --> D2[保持低计算和内存开销 / Maintains low computational and memory overhead]"
    },
    {
      "title": "AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents",
      "authors": "Bhanu Prakash Vangala, Ali Adibifar, Tanu Malik, Ashish Gehani",
      "institution": "University of Missouri, SRI International",
      "link": "https://arxiv.org/pdf/2512.22387",
      "code": null,
      "tags": [
        "agent system",
        "reproducibility",
        "dependency management",
        "code generation",
        "large language models",
        "empirical study"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3f3a69233ca5eacf2fea5882b11aeb102413519f3be78440b3532150966328b_w640_q70.webp",
      "contributions": "1. Introduces a three-layer dependency framework (claimed, working, runtime) to quantify the execution reproducibility of LLM-generated code. 2. Conducts an empirical study evaluating three state-of-the-art LLM coding agents across 300 projects in three programming languages, revealing low out-of-the-box execution success rates. 3. Discovers a significant hidden dependency problem, with an average 13.5x expansion from declared to actual runtime dependencies.",
      "summary": "This paper investigates the reproducibility of code generated by LLM-based coding agents. It proposes a three-layer dependency framework and conducts an empirical study on 300 projects, finding that only 68.3% execute successfully out-of-the-box and that actual runtime dependencies are significantly larger than declared ones. The study concludes that AI-generated code currently suffers from major reproducibility issues due to dependency gaps and code generation errors.",
      "mindmap": "graph TB\n        Root[”AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents”] --> Problem[”核心问题/Problem: Is AI-generated code reproducible?”]\n        Root --> Method[”主要方法/Method: Empirical study using a three-layer dependency framework on 300 projects from 3 LLM agents.”]\n        Root --> Results[”关键结果/Results: Low out-of-the-box execution rate (68.3%) and large hidden dependencies (13.5x expansion).”]"
    },
    {
      "title": "Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration",
      "authors": "Bruno Mlodozeniec, Pierre Ablin, Louis Béthune, Dan Busbridge, Michal Klein, Jason Ramapuram, Marco Cuturi",
      "institution": "Apple, University of Cambridge",
      "link": "https://arxiv.org/pdf/2512.22382",
      "code": null,
      "tags": [
        "llm training",
        "hyperparameter transfer",
        "Complete(d)P parameterisation",
        "per-module hyperparameter optimisation",
        "scaling laws",
        "evolutionary strategy"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06e0593eb453191fce60eeebdd4eb6147ab462084db9eb8d81ea8e2486d468be_w640_q70.webp",
      "contributions": "1. Proposes the Complete(d)P parameterisation, a unified framework for scaling hyperparameters across model width, depth, batch size, and training duration. 2. Investigates and enables the transfer of per-module hyperparameters (e.g., learning rates, weight decay) across model scales, moving beyond global hyperparameter transfer. 3. Provides practical guidelines for navigating the high-dimensional per-module hyperparameter optimization landscape and demonstrates significant training speed improvements in Large Language Models.",
      "summary": "This paper addresses the challenge of hyperparameter transfer across different model scales and configurations. It introduces the Complete(d)P parameterisation to unify scaling across width, depth, batch size, and duration, and demonstrates that with this method, even granular per-module hyperparameters can be optimized on a small model and successfully transferred to much larger models, leading to faster training and improved performance.",
      "mindmap": "graph TB\n        A[Completed Hyperparameter Transfer<br/>超参数迁移研究] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Hyperparameter tuning is critical for large models<br/>大模型超参数调优至关重要]\n        B --> B2[Transferring optimal HPs across scales is challenging<br/>跨规模最优超参数迁移困难]\n        C --> C1[Propose Complete(d)P parameterisation<br/>提出Complete(d)P参数化方法]\n        C --> C2[Enable per-module HP optimisation & transfer<br/>实现模块级超参数优化与迁移]\n        D --> D1[Direct HP transfer to ~600x larger scale<br/>超参数可直接迁移至约600倍规模]\n        D --> D2[Per-module HPs yield training speedup<br/>模块级超参数带来训练加速]"
    },
    {
      "title": "LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition",
      "authors": "Elsen Ronando, Sozo Inoue",
      "institution": "Kyushu Institute of Technology, Universitas 17 Agustus 1945 Surabaya",
      "link": "https://arxiv.org/pdf/2512.22385",
      "code": null,
      "tags": [
        "few-shot learning",
        "exemplar selection",
        "large language model",
        "human activity recognition",
        "facility-location optimization",
        "PageRank"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90a149428a6ff84d38e1ab6a741982394b05c9d5b98eaaa538dcd12183dbe7bf_w640_q70.webp",
      "contributions": "1. Proposes an LLM-Guided Exemplar Selection framework that incorporates semantic reasoning via LLM-generated knowledge priors (feature importance, inter-class confusability, budget multipliers) for HAR. 2. Integrates these semantic priors with multiple geometric and structural cues (margin-based validation, PageRank centrality, hubness penalization, facility-location optimization) for a unified exemplar scoring and selection process. 3. Demonstrates superior performance (88.78% macro F1-score on UCI-HAR) under strict few-shot conditions compared to classical selection methods like random sampling, herding, and k-center.",
      "summary": "This paper addresses the limitation of relying on large labeled datasets and purely geometric exemplar selection in Human Activity Recognition (HAR) by proposing an LLM-Guided Exemplar Selection framework. The method uses an LLM to generate semantic knowledge priors, which are combined with structural and geometric cues to select a compact, informative set of exemplars for few-shot learning. Evaluated on the UCI-HAR dataset, the framework outperforms classical selection approaches, showing that integrating semantic reasoning improves representative exemplar selection for wearable-sensor HAR.",
      "mindmap": "graph TB\n        A[LLM-Guided Exemplar Selection for Few-Shot HAR] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[依赖大数据集与几何选择 / Reliance on large datasets & geometric selection]\n        B --> B2[难以区分相似活动 / Hard to distinguish similar activities]\n        C --> C1[LLM生成语义先验 / LLM-generated semantic priors]\n        C --> C2[结合多线索优化 / Combine multiple cues for optimization]\n        D --> D1[性能超越基线 / Outperforms baselines (88.78% F1)]\n        D --> D2[语义先验有效 / Semantic priors are effective]"
    },
    {
      "title": "HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification",
      "authors": "Bhanu Prakash Vangala, Sajid Mahmud, Pawan Neupane, Joel Selvaraj, Jianlin Cheng",
      "institution": "University of Missouri",
      "link": "https://arxiv.org/pdf/2512.22396",
      "code": null,
      "tags": [
        "hallucination detection",
        "hallucination detection",
        "retrieval-augmented verification",
        "contradiction graph",
        "Paraphrased Hallucination Consistency Score (PHCS)",
        "materials science"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2d61ec50b266277e33c913c31df1946cc27990dbacfbd8d5b4979a627f3fa00_w640_q70.webp",
      "contributions": "1. Introduces HalluMatData, a benchmark dataset for evaluating hallucination detection in AI-generated materials science content. 2. Proposes HalluMatDetector, a multi-stage hallucination detection framework integrating intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment. 3. Introduces the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries.",
      "summary": "This paper addresses the problem of factual hallucinations in LLM-generated materials science content. It proposes HalluMatDetector, a multi-stage verification framework that combines intrinsic checks, retrieval, and contradiction analysis to detect and mitigate errors. The method reduces hallucination rates by 30% compared to standard LLM outputs and introduces a new metric (PHCS) for evaluating response consistency.",
      "mindmap": "graph TB\n        Root[”HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content”] --> Problem[”核心问题/Problem: LLM Hallucinations in Scientific Content”]\n        Root --> Method[”主要方法/Method: Multi-Stage Verification Framework (HalluMatDetector)”]\n        Root --> Results[”关键结果/Results: 30% Hallucination Reduction & New Metric (PHCS)”]"
    },
    {
      "title": "Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings",
      "authors": "Ozan Oguztuzun, Cerag Oguztuzun",
      "institution": "Case Western Reserve University",
      "link": "https://arxiv.org/pdf/2512.22398",
      "code": null,
      "tags": [
        "llm inference",
        "knowledge graph embeddings",
        "inference-time personalization",
        "parameter-efficient adaptation",
        "structure-gated adaptation",
        "frozen backbone"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b860d25f74e154dce6d1b1a346b065fe4b4e238a600b81e91ad6a825aa744e1c_w640_q70.webp",
      "contributions": "1. A post-hoc personalization mechanism that operates at inference time on frozen KG embeddings without backbone updates. 2. A structure-gated adaptation method that conditions candidate rankings on profile features via graph-derived gates. 3. New evaluation metrics for personalization (Alignment@k and Counterfactual Responsiveness) to quantify alignment and causal responsiveness.",
      "summary": "The paper addresses the problem that foundation models for knowledge graphs perform well for groups but fail to capture individual user preferences. It proposes GatedBias, a lightweight framework that adds interpretable, per-entity biases to frozen KG embeddings at inference time using profile features and graph-derived gates, requiring only ~300 parameters. The method significantly improves personalized ranking alignment on benchmark datasets while preserving global accuracy, demonstrating that parameter-efficient and causally verifiable personalization is possible.",
      "mindmap": "graph TB\n        Root[”Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings”] --> Problem[”核心问题/Problem: Foundation KG models fail to capture individual user preferences”]\n        Root --> Method[”主要方法/Method: GatedBias framework using structure-gated adaptation on frozen embeddings”]\n        Root --> Results[”关键结果/Results: Improves alignment, preserves cohort performance, parameter-efficient”]"
    },
    {
      "title": "BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks",
      "authors": "Omar Alsaqa, Linh Thi Hoang, Muhammed Fatih Balin",
      "institution": "Wilfrid Laurier University, Singapore Management University, Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22388",
      "code": null,
      "tags": [
        "others",
        "Graph Neural Networks",
        "Multi-armed Bandits",
        "Layer-wise Sampling",
        "Node Importance",
        "Efficient Training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1be93ab34a4af58594bc48c8d52cc9f7177c298fc314982c8ac7ae27b2086b70_w640_q70.webp",
      "contributions": "1. Introduces BLISS, a novel adaptive sampling strategy using multi-armed bandits to dynamically select informative nodes at each GNN layer. 2. Balances exploration and exploitation to ensure comprehensive graph coverage and adapts to evolving node importance, unlike static methods. 3. Demonstrates versatility by integrating with different GNN architectures (GCNs and GATs) and maintaining or exceeding full-batch training accuracy.",
      "summary": "The paper addresses the computational bottleneck in training Graph Neural Networks (GNNs) on large graphs by proposing BLISS, a Bandit Layer Importance Sampling Strategy. BLISS uses multi-armed bandits to dynamically and adaptively sample the most informative nodes at each layer. Experiments show that this method maintains or even surpasses the accuracy of full-batch training while being more efficient.",
      "mindmap": "graph TB\n        A[BLISS: Bandit Layer Importance Sampling Strategy] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[训练大型图神经网络的计算成本高/High computational cost for training GNNs on large graphs]\n        C --> C1[使用多臂老虎机动态选择信息节点/Using multi-armed bandits to dynamically select informative nodes]\n        C --> C2[平衡探索与利用，自适应节点重要性/Balancing exploration and exploitation, adapting to node importance]\n        D --> D1[保持或超过全批次训练的精度/Maintains or exceeds full-batch training accuracy]"
    },
    {
      "title": "Efficient Multi-Model Orchestration for Self-Hosted Large Language Models",
      "authors": "Bhanu Prakash Vangala, Tanu Malik",
      "institution": "University of Missouri",
      "link": "https://arxiv.org/pdf/2512.22402",
      "code": null,
      "tags": [
        "llm inference",
        "Kubernetes",
        "Helm",
        "DistilBERT",
        "scale-to-zero",
        "hybrid routing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e9270400ac5f7fbf1ac4048cb82d9527c762106e232f9cd97653eb0ab3bdb4_w640_q70.webp",
      "contributions": "1. A unified Helm-based deployment system for self-hosted LLMs on Kubernetes, 2. An adaptive scale-to-zero automation mechanism for efficient GPU resource utilization, 3. A hybrid routing module combining keyword heuristics and a lightweight DistilBERT classifier to balance cost, latency, and accuracy.",
      "summary": "The paper introduces \"Pick and Spin,\" a framework for efficient orchestration of self-hosted large language models. It addresses challenges in GPU utilization and workload routing by integrating Kubernetes-based deployment, adaptive scaling, and a hybrid routing strategy. The system demonstrates significant improvements in success rate, latency, and cost compared to static deployments.",
      "mindmap": "graph TB\n        A[Efficient Multi-Model Orchestration for Self-Hosted LLMs] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Self-hosted LLM deployment challenges: GPU utilization, workload routing, reliability/自托管LLM部署挑战：GPU利用率、工作负载路由、可靠性]\n        C --> C1[Pick and Spin Framework: Kubernetes, Helm, scale-to-zero, hybrid routing/Pick and Spin框架：Kubernetes, Helm, 缩容至零, 混合路由]\n        D --> D1[21.6% higher success rate, 30% lower latency, 33% lower cost/成功率提升21.6%，延迟降低30%，成本降低33%]"
    },
    {
      "title": "Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving",
      "authors": "Rui Li, Zhaoning Zhang, Libo Zhang, Huaimin Wang, Xiang Fu, Zhiquan Lai",
      "institution": "National University of Defense Technology",
      "link": "https://arxiv.org/pdf/2512.22420",
      "code": null,
      "tags": [
        "llm inference",
        "speculative decoding",
        "dynamic adaptation",
        "multi-armed bandit",
        "throughput optimization",
        "latency reduction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c6466394cd16e760ca78e05f13eba9852a284e7e8231b58de2c71fbee1e7b39_w640_q70.webp",
      "contributions": "1. Identifies the critical trade-off in speculative decoding: beneficial in memory-bound (low-load) scenarios but detrimental in compute-bound (high-load) scenarios due to verification overhead. 2. Proposes Nightjar, a novel learning-based algorithm that dynamically adapts the speculative length (or disables SD) based on real-time request load and batch size. 3. Demonstrates significant performance gains, achieving up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding.",
      "summary": "The paper addresses the inefficiency of fixed-length speculative decoding in LLM serving, which fails to adapt to dynamic request loads. It proposes Nightjar, a learning-based algorithm that dynamically selects the optimal speculative length. Experiments show Nightjar significantly improves throughput and reduces latency compared to standard speculative decoding.",
      "mindmap": "graph TB\n        A[Nightjar: Dynamic Adaptive Speculative Decoding] --> B[核心问题/Problem: Fixed speculative length fails under dynamic loads]\n        A --> C[主要方法/Method: Learning-based algorithm adapts speculative length]\n        A --> D[关键结果/Results: Higher throughput, lower latency]"
    },
    {
      "title": "A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot",
      "authors": "Amro Gamar, Ahmed Abduljalil, Alargam Mohammed, Ali Elhenidy, Abeer Tawakol",
      "institution": "Mansoura University, Egypt",
      "link": "https://arxiv.org/pdf/2512.22408",
      "code": null,
      "tags": [
        "on-device ai",
        "Heterogeneous Computing",
        "ROS 2",
        "FreeRTOS",
        "PID Control",
        "AWS IoT"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49a4266e78db3945d26829ffd5e030ccc9fa68be1c543cca653fc81df446dfe_w640_q70.webp",
      "contributions": "1. Developed a heterogeneous computing architecture combining a Raspberry Pi 5 with ROS 2 for high-level AI perception/path planning and an ESP32 with FreeRTOS for real-time motor control. 2. Implemented a low-latency, reliable communication link between the ROS 2 host and the embedded controller to ensure system coordination. 3. Enhanced system reliability through deterministic PID-based motor control with static memory allocation and integrated AWS IoT monitoring with a firmware-level motor shutdown failsafe.",
      "summary": "This paper presents the development of an autonomous delivery robot using a unified, multi-disciplinary approach. It employs a heterogeneous computing architecture to handle AI-based navigation on a Raspberry Pi and real-time motor control on an ESP32, addressing challenges like algorithm optimization and inter-processor communication. The result is a robust, operational system demonstrated to be capable of real-world deployment.",
      "mindmap": "graph TB\n        Root[”A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Limitations of human-operated last-mile delivery (cost, safety, reliability)”] --> P1[”子问题/Sub-Problem<br>Need for autonomous, cost-efficient delivery robot”]\n        Method[”主要方法/Method<br>Unified multi-disciplinary approach”] --> M1[”异构计算/Heterogeneous Computing<br>RPi 5 (ROS 2) for AI & ESP32 (FreeRTOS) for control”]\n        Method --> M2[”关键技术/Key Tech<br>Low-latency comms, PID control, AWS IoT, failsafe”]\n        Results[”关键结果/Results<br>Robust, operational autonomous delivery system”] --> R1[”成果/Outcome<br>Deterministic motor control & enhanced reliability”]"
    },
    {
      "title": "Emergence of Human to Robot Transfer in Vision-Language-Action Models",
      "authors": "Simar Kareer, Karl Pertsch, James Darpinian, Judy Hoffman, Danfei Xu, Sergey Levine, Chelsea Finn, Suraj Nair",
      "institution": "Physical Intelligence, Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22414",
      "code": null,
      "tags": [
        "robot learning",
        "vision-language-action models",
        "human-to-robot transfer",
        "co-training",
        "emergent capability",
        "embodiment-agnostic representations"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c14383fd88b5d16af8c13ba59202c2143ecd1d25b65a10248ec614491595a50_w640_q70.webp",
      "contributions": "1. Introduces a simple co-training recipe for training Vision-Language-Action (VLA) models on a mix of human video and robot data. 2. Discovers and demonstrates that the ability to transfer skills from human videos to robot policies is an emergent property that appears with sufficient scale and diversity in robot pre-training data. 3. Provides analysis suggesting the emergent capability arises from the model learning embodiment-agnostic representations through diverse pre-training.",
      "summary": "This paper investigates whether Vision-Language-Action (VLA) models can learn to transfer skills from human videos to robots, a task that is typically challenging. The authors propose a simple co-training method and find that this human-to-robot transfer capability emerges as a property of scale when the model is pre-trained on a sufficiently large and diverse dataset of robot tasks. Their experiments show that with diverse pre-training, leveraging human data can nearly double performance on tasks seen only in human videos.",
      "mindmap": "graph TB\n        A[Emergence of Human to Robot Transfer in Vision-Language-Action Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Can VLA models learn from human videos for robot control?]\n        C[主要方法/Method: Simple co-training recipe on human & robot data]\n        D[关键结果/Results: Transfer emerges with scale; performance nearly doubles]"
    },
    {
      "title": "Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy",
      "authors": "Amil Khan, Matheus Palhares Viana, Suraj Mishra, B.S. Manjunath",
      "institution": "UC Santa Barbara, Allen Institute for Cell Sciences",
      "link": "https://arxiv.org/pdf/2512.22423",
      "code": null,
      "tags": [
        "medical image segmentation",
        "hyperspherical learning",
        "native sparse attention",
        "anisotropic patch embed"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp",
      "contributions": "1. A 4B-parameter foundation model (Bright-4B) that learns on the unit hypersphere for segmenting subcellular structures directly from 3D brightfield volumes. 2. Novel architectural components: a hardware-aligned Native Sparse Attention mechanism, depth-width residual HyperConnections, and a soft Mixture-of-Experts for adaptive capacity. 3. A plug-and-play anisotropic patch embed that respects confocal point-spread and axial thinning for geometry-faithful 3D tokenization.",
      "summary": "The paper introduces Bright-4B, a 4-billion parameter foundation model designed for volumetric segmentation of subcellular structures directly from label-free 3D brightfield microscopy images. It employs novel architectural components like hyperspherical learning, native sparse attention, and an anisotropic patch embed to handle 3D context and anisotropic sampling. The model outperforms contemporary baselines in preserving fine structural detail across depth and cell types without requiring fluorescence or heavy post-processing.",
      "mindmap": "graph TB\n        A[Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy] --> B(核心问题/Problem: Robust 3D segmentation in brightfield microscopy depends on fluorescence or heavy post-processing.)\n        A --> C(主要方法/Method: A 4B-parameter foundation model with hyperspherical learning, native sparse attention, hyperconnections, mixture-of-experts, and anisotropic patch embed.)\n        A --> D(关键结果/Results: Produces accurate segmentations from brightfield alone, outperforms baselines, preserves detail across depth and cell types.)"
    },
    {
      "title": "FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning",
      "authors": "Ujunwa Mgboh, Rafi Ibn Sultan, Joshua Kim, Kundan Thind, Dongxiao Zhu",
      "institution": "Wayne State University, Henry Ford Health",
      "link": "https://arxiv.org/pdf/2512.22425",
      "code": null,
      "tags": [
        "medical image analysis",
        "transformer",
        "fluence map prediction",
        "physics-informed loss",
        "two-stage regression",
        "Swin UNETR"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d0e329d1bfd4c1f892f7333af6a3a4e76c5219cd192f715a25e502a35ef178_w640_q70.webp",
      "contributions": "1. Proposed FluenceFormer, a backbone-agnostic transformer framework for direct, geometry-aware fluence map regression. 2. Introduced a unified two-stage design (dose prior prediction followed by geometry-conditioned fluence regression) and the physics-informed Fluence-Aware Regression (FAR) loss. 3. Demonstrated the framework's generality across multiple transformer backbones and achieved state-of-the-art performance, significantly reducing energy error.",
      "summary": "This paper introduces FluenceFormer, a transformer-based framework for automating radiotherapy planning by predicting multi-beam fluence maps. The method uses a two-stage, geometry-aware regression approach with a novel physics-informed loss function. The results show that FluenceFormer outperforms existing methods, achieving a low energy error and improved structural fidelity in fluence map prediction.",
      "mindmap": "graph TB\n        A[FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Ill-posed inverse problem: complex anatomy-beam relationship / 病态逆问题: 解剖结构与射束强度的复杂关系]\n        B --> B2[CNN struggles with long-range dependencies / CNN难以捕捉长程依赖]\n        C --> C1[Two-stage transformer framework / 两阶段Transformer框架]\n        C1 --> C1_1[Stage 1: Global dose prior / 阶段1: 全局剂量先验]\n        C1 --> C1_2[Stage 2: Geometry-conditioned fluence regression / 阶段2: 几何条件化的注量图回归]\n        C --> C2[Fluence-Aware Regression (FAR) loss / 注量感知回归损失]\n        D --> D1[Reduced Energy Error to 4.5% / 能量误差降低至4.5%]\n        D --> D2[Improved structural fidelity (p<0.05) / 结构保真度显著提升]\n        D --> D3[Outperformed benchmark CNN & single-stage methods / 超越基准CNN与单阶段方法]"
    },
    {
      "title": "SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems",
      "authors": "Khalfalla Awedat, Mohamed Abidalrekab, Gurcan Comert, Mustafa Ayad",
      "institution": "SUNY Morrisville College, Portland State University, North Carolina A&T State University, SUNY Oswego",
      "link": "https://arxiv.org/pdf/2512.22439",
      "code": null,
      "tags": [
        "point cloud processing",
        "Graph Attention Networks",
        "LiDAR reconstruction",
        "beam dropout",
        "gated residual fusion",
        "sparse point cloud"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68e019420f70e8da99107deedb1af90b7e95ec95b9487f8fc6c872f9994d15e1_w640_q70.webp",
      "contributions": "1. Proposes SuperiorGAT, a novel graph attention-based framework for reconstructing missing elevation data in sparse LiDAR point clouds. 2. Introduces a beam-aware graph modeling approach for LiDAR scans combined with gated residual fusion and feed-forward refinement to achieve accurate reconstruction without increasing network depth. 3. Demonstrates superior performance in reconstruction error and geometric consistency across diverse environments compared to PointNet and deeper GAT baselines, validated through structured beam dropout simulation.",
      "summary": "This paper addresses the problem of LiDAR beam dropout and sparse resolution in autonomous systems by proposing SuperiorGAT, a graph attention network framework that reconstructs missing elevation information. The method models LiDAR scans as beam-aware graphs and uses gated residual fusion for accurate reconstruction without deeper networks. The results show it achieves lower error and better geometric consistency than baselines, offering a computationally efficient way to improve LiDAR resolution.",
      "mindmap": "graph TB\n        A[SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction] --> B[核心问题/Problem: LiDAR垂直分辨率固定与光束丢失导致点云稀疏]\n        A --> C[主要方法/Method: 基于光束感知图与门控残差融合的图注意力网络]\n        A --> D[关键结果/Results: 重建误差更低，几何一致性更好，结构完整性保持]"
    },
    {
      "title": "Monadic Context Engineering",
      "authors": "Yifan Zhang, Mengdi Wang",
      "institution": "Princeton University",
      "link": "https://arxiv.org/pdf/2512.22431",
      "code": "https://github.com/yifanzhang-pro/monadic-context-engineering",
      "tags": [
        "agent system",
        "Monadic Context Engineering",
        "Monad Transformers",
        "Meta-Agents",
        "computational contexts",
        "algebraic structures"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/635ff6dca4b79fe5e98a96641cbb26356935e3090aa65b20972b744e69151810_w640_q70.webp",
      "contributions": "1. Proposes Monadic Context Engineering (MCE), a novel architectural paradigm using Functors, Applicatives, and Monads to provide a formal foundation for AI agent design. 2. Demonstrates how Monads and Applicatives manage sequential composition and parallel execution, and how Monad Transformers enable systematic composition of capabilities like state and error handling. 3. Extends the MCE framework to describe Meta-Agents for generative orchestration, dynamically creating and managing sub-agent workflows via metaprogramming.",
      "summary": "This paper addresses the brittleness and complexity in current AI agent architectures by introducing Monadic Context Engineering (MCE), a paradigm that leverages algebraic structures like Monads to formally manage state, errors, and concurrency within agent workflows. The proposed method enables the construction of complex, resilient agents from simple, verifiable components and is extended to support generative orchestration via Meta-Agents. The work concludes that MCE provides a principled foundation for building robust and scalable autonomous agent systems.",
      "mindmap": "graph TB\n        Root[”Monadic Context Engineering”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”当前代理架构脆弱/Current agent architectures are brittle”]\n        Problem --> P2[”状态、错误、并发管理困难/Difficulties in state, error, concurrency management”]\n        Method --> M1[”引入单子上下文工程/Introduce Monadic Context Engineering (MCE)”]\n        Method --> M2[”利用函子、应用函子、单子/Leverage Functors, Applicatives, Monads”]\n        Method --> M3[”使用单子变换器组合能力/Use Monad Transformers to compose capabilities”]\n        Results --> R1[”提供形式化基础/Provides a formal foundation”]\n        Results --> R2[”支持构建复杂、鲁棒的代理/Enables building complex, resilient agents”]\n        Results --> R3[”扩展至元代理进行生成式编排/Extends to Meta-Agents for generative orchestration”]"
    },
    {
      "title": "HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG",
      "authors": "Cattalyya Nuengsigkapian",
      "institution": "Google",
      "link": "https://arxiv.org/pdf/2512.22442",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "hierarchical filtering",
        "two-pass generation",
        "citation verification",
        "query formulation",
        "model cascade"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9840d615edee0e72c18b93838479ebb342195ea1805803858fb9effaf9ba2e95_w640_q70.webp",
      "contributions": "1. Proposes a hierarchical content filtering pipeline to replace standard vector similarity search, improving context precision. 2. Introduces a model cascade strategy using a cost-efficient model (Gemini 2.5 Flash) for filtering and a powerful model (Gemini 2.5 Pro) for final generation. 3. Demonstrates significant performance gains on the MMU-RAGent benchmark and a custom dataset for post-cutoff knowledge.",
      "summary": "This paper presents HiFi-RAG, a system designed to improve open-domain RAG by addressing irrelevant retrieved information. The method uses a multi-stage pipeline with hierarchical filtering and a two-pass generation strategy employing different LLMs for efficiency and quality. The system won a NeurIPS 2025 competition and showed substantial improvements over baselines in evaluation metrics.",
      "mindmap": "graph TB\n        A[HiFi-RAG] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[开放域RAG中的无关信息与意图对齐/Open-domain RAG faces irrelevant info & intent misalignment]\n        C --> C1[分层过滤与两阶段生成/Hierarchical Filtering & Two-Pass Generation]\n        C1 --> C2[使用Gemini Flash进行过滤/Use Gemini Flash for filtering]\n        C1 --> C3[使用Gemini Pro进行生成/Use Gemini Pro for generation]\n        D --> D1[在MMU-RAGent上超越基线/Outperforms baseline on MMU-RAGent]\n        D --> D2[在自定义测试集上显著提升/Substantial gains on custom test set]"
    },
    {
      "title": "Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework",
      "authors": "Zhicheng Zhao, Yuancheng Xu, Andong Lu, Chenglong Li, Jin Tang",
      "institution": "Anhui University, China Electronics Technology Group Corporation (38th Research Institute)",
      "link": "https://arxiv.org/pdf/2512.22447",
      "code": null,
      "tags": [
        "object detection",
        "optical-SAR fusion",
        "missing modality",
        "quality-aware fusion",
        "dynamic fusion",
        "orthogonal constraint"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69b5db28a2b2a43b3d55d1592c7f26e5ce4421dd707ae3e19282c69c4e894e50_w640_q70.webp",
      "contributions": "1. Proposed a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection under missing or degraded modalities. 2. Designed a Dynamic Modality Quality Assessment (DMQA) module that uses learnable reference tokens to iteratively assess feature reliability and identify degraded regions. 3. Developed an Orthogonal Constraint Normalization Fusion (OCNF) module that uses orthogonal constraints to preserve modality independence and dynamically adjust fusion weights based on reliability scores to suppress unreliable features.",
      "summary": "This paper addresses the problem of robust object detection using optical and SAR images when one modality is missing or degraded. The proposed QDFNet method dynamically assesses feature quality and adaptively fuses information using learnable tokens and orthogonal constraints. Experiments show it outperforms other methods, especially when modalities are partially missing.",
      "mindmap": "graph TB\n        Root[”Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework”] --> Problem[”核心问题/Problem: Optical-SAR image pairs are often misaligned or missing, degrading fusion-based detection.”]\n        Root --> Method[”主要方法/Method: Proposes QDFNet with DMQA (quality assessment) and OCNF (orthogonal fusion) modules.”]\n        Root --> Results[”关键结果/Results: Superior performance on SpaceNet6-OTD and OGSOD-2.0 datasets, especially under missing data.”]"
    },
    {
      "title": "AMBIT: Augmenting Mobility Baselines with Interpretable Trees",
      "authors": "Qizhi Wang",
      "institution": "PingCAP, Data & AI-Innovation Lab",
      "link": "https://arxiv.org/pdf/2512.22466",
      "code": null,
      "tags": [
        "urban computing",
        "spatial data science",
        "origin-destination flow prediction",
        "spatial interaction models",
        "gradient-boosted trees",
        "SHAP analysis",
        "gray-box model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d0d9ab13b8b2f60e318c90f38821b29e87e0bdd9bf0d9bc963b48c5ac7c2759_w640_q70.webp",
      "contributions": "1. Conducts a comprehensive audit of classical spatial interaction models on high-resolution mobility data, identifying PPML gravity as the strongest physical baseline. 2. Proposes AMBIT, a gray-box framework that augments interpretable physical baselines with gradient-boosted trees to learn residuals, balancing accuracy and interpretability. 3. Demonstrates that physics-grounded and POI-anchored residual learners achieve competitive accuracy and robust spatial generalization, providing a reproducible pipeline with diagnostics for urban decision-making.",
      "summary": "This paper proposes AMBIT, a gray-box framework that combines interpretable physical mobility baselines with gradient-boosted trees to predict origin-destination flows. It shows that learning residuals on top of physics-based models can achieve accuracy close to strong black-box predictors while maintaining interpretable structure, with POI-anchored residuals being particularly robust for spatial generalization.",
      "mindmap": "graph TB\n        A[AMBIT: Augmenting Mobility Baselines with Interpretable Trees] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Conflicting needs for high accuracy and clear interpretability in OD flow prediction]\n        C[主要方法/Method: Gray-box framework augmenting physical baselines with interpretable tree models]\n        D[关键结果/Results: Physics-grounded residuals approach black-box accuracy; POI-anchored residuals are most robust]"
    },
    {
      "title": "The Bayesian Geometry of Transformer Attention",
      "authors": "Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra",
      "institution": "Columbia University, Dream Sports, Google DeepMind",
      "link": "https://arxiv.org/pdf/2512.22471",
      "code": null,
      "tags": [
        "interpretability",
        "Bayesian inference",
        "transformer attention",
        "mechanistic interpretability",
        "Bayesian wind tunnels",
        "geometric analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5333e070f21cfd2abea4d13cddb84bf6d9f3c3124c93bf9142879f24f1e739b1_w640_q70.webp",
      "contributions": "1. Introduces \"Bayesian wind tunnels\" as controlled environments to rigorously test if transformers perform Bayesian inference, where true posteriors are known and memorization is impossible. 2. Demonstrates that small transformers implement Bayesian inference via a consistent geometric mechanism (residual streams as belief substrate, FFNs for updates, attention for routing), while MLPs fail. 3. Identifies specific geometric diagnostics (orthogonal key bases, query-key alignment, low-dimensional value manifold) and a frame-precision dissociation during training.",
      "summary": "This paper investigates whether transformers perform genuine Bayesian inference. To test this, the authors create controlled \"Bayesian wind tunnel\" tasks with known posteriors and show that small transformers accurately reproduce Bayesian posteriors via a specific geometric mechanism, while MLPs fail, establishing attention as crucial for this capability.",
      "mindmap": "graph TB\n        Root[”The Bayesian Geometry of Transformer Attention<br/>Transformer注意力的贝叶斯几何”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br/>Do transformers perform genuine Bayesian inference or just pattern matching?<br/>Transformer是进行真正的贝叶斯推理还是仅仅模式匹配?”]\n        Method[”主要方法/Method<br/>Construct 'Bayesian wind tunnels' with known posteriors<br/>构建具有已知后验的'贝叶斯风洞'”]\n        Results[”关键结果/Results<br/>Transformers implement Bayesian inference via geometric mechanism; MLPs fail<br/>Transformer通过几何机制实现贝叶斯推理；MLP失败”]"
    },
    {
      "title": "DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior",
      "authors": "Sadia Asif, Israel Antonio Rosales Laguan, Haris Khan, Shumaila Asif, Muneeb Asif",
      "institution": "Rensselaer Polytechnic Institute, National University of Sciences and Technology",
      "link": "https://arxiv.org/pdf/2512.22470",
      "code": "https://github.com/sadia-sigma-lab/Benchmark-dataset-for-dark-patterns-in-llms",
      "tags": [
        "ai safety & alignment",
        "manipulation detection",
        "safety benchmark",
        "harm categorization",
        "multi-layer analysis",
        "autonomy harm"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b68b7543a951540a0b9d1fde4bdde15299ee5f4f63a4526b64cdf4ab7f7a4c28_w640_q70.webp",
      "contributions": "1. Introduces DarkPatterns-LLM, the first comprehensive benchmark dataset with 401 expert-annotated examples for fine-grained detection of manipulative LLM behaviors across seven harm categories. 2. Proposes a novel four-layer diagnostic framework (MGD, MSIAN, THP, DCRA) for nuanced analysis of manipulative content, moving beyond coarse binary safety labels. 3. Provides an empirical evaluation revealing significant performance disparities (65.2%-89.7%) among state-of-the-art LLMs and identifies consistent weaknesses, particularly in detecting autonomy-undermining patterns.",
      "summary": "This paper addresses the lack of nuanced benchmarks for detecting manipulative behaviors in Large Language Models (LLMs). It introduces DarkPatterns-LLM, a new dataset and a four-layer analytical framework for fine-grained assessment across multiple harm categories. The evaluation shows current LLMs have significant and varied weaknesses in manipulation detection, establishing a standardized benchmark for developing more trustworthy AI.",
      "mindmap": "graph TB\n        A[DarkPatterns-LLM: 检测有害AI行为] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem] --> B1[现有安全基准过于粗糙/Existing safety benchmarks are coarse]\n        B --> B2[无法捕捉操纵的微妙机制/Fail to capture nuanced manipulation mechanisms]\n        C[主要方法/Method] --> C1[新基准数据集/New benchmark dataset (401 examples)]\n        C --> C2[七类危害分类/Seven harm categories]\n        C --> C3[四层分析框架/Four-layer analytical pipeline (MGD, MSIAN, THP, DCRA)]\n        D[关键结果/Results] --> D1[模型性能差异大/Model performance varies widely (65.2%-89.7%)]\n        D --> D2[自主性危害检测弱/Weakness in detecting autonomy harm]\n        D --> D3[首个标准化多维度基准/First standardized multi-dimensional benchmark]"
    },
    {
      "title": "SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding for Fine-Grained sEMG-Based Movement Decoding",
      "authors": "Zihan Weng, Chanlin Yi, Pouya Bashivan, Jing Lu, Fali Li, Dezhong Yao, Jingming Hou, Yangsong Zhang, Peng Xu",
      "institution": "University of Electronic Science and Technology of China",
      "link": "https://arxiv.org/pdf/2512.22481",
      "code": null,
      "tags": [
        "biomedical signal processing",
        "self-supervised learning",
        "surface electromyography",
        "rotary position encoding",
        "spectral pre-training",
        "movement decoding"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5801cbdcbac93bf7df15e18531c5ff2653259e25003568da5b53b240d06d32c_w640_q70.webp",
      "contributions": "1. A novel self-supervised pre-training task that uses masked prediction of clustered STFT pseudo-labels to learn robust, physiologically relevant frequency patterns from sEMG signals. 2. A novel Cylindrical Rotary Position Embedding (CyRoPE) that factorizes embeddings along temporal and annular spatial dimensions to explicitly model the cylindrical topology of forearm electrode arrays. 3. The SPECTRE framework, which integrates these contributions to establish a new state-of-the-art for fine-grained movement decoding, validated on multiple datasets including data from individuals with amputation.",
      "summary": "The paper introduces SPECTRE, a domain-specific self-supervised learning framework for decoding fine-grained movements from surface electromyography (sEMG) signals. It proposes a spectral pre-training task using masked pseudo-label prediction and a novel cylindrical rotary position encoding to model sensor topology. Evaluations show SPECTRE significantly outperforms existing supervised and generic self-supervised baselines, providing a robust foundation for practical myoelectric interfaces.",
      "mindmap": "graph TB\n        A[SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Decoding fine-grained movement from noisy, non-stationary sEMG signals for prosthetic control]\n        C[主要方法/Method: Domain-specific SSL with spectral pre-training and Cylindrical Rotary Position Embedding (CyRoPE)]\n        D[关键结果/Results: New SOTA performance, outperforms supervised & generic SSL baselines, validated on amputation data]"
    },
    {
      "title": "Hierarchical Pedagogical Oversight: A Multi-Agent Adversarial Framework for Reliable AI Tutoring",
      "authors": "Saisab Sadhu, Ashim Dhor",
      "institution": "Indian Institute of Science Education and Research Bhopal",
      "link": "https://arxiv.org/pdf/2512.22496",
      "code": null,
      "tags": [
        "agent system",
        "adversarial reasoning",
        "multi-agent system",
        "pedagogical oversight",
        "hierarchical framework",
        "low-compute inference"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67b6c2853ea8b40662f9788f9062b5488d7ec541a754a445a856888b9fb9300c_w640_q70.webp",
      "contributions": "1. Introduces Hierarchical Pedagogical Oversight (HPO), a novel multi-agent adversarial framework designed to improve the reliability of AI tutoring by separating pedagogical generation from evaluation. 2. Adapts structured adversarial synthesis to educational assessment, enforcing a dialectical debate between opposing pedagogical critics to mitigate sycophancy and superficial consensus. 3. Demonstrates that the adversarial protocol enables a small 8B-parameter model to outperform GPT-4o on pedagogical oversight while using significantly fewer computational resources.",
      "summary": "The paper addresses the problem of unreliable AI tutors (LLMs) that often validate incorrect student answers. It proposes the Hierarchical Pedagogical Oversight (HPO) framework, which uses a structured multi-agent adversarial debate to assess tutoring quality. The main conclusion is that this adversarial approach enables a much smaller model to outperform a much larger one (GPT-4o) on a pedagogical reasoning benchmark, establishing it as a critical mechanism for reliable, low-compute oversight.",
      "mindmap": "graph TB\n        A[Hierarchical Pedagogical Oversight<br>分层教学监督框架] --> B[Problem: LLMs as tutors are unreliable<br>问题: LLM导师不可靠]\n        A --> C[Method: Multi-Agent Adversarial Framework<br>方法: 多智能体对抗框架]\n        A --> D[Results: 8B model beats GPT-4o, low-compute<br>结果: 8B模型超越GPT-4o, 低计算]"
    },
    {
      "title": "ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching and Hierarchical Text Representation",
      "authors": "Suhua Wang, Zifan Wang, Xiaoxin Sun, D. J. Wang, Zhanbo Liu, Xin Li",
      "institution": "Northeast Normal University, Changchun Humanities and Sciences College, Zhejiang University",
      "link": "https://arxiv.org/pdf/2512.22491",
      "code": null,
      "tags": [
        "speech synthesis",
        "flow matching",
        "hierarchical attention",
        "low-resource TTS",
        "agglutinative language",
        "non-autoregressive generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/118d24570c94614dbb94eaabcc1dc47ba64643f094c4ea3727d4674221bf53fc_w640_q70.webp",
      "contributions": "1. Proposes a novel hierarchical text representation and cross-modal attention mechanism to handle Manchu's agglutinative phonology. 2. Introduces an end-to-end speech synthesis model integrating deep convolutional networks with a flow-matching Transformer for efficient, non-autoregressive generation. 3. Constructs the first public Manchu TTS dataset and employs data augmentation to address severe data scarcity.",
      "summary": "This paper proposes ManchuTTS, a novel text-to-speech system designed for the endangered and agglutinative Manchu language. The method uses a three-tier text representation and a flow-matching Transformer with hierarchical guidance to tackle data scarcity and complex phonology. Experiments show it achieves a high MOS score of 4.52 and significantly improves pronunciation accuracy and prosodic naturalness compared to baselines.",
      "mindmap": "graph TB\n        A[ManchuTTS: 满语语音合成] --> B1(核心问题/Problem)\n        A --> B2(主要方法/Method)\n        A --> B3(关键结果/Results)\n        B1 --> C1[数据稀缺/Data Scarcity]\n        B1 --> C2[粘着语语音学/Agglutinative Phonology]\n        B2 --> D1[三层文本表示/Three-tier Text Representation]\n        B2 --> D2[流匹配Transformer/Flow-matching Transformer]\n        B2 --> D3[分层对比损失/Hierarchical Contrastive Loss]\n        B3 --> E1[MOS得分4.52/MOS Score 4.52]\n        B3 --> E2[AWPA提升31%/AWPA +31%]\n        B3 --> E3[韵律自然度提升27%/Prosodic Naturalness +27%]"
    },
    {
      "title": "Role-Based Fault Tolerance System for LLM RL Post-Training",
      "authors": "Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin",
      "institution": "Zhejiang University, State Key Laboratory of Mathematical Engineering and Advanced Computing",
      "link": "https://arxiv.org/pdf/2512.22492",
      "code": null,
      "tags": [
        "fault-tolerance",
        "role-based fault tolerance",
        "RL post-training",
        "UCX communication",
        "warm standby",
        "Effective Training Time Ratio (ETTR)"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceff77cfb9d0c7d7e763916b77716ee6534c3aa3d54d41253fef6ea4d9a86ec0_w640_q70.webp",
      "contributions": "1. Proposes a role-based fault isolation and recovery system (RobustRL) for RL post-training, enabling recovery of only the failed component (trainer, rollout) instead of restarting the entire task. 2. Introduces a role-aware monitoring mechanism to accurately detect failures and avoid false positives/delays specific to different RL roles. 3. Implements dynamic, UCX-based point-to-point communication to reconnect recovered roles and synchronize weights immediately, replacing static collective communication.",
      "summary": "The paper addresses the lack of fault tolerance for RL post-training of LLMs, which interleaves training and inference workloads. It proposes RobustRL, a system that isolates and recovers failed roles (e.g., trainer, rollout) individually using a Detect-Restart-Reconnect paradigm, instead of restarting the entire job. This approach significantly improves the Effective Training Time Ratio and reduces end-to-end training time compared to baseline methods.",
      "mindmap": "graph TB\n        A[Role-Based Fault Tolerance System for LLM RL Post-Training] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[RL后训练混合训练与推理工作负载，易受双方故障影响/RL post-training mixes training & inference, vulnerable to faults from both]\n        B --> B2[现有容错框架未针对RL的异步执行优化/Existing FT frameworks not optimized for RL's async execution]\n        C --> C1[基于角色的故障隔离与恢复/Role-based fault isolation & recovery]\n        C --> C2[检测-重启-重连范式/Detect-Restart-Reconnect paradigm]\n        C2 --> C21[角色感知监控/Role-aware monitoring]\n        C2 --> C22[非中断式重启/Non-disruptive restart with warm standbys]\n        C2 --> C23[动态UCX点对点通信重连/Dynamic UCX P2P reconnection]\n        D --> D1[ETTR超过80%，优于基线的60%/ETTR >80%, better than baseline 60%]\n        D --> D2[端到端训练时间加快8.4%-17.4%/End-to-end training time 8.4%-17.4% faster]"
    },
    {
      "title": "Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals",
      "authors": "Lucky Susanto, Anasta Pranawijayana, Cortino Sukotjo, Soni Prasad, Derry Wijaya",
      "institution": "Monash University Indonesia, University of Pittsburgh, University of North Carolina Adams School of Dentistry, Boston University",
      "link": "https://arxiv.org/pdf/2512.22508",
      "code": null,
      "tags": [
        "hallucination detection",
        "correctness prediction",
        "metadata signals",
        "prompting strategies",
        "log probability",
        "response consistency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbe159260b97cf5e7a59edbee0a23b697a76a89a0fdbf6e0c9797739cc322b42_w640_q70.webp",
      "contributions": "1. Proposes a novel method to predict LLM correctness (not just hallucination) using metadata and hallucination signals in a high-stakes medical domain (prosthodontics). 2. Demonstrates that metadata-based predictors can improve accuracy over a naive baseline and achieve high precision, but are not reliable for directly predicting hallucination. 3. Shows that prompting strategies significantly alter model internal behavior and the predictive utility of metadata, despite not changing overall task accuracy.",
      "summary": "This study investigates predicting the correctness of LLM answers on a prosthodontics exam using metadata (like log probability and consistency) and hallucination signals. The method, applied to GPT-4o and OSS-120B across different prompts, shows improved accuracy over a baseline but is not yet robust for high-stakes deployment. The research highlights that prompting strategies change model behavior and metadata utility, offering a direction for reliability signals.",
      "mindmap": "graph TB\n        A[Predicting LLM Correctness in Prosthodontics<br/>预测LLM在口腔修复学中的正确性] --> B(Problem/核心问题: LLM hallucinations in high-stakes healthcare domains<br/>高风险医疗领域中的LLM幻觉问题)\n        A --> C(Method/主要方法: Use metadata & hallucination signals to build correctness predictors<br/>使用元数据和幻觉信号构建正确性预测器)\n        A --> D(Results/关键结果: Improved accuracy & precision; metadata not reliable for hallucination prediction; prompting alters behavior<br/>提升准确率和精确度；元数据不能可靠预测幻觉；提示策略改变模型行为)"
    },
    {
      "title": "Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks",
      "authors": "Jihang Wang, Dongcheng Zhao, Ruolin Chen, Qian Zhang, Yi Zeng",
      "institution": "Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences; Beijing Institute of AI Safety and Governance",
      "link": "https://arxiv.org/pdf/2512.22522",
      "code": null,
      "tags": [
        "adversarial robustness",
        "Spiking Neural Networks",
        "surrogate gradient",
        "adversarial attack",
        "gradient vanishing",
        "adaptive optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7b10ed2a96f6e46c5c2afb21a8e1184dd9c5e1f342efdb93f3cd317a08c20ea_w640_q70.webp",
      "contributions": "1. Theoretical analysis of gradient vanishing in surrogate gradient methods for SNNs. 2. Proposal of Adaptive Sharpness Surrogate Gradient (ASSG) to adaptively adjust the surrogate function for more accurate gradients. 3. Design of Stable Adaptive Projected Gradient Descent (SA-PGD), an adversarial attack with adaptive step size for stable convergence under imprecise gradients.",
      "summary": "This paper addresses the unreliable evaluation of adversarial robustness in Spiking Neural Networks (SNNs) caused by gradient vanishing from surrogate gradients. The authors propose a framework combining an adaptive surrogate gradient method (ASSG) and an adaptive-step attack (SA-PGD) to generate stronger attacks. Experiments show this framework significantly increases attack success rates, revealing that current SNN robustness is overestimated and highlighting the need for more reliable adversarial training.",
      "mindmap": "graph TB\n        Root(”Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”梯度消失/Gradient Vanishing”)\n        Problem --> P2(”对抗评估不可靠/Unreliable Adversarial Evaluation”)\n        Method --> M1(”理论分析梯度消失/Theoretical Analysis of Gradient Vanishing”)\n        Method --> M2(”自适应锐度替代梯度/Adaptive Sharpness Surrogate Gradient (ASSG)”)\n        Method --> M3(”稳定自适应投影梯度下降/Stable Adaptive PGD (SA-PGD)”)\n        Results --> R1(”攻击成功率大幅提升/Substantially Increased Attack Success Rate”)\n        Results --> R2(”揭示鲁棒性被高估/Revealed Overestimated Robustness”)\n        Results --> R3(”提供更可靠评估/Provided More Reliable Evaluation”)"
    },
    {
      "title": "Multi-AI Agent Framework Reveals the \"Oxide Gatekeeper\" in Aluminum Nanoparticle Oxidation",
      "authors": "Yiming Lu, Tingyu Lu, Di Zhang, Lili Ye, Hao Li",
      "institution": "Tohoku University, Dalian University of Technology",
      "link": "https://arxiv.org/pdf/2512.22529",
      "code": null,
      "tags": [
        "agent system",
        "multi-AI agent",
        "machine learning potential",
        "human-in-the-loop",
        "self-auditing",
        "scalable molecular dynamics"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42c9680d91fb0cb7f7611fd33feaef10bdab707dc569594f3d51f8af4d9e0651_w640_q70.webp",
      "contributions": "1. Developed a novel \"human-in-the-loop\" closed-loop framework utilizing self-auditing AI agents to validate and evolve a machine learning potential, ensuring quantum accuracy while achieving near-linear scalability to million-atom systems and nanosecond timescales. 2. Discovered a temperature-regulated dual-mode oxidation mechanism for aluminum nanoparticles, where the oxide shell acts as a dynamic \"gatekeeper\" via a \"breathing mode\" at moderate temperatures and transitions to a catastrophic \"rupture mode\" above a critical threshold. 3. Resolved a long-standing controversy by demonstrating that aluminum cation outward diffusion, not oxygen transport, is the dominant mass transfer mechanism across all temperature regimes, with diffusion coefficients 2-3 orders of magnitude higher.",
      "summary": "This paper introduces a human-in-the-loop multi-AI agent framework to develop and validate a highly accurate and scalable machine learning potential for molecular dynamics simulations. Using this method, the study reveals a dual-mode oxidation mechanism in aluminum nanoparticles and conclusively shows that aluminum cation diffusion, not oxygen transport, dominates the oxidation process.",
      "mindmap": "graph TB\n        A[Multi-AI Agent Framework Reveals the ”Oxide Gatekeeper” in Aluminum Nanoparticle Oxidation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem]\n        B --> B1[原子尺度氧化机制未知/Atomic-scale oxidation mechanism unknown]\n        B --> B2[计算瓶颈:精度与规模难以兼得/Computational bottleneck: trade-off between accuracy and scale]\n        C[主要方法/Method]\n        C --> C1[人机协同闭环框架/Human-in-the-loop closed-loop framework]\n        C --> C2[自审AI代理验证MLP演化/Self-auditing AI Agents validate MLP evolution]\n        D[关键结果/Results]\n        D --> D1[发现双模式氧化机制/Discovered dual-mode oxidation mechanism]\n        D --> D2[揭示铝离子扩散主导/Revealed Al cation diffusion dominates]\n        D --> D3[建立统一原子尺度设计框架/Established unified atomic-scale design framework]"
    },
    {
      "title": "CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation",
      "authors": "Qinglin Zeng, Kaitong Cai, Ruiqi Chen, Qinhan Lv, Keze Wang",
      "institution": "Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.22536",
      "code": null,
      "tags": [
        "video generation",
        "closed-loop framework",
        "entity-level memory",
        "vision-language verification",
        "pacing-aware editing",
        "multi-agent collaboration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfaf9ffbd76c531ee1d089b472175957646bd5b08a39cad1cce07b642bd237a4_w640_q70.webp",
      "contributions": "1. Proposes CoAgent, a collaborative closed-loop framework that formulates video generation as a plan-synthesize-verify-edit process. 2. Introduces a Global Context Manager to maintain entity-level memory for cross-shot identity and appearance consistency. 3. Employs a Verifier Agent with vision-language reasoning to evaluate intermediate results and trigger selective regeneration for quality control.",
      "summary": "The paper addresses the challenge of maintaining narrative coherence and visual consistency in long-form video generation. It proposes CoAgent, a collaborative multi-agent framework that uses a closed-loop plan-synthesize-verify-edit pipeline with entity memory and consistency verification. Experiments show that CoAgent significantly improves coherence, consistency, and narrative quality in generated videos.",
      "mindmap": "graph TB\n        A[CoAgent: Coherent Video Generation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Open-domain video generation lacks coherence and consistency/开放域视频生成缺乏连贯性和一致性]\n        C --> C1[Plan-Synthesize-Verify-Edit Pipeline/计划-合成-验证-编辑流程]\n        C1 --> C2[Storyboard Planner/故事板规划器]\n        C1 --> C3[Global Context Manager/全局上下文管理器]\n        C1 --> C4[Verifier Agent/验证智能体]\n        C1 --> C5[Pacing-aware Editor/节奏感知编辑器]\n        D --> D1[Improves coherence, consistency, narrative quality/提升连贯性、一致性、叙事质量]"
    },
    {
      "title": "Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains",
      "authors": "Jesen Zhang, Ningyuan Liu, Kaitong Cai, Sidi Liu, Jing Yang, Ziliang Chen, Xiaofei Sun, Keze Wang",
      "institution": "Sun Yat-sen University, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.22545",
      "code": null,
      "tags": [
        "multimodal reasoning",
        "self-rewarded learning",
        "process alignment",
        "multimodal large language models",
        "reasoning coherence",
        "visual grounding"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/629536dd38b71477a18bd2e7208023831047c11b4eafeff5c5c094c124751f98_w640_q70.webp",
      "contributions": "1. A lightweight, label-free framework (SR-MCR) that aligns multimodal reasoning by constructing a self-reward from intrinsic process signals (semantic alignment, lexical fidelity, non-redundancy, visual grounding, step consistency). 2. A normalized, reliability-weighted reward mechanism that adaptively combines multiple self-referential cues to provide fine-grained, process-level guidance. 3. A critic-free GRPO objective enhanced with a confidence-aware cooling mechanism to stabilize training and suppress trivial or overconfident generations.",
      "summary": "The paper addresses the problem of multimodal LLMs producing fluent but unreliable reasoning with poor step coherence and visual grounding. It proposes SR-MCR, a self-rewarded framework that uses multiple intrinsic process signals from model outputs to create a fine-grained reward for alignment, achieving state-of-the-art accuracy and improved reasoning coherence on visual benchmarks.",
      "mindmap": "graph TB\n        Root[Self-Rewarded Multimodal Coherent Reasoning<br>自我奖励多模态连贯推理] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>Fluent but unreliable reasoning,<br>weak coherence & grounding] --> P1[现有方法缺陷/Existing Method Flaws<br>Supervises only final answer]\n        Method[主要方法/Method<br>SR-MCR Framework] --> M1[自我奖励/Self-Reward<br>Five intrinsic process cues]\n        Method --> M2[优化目标/Optimization<br>GRPO with cooling mechanism]\n        Results[关键结果/Results<br>Evaluation & Ablation] --> R1[性能提升/Performance Gain<br>SOTA accuracy (81.4%)]\n        Results --> R2[消融研究/Ablation Study<br>Confirms contributions]"
    },
    {
      "title": "TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting",
      "authors": "Jaebin Lee, Hankook Lee",
      "institution": "Sungkyunkwan University",
      "link": "https://arxiv.org/pdf/2512.22550",
      "code": "https://github.com/efficient-learning-lab/TimePerceiver",
      "tags": [
        "time-series forecasting",
        "encoder-decoder",
        "latent bottleneck representations",
        "learnable queries",
        "generalized forecasting"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/091fdcd1058e0acfad18820d025c1fe50ff4315cbd5ec8a06f5d86eb9767513c_w640_q70.webp",
      "contributions": "1. Generalizes the time-series forecasting task to include diverse objectives like extrapolation, interpolation, and imputation. 2. Proposes a novel encoder-decoder architecture with latent bottleneck representations to capture temporal and cross-channel dependencies. 3. Introduces learnable queries for decoding to effectively retrieve information for arbitrarily positioned target timestamps.",
      "summary": "This paper proposes TimePerceiver, a unified encoder-decoder framework for generalized time-series forecasting. It handles diverse prediction objectives by using latent bottleneck encodings and learnable query-based decoding. Extensive experiments show it consistently outperforms prior state-of-the-art methods.",
      "mindmap": "graph TB\n        A[TIMEPERCEIVER] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法侧重编码器，预测与训练分离/Prior work focuses on encoder, treats prediction & training separately]\n        C --> C1[广义预测任务: 外推、插值、填补/Generalized forecasting: extrapolation, interpolation, imputation]\n        C --> C2[编码: 潜在瓶颈表示/Encoding: Latent bottleneck representations]\n        C --> C3[解码: 可学习查询/Decoding: Learnable queries]\n        D --> D1[性能显著超越SOTA/Outperforms SOTAs significantly]"
    },
    {
      "title": "Learning When Not to Attend Globally",
      "authors": "Xuan Luo, Kailai Zhang, Xifeng Yan",
      "institution": "UC Santa Barbara",
      "link": "https://arxiv.org/pdf/2512.22562",
      "code": null,
      "tags": [
        "llm inference",
        "All-or-Here Attention",
        "sliding window attention",
        "conditional computation",
        "binary router",
        "context dependency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edc0024b0088e710cd3ce9c0be8276b43396c11c0424f0f6340e0f97d63982e6_w640_q70.webp",
      "contributions": "1. Proposes All-or-Here Attention (AHA), a novel attention mechanism that dynamically toggles between full and local sliding window attention using a binary router per head. 2. Demonstrates empirically that full attention is largely redundant, showing up to 93% of full attention operations can be replaced with local attention without performance loss. 3. Identifies a long-tail distribution in context dependency, revealing that the need for global context decays rapidly as the local window expands.",
      "summary": "The paper addresses the computational inefficiency of full self-attention in LLMs by proposing All-or-Here Attention (AHA), which learns to dynamically switch between full and local sliding window attention for each token. The results show that most full attention operations are unnecessary, and efficient inference can be achieved with on-demand global context access.",
      "mindmap": "graph TB\n        A[Learning When Not to Attend Globally] --> B[核心问题/Problem: Quadratic complexity of full self-attention is inefficient]\n        A --> C[主要方法/Method: All-or-Here Attention (AHA) with binary router to toggle between full and local sliding window attention]\n        A --> D[关键结果/Results: Up to 93% full attention replaced without loss; reveals long-tail context dependency]"
    },
    {
      "title": "RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure",
      "authors": "Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang",
      "institution": "HKUST, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.22560",
      "code": "https://github.com/alibaba/ROLL",
      "tags": [
        "agent system",
        "disaggregated infrastructure",
        "hardware-affinity mapping",
        "fine-grained asynchrony"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc8e408c613097b7b60bc32e41ec3137faa8dd94184658c8a802b65cbf57c593_w640_q70.webp",
      "contributions": "1. A hardware-affinity workload mapping strategy that routes compute-bound and bandwidth-bound tasks to best-fit GPU devices. 2. A fine-grained asynchrony mechanism that manages execution at the trajectory level to mitigate resource bubbles and improve utilization. 3. A statefulness-aware computation design that offloads stateless components to serverless infrastructure for elastic scaling.",
      "summary": "The paper presents RollArt, a distributed system designed to scale agentic reinforcement learning training on disaggregated infrastructure. It addresses the heterogeneity of agentic RL workloads by proposing three core techniques: hardware-affinity workload mapping, fine-grained asynchrony, and statefulness-aware computation. The system demonstrates significant improvements in training throughput, achieving 1.35-2.05x speedup over baselines, and scales to thousands of GPUs.",
      "mindmap": "graph TB\n        Root[”RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure”] --> Problem[”核心问题/Problem: Agentic RL workloads are heterogeneous, causing inefficiency in monolithic infrastructure.”]\n        Root --> Method[”主要方法/Method: Disaggregated system with hardware-affinity mapping, fine-grained asynchrony, and statefulness-aware computation.”]\n        Root --> Results[”关键结果/Results: Achieves 1.35-2.05x training speedup and scales to >3000 GPUs.”]"
    },
    {
      "title": "Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI",
      "authors": "Rajesh P. N. Rao, Vishwas Sathish, Linxing Preston Jiang, Matthew Bryan, Prashant Rangarajan",
      "institution": "University of Washington",
      "link": "https://arxiv.org/pdf/2512.22568",
      "code": null,
      "tags": [
        "cognitive architectures",
        "predictive coding",
        "compositional structure",
        "episodic memory",
        "action integration",
        "foundation models"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c90f622f122bb0e11d5909a8de5e173e03638e3854680c6b2a0139a74ea9314_w640_q70.webp",
      "contributions": "1. Proposes integrating actions, compositional structure, and episodic memory into foundation models to address their deficiencies. 2. Presents neuroscience evidence to support the importance of these components for achieving human-like AI. 3. Compares the proposal to current trends like chain-of-thought reasoning and retrieval-augmented generation, suggesting new brain-inspired augmentation methods.",
      "summary": "The paper argues that current foundation models, despite their success, lack key components of brain-inspired predictive coding models: action integration, compositional structure, and episodic memory. It proposes integrating these components to address issues like hallucinations, lack of grounding, and poor interpretability, aiming for safer and more human-like AI. The conclusion advocates for renewed collaboration between neuroscience and AI to achieve these goals.",
      "mindmap": "graph TB\n        Root[”Lessons from Neuroscience for AI / 神经科学对AI的启示”] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题/Problem”] --> P1[”Current AI lacks key brain features / 当前AI缺乏关键大脑特征”]\n        Problem --> P2[”Deficiencies: hallucinations, no grounding / 缺陷：幻觉，缺乏根基”]\n    \n        Method[”主要方法/Method”] --> M1[”Integrate brain components / 整合大脑组件”]\n        M1 --> M1_1[”Actions / 行动”]\n        M1 --> M1_2[”Compositional Structure / 组合结构”]\n        M1 --> M1_3[”Episodic Memory / 情景记忆”]\n    \n        Results[”关键结果/Results”] --> R1[”Address AI deficiencies / 解决AI缺陷”]\n        Results --> R2[”Enable safe, interpretable AI / 实现安全、可解释的AI”]\n        Results --> R3[”Path to human-like AI / 通向类人AI之路”]"
    },
    {
      "title": "SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G",
      "authors": "Yong Xiao, Xubo Li, Haoran Zhou, Yingyu Li, Yayu Gao, Guangming Shi, Ping Zhang, Marwan Krunz",
      "institution": "Huazhong University of Science and Technology, Peng Cheng Laboratory, Pazhou Laboratory (Huangpu), China University of Geosciences (Wuhan), Xidian University, Beijing University of Posts and Telecommunications, University of Arizona",
      "link": "https://arxiv.org/pdf/2512.22579",
      "code": null,
      "tags": [
        "agent system",
        "Agentic AI Networking",
        "Multi-agent Multi-objective Optimization",
        "Model Partition and Sharing (MoPS)",
        "Cross-layer Optimization",
        "Pareto-optimal Solution"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eee5846572238b17d03d837b7c31a7bd60644abe5d9070207f45cd166b326470_w640_q70.webp",
      "contributions": "1. Proposes SANet, a semantic-aware Agentic AI Networking architecture that infers user semantic goals and automatically assigns cross-layer agents to fulfill them. 2. Formulates the decentralized optimization as a multi-agent multi-objective problem, proposes three novel evaluation metrics, and develops a Model Partition and Sharing (MoPS) framework for efficient model deployment. 3. Derives theoretical bounds proving a three-way tradeoff among optimization, generalization, and conflicting errors, and validates the framework with a hardware prototype showing significant performance gains and computational efficiency.",
      "summary": "This paper proposes SANet, a semantic-aware Agentic AI networking framework for 6G that uses AI agents to infer user goals and perform cross-layer optimization. It formulates the problem as multi-agent multi-objective optimization, introduces a model partition and sharing method, and proves a theoretical tradeoff. Experiments on a hardware prototype show the framework achieves up to 14.61% performance gain while requiring only 44.37% of the FLOPs of state-of-the-art methods.",
      "mindmap": "graph TB\n        Root[SANet: A Semantic-aware Agentic AI Networking Framework] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[核心问题/Problem] --> P1[缺乏支持自动目标发现与多智能体编排的框架 / Lack of framework for automatic goal discovery and multi-agent orchestration]\n        Problem --> P2[协作智能体可能存在目标冲突 / Collaborating agents may have conflicting objectives]\n    \n        Method[主要方法/Method] --> M1[语义感知架构推断用户目标并分配智能体 / Semantic-aware architecture infers user goal and assigns agents]\n        Method --> M2[将问题建模为多智能体多目标优化 / Formulates as multi-agent multi-objective problem]\n        Method --> M3[提出模型分区与共享框架 / Proposes Model Partition and Sharing (MoPS) framework]\n        Method --> M4[提出两种分布式优化算法 / Proposes two decentralized optimization algorithms]\n    \n        Results[关键结果/Results] --> R1[理论证明三方面误差的权衡 / Theoretical proof of three-way error tradeoff]\n        Results --> R2[硬件原型验证性能提升与计算效率 / Hardware prototype validates performance gain and computational efficiency]"
    },
    {
      "title": "Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care",
      "authors": "Tao Zhou, Lingyu Shu, Zixing Zhang, Jing Han",
      "institution": "Hunan University, University of Cambridge",
      "link": "https://arxiv.org/pdf/2512.22601",
      "code": "https://github.com/SmileHnu/Tyee",
      "tags": [
        "others",
        "physiological signal analysis",
        "unified data interface",
        "modular architecture",
        "end-to-end workflow",
        "configurable toolkit"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c4a4d498d8e9b9f7b3a7f4413e2399342920644addd35cc3e7401a5ebadf033_w640_q70.webp",
      "contributions": "1. A unified data interface and configurable preprocessing pipeline for 12 physiological signal modalities. 2. A modular and extensible architecture enabling flexible integration and rapid prototyping. 3. An end-to-end workflow configuration system promoting reproducible and scalable experimentation.",
      "summary": "The paper introduces Tyee, a configurable deep learning toolkit designed to address challenges in physiological signal analysis, such as heterogeneous data and fragmented pipelines. Its unified, modular design allows for flexible and reproducible experimentation. The toolkit demonstrates strong performance, achieving state-of-the-art results on 12 out of 13 evaluated datasets.",
      "mindmap": "graph TB\n        Root[Tyee: A Unified Toolkit for Intelligent Physiological Health Care] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[异构数据格式/Heterogeneous Data Formats]\n        Problem --> P2[不一致的预处理/Inconsistent Preprocessing]\n        Problem --> P3[碎片化模型管道/Fragmented Model Pipelines]\n        Problem --> P4[不可复现的实验/Non-reproducible Experiments]\n        Method --> M1[统一数据接口/Unified Data Interface]\n        Method --> M2[模块化架构/Modular Architecture]\n        Method --> M3[端到端工作流配置/End-to-end Workflow Configuration]\n        Results --> R1[性能优异/Outperforms Baselines]\n        Results --> R2[12/13数据集SOTA/State-of-the-art on 12 of 13 Datasets]\n        Results --> R3[开源工具包/Open-source Toolkit]"
    },
    {
      "title": "Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation",
      "authors": "Junshu Dai, Yu Wang, Tongya Zheng, Wei Ji, Qinghong Guo, Ji Cao, Jie Song, Canghong Jin, Mingli Song",
      "institution": "Zhejiang University, Hangzhou City University, Nanjing University",
      "link": "https://arxiv.org/pdf/2512.22605",
      "code": "https://anonymous.4open.science/r/M3ob-62EF",
      "tags": [
        "location-based recommendation",
        "multi-modal learning",
        "spatial-temporal knowledge graph",
        "cross-modal alignment"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0c70167dfab9ddb767e6d0d1161ce4f31f482e064a77521ffa2e25c598f1d2_w640_q70.webp",
      "contributions": "1. Proposes a unified spatial-temporal relational graph (STRG) for multi-modal representation, enhanced by LLMs. 2. Designs a gating mechanism to fuse spatial-temporal graph representations from different modalities. 3. Introduces an STKG-guided cross-modal alignment method to inject dynamic knowledge into static image representations.",
      "summary": "This paper addresses the limited generalization of next location recommendation methods by proposing M³ob, a framework that leverages multi-modal spatial-temporal knowledge. It constructs a unified graph representation and uses a gating mechanism with cross-modal alignment to capture mobility dynamics. Experiments on six datasets show the method improves performance in both normal and abnormal scenarios.",
      "mindmap": "graph TB\n    A[”Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation”] --> B[”核心问题/Problem: Existing methods have limited generalization; unimodal suffers from sparsity, multi-modal struggles with semantic gap.”]\n    A --> C[”主要方法/Method: Constructs LLM-enhanced spatial-temporal knowledge graph (STKG) and unified STRG; uses gating fusion and STKG-guided cross-modal alignment.”]\n    A --> D[”关键结果/Results: Achieves consistent improvements on six datasets and shows strong generalization in abnormal scenarios.”]"
    },
    {
      "title": "LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation",
      "authors": "Zhongyang Liu, Haoyu Pei, Xiangyi Xiao, Xiaocong Du, Yihui Li, Suting Hong, Kunpeng Zhang, Haipeng Zhang",
      "institution": "ShanghaiTech University, Xi’an Jiaotong-Liverpool University, University of Maryland",
      "link": "https://arxiv.org/pdf/2512.22608",
      "code": null,
      "tags": [
        "agent system",
        "multi-agent system",
        "graph neural network",
        "collective decision-making",
        "startup success prediction",
        "role-playing agents"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5560279fcb9dca880844ffe21da36f9901c81ce8898e265fff9cc80a5e7cfb8a_w640_q70.webp",
      "contributions": "1. Proposes SimVC-CAS, a novel collective agent system that reformulates startup financing prediction as a multi-agent group decision-making task, moving beyond single decision-maker models. 2. Introduces role-playing agents with unique traits and a GNN-based supervised interaction module to capture heterogeneous investor evaluations and behavioral dynamics within a co-investment network. 3. Demonstrates significant predictive performance improvement (e.g., ~25% relative improvement in average precision@10) on real-world PitchBook data with strict leakage controls, while providing interpretable, multi-perspective reasoning.",
      "summary": "This paper addresses the challenge of predicting startup success by simulating venture capital decision-making as a collective process. It proposes SimVC-CAS, a multi-agent system where role-playing LLM agents interact via a GNN module to model investor networks. The method significantly outperforms previous approaches in predicting financing outcomes and offers interpretable reasoning from multiple investor perspectives.",
      "mindmap": "graph TB\n        A[LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[预测初创企业成功/Predicting Startup Success]\n        B --> B2[现有方法忽略投资者群体动态/Existing methods overlook investor group dynamics]\n        C --> C1[提出SimVC-CAS集体代理系统/Propose SimVC-CAS collective agent system]\n        C --> C2[角色扮演代理与GNN交互模块/Role-playing agents & GNN-based interaction module]\n        D --> D1[预测准确性显著提升/Significantly improved predictive accuracy]\n        D --> D2[提供可解释的多视角推理/Provides interpretable, multi-perspective reasoning]"
    },
    {
      "title": "Chord Recognition with Deep Learning",
      "authors": "Pierre Mackenzie",
      "institution": "University of Edinburgh",
      "link": "https://arxiv.org/pdf/2512.22621",
      "code": null,
      "tags": [
        "music information retrieval",
        "chord recognition",
        "deep learning",
        "generative models",
        "pitch augmentation",
        "beat detection"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53155c1b8ae949083d40642ac5cf37ed34863c9840190ad8076fa052bb0f3185_w640_q70.webp",
      "contributions": "1. Identifies and analyzes the poor performance of chord classifiers on rare chords, providing a key insight into a major limitation of current methods. 2. Demonstrates that pitch augmentation is an effective technique for boosting chord recognition accuracy. 3. Improves model interpretability by integrating beat detection into the model's output, leading to some of the best reported results in the field.",
      "summary": "This thesis investigates the slow progress in automatic chord recognition despite the use of deep learning. It experiments with existing methods and generative models, finding that pitch augmentation improves accuracy while generative features do not help. The work concludes by enhancing model interpretability with beat detection, achieving state-of-the-art results and suggesting synthetic data as a promising future direction.",
      "mindmap": "graph TB\n        Root(Chord Recognition with Deep Learning) --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1(进展缓慢/Slow Progress)\n        Method --> M1(实验现有方法/Experiment with Existing Methods)\n        Method --> M2(测试生成模型假设/Test Generative Model Hypotheses)\n        Results --> R1(罕见和弦表现差/Poor Performance on Rare Chords)\n        Results --> R2(音高增强提升准确率/Pitch Augmentation Boosts Accuracy)\n        Results --> R3(节拍检测提升可解释性/Beat Detection Improves Interpretability)"
    },
    {
      "title": "The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?",
      "authors": "Paul Schneider, Amalie Schramm",
      "institution": "PRIORB",
      "link": "https://arxiv.org/pdf/2512.22625",
      "code": null,
      "tags": [
        "forecasting",
        "large language models",
        "deliberation",
        "multi-agent",
        "forecasting accuracy",
        "log loss"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0685f113c8c22bd110f615c357b7de1633adf1104f67be97e690bae6bee70345_w640_q70.webp",
      "contributions": "1. Introduces and tests a structured deliberation intervention for LLMs, where models review each other's forecasts before updating, as a novel method for improving AI-based forecasting. 2. Systematically evaluates the intervention across four distinct scenarios (diverse/homogeneous models with distributed/shared information), identifying that accuracy improvement is specific to diverse models with shared information. 3. Provides empirical evidence that deliberation can be a viable strategy for improving LLM forecasting, while also revealing the unexpected finding that providing additional contextual information did not improve accuracy.",
      "summary": "This study investigates whether allowing large language models (LLMs) to deliberate by reviewing each other's forecasts improves their forecasting accuracy. The method was tested on 202 binary questions across different model group compositions and information-sharing scenarios. The main conclusion is that deliberation significantly improves accuracy for diverse LLM groups with shared information, but not for homogeneous groups, suggesting it as a viable strategy for enhancing LLM-based forecasting.",
      "mindmap": "graph TB\n        Root[”The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?”]\n        Root --> Problem[”核心问题/Problem<br>Does structured deliberation improve LLM forecasting accuracy?”]\n        Root --> Method[”主要方法/Method<br>LLMs review each other's forecasts before updating across four scenarios.”]\n        Root --> Results[”关键结果/Results<br>Accuracy improved for diverse models with shared info; no benefit for homogeneous groups.”]"
    },
    {
      "title": "DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation",
      "authors": "Shiyan Liu, Jian Ma, Rui Qu",
      "institution": "Huazhong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.22629",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "probabilistic scoring",
        "Swiss-system tournament",
        "explainable evaluation",
        "evidence-coupled framework",
        "ranking fidelity"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6045a90fb152ced5737d976a17be84cd02b0a0396b4dd9cb385e9ba4d9063de6_w640_q70.webp",
      "contributions": "1. Introduces DICE, a two-stage, evidence-coupled framework for explainable and robust RAG evaluation using probabilistic \\{A, B, Tie\\} scoring. 2. Employs a Swiss-system tournament to reduce computational complexity from O(N²) to O(N log N) for efficient multi-system comparisons. 3. Demonstrates high agreement (85.7%) with human experts on a Chinese financial QA dataset, outperforming existing LLM-based metrics like RAGAS.",
      "summary": "The paper addresses the lack of interpretability and efficiency in evaluating Retrieval-Augmented Generation (RAG) systems. It proposes DICE, a framework that uses probabilistic scoring and a Swiss-system tournament to provide transparent, confidence-aware judgments while reducing computational cost. The method shows strong agreement with human experts, establishing it as an explainable and efficient paradigm for trustworthy RAG assessment.",
      "mindmap": "graph TB\n        Root[DICE: Discrete Interpretable Comparative Evaluation] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[核心问题/Problem] --> P1[现有指标问题/Existing Metrics Issues]\n        P1 --> P1_1[可解释性有限/Limited Interpretability]\n        P1 --> P1_2[不确定性量化不足/Inadequate Uncertainty Quantification]\n        P1 --> P1_3[计算效率低/Computational Inefficiency]\n    \n        Method[主要方法/Method] --> M1[两阶段证据耦合框架/Two-Stage Evidence-Coupled Framework]\n        M1 --> M1_1[概率{A,B,Tie}评分/Probabilistic Scoring]\n        M1 --> M1_2[可解释推理痕迹/Interpretable Reasoning Traces]\n        Method --> M2[瑞士制锦标赛/Swiss-System Tournament]\n        M2 --> M2_1[降低复杂度/Reduces O(N²) to O(N log N)]\n    \n        Results[关键结果/Results] --> R1[效率提升/Efficiency Gain]\n        R1 --> R1_1[计算量减少42.9%/42.9% Reduction]\n        Results --> R2[评估有效性/Evaluation Validity]\n        R2 --> R2_1[与专家85.7%一致/85.7% Human Agreement]"
    },
    {
      "title": "Scaling Unverifiable Rewards: A Case Study on Visual Insights",
      "authors": "Shuyu Gan, James Mooney, Pan Hao, Renxiang Wang, Mingyi Hong, Qianwen Wang, Dongyeop Kang",
      "institution": "University of Minnesota",
      "link": "https://arxiv.org/pdf/2512.22650",
      "code": "https://minnesotanlp.github.io/insight-scaling-webpage",
      "tags": [
        "agent system",
        "Test-Time Scaling",
        "multi-agent pipeline",
        "process-based refinement",
        "LLM-as-Judge",
        "unverifiable rewards"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e14b2c6ac8597444bea3610d692bad067ed798ec62c0920b0fe7c54b7fcd14_w640_q70.webp",
      "contributions": "1. Proposed Selective Test-Time Scaling, a process-based refinement framework that scales inference across stages in multi-agent pipelines instead of repeated refinement over time. 2. Designed a reliable LLM-based judge model aligned with human experts for evaluating visual insights. 3. Demonstrated improved insight quality under fixed compute budget in a data science pipeline application.",
      "summary": "This paper addresses the challenge of scaling LLM agents for tasks with unverifiable rewards by introducing Selective Test-Time Scaling, which distributes compute across pipeline stages and prunes low-quality branches early using process-specific judges. Applied to generating visual insights from datasets, the method increases mean quality scores and reduces variance compared to traditional time-based refinement. The work provides a foundation for scaling complex, open-ended tasks like scientific discovery and story generation.",
      "mindmap": "graph TB\n        A[Scaling Unverifiable Rewards: A Case Study on Visual Insights] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[多阶段任务缺乏可验证奖励/Multi-stage tasks lack verifiable rewards]\n        B --> B2[基于评判的优化易累积误差/Judge-based refinement prone to error accumulation]\n        C --> C1[选择性测试时扩展/Selective Test-Time Scaling]\n        C --> C2[跨阶段分配计算资源/Distribute compute across stages]\n        C --> C3[早期剪枝低质量分支/Prune low-quality branches early]\n        D --> D1[提升平均分数/Increased mean scores (61.64 to 65.86)]\n        D --> D2[降低方差/Reduced variance]\n        D --> D3[与人类专家对齐的评判模型/Judge model aligned with human experts]"
    },
    {
      "title": "Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains",
      "authors": "Qiankun Li, Feng He, Huabao Chen, Xin Ning, Kun Wang, Zengfu Wang",
      "institution": "University of Science and Technology of China, AnnLab (Institute of Semiconductors, Chinese Academy of Sciences), Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.22664",
      "code": "https://github.com/qklee-lz/CLAdapter",
      "tags": [
        "transfer learning / domain adaptation",
        "Cluster Attention Adapter",
        "adapter tuning",
        "data-limited domains",
        "vision foundation models",
        "adaptive transfer"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d174c6a31de34c34ee98111995fd6b43142db3342aa6c7a647cb2a8121615532_w640_q70.webp",
      "contributions": "1. Proposes a novel Cluster Attention Adapter (CLAdapter) that refines and adapts pre-trained representations to data-limited downstream tasks using attention mechanisms and cluster centers. 2. Designs a unified interface for seamless integration with diverse model architectures (CNNs, Transformers) in both 2D and 3D contexts. 3. Demonstrates state-of-the-art performance through extensive experiments on 10 datasets spanning diverse scientific domains.",
      "summary": "This paper addresses the challenge of adapting large-scale pre-trained vision foundation models to specialized, data-limited scientific domains. It proposes a novel Cluster Attention Adapter (CLAdapter) that personalizes feature enhancement for different downstream tasks, enabling effective adaptive transfer. Extensive experiments across 10 diverse datasets show that CLAdapter achieves state-of-the-art performance, demonstrating its effectiveness in unleashing the potential of foundation models for scientific applications.",
      "mindmap": "graph TB\n        A[Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[下游任务数据稀缺/Data-limited downstream tasks]\n        C --> C1[提出CLAdapter/Propose CLAdapter]\n        C1 --> C2[注意力与聚类中心/Attention & Cluster Centers]\n        C2 --> C3[个性化特征增强/Personalized Feature Enhancement]\n        D --> D1[10个数据集实验/Experiments on 10 datasets]\n        D1 --> D2[实现SOTA性能/Achieves SOTA performance]"
    },
    {
      "title": "Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos",
      "authors": "Shravan Saranyan, Pramit Saha",
      "institution": "Branham High School, University of Oxford",
      "link": "https://arxiv.org/pdf/2512.22657",
      "code": null,
      "tags": [
        "medical image analysis",
        "3D Inception",
        "two-stream networks",
        "CNN-RNN",
        "EchoNet-Dynamic",
        "video analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp",
      "contributions": "1. A systematic investigation and comparison of multiple deep learning architectures (3D Inception, two-stream, CNN-RNN) for automated LVEF estimation from echocardiography videos. 2. Identification that modified 3D Inception architectures achieve the best performance (RMSE of 6.79%) on the EchoNet-Dynamic dataset. 3. Key insights on model design, including the tendency for smaller, simpler models to generalize better and the high sensitivity of performance to hyperparameters like kernel size and normalization.",
      "summary": "This paper investigates deep learning models for automating the estimation of left ventricular ejection fraction (LVEF) from echocardiography videos to address the time-consuming and variable nature of manual assessment. The authors systematically evaluate and modify several architectures, including 3D Inception, two-stream, and CNN-RNN models, finding that a modified 3D Inception model performs best. The study concludes that while these models show promise, they are prone to overfitting and their performance is highly sensitive to specific hyperparameter choices.",
      "mindmap": "graph TB\n        A[Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[手动评估LVEF耗时且存在观察者间差异/Manual LVEF assessment is time-consuming and has inter-observer variability]\n        C --> C1[系统评估3D Inception、双流和CNN-RNN架构/Systematically evaluate 3D Inception, two-stream, and CNN-RNN architectures]\n        C --> C2[在EchoNet-Dynamic数据集上训练和评估/Train and evaluate on the EchoNet-Dynamic dataset]\n        D --> D1[改进的3D Inception架构表现最佳，RMSE为6.79%/Modified 3D Inception achieves best performance (RMSE 6.79%)]\n        D --> D2[更小、更简单的模型泛化能力更好/Smaller, simpler models generalize better]"
    },
    {
      "title": "Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2",
      "authors": "Pere Martra",
      "institution": "Universidad Internacional Menéndez Pelayo (UIMP)",
      "link": "https://arxiv.org/pdf/2512.22671",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "width pruning",
        "expansion ratio",
        "Maximum Absolute Weight (MAW)",
        "GLU-MLP",
        "instruction-following"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4df6003ce0dd5672f67ef0c36b943a93c7cbb475cc96f2c7592e1f230af17d53_w640_q70.webp",
      "contributions": "1. Systematically characterizes a capability dichotomy where structured width pruning degrades parametric knowledge (e.g., MMLU) but significantly improves instruction-following (e.g., IFEval). 2. Discovers and quantifies a robust inverse correlation between factual knowledge and truthfulness, linking knowledge degradation under pruning to improved misconception discrimination. 3. Identifies the expansion ratio as a critical architectural parameter that selectively modulates cognitive capabilities, rather than just a compression metric, and quantifies its context-dependent efficiency trade-offs.",
      "summary": "This paper investigates structured width pruning of GLU-MLP layers in Llama-3.2 models using the Maximum Absolute Weight (MAW) criterion. It finds that pruning creates a dichotomy: while parametric knowledge degrades, instruction-following improves and multi-step reasoning remains robust, challenging the assumption of uniform degradation. The main conclusion is that width pruning acts as a selective filter, reducing knowledge but preserving or enhancing behavioral alignment, with identified trade-offs in efficiency.",
      "mindmap": "graph TB\n        A[Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: How does structured width pruning affect different LLM capabilities?]\n        C[主要方法/Method: MAW-guided pruning of GLU-MLP layers, varying expansion ratio]\n        D[关键结果/Results: Knowledge ↓, Instruction-following ↑, Truthfulness ↑, Efficiency trade-offs]"
    },
    {
      "title": "Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency",
      "authors": "Yoshith Roy Kotla, Varshith Roy Kotla",
      "institution": "The ICFAI Foundation for Higher Education",
      "link": "https://arxiv.org/pdf/2512.22682",
      "code": null,
      "tags": [
        "uncertainty quantification",
        "conformal prediction",
        "adaptive prediction sets",
        "vocabulary-aware",
        "coverage-efficiency tradeoff",
        "marginal coverage"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fb40fe541a33e11ac6d9b3f6f3fac213d5602d391cc590303cb8079bf97a840_w640_q70.webp",
      "contributions": "1. Identified and formally characterized the coverage-efficiency tradeoff unique to applying conformal prediction to next-token prediction in LLMs with large vocabularies. 2. Proposed Vocabulary-Aware Conformal Prediction (VACP), a framework using semantic masking and temperature-adjusted scoring to reduce the effective prediction space. 3. Provided a theoretical analysis of when vocabulary reduction preserves conformal validity and demonstrated a 197x improvement in prediction set efficiency on benchmarks while maintaining coverage guarantees.",
      "summary": "This paper addresses the problem that naive conformal prediction for LLM next-token prediction produces uninformatively large prediction sets due to large vocabularies. It proposes Vocabulary-Aware Conformal Prediction (VACP), which uses semantic masking and hierarchical conformalization to drastically reduce set size. The method achieves near-target coverage while improving set efficiency by 197x, making conformal prediction practical for LLMs.",
      "mindmap": "graph TB\n        A[Conformal Prediction Sets for LLMs] --> B[核心问题/Problem: 标准置信预测在大型词汇表中产生巨大且无用的预测集]\n        A --> C[主要方法/Method: 提出词汇感知置信预测(VACP), 使用语义掩码和分层校准]\n        A --> D[关键结果/Results: 在保持90%覆盖率的同时, 将平均预测集大小从847个词元减少到4.3个]"
    },
    {
      "title": "TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning",
      "authors": "Xiang Cheng, Yulan Hu, Xiangwen Zhang, Lu Xu, Zheng Pan, Xin Li, Yong Liu",
      "institution": "Gaoling School of Artificial Intelligence, Renmin University of China; AMAP, Alibaba Group; National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.22673",
      "code": null,
      "tags": [
        "agent system",
        "travel planning benchmark",
        "multi-turn interaction",
        "tool-augmented agents",
        "deterministic sandbox",
        "real-world user requests"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9e23f8223243d3734eaad4483f929312f198ee51f05e74faf519458ec18add0_w640_q70.webp",
      "contributions": "1. Introduces TravelBench, a real-world travel planning benchmark featuring multi-turn user-agent interaction and tool use, addressing limitations of prior static benchmarks. 2. Constructs a controlled sandbox environment with 10 deterministic travel-domain tools (e.g., POI search, route planning) to enable stable and reproducible evaluation of agent reasoning. 3. Collects and curates a diverse dataset of 1,103 instances (multi-turn, single-turn, unsolvable) from real user scenarios to comprehensively evaluate different aspects of agent performance.",
      "summary": "This paper introduces TravelBench, a new benchmark for evaluating LLM agents in realistic travel planning, which features multi-turn interaction and a sandbox of deterministic tools. The benchmark addresses the limitations of prior work by supporting dynamic user interaction and long-horizon planning. The authors evaluate several LLMs on TravelBench, providing a practical testbed for advancing agent capabilities in planning and tool use.",
      "mindmap": "graph TB\n        A[TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有旅行规划基准缺乏多轮交互和真实场景覆盖/Existing travel planning benchmarks lack multi-turn interaction and real-world coverage]\n        C --> C1[构建包含多轮对话、单轮查询和不可解请求的真实数据集/Build a real-world dataset with multi-turn dialogues, single-turn queries, and unsolvable requests]\n        C --> C2[创建具有10个确定性工具的沙盒环境/Create a sandbox environment with 10 deterministic tools]\n        D --> D1[为LLM智能体评估提供了实用且可复现的基准/Provides a practical and reproducible benchmark for LLM agent evaluation]"
    },
    {
      "title": "Learning with the $p$-adics",
      "authors": "André F. T. Martins",
      "institution": "Instituto Superior Técnico, Universidade de Lisboa; Instituto de Telecomunicações",
      "link": "https://arxiv.org/pdf/2512.22692",
      "code": null,
      "tags": [
        "representation learning",
        "p-adic numbers",
        "ultrametric space",
        "hierarchical representation",
        "non-archimedean geometry",
        "semantic networks"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec090da18463864436ff27e6cb4651eb7e01719b66dcdae5d6644770e6a7b488_w640_q70.webp",
      "contributions": "1. Proposes using the p-adic number field (Q_p) as an alternative to real numbers for machine learning, leveraging its ultrametric and hierarchical structure. 2. Develops foundational building blocks for classification, regression, and representation learning models and algorithms within the p-adic framework. 3. Demonstrates a novel application by representing simple Quillian semantic networks as compact p-adic linear networks, which is not achievable with real numbers.",
      "summary": "This paper explores using p-adic numbers, an ultrametric and non-archimedean field, as an alternative to real numbers for machine learning. It introduces theoretical models and algorithms for classification, regression, and representation learning, showing that p-adics enable compact representations of hierarchical structures like semantic networks. The work opens new research directions by leveraging the unique geometric properties of p-adic spaces.",
      "mindmap": "graph TB\n    A[Learning with the p-adics] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[现有ML基于实数域/Existing ML uses real numbers]\n    B --> B2[是否可用其他域?/Alternative fields possible?]\n    C --> C1[研究p-adic数域/Study p-adic number field Q_p]\n    C --> C2[利用超度量结构/Exploit ultrametric structure]\n    D --> D1[构建分类回归模型/Build classification & regression models]\n    D --> D2[表示学习与语义网络/Representation learning & semantic networks]\n    D --> D3[开启新研究方向/Open new research directions]"
    },
    {
      "title": "GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages",
      "authors": "Ahmed Abdullah, Sana Fatima, Haroon Mahmood",
      "institution": "FAST-National University, Al Ain University",
      "link": "https://arxiv.org/pdf/2512.22705",
      "code": null,
      "tags": [
        "hope speech detection",
        "transformer models",
        "multilingual classification",
        "low-resource languages",
        "XLM-RoBERTa",
        "UrduBERT"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c79c484e6d35762080aa8d6e1dbf075222d30335d656555a46ddf73380d7fe88_w640_q70.webp",
      "contributions": "1. Proposes a multilingual framework for hope speech detection, specifically addressing the underrepresentation of low-resource languages like Urdu. 2. Applies and evaluates multiple pretrained transformer models (XLM-RoBERTa, mBERT, EuroBERT, UrduBERT) on the PolyHope-M 2025 benchmark for this task. 3. Demonstrates strong performance, achieving high F1-scores for Urdu classification, validating the use of existing multilingual models in low-resource settings.",
      "summary": "This paper addresses the lack of resources for hope speech detection in low-resource languages by proposing a multilingual framework using pretrained transformer models like XLM-RoBERTa and UrduBERT. The method involves simple preprocessing and training classifiers, which achieve high F1-scores on the PolyHope-M 2025 benchmark, particularly for Urdu. The results show that existing multilingual models can be effectively implemented to identify hope speech and foster positive digital discourse in low-resource environments.",
      "mindmap": "graph TB\n        A[GHaLIB: 多语言希望语音检测框架 / GHaLIB: Multilingual Hope Speech Detection Framework] --> B[核心问题 / Problem]\n        A --> C[主要方法 / Method]\n        A --> D[关键结果 / Results]\n        B --> B1[希望语音在NLP中代表性不足 / Hope speech underrepresented in NLP]\n        B --> B2[低资源语言(如乌尔都语)缺乏资源 / Lack of resources for low-resource languages (e.g., Urdu)]\n        C --> C1[使用预训练多语言Transformer模型 / Use pretrained multilingual Transformer models]\n        C --> C2[简单预处理与分类器训练 / Simple preprocessing & classifier training]\n        D --> D1[乌尔都语二元分类F1: 95.2% / Urdu binary F1: 95.2%]\n        D --> D2[乌尔都语多类分类F1: 65.2% / Urdu multi-class F1: 65.2%]\n        D --> D3[多语言模型适用于低资源环境 / Multilingual models viable for low-resource settings]"
    },
    {
      "title": "Memento-II: Learning by Stateful Reflective Memory",
      "authors": "Jun Wang",
      "institution": "University College London (UCL)",
      "link": "https://arxiv.org/pdf/2512.22716",
      "code": null,
      "tags": [
        "reinforcement learning",
        "stateful reflective decision process",
        "episodic memory",
        "policy iteration",
        "continual learning",
        "retrieval-augmented generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp",
      "contributions": "1. Introduces the Stateful Reflective Decision Process (SRDP), a formal theoretical framework that models continual learning in LLM agents as a two-stage read-write interaction with episodic memory, linking it to policy evaluation and improvement. 2. Provides a theoretical analysis showing that the reflective learning process induces an equivalent Markov Decision Process, enabling the use of classical dynamic programming and RL tools, and establishes convergence guarantees when instantiated with entropy-regularised policy iteration. 3. Unifies heuristic approaches like case-based reasoning and retrieval-augmented generation with principled reinforcement learning, offering a rigorous mathematical foundation for building memory-augmented agents capable of online adaptation without parameter updates.",
      "summary": "This paper proposes a theoretical framework for continual learning in LLM agents that uses episodic memory and reflection instead of back-propagation. The core method formalizes learning as a Stateful Reflective Decision Process, where writing to memory is policy evaluation and reading from it is policy improvement. The main conclusion is that this framework provides a principled, convergent foundation for agents to self-improve through interaction without fine-tuning.",
      "mindmap": "graph TB\n        A[Memento-II: Learning by Stateful Reflective Memory] --> B[核心问题/Problem: 缺乏理论解释/Lack of theoretical explanation for memory-based continual learning in LLM agents]\n        A --> C[主要方法/Method: 状态化反思决策过程/Stateful Reflective Decision Process (SRDP) with read-write episodic memory]\n        A --> D[关键结果/Results: 提供理论框架与收敛保证/Provides theoretical framework and convergence guarantees for optimal policy]\n        C --> E[写入对应策略评估/Writing corresponds to policy evaluation]\n        C --> F[读取对应策略改进/Reading corresponds to policy improvement]"
    },
    {
      "title": "FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents",
      "authors": "Jiaqi Shao, Yufeng Miao, Wei Zhang, Bing Luo",
      "institution": "Hong Kong University of Science and Technology, Duke Kunshan University, Microsoft AI",
      "link": "https://arxiv.org/pdf/2512.22733",
      "code": "https://github.com/SHAO-Jiaqi757/FoldAct",
      "tags": [
        "agent system",
        "context folding",
        "long-horizon RL",
        "non-stationary observation",
        "gradient dilution",
        "selective segment training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebdb4b7ca8ea3a44c0e368eed5fbbebfc656b663cf280d1891abfbf823c742fa_w640_q70.webp",
      "contributions": "1. Separated loss computation for independent gradient signals on summary and action tokens to address gradient dilution. 2. Full context consistency loss to reduce distribution shift caused by policy-dependent observation changes. 3. Selective segment training to reduce computational cost by processing unique contexts efficiently.",
      "summary": "The paper identifies that treating context folding (history summarization) as a standard action in long-horizon RL for LLMs creates a non-stationary observation distribution, leading to training instability and inefficiency. It proposes FoldAct, a framework with three innovations—separated loss, consistency loss, and selective training—to stabilize training and improve efficiency. The method achieves stable training and a 5.19× speedup.",
      "mindmap": "graph TB\n        A[FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents] --> B[核心问题 / Problem]\n        A --> C[主要方法 / Method]\n        A --> D[关键结果 / Results]\n        B --> B1[非平稳观测分布 / Non-stationary Observation Distribution]\n        B --> B2[梯度稀释 / Gradient Dilution]\n        B --> B3[计算成本高 / High Computational Cost]\n        C --> C1[分离损失计算 / Separated Loss Computation]\n        C --> C2[全上下文一致性损失 / Full Context Consistency Loss]\n        C --> C3[选择性片段训练 / Selective Segment Training]\n        D --> D1[稳定训练 / Stable Training]\n        D --> D2[5.19倍加速 / 5.19× Speedup]"
    },
    {
      "title": "Harnessing Large Language Models for Biomedical Named Entity Recognition",
      "authors": "Jian Chen, Leilei Su, Cong Sun",
      "institution": "Hainan University, Weill Cornell Medicine",
      "link": "https://arxiv.org/pdf/2512.22738",
      "code": null,
      "tags": [
        "named entity recognition",
        "instruction tuning",
        "data filtering",
        "weak-to-strong learning",
        "biomedical named entity recognition",
        "json generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e941de51836d02e0004ef558a69019ce22af7124cd10760a2c904f6329cfa1_w640_q70.webp",
      "contributions": "1. Proposes BioSelectTune, a data-centric framework for fine-tuning LLMs for BioNER that prioritizes data quality. 2. Introduces a Hybrid Superfiltering strategy, a weak-to-strong data curation method to distill a high-impact training dataset. 3. Reformulates BioNER as a structured JSON generation task to leverage LLMs' instruction-following capabilities.",
      "summary": "The paper addresses the challenge of adapting general-domain LLMs to Biomedical Named Entity Recognition (BioNER) by proposing BioSelectTune, a framework that uses a novel Hybrid Superfiltering data curation strategy and formulates BioNER as a JSON generation task. The method achieves state-of-the-art performance on multiple benchmarks, outperforming specialized models even when trained on only 50% of the curated data.",
      "mindmap": "graph TB\n        A[Harnessing LLMs for BioNER] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLMs lack domain knowledge for BioNER / LLMs缺乏生物医学领域知识]\n        B --> B2[Low-quality data degrades performance / 低质量数据导致性能下降]\n        C --> C1[BioSelectTune Framework / BioSelectTune框架]\n        C1 --> C2[Reformulate as JSON generation / 重构为JSON生成任务]\n        C1 --> C3[Hybrid Superfiltering / 混合超级过滤策略]\n        D --> D1[SOTA on benchmarks / 基准测试达到SOTA]\n        D --> D2[Outperforms BioMedBERT / 超越BioMedBERT]\n        D --> D3[50% data surpasses baseline / 50%数据超越全量基线]"
    },
    {
      "title": "Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning",
      "authors": "Hanze Meng, Jianhao Cao, Rachel Pottinger",
      "institution": "University of British Columbia",
      "link": "https://arxiv.org/pdf/2512.22742",
      "code": "https://github.com/fripSideMeng/PACTA",
      "tags": [
        "llm training",
        "Column Type Annotation",
        "Prompt Augmentation",
        "LoRA",
        "Parameter-Efficient Fine-Tuning",
        "Prompt Sensitivity"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95317c9af6072a1e0ffbb34950b7d9da057c55baaf301c56bb751746b366785a_w640_q70.webp",
      "contributions": "1. Proposes a parameter-efficient fine-tuning framework for Column Type Annotation (CTA) using Low-Rank Adaptation (LoRA) to reduce computational cost. 2. Introduces a prompt augmentation strategy during training to mitigate model sensitivity to variations in prompt wording and structure. 3. Demonstrates robust and stable performance across diverse datasets and prompt templates, achieving higher weighted F1 scores than single-template fine-tuning.",
      "summary": "This paper addresses the challenges of prompt sensitivity and high computational cost in using Large Language Models (LLMs) for Column Type Annotation. It proposes a parameter-efficient framework that fine-tunes LLMs using LoRA on prompt-augmented data. The method achieves robust performance across different prompts and datasets while requiring significantly fewer trainable parameters.",
      "mindmap": "graph TB\n        Root(”Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”现有方法对提示词敏感/Existing methods are sensitive to prompts”)\n        Problem --> P2(”完全微调成本高昂/Full fine-tuning is computationally prohibitive”)\n        Method --> M1(”使用LoRA进行参数高效微调/Parameter-efficient fine-tuning with LoRA”)\n        Method --> M2(”使用增强的提示数据进行训练/Training on prompt-augmented data”)\n        Results --> R1(”对不同提示模式性能稳定/Stable performance across diverse prompts”)\n        Results --> R2(”获得更高的加权F1分数/Higher weighted F1 scores”)"
    },
    {
      "title": "Active Constraint Learning in High Dimensions from Demonstrations",
      "authors": "Zheng Qiu, Chih-Yuan Chiu, Glen Chou",
      "institution": "Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22757",
      "code": null,
      "tags": [
        "robot learning",
        "active learning",
        "constraint inference",
        "Gaussian processes",
        "learning from demonstration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5acbf135d5aa431114b6b72db2fb101427cb6bd1e55fb1a8afd3028c0874cb4_w640_q70.webp",
      "contributions": "1. Proposes an iterative active constraint learning (ACL) algorithm that intelligently queries for new demonstrations to reduce constraint uncertainty. 2. Integrates a Gaussian process (GP) model within the learning from demonstrations (LfD) paradigm to represent and infer unknown constraints. 3. Demonstrates superior performance over a random-sampling baseline in recovering nonlinear constraints from sparse, informative demonstrations in high-dimensional settings with nonlinear dynamics.",
      "summary": "This paper addresses the data inefficiency of learning unknown constraints from demonstrations by proposing an active learning algorithm. The method iteratively trains a Gaussian process on demonstration data to model constraints and uses the model's uncertainty to query for new, informative start/goal states to generate more demonstrations. Experiments show the approach outperforms a random-sampling baseline in accurately inferring constraints from fewer demonstrations in high-dimensional, nonlinear environments.",
      "mindmap": "graph TB\n        Root(”Active Constraint Learning in High Dimensions from Demonstrations”) --> Problem(”核心问题/Problem: Data-inefficient constraint inference from demonstrations”)\n        Root --> Method(”主要方法/Method: Iterative active learning with Gaussian Processes”)\n        Root --> Results(”关键结果/Results: Outperforms baseline with sparse, informative demonstrations”)"
    },
    {
      "title": "Understanding the Mechanisms of Fast Hyperparameter Transfer",
      "authors": "Nikhil Ghosh, Denny Wu, Alberto Bietti",
      "institution": "Flatiron Institute, New York University",
      "link": "https://arxiv.org/pdf/2512.22768",
      "code": null,
      "tags": [
        "hyperparameter optimization",
        "hyperparameter transfer",
        "scale-aware hyperparameters",
        "Maximal Update Parameterization (μP)",
        "compute-optimal grid search"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b19c9825d64e8e70117e18cd478466aaf2627a7a5167d401bcac21353870d508_w640_q70.webp",
      "contributions": "1. Develops a formal conceptual framework defining \"fast\" hyperparameter transfer and proves its equivalence to \"useful\" transfer for compute-optimal grid search. 2. Demonstrates that the fast transfer property is not universal and depends critically on problem structure, showing synthetic cases where it succeeds or fails. 3. Proposes and provides empirical evidence for a mechanistic hypothesis explaining fast transfer, decomposing the loss reduction into width-stable and width-sensitive components.",
      "summary": "This paper investigates the mechanisms behind fast hyperparameter transfer, a strategy to reduce tuning costs by transferring optimal hyperparameters from small to large models. It formally defines fast transfer and shows it is computationally advantageous, then explains the phenomenon by hypothesizing a decomposition of the optimization trajectory into stable and sensitive components, supported by empirical evidence.",
      "mindmap": "graph TB\n        Root[”Understanding the Mechanisms of Fast Hyperparameter Transfer<br>理解快速超参数迁移的机制”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Standard HP tuning is too expensive for large models.<br>标准HP调优对于大模型过于昂贵”] --> P1[”子问题/Sub-problem<br>How to define and understand 'fast' HP transfer?<br>如何定义和理解'快速'HP迁移？”]\n        Method[”主要方法/Method<br>Develop a formal framework for HP transfer.<br>建立HP迁移的形式化框架”] --> M1[”方法步骤/Step<br>Define 'fast' vs 'useful' transfer.<br>定义'快速'与'有用'迁移”]\n        Method --> M2[”方法步骤/Step<br>Analyze problem structure & µP.<br>分析问题结构与µP”]\n        Method --> M3[”方法步骤/Step<br>Propose trajectory decomposition hypothesis.<br>提出轨迹分解假设”]\n        Results[”关键结果/Results<br>Fast transfer is equivalent to useful transfer.<br>快速迁移等价于有用迁移”] --> R1[”结果/Result<br>Transfer success depends on problem structure.<br>迁移成功取决于问题结构”]\n        Results --> R2[”结果/Result<br>Empirical evidence supports the hypothesis.<br>实证证据支持该假设”]"
    },
    {
      "title": "Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting",
      "authors": "Yiqian Li, Wen Jiang, Kostas Daniilidis",
      "institution": "University of Pennsylvania",
      "link": "https://arxiv.org/pdf/2512.22771",
      "code": null,
      "tags": [
        "3d reconstruction",
        "3D Gaussian Splatting",
        "Next Best View",
        "Active Learning",
        "Fisher Information",
        "Dynamic Scene Modeling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/512528158b99bf9d8d5519331a5d1557baee5b3050273bff66df842767a73963_w640_q70.webp",
      "contributions": "1. Formulates the next-best-view selection problem for dynamic and semantic 3D scenes as an active learning problem. 2. Proposes an active learning algorithm using Fisher Information to quantify view informativeness for both semantic Gaussian parameters and deformation networks. 3. Provides a unified framework that jointly handles semantic reasoning and dynamic scene modeling, outperforming heuristic and random baselines.",
      "summary": "This paper addresses the challenge of selecting the most informative camera views for training dynamic and semantic 3D Gaussian Splatting models. It proposes an active learning method based on Fisher Information to prioritize frames that maximize information gain for both geometry and semantics. The approach improves rendering quality and segmentation performance compared to random or uncertainty-based selection strategies.",
      "mindmap": "graph TB\n    A[Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting] --> B(核心问题/Problem: Data redundancy in dynamic & semantic scene understanding, need for efficient view selection)\n    A --> C(主要方法/Method: Active learning with Fisher Information to quantify informativeness of views for semantic Gaussians & deformation networks)\n    A --> D(关键结果/Results: Improved rendering quality & semantic segmentation, outperforms random & heuristic baselines)"
    },
    {
      "title": "Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning",
      "authors": "Kasra Jalaldoust, Elias Bareinboim",
      "institution": "Columbia University",
      "link": "https://arxiv.org/pdf/2512.22777",
      "code": null,
      "tags": [
        "causal inference",
        "causal transportability",
        "domain adaptation",
        "few-shot learning",
        "circuit composition",
        "distribution shift"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b025a886304d6097d2893ec4af3bb34a59528db65e22ddf5b4dfff4439a096_w640_q70.webp",
      "contributions": "1. Proposed Circuit-TR, an algorithm for zero-shot compositional generalization based on causal transportability theory, using modules learned from source data. 2. Introduced a supervised domain adaptation scheme that leverages circuit transportability without requiring an explicit causal graph, using only limited target data. 3. Provided theoretical characterization of few-shot learnable tasks using graphical circuit transportability criteria, linking generalizability to circuit size complexity.",
      "summary": "This paper addresses the problem of generalization under distribution shift by proposing a method based on causal transportability theory. The method, Circuit-TR, learns local predictors (modules) from source data and composes them into a circuit for prediction in a target domain, enabling both zero-shot and few-shot adaptation. The theoretical results connect few-shot learnability to circuit transportability criteria and complexity, which are supported by simulations.",
      "mindmap": "graph TB\n    A[Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning] --> B[核心问题/Problem: Generalization under distribution shift]\n    A --> C[主要方法/Method: Circuit-TR algorithm based on causal transportability]\n    A --> D[关键结果/Results: Theoretical characterization of few-shot learnability]\n    B --> B1[领域泛化与适应/Domain Generalization & Adaptation]\n    C --> C1[模块学习与电路组合/Module Learning & Circuit Composition]\n    C --> C2[因果图与机制共享/Causal Graph & Mechanism Sharing]\n    D --> D1[可迁移性标准/Transportability Criteria]\n    D --> D2[电路规模复杂度/Circuit Size Complexity]"
    },
    {
      "title": "GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks",
      "authors": "Xuyan Li, Jie Wang, Zheng Yan",
      "institution": "Xidian University",
      "link": "https://arxiv.org/pdf/2512.22772",
      "code": null,
      "tags": [
        "graph neural networks",
        "temporal graph neural networks",
        "explainable ai",
        "graph explanation",
        "recurrent neural networks",
        "breadth-first search"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0722c64cfeebe092d7b253f417faaa51990e69198068745c39fe241716082408_w640_q70.webp",
      "contributions": "1. Proposes GRExplainer, a universal explanation method applicable to both snapshot-based and event-based Temporal Graph Neural Networks (TGNNs). 2. Introduces an efficient approach using breadth-first search and temporal information to construct node sequences, reducing computational cost. 3. Designs a user-friendly generative model based on Recurrent Neural Networks (RNNs) for automated and continuous explanation generation.",
      "summary": "This paper addresses the lack of explainability in Temporal Graph Neural Networks (TGNNs) by proposing GRExplainer, a universal and efficient method that uses node sequences and an RNN-based generative model to provide explanations. Experiments on six datasets with three TGNNs demonstrate that GRExplainer outperforms existing methods in generality, efficiency, and user-friendliness.",
      "mindmap": "graph TB\n        A[GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[TGNN缺乏透明度和可解释性/Lack of TGNN transparency & explainability]\n        B --> B2[现有方法通用性差、效率低、不友好/Existing methods lack generality, efficiency, user-friendliness]\n        C --> C1[提取节点序列作为统一特征/Extract node sequences as unified features]\n        C --> C2[使用BFS和时间信息构建序列/Use BFS & temporal info to construct sequences]\n        C --> C3[基于RNN的生成模型/RNN-based generative model]\n        D --> D1[在6个数据集上实验/Experiments on 6 datasets]\n        D --> D2[优于现有基线方法/Outperforms existing baselines]\n        D --> D3[通用、高效、用户友好/Generality, efficiency, user-friendliness]"
    },
    {
      "title": "CNSight: Evaluation of Clinical Note Segmentation Tools",
      "authors": "Risha Surana, Adrian Law, Sunwoo Kim, Rishab Sridhar, Angxiao Han, Peiyu Hong",
      "institution": "University of Southern California",
      "link": "https://arxiv.org/pdf/2512.22795",
      "code": null,
      "tags": [
        "text segmentation",
        "clinical note segmentation",
        "transformer models",
        "large language models",
        "MIMIC-IV",
        "rule-based baselines"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be9738f44f0f558dc344bd32b267879341b566035c5dbe67f97ac2e4529b479_w640_q70.webp",
      "contributions": "1. A comprehensive evaluation of diverse methods (rule-based, domain-specific transformers, and large language models) for the task of clinical note segmentation. 2. The curation and use of a dataset of 1,000 notes from MIMIC-IV for benchmarking segmentation performance. 3. Empirical findings that large API-based models (e.g., GPT-5-mini) achieve the best overall performance, while lightweight baselines remain competitive only on structured tasks.",
      "summary": "This paper evaluates various methods for segmenting unstructured clinical notes into distinct sections. It compares rule-based baselines, domain-specific transformers, and large language models on a curated dataset from MIMIC-IV. The main conclusion is that large API-based models like GPT-5-mini achieve the best overall segmentation performance, providing guidance for method selection in downstream clinical applications.",
      "mindmap": "graph TB\n        Root[CNSight: 临床笔记分割工具评估 / CNSight: Evaluation of Clinical Note Segmentation Tools]\n        Root --> Problem[临床笔记非结构化 / Clinical Notes Unstructured]\n        Root --> Method[评估规则/变换器/大语言模型 / Evaluate Rule-based/Transformer/LLMs]\n        Root --> Results[大模型性能最佳 / Large Models Best Performance]"
    },
    {
      "title": "SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance",
      "authors": "Shuai Chen, Chen Wang, Ziran Wang",
      "institution": "School of Mechanical Engineering, Shandong University",
      "link": "https://arxiv.org/pdf/2512.22792",
      "code": null,
      "tags": [
        "open-set recognition",
        "spherical normalization",
        "Mahalanobis distance",
        "electronic nose",
        "open-set recognition",
        "feature drift"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d69d593ebbfea881a49530f570b5c2a934cb8b5cd782c1d7e7c9fba99a92906_w640_q70.webp",
      "contributions": "1. A geometric decoupling mechanism using cascaded batch normalization and L2 normalization to project features onto a unit hypersphere, eliminating signal intensity fluctuations. 2. The introduction of Mahalanobis distance as a scoring mechanism to construct adaptive ellipsoidal decision boundaries that account for anisotropic feature distributions. 3. A universal, architecture-agnostic framework (SNM-Net) that can be seamlessly integrated with various backbone networks (CNN, RNN, Transformer) for robust open-set gas recognition.",
      "summary": "This paper proposes SNM-Net, a universal framework for robust open-set gas recognition in electronic nose systems. It addresses signal drift and unknown interference by projecting features onto a hypersphere for intensity normalization and using Mahalanobis distance for scoring. The method achieves state-of-the-art performance with high accuracy and exceptional robustness across different sensor conditions.",
      "mindmap": "graph TB\n        Root[”SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Feature drift & unknown gas interference in E-nose”] --> P1[”信号漂移/Feature Distribution Shift”]\n        Problem --> P2[”未知气体干扰/Unknown Gas Interference”]\n        Method[”主要方法/Method<br>SNM-Net Framework”] --> M1[”几何解耦/Geometric Decoupling<br>Cascaded Batch & L2 Norm”]\n        Method --> M2[”马氏距离评分/Mahalanobis Distance Scoring”]\n        Method --> M3[”架构无关/Architecture-Agnostic<br>CNN, RNN, Transformer”]\n        Results[”关键结果/Results<br>State-of-the-art performance”] --> R1[”高AUROC/High AUROC: 0.9977”]\n        Results --> R2[”高未知气体检测率/High Unknown Detection: 99.57%”]\n        Results --> R3[”强鲁棒性/High Robustness<br>Low std. dev.”]"
    },
    {
      "title": "Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach",
      "authors": "Minh Bui, Simon Monckton, Mo Chen",
      "institution": "Simon Fraser University, Defense Research & Development Canada (DRDC)",
      "link": "https://arxiv.org/pdf/2512.22793",
      "code": null,
      "tags": [
        "differential games",
        "Hamilton-Jacobi reachability",
        "reach-avoid games",
        "dimensionality decomposition",
        "UAVs",
        "tracking control"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d14da164894760eae683bf30139829cd77a6bbd67cf14fee19eb09a05cb31eff_w640_q70.webp",
      "contributions": "1. A novel dimensionality reduction framework for 3D reach-avoid games by decomposing the problem into horizontal and vertical sub-games., 2. A Hamilton-Jacobi-based tracking control algorithm to reconstruct the solution from sub-games, guaranteeing capture and subsequent tracking of the attacker., 3. Theoretical proof of the conditions for maintaining capture guarantees and empirical validation in both numerical simulations and a physics simulator (Gazebo).",
      "summary": "This paper tackles the high-dimensional challenge of 3D reach-avoid differential games for UAVs by proposing a decomposition approach that splits the problem into horizontal and vertical sub-games, solves them using Hamilton-Jacobi reachability analysis, and uses a novel tracking control to reconstruct the solution. The method is proven to maintain optimality and capture guarantees, and its effectiveness is successfully demonstrated through simulations and a physics simulator for quadrotor capture.",
      "mindmap": "graph TB\n        A[论文标题: Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[3D追逃博弈高维挑战/High Dimensionality of 3D Reach-Avoid Games]\n        B --> B2[现有方法局限性/Limitations of Existing Approaches]\n        C --> C1[维度分解/Dimensionality Decomposition]\n        C1 --> C1_1[水平子博弈/Horizontal Sub-game]\n        C1 --> C1_2[垂直子博弈/Vertical Sub-game]\n        C --> C2[HJ可达性分析/HJ Reachability Analysis]\n        C --> C3[HJ跟踪控制/HJ-based Tracking Control]\n        D --> D1[保持最优性与保证/Maintains Optimality & Guarantees]\n        D --> D2[仿真验证/Simulation Validation]\n        D --> D3[物理模拟器成功捕获/Successful Capture in Physics Simulator]"
    },
    {
      "title": "MoR: Mixture Of Representations For Mixed-Precision Training",
      "authors": "Bor-Yiing Su, Peter Dykas, Mike Chrzanowski, Jatin Chhugani",
      "institution": "Nvidia, Meta",
      "link": "https://arxiv.org/pdf/2512.22804",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "mixed-precision training",
        "FP8",
        "dynamic quantization",
        "tensor representation",
        "low-precision training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14c70a65c4f996e1c88d9996c77e4d780e13466bf11a0601ad82d27376753968_w640_q70.webp",
      "contributions": "1. Proposes Mixture-of-Representations (MoR), a novel per-tensor and sub-tensor level quantization framework that dynamically selects numerical representations based on tensor properties. 2. Introduces and experiments with concrete algorithms that dynamically choose between FP8 and BF16 representations at different granularities. 3. Demonstrates a universal approach that preserves model quality across datasets and achieves state-of-the-art results with 98.38% of tensors quantized to FP8, showing potential for even lower precision formats like NVFP4.",
      "summary": "This paper introduces MoR, a dynamic quantization framework for mixed-precision training that analyzes tensor properties to select between representations like FP8 and BF16. It achieves high FP8 quantization rates (98.38%) while maintaining model quality, offering a robust approach for low-precision training that can be combined with other methods for even lower precision formats.",
      "mindmap": "graph TB\n        Root[MoR: Mixture Of Representations For Mixed-Precision Training] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>Successful mixed-precision training requires the right combination of methods.]\n        Method[主要方法/Method<br>Dynamic, property-aware quantization framework selecting between representations (e.g., FP8/BF16).]\n        Results[关键结果/Results<br>Achieves 98.38% FP8 quantization, preserves model quality, enables lower precision formats.]"
    },
    {
      "title": "EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation",
      "authors": "Libo Zhang, Zekun Li, Tianyu Li, Zeyu Cao, Rui Xu, Xiaoxiao Long, Wenjia Wang, Jingbo Wang, Yuan Liu, Wenping Wang, Daquan Zhou, Taku Komura, Zhiyang Dou",
      "institution": "THU, Brown, Georgia Tech, Cambridge, HKU, NJU, CUHK, HKUST, TAMU, PKU, MIT",
      "link": "https://arxiv.org/pdf/2512.22808",
      "code": null,
      "tags": [
        "human motion generation",
        "egocentric video",
        "3D human reaction",
        "autoregressive generation",
        "VQ-VAE",
        "GPT"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/157e088cb49368b1dbc239ff6380ef8897576c9c3fa92a5bf6737c3a5029e927_w640_q70.webp",
      "contributions": "1. Constructed the Human Reaction Dataset (HRD), a spatially aligned egocentric video-reaction dataset to address data scarcity and misalignment in existing resources. 2. Proposed EgoReAct, the first autoregressive framework for real-time, 3D-aligned human reaction motion generation from streaming egocentric video. 3. Incorporated 3D dynamic features (metric depth, head dynamics) into the generation pipeline to enhance spatial grounding and realism.",
      "summary": "This paper tackles the challenge of generating realistic and spatially aligned 3D human reactions from egocentric video streams. The authors propose EgoReAct, an autoregressive framework that uses a VQ-VAE and a GPT to generate motions in real-time, enhanced by 3D features. Experiments show the method achieves superior realism, spatial consistency, and efficiency while maintaining strict causality.",
      "mindmap": "graph TB\n        A[EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有数据空间不一致/Existing data spatial misalignment]\n        B --> B2[因果生成与3D对齐的挑战/Causal generation & 3D alignment challenge]\n        C --> C1[构建HRD数据集/Build HRD dataset]\n        C --> C2[VQ-VAE压缩运动/VQ-VAE compresses motion]\n        C --> C3[GPT自回归生成/GPT autoregressive generation]\n        C --> C4[融入3D动态特征/Incorporate 3D dynamic features]\n        D --> D1[更高的真实感与空间一致性/Higher realism & spatial consistency]\n        D --> D2[实时生成效率/Real-time generation efficiency]\n        D --> D3[保持严格因果性/Maintains strict causality]"
    },
    {
      "title": "FasterPy: An LLM-based Code Execution Efficiency Optimization Framework",
      "authors": "Yue Wu, Minghao Han, Ruiyin Li, Peng Liang, Amjed Tahir, Zengyang Li, Qiong Feng, Mojtaba Shahin",
      "institution": "Wuhan University, Carnegie Mellon University, Massey University, Central China Normal University, Nanjing University of Science and Technology, RMIT University",
      "link": "https://arxiv.org/pdf/2512.22827",
      "code": "https://github.com/WuYue22/fasterpy",
      "tags": [
        "rag (retrieval-augmented generation)",
        "Code Optimization",
        "Retrieval-Augmented Generation (RAG)",
        "Low-Rank Adaptation (LoRA)",
        "Large Language Models (LLMs)",
        "Python"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49aae1b7cd12cfd30401a619c9b06d4bccc853d1d14eed57af87eb6c80858f31_w640_q70.webp",
      "contributions": "1. Proposes FasterPy, a low-cost and efficient framework that adapts LLMs for Python code execution efficiency optimization. 2. Combines Retrieval-Augmented Generation (RAG) with a knowledge base of performance-improving code pairs and Low-Rank Adaptation (LoRA) to enhance optimization performance. 3. Demonstrates superior performance over existing models on the Performance Improving Code Edits (PIE) benchmark.",
      "summary": "This paper introduces FasterPy, a framework that uses Large Language Models (LLMs) enhanced with Retrieval-Augmented Generation (RAG) and Low-Rank Adaptation (LoRA) to automatically optimize Python code for better execution efficiency. It addresses the limitations of traditional rule-based and data-intensive ML methods by providing a more scalable and cost-effective solution. Experimental results show that FasterPy outperforms existing models on standard benchmarks.",
      "mindmap": "graph TB\n        A[FasterPy: An LLM-based Code Execution Efficiency Optimization Framework] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[传统方法成本高，可扩展性差/Traditional methods are costly and hard to scale]\n        C --> C1[结合RAG与LoRA的LLM框架/LLM framework combining RAG and LoRA]\n        D --> D1[在PIE基准上表现优异/Outperforms existing models on PIE benchmark]"
    },
    {
      "title": "AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning",
      "authors": "Shihao Cai, Runnan Fang, Jialong Wu, Baixuan Li, Xinyu Wang, Yong Jiang, Liangcai Su, Liwen Zhang, Wenbiao Yin, Zhen Zhang, Fuli Feng, Pengjun Xie, Xiaobin Wang",
      "institution": "Tongyi Lab, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.22857",
      "code": null,
      "tags": [
        "agent system",
        "automated environment synthesis",
        "environment-level RL",
        "agentic reinforcement learning",
        "simulated user",
        "policy optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf43f01b4afce8af27cc99730129e26bd5b170c90172ddf77134a48ec54cccb0_w640_q70.webp",
      "contributions": "1. A unified, automated pipeline for synthesizing scalable simulated environments with high-difficulty, easily verifiable tasks. 2. An Environment-level Relative Policy Optimization (ERPO) algorithm that mitigates simulated user instability and performs advantage estimation at the environment level. 3. Comprehensive validation on agentic benchmarks demonstrating effectiveness and out-of-domain generalization.",
      "summary": "This paper proposes AutoForge, a framework to automate the synthesis of challenging simulated environments for training language-based agents via reinforcement learning. It introduces an environment-level RL algorithm to improve training stability and efficiency by handling simulated user instability and heterogeneous environments. Evaluations show the method is effective and generalizes well to out-of-domain tasks.",
      "mindmap": "graph TB\n        A[AutoForge] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[环境合成半自动/Semi-automated Environment Synthesis]\n        B --> B2[任务难度不足/Insufficient Task Difficulty]\n        B --> B3[模拟用户不稳定/Simulated User Instability]\n        C --> C1[自动化环境合成管道/Automated Environment Synthesis Pipeline]\n        C --> C2[环境级RL算法/Environment-level RL Algorithm (ERPO)]\n        D --> D1[基准测试有效/Effective on Benchmarks (τ-bench, etc.)]\n        D --> D2[域外泛化强/Strong Out-of-domain Generalization]"
    },
    {
      "title": "The body is not there to compute: Comment on \"Informational embodiment: Computational role of information structure in codes and robots\" by Pitti et al",
      "authors": "Matej Hoffmann",
      "institution": "Czech Technical University in Prague",
      "link": "https://arxiv.org/pdf/2512.22868",
      "code": null,
      "tags": [
        "embodied cognition",
        "robotics",
        "morphological computation",
        "embodiment",
        "information theory",
        "passive dynamic walker"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c58366db344796ab1e3ca689f71030b6079280073a3b1974c0cb683e87c3918c_w640_q70.webp",
      "contributions": "1. Critiques the application of computational and informational frameworks to biological and robotic bodies, arguing it is a misleading metaphor. 2. Distinguishes between the physical, non-computational role of body morphology and the metaphorical concept of \"morphological computation\". 3. Proposes that the primary function of bodies is not to compute, challenging a core premise of the target article.",
      "summary": "This commentary argues against the central thesis of a target article that applies computational and informational concepts to understand animal and robot bodies. The author contends that the concept of \"morphological computation\" is merely a metaphor and that the body's main role is physical, not computational. The core conclusion is that bodies are not fundamentally for computing, challenging an informational embodiment perspective.",
      "mindmap": "graph TB\n        Root[The body is not there to compute<br>身体不是为了计算] --> Problem[核心问题/Problem<br>Is the body's primary role computational?<br>身体的主要作用是计算吗？]\n        Root --> Method[主要方法/Method<br>Conceptual critique of ”morphological computation”<br>对”形态计算”的概念性批判]\n        Root --> Results[关键结果/Results<br>Body's role is physical, not computational<br>身体的作用是物理的，而非计算的]"
    },
    {
      "title": "Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks",
      "authors": "Maksim Kryzhanovskiy, Svetlana Glazyrina, Roman Ischenko, Konstantin Vorontsov",
      "institution": "Lomonosov Moscow State University, Institute for Artificial Intelligence, Lomonosov Moscow State University",
      "link": "https://arxiv.org/pdf/2512.22876",
      "code": null,
      "tags": [
        "multi-agent reinforcement learning",
        "Reinforcement Networks",
        "directed acyclic graph (DAG)",
        "credit assignment",
        "LevelEnv",
        "hierarchical RL"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b613175c92e9bdddfbd57ed84d044a5846b7b8148cca8ab0405e69847f66c33a_w640_q70.webp",
      "contributions": "1. Introduces the Reinforcement Networks framework, a general approach for collaborative MARL that organizes agents as vertices in a directed acyclic graph (DAG)., 2. Formalizes training and inference methods for the framework and connects it to the LevelEnv concept for reproducible construction and evaluation., 3. Demonstrates improved performance over standard MARL baselines and unifies hierarchical, modular, and graph-structured views of MARL.",
      "summary": "The paper addresses the challenge of end-to-end training for AI systems with multiple learnable components. It proposes Reinforcement Networks, a framework that organizes agents in a directed acyclic graph for flexible credit assignment and coordination in multi-agent reinforcement learning. The method shows improved performance over baselines and provides a principled foundation for designing complex multi-agent systems.",
      "mindmap": "graph TB\n        A[Reinforcement Networks] --> B[核心问题/Problem: End-to-end training of multi-component AI systems]\n        A --> C[主要方法/Method: MARL agents organized in a DAG (Reinforcement Networks)]\n        A --> D[关键结果/Results: Improved performance, unified framework for structured MARL]"
    },
    {
      "title": "SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation",
      "authors": "Hasan Faraz Khan, Noor Fatima, Muzammil Behzad",
      "institution": "King Fahd University of Petroleum and Minerals, SDAIA-KFUPM Joint Research Center for Artificial Intelligence",
      "link": "https://arxiv.org/pdf/2512.22878",
      "code": null,
      "tags": [
        "medical image segmentation",
        "multimodal fusion",
        "text-guided segmentation",
        "transformer-based architecture",
        "lightweight model",
        "3D segmentation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/991651742357d8ab55dc27a29fa78a0c7b0f65e13d3fe1d6d260f8acea2238d9_w640_q70.webp",
      "contributions": "1. Proposes SwinTF3D, a lightweight multimodal fusion model for text-guided 3D medical image segmentation, integrating visual and linguistic representations. 2. Introduces an efficient fusion mechanism to align semantic text prompts with spatial structures in volumetric medical images. 3. Demonstrates competitive performance and significant efficiency gains on the BTCV dataset, offering a practical and interpretable paradigm for interactive clinical segmentation.",
      "summary": "The paper proposes SwinTF3D, a lightweight multimodal model that uses a transformer-based visual encoder and a text encoder to perform text-guided 3D medical image segmentation. It achieves competitive accuracy on the BTCV dataset with low computational overhead, establishing a practical paradigm for interactive, resource-efficient clinical imaging.",
      "mindmap": "graph TB\n        Root[SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Existing 3D segmentation models lack semantic understanding and adaptability to user-defined tasks] --> Problem_Sub[问题细节/Problem Details: Rely on visual-only learning, ineffective for flexible objectives]\n        Method[主要方法/Method: Lightweight multimodal fusion of transformer-based visual encoder and compact text encoder] --> Method_Sub[方法细节/Method Details: Efficient fusion mechanism aligns semantic cues with spatial structures]\n        Results[关键结果/Results: Achieves competitive Dice/IoU scores on BTCV dataset with low computational overhead] --> Results_Sub[结果细节/Results Details: Generalizes well, offers efficiency gains, establishes an interpretable paradigm]"
    },
    {
      "title": "Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations",
      "authors": "Tao Li, Quanyan Zhu",
      "institution": "City University of Hong Kong, New York University",
      "link": "https://arxiv.org/pdf/2512.22883",
      "code": null,
      "tags": [
        "cyber resilience",
        "agentic AI",
        "game theory",
        "autonomous agents",
        "system-theoretic framework",
        "equilibrium-based design"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1d5f82743a059040190978c2a78338bb73c72bc9cec9a0aafe00a0d12f0f24d_w640_q70.webp",
      "contributions": "1. Proposes a paradigm shift from prevention-centric security to agentic cyber resilience, arguing for systems that anticipate, maintain, recover, and learn under attack. 2. Develops a system-level framework and general architecture for designing AI workflows where autonomous agents participate in sensing, reasoning, and action. 3. Demonstrates how game-theoretic formulations provide a unifying design language for analyzing coupled attacker-defender workflows and enable equilibrium-based resiliency design, illustrated with case studies.",
      "summary": "This paper argues that the rise of foundation-model-based AI necessitates a shift from traditional prevention-focused cybersecurity to a new paradigm of agentic cyber resilience. It proposes a system-theoretic framework for designing autonomous AI workflows and uses game theory as a unifying language to model attacker-defender dynamics, concluding that equilibrium-based design enables system-level resilience as demonstrated in case studies like automated penetration testing.",
      "mindmap": "graph TB\n        Root(”Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations”) --> Problem(”核心问题/Problem: Traditional static, human-centered security architectures are mismatched with AI-driven, adaptive cyber threats.”)\n        Root --> Method(”主要方法/Method: Proposes a shift to agentic cyber resilience and a system-level framework using game theory to design autonomous AI workflows.”)\n        Root --> Results(”关键结果/Results: Equilibrium-based design enables system-level resiliency, illustrated through case studies in automated pentesting and cyber deception.”)"
    },
    {
      "title": "DECEPTICON: How Dark Patterns Manipulate Web Agents",
      "authors": "Phil Cuvin, Hao Zhu, Diyi Yang",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.22894",
      "code": "https://agentdarkpatterns.org",
      "tags": [
        "agent system",
        "dark patterns",
        "web agents",
        "adversarial robustness",
        "deceptive UI",
        "agent testing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8998ab43971416f683709173f00be5e8d5373de89f15ac199ec42d645a75b8b6_w640_q70.webp",
      "contributions": "1. Introduces DECEPTICON, a novel environment for testing dark patterns in isolation with 700 web navigation tasks, 2. Demonstrates that dark patterns successfully manipulate agent trajectories in over 70% of tasks, significantly higher than human susceptibility, 3. Shows that larger, more capable models are more susceptible to dark patterns, and existing countermeasures like in-context prompting and guardrail models fail to mitigate the risk effectively.",
      "summary": "This paper introduces DECEPTICON, a testing environment to evaluate how dark patterns manipulate web agents, revealing that these deceptive UI designs successfully steer agent actions in over 70% of tasks, with larger models being more vulnerable and current defenses ineffective. The findings highlight an urgent need for robust defenses against such manipulative designs in agent systems.",
      "mindmap": "graph TB\n        A[DECEPTICON: How Dark Patterns Manipulate Web Agents] --> B[核心问题/Problem: Dark patterns manipulate users and pose risks to agent robustness]\n        A --> C[主要方法/Method: DECEPTICON environment with 700 tasks to test dark patterns in isolation]\n        A --> D[关键结果/Results: Dark patterns steer agents in >70% tasks, larger models more susceptible, defenses fail]"
    },
    {
      "title": "HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery",
      "authors": "Yaping Zhang, Qixuan Zhang, Xingquan Zhang, Zhiyuan Chen, Wenwen Zhuang, Yupu Liang, Lu Xiang, Yang Zhao, Jiajun Zhang, Yu Zhou, Chengqing Zong",
      "institution": "Institute of Automation, Chinese Academy of Sciences; University of the Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.22899",
      "code": null,
      "tags": [
        "benchmark evaluation",
        "scientific intelligence",
        "hierarchical benchmark",
        "multi-disciplinary evaluation",
        "multimodal inputs",
        "dependency-aware framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d6954386f880faf48b86cfbd5d72eb78413ee39a02fe0f600306713ecc9bda_w640_q70.webp",
      "contributions": "1. Introduces HiSciBench, a novel hierarchical benchmark spanning five levels (Scientific Literacy to Scientific Discovery) to evaluate the complete scientific workflow. 2. Provides a comprehensive, multi-disciplinary dataset of 8,735 instances across six scientific fields, supporting multimodal and cross-lingual inputs. 3. Establishes an integrated, dependency-aware evaluation framework that reveals significant performance gaps in foundation models, especially on higher-order discovery tasks.",
      "summary": "The paper introduces HiSciBench, a hierarchical and multi-disciplinary benchmark designed to evaluate the full spectrum of scientific intelligence in foundation models, from basic literacy to creative discovery. It contains thousands of multimodal instances across six disciplines and uses a dependency-aware framework for evaluation. The evaluation of leading models shows a sharp performance decline on complex discovery tasks, highlighting a key capability gap and setting a new standard for assessing scientific AI.",
      "mindmap": "graph TB\n        A[HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Existing benchmarks are fragmented and fail to reflect the hierarchical, multi-disciplinary nature of real scientific inquiry.]\n        C[主要方法/Method: Proposes HiSciBench, a 5-level hierarchical benchmark covering six disciplines with multimodal support and an integrated evaluation framework.]\n        D[关键结果/Results: Models show a large performance gap (69% on basic tasks vs. 25% on discovery), establishing a new evaluation standard.]"
    },
    {
      "title": "A Neural Network-Based Real-time Casing Collar Recognition System for Downhole Instruments",
      "authors": "Si-Yu Xiao, Xin-Di Zhao, Xiang-Zhan Wang, Tian-Hao Mao, Ying-Kai Liao, Xing-Yu Liao, Yu-Qiao Chen, Jun-Jie Wang, Shuang Liu, Tu-Pei Chen, Yang Liu",
      "institution": "University of Electronic Science and Technology of China, China National Petroleum Corporation Logging Co., Ltd., Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.22901",
      "code": null,
      "tags": [
        "on-device ai",
        "Casing Collar Locator (CCL)",
        "ARM Cortex-M7",
        "Depthwise Separable Convolutions",
        "MACs",
        "Inference Latency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/235a8bd15d5c0da93f1ea31dd4b6da44b238cc7be57fd42a7bef3e629f9c6495_w640_q70.webp",
      "contributions": "1. Proposes an in-situ, real-time collar recognition system using embedded neural networks to overcome signal degradation in traditional surface-based monitoring. 2. Introduces lightweight \"Collar Recognition Nets\" (CRNs) optimized for resource-constrained ARM Cortex-M7 microprocessors, using temporal and depthwise separable convolutions. 3. Demonstrates a highly efficient model achieving 8,208 MACs, an F1 score of 0.972, and an average inference latency of 343.2 µs, proving feasibility for downhole power/space constraints.",
      "summary": "This paper addresses the problem of accurate downhole positioning in oil/gas operations by developing a real-time, embedded neural network system for casing collar recognition. The method introduces lightweight \"Collar Recognition Nets\" optimized for ARM Cortex-M7 processors, achieving high accuracy with minimal computational cost. The results demonstrate that robust, autonomous signal processing is feasible within the severe power and space limitations of downhole instrumentation.",
      "mindmap": "graph TB\n        Root[”A Neural Network-Based Real-time Casing Collar Recognition System<br>基于神经网络的实时套管接箍识别系统”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”信号衰减导致井下定位不准确<br>Signal degradation compromises downhole positioning”]\n        Method[”为ARM Cortex-M7优化的轻量级CRN网络<br>Lightweight CRNs optimized for ARM Cortex-M7”]\n        Results[”8208 MACs, F1=0.972, 343.2µs延迟<br>8208 MACs, F1=0.972, 343.2µs latency”]"
    },
    {
      "title": "SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning",
      "authors": "Xiaotian Ren, Nuerxiati Abudurexiti, Zhengyong Jiang, Angelos Stefanidis, Hongbin Liu, Jionglong Su",
      "institution": "Not explicitly stated in provided content.",
      "link": "https://arxiv.org/pdf/2512.22895",
      "code": null,
      "tags": [
        "reinforcement learning",
        "hierarchical deep reinforcement learning",
        "portfolio management",
        "dynamic asset grouping",
        "utility-based capital allocation",
        "SHAP interpretability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07244b408b9238d10b2d5561e0007db8732b1d4e9e79bda5477bef5db2dd385c_w640_q70.webp",
      "contributions": "1. Proposes a hierarchical DRL framework (SAMP-HDRL) that integrates dynamic asset grouping, upper-lower agent coordination, and a utility-based capital allocation mechanism for robust portfolio management. 2. Demonstrates superior performance through extensive backtests across multiple market regimes, showing consistent improvements in return and risk-adjusted metrics over traditional and DRL baselines. 3. Provides interpretability via SHAP analysis, revealing a complementary \"diversified + concentrated\" decision pattern across agent layers.",
      "summary": "This paper tackles portfolio optimization in non-stationary markets by proposing SAMP-HDRL, a hierarchical deep reinforcement learning framework that segments assets, coordinates global and local agents, and uses a utility-based capital allocator. The method outperforms numerous baselines in backtests, achieving higher returns and risk-adjusted ratios, and its decisions are made interpretable through SHAP analysis, revealing a combined diversified and concentrated investment strategy.",
      "mindmap": "graph TB\n        Root[SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Portfolio optimization in non-stationary markets with regime shifts and limited DRL interpretability] --> P1[挑战/Challenges: Dynamic correlations, regime shifts]\n        Method[主要方法/Method: Hierarchical DRL with segmented allocation] --> M1[上层代理/Upper-level Agent: Extracts global market signals]\n        Method --> M2[动态资产分组/Dynamic Asset Grouping: Partitions market into subsets]\n        Method --> M3[下层代理/Lower-level Agents: Perform intra-group allocation]\n        Method --> M4[效用资本分配/Utility-based Capital Allocation: Integrates risky & risk-free assets]\n        Results[关键结果/Results: Outperforms baselines, provides interpretability] --> R1[性能/Performance: Higher Return, Sharpe, Sortino, Omega ratios]\n        Results --> R2[可解释性/Interpretability: SHAP reveals ”diversified + concentrated” mechanism]"
    },
    {
      "title": "Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning",
      "authors": "Ünver Çiftçi",
      "institution": "Tekirdağ Namık Kemal University",
      "link": "https://arxiv.org/pdf/2512.22910",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Q-learning",
        "ensemble learning",
        "satisficing",
        "distillation",
        "bounded rationality"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d12c2382ed5ee9e4da47d1775097d950b626f676e5d3552cd0ff19b6c385b2a_w640_q70.webp",
      "contributions": "1. Proposes a two-phase framework (Sat-EnQ) that first trains an ensemble of lightweight Q-networks using a satisficing objective to limit early value growth and reduce variance. 2. Provides theoretical proof that the satisficing objective induces bounded updates and cannot increase target variance, with a corollary for substantial reduction. 3. Demonstrates empirical results including significant variance reduction, elimination of catastrophic failures, robustness to noise, and improved compute efficiency compared to baseline methods.",
      "summary": "The paper addresses the instability of deep Q-learning, especially early in training, by introducing Sat-EnQ. This framework first trains a satisficing ensemble of weak Q-learners to produce stable, low-variance estimates, then distills and fine-tunes the ensemble. The method significantly improves training reliability, robustness, and computational efficiency compared to standard approaches.",
      "mindmap": "graph TB\n        A[Sat-EnQ] --> B[核心问题/Problem: Deep Q-Learning Instability]\n        A --> C[主要方法/Method: Two-Phase Satisficing Ensemble]\n        A --> D[关键结果/Results: Variance Reduction & Robustness]\n        B --> B1[早期训练不稳定/Early Training Instability]\n        B --> B2[高方差与灾难性失败/High Variance & Catastrophic Failure]\n        C --> C1[阶段1: 满足化集成训练/Phase 1: Satisficing Ensemble Training]\n        C --> C2[阶段2: 蒸馏与微调/Phase 2: Distillation & Fine-tuning]\n        D --> D1[3.8倍方差降低/3.8x Variance Reduction]\n        D --> D2[0%灾难性失败/0% Catastrophic Failure]\n        D --> D3[2.5倍计算效率提升/2.5x Compute Efficiency]"
    },
    {
      "title": "Multimodal Fact-Checking: An Agent-based Approach",
      "authors": "Danni Xu, Shaojing Fan, Xuanang Cheng, Mohan Kankanhalli",
      "institution": "National University of Singapore (NUS)",
      "link": "https://arxiv.org/pdf/2512.22933",
      "code": null,
      "tags": [
        "multimodal fact-checking",
        "multimodal misinformation",
        "agent-based reasoning",
        "explainable dataset",
        "vision-language models",
        "evidence retrieval"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bbe58d9ac10920f1b315029a664c297bd8051834b3724dbf3fa80f26372bec_w640_q70.webp",
      "contributions": "1. Introduces RW-Post, a high-quality, explainable dataset for real-world multimodal fact-checking that aligns claims with original social media posts and provides detailed reasoning and evidence. 2. Proposes AgentFact, a novel agent-based multimodal fact-checking framework that emulates the human verification workflow through five specialized, collaboratively working agents. 3. Demonstrates that the synergy between the new dataset and the agent framework substantially improves both the accuracy and interpretability of multimodal fact-checking.",
      "summary": "This paper addresses the challenge of automated multimodal fact-checking by introducing a new dataset (RW-Post) and an agent-based framework (AgentFact). The dataset provides real-world misinformation instances with reasoning and evidence, while the framework uses specialized agents to collaboratively perform verification tasks. The combined approach is shown to significantly enhance the accuracy and explainability of fact-checking systems.",
      "mindmap": "graph TB\n        A[Multimodal Fact-Checking: An Agent-based Approach] --> B[核心问题/Problem: 多模态虚假信息传播与现有方法在推理和证据利用上的不足 / The spread of multimodal misinformation and the limitations of existing methods in reasoning and evidence utilization]\n        A --> C[主要方法/Method: 提出RW-Post数据集和AgentFact智能体框架 / Proposes the RW-Post dataset and the AgentFact agent-based framework]\n        A --> D[关键结果/Results: 显著提升了多模态事实核查的准确性和可解释性 / Substantially improves the accuracy and interpretability of multimodal fact-checking]"
    },
    {
      "title": "Geometric Structural Knowledge Graph Foundation Model",
      "authors": "Ling Xin, Mojtaba Nayyeri, Zahra Makki Nayeri, Steffen Staab",
      "institution": "University of Stuttgart, University of Southampton, Shahrood University of Technology",
      "link": "https://arxiv.org/pdf/2512.22931",
      "code": null,
      "tags": [
        "knowledge graph reasoning",
        "structural foundation model",
        "geometric attention",
        "inductive link prediction",
        "multi-head transformation",
        "relational fusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1926af9f65861ace968c86c6c18e3eaa892e2114d0d505e3fce8a7ae39975f1_w640_q70.webp",
      "contributions": "1. Proposes Gamma, a novel structural KG foundation model that replaces the single relational transformation with multiple parallel geometric transformations (real, complex, split-complex, dual). 2. Introduces a relational conditioned attention fusion mechanism with entropy regularization to adaptively fuse these geometric representations at the link level. 3. Provides a full formalization of the algebraic message functions and demonstrates through extensive experiments on 56 KGs that Gamma consistently outperforms the prior state-of-the-art (Ultra) in zero-shot inductive link prediction.",
      "summary": "This paper identifies a key limitation in existing structural knowledge graph foundation models: their reliance on a single relational transformation limits their ability to capture diverse relational patterns. To address this, the authors propose Gamma, a new model that employs multi-head geometric attention, using parallel transformations from different algebraic spaces and a fusion mechanism to adaptively combine them. Comprehensive experiments show that Gamma outperforms the previous best model, Ultra, in zero-shot inductive link prediction across diverse benchmarks, demonstrating the benefit of complementary geometric representations.",
      "mindmap": "graph TB\n        A[Geometric Structural Knowledge Graph Foundation Model] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法依赖单一关系转换，表达能力受限/Existing methods rely on single relational transformation, limiting expressiveness]\n        C --> C1[引入多头几何注意力/Multi-head geometric attention]\n        C --> C2[并行多种几何变换/Parallel geometric transformations]\n        C --> C3[关系条件注意力融合/Relational conditioned attention fusion]\n        D --> D1[在56个KG上超越ULTRA/Outperforms ULTRA on 56 KGs]\n        D --> D2[零样本归纳链接预测性能提升/Improves zero-shot inductive link prediction]"
    },
    {
      "title": "Heterogeneity in Multi-Agent Reinforcement Learning",
      "authors": "Tianyi Hu, Zhiqiang Pu, Yuan Wang, Tenghai Qiu, Min Chen, Xin Yu",
      "institution": "Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.22941",
      "code": "https://github.com/Harry67Hu/HetDPS",
      "tags": [
        "multi-agent reinforcement learning",
        "heterogeneity",
        "multi-agent reinforcement learning",
        "parameter sharing",
        "heterogeneity distance",
        "dynamic algorithm"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de4448c413f180749bc7f2220bea2793dad9a358fb068164020bd7b0421e5b05_w640_q70.webp",
      "contributions": "1. Proposes a systematic categorization of heterogeneity in MARL into five types with mathematical definitions. 2. Defines a heterogeneity distance and introduces a practical method to quantify agent heterogeneity. 3. Designs a heterogeneity-based dynamic parameter sharing algorithm that demonstrates better interpretability and adaptability compared to baselines.",
      "summary": "This paper addresses the lack of a rigorous definition and understanding of heterogeneity in multi-agent reinforcement learning (MARL). It proposes a methodology to define, quantify, and utilize heterogeneity, culminating in a dynamic parameter sharing algorithm. Experiments show this algorithm offers improved interpretability and adaptability over other parameter-sharing methods.",
      "mindmap": "graph TB\n        Root[”Heterogeneity in Multi-Agent Reinforcement Learning<br/>多智能体强化学习中的异质性”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”缺乏对异质性的严格定义<br/>Lacks rigorous definition of heterogeneity”]\n        Method --> M1[”定义与分类<br/>Definition & Categorization”]\n        Method --> M2[”量化方法<br/>Quantification Method”]\n        Method --> M3[”应用算法<br/>Application Algorithm”]\n        M1 --> M1_1[”五类异质性<br/>Five types of heterogeneity”]\n        M2 --> M2_1[”异质性距离<br/>Heterogeneity distance”]\n        M3 --> M3_1[”动态参数共享<br/>Dynamic Parameter Sharing”]\n        Results --> R1[”有效识别与量化<br/>Effective identification & quantification”]\n        Results --> R2[”算法性能优越<br/>Algorithm outperforms baselines”]"
    },
    {
      "title": "APO: Alpha-Divergence Preference Optimization",
      "authors": "Wang Zixian",
      "institution": "China Mobile Communications Group Shandong Co., Ltd. Tai’an Branch",
      "link": "https://arxiv.org/pdf/2512.22953",
      "code": null,
      "tags": [
        "reinforcement learning from human feedback (rlhf)",
        "alpha-divergence",
        "preference optimization",
        "mode collapse",
        "anchored coordinates",
        "gradient variance"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a407212dc95985ef8918d58e7c65f70fd3f6adf8764c95f85871cd1924b3528_w640_q70.webp",
      "contributions": "1. Introduces APO, an anchored framework using Csiszár alpha-divergence to continuously interpolate between forward and reverse KL behavior for RLHF. 2. Derives unified gradient dynamics parameterized by alpha and analyzes gradient variance properties. 3. Proposes a practical reward-and-confidence-guarded alpha schedule to transition from mode-covering to mode-seeking behavior safely.",
      "summary": "This paper addresses the trade-off between stable but under-exploitative mode-covering updates and high-reward but unstable mode-seeking updates in LLM alignment. It proposes APO, an anchored preference optimization framework that uses alpha-divergence to smoothly interpolate between these regimes via a guarded schedule. Experiments show APO achieves competitive performance while maintaining training stability.",
      "mindmap": "graph TB\n        Root[APO: Alpha-Divergence Preference Optimization] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[两种分歧权衡 / Two Divergence Trade-off]\n        P1 --> P2[前向KL覆盖但保守 / Forward KL: Mode-Covering but Conservative]\n        P1 --> P3[反向KL寻求但易崩溃 / Reverse KL: Mode-Seeking but Collapses]\n        Method[主要方法/Method] --> M1[锚定框架 / Anchored Framework]\n        M1 --> M2[使用α-散度插值 / Use α-Divergence to Interpolate]\n        M2 --> M3[调度α值 / Schedule α Value]\n        Results[关键结果/Results] --> R1[竞争性性能 / Competitive Performance]\n        Results --> R2[保持稳定性 / Maintains Training Stability]"
    },
    {
      "title": "OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding",
      "authors": "Wenyuan Huang, Zhao Wang, Zhou Wei, Ting Huang, Fang Zhao, Jian Yang, Zhenyu Zhang",
      "institution": "Nanjing University, China Mobile Zijin Innovation Institute",
      "link": "https://arxiv.org/pdf/2512.23020",
      "code": null,
      "tags": [
        "3D visual grounding",
        "open-world",
        "zero-shot",
        "active cognition-based reasoning",
        "object lookup table",
        "visual language models"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dca85890cab234049b1698ab9f6ade12ea02b16f297cec04cbcb907dcdffb7be_w640_q70.webp",
      "contributions": "1. Proposes OpenGround, a novel zero-shot framework for open-world 3D visual grounding that overcomes the limitation of pre-defined object categories. 2. Introduces the Active Cognition-based Reasoning (ACR) module to progressively augment VLM cognition via a cognitive task chain and a dynamically updated Object Lookup Table (OLT). 3. Presents a new dataset named OpenTarget with over 7000 object-description pairs to evaluate open-world 3D grounding performance.",
      "summary": "This paper addresses the limitation of existing 3D visual grounding methods that rely on a pre-defined object lookup table, which restricts their use in open-world scenarios. The authors propose OpenGround, a zero-shot framework featuring an Active Cognition-based Reasoning module that dynamically expands the model's cognitive scope to handle undefined objects. The method achieves competitive or state-of-the-art results on standard benchmarks and shows a 17.6% improvement on their new OpenTarget dataset.",
      "mindmap": "graph TB\n        A[OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法依赖预定义对象表，无法处理未定义目标/Existing methods rely on pre-defined OLT, limiting open-world application]\n        C --> C1[提出OpenGround框架与主动认知推理模块/Propose OpenGround framework with Active Cognition-based Reasoning (ACR) module]\n        C1 --> C2[通过认知任务链和动态更新的OLT增强VLM认知/Enhance VLM cognition via cognitive task chain and dynamically updated OLT]\n        D --> D1[Nr3D上表现有竞争力，ScanRefer上达到SOTA/Competitive on Nr3D, SOTA on ScanRefer]\n        D --> D2[在OpenTarget数据集上提升17.6%/17.6% improvement on OpenTarget dataset]"
    },
    {
      "title": "LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models",
      "authors": "Wenxuan Xu, Arvind Pillai, Subigya Nepal, Amanda C Collins, Daniel M Mackin, Michael V Heinz, Tess Z Griffin, Nicholas C Jacobson, Andrew Campbell",
      "institution": "Dartmouth College, University of Virginia, Massachusetts General Hospital, Harvard Medical School",
      "link": "https://arxiv.org/pdf/2512.23025",
      "code": null,
      "tags": [
        "multimodal language models",
        "multimodal sensing",
        "time-series encoding",
        "ecological momentary assessment (EMA)"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4b276805556458f16b63a7994f848b1c3a3a24eeed8ecb80496361d925fb9d8_w640_q70.webp",
      "contributions": "1. Introduces LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental health narratives. 2. Constructs a large-scale dataset of over 100,000 sensor-text QA pairs by transforming Ecological Momentary Assessment (EMA) responses. 3. Trains a patch-level encoder to project raw sensor time-series signals directly into an LLM's representation space for native integration.",
      "summary": "The paper addresses the challenge of translating long-duration, multimodal sensor data into interpretable natural language for mental health assessment. It proposes the LENS framework, which creates a large sensor-text dataset and trains a specialized encoder to align sensor signals with an LLM, enabling the generation of clinically meaningful narratives. The results show LENS outperforms baselines on NLP and clinical metrics, and is validated by mental health professionals.",
      "mindmap": "graph TB\n        Root[LENS: LLM-Enabled Narrative Synthesis] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[传感器数据难以转化为自然语言/Sensor data hard to translate to text]\n        Problem --> P2[缺乏配对数据集/Lack of paired sensor-text datasets]\n        Method[主要方法/Method] --> M1[构建大规模传感器-文本QA数据集/Build large-scale sensor-text QA dataset]\n        Method --> M2[训练补丁级编码器对齐LLM/Train patch-level encoder to align with LLM]\n        Results[关键结果/Results] --> R1[在NLP和症状指标上超越基线/Outperforms baselines on NLP & symptom metrics]\n        Results --> R2[临床医生认为叙述全面有意义/Clinicians find narratives comprehensive & meaningful]"
    },
    {
      "title": "An Architecture-Led Hybrid Report on Body Language Detection Project",
      "authors": "Thomson Tong, Diba Darooneh",
      "institution": "None",
      "link": "https://arxiv.org/pdf/2512.23028",
      "code": "BodyLanguageDetection",
      "tags": [
        "video understanding",
        "vision-language models",
        "structured generation",
        "bounding boxes",
        "mixture-of-experts",
        "video analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a618c048bc336ad2ade96a7a97cf301fb10fee2c9c8e7bc16556348f1c0c4b9d_w640_q70.webp",
      "contributions": "1. Provides an architecture-led analysis of two modern VLMs (Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct) for a practical task. 2. Maps model architectural properties to a concrete video-to-artifact pipeline for person detection and attribute extraction. 3. Explicitly defines and analyzes critical system constraints and limitations arising from model behavior, such as semantic vs. syntactic correctness and frame-local identifiers.",
      "summary": "This report analyzes two vision-language models (VLMs) and connects their architectures to a practical system for detecting people and their emotions in video frames. The system prompts VLMs to generate structured outputs like bounding boxes, validates the output structure, and can render annotated videos. The core conclusion is that understanding model architecture is crucial for designing robust interfaces and making defensible claims, as VLMs can produce syntactically correct but semantically incorrect outputs.",
      "mindmap": "graph TB\n        A[An Architecture-Led Hybrid Report on Body Language Detection Project] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[如何基于VLM架构构建可靠的应用系统/How to build reliable application systems based on VLM architecture]\n        C --> C1[分析两种VLM架构并映射到视频处理流程/Analyze two VLM architectures and map to a video processing pipeline]\n        C --> C2[系统采样视频帧，提示VLM生成结构化输出/System samples video frames, prompts VLM for structured output]\n        C --> C3[使用预定义模式验证输出结构/Validate output structure with predefined schema]\n        D --> D1[结构化输出可能语法正确但语义错误/Structured outputs can be syntactically valid but semantically incorrect]\n        D --> D2[模式验证是结构性的，非几何正确性/Schema validation is structural, not geometric]\n        D --> D3[理解架构对设计稳健接口和评估至关重要/Understanding architecture is critical for robust interface design and evaluation]"
    },
    {
      "title": "Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware",
      "authors": "Alex Khalil, Guillaume Heilles, Maria Parraga, Simon Heilles",
      "institution": "UCLouvain, Universidad Espíritu Santo, DENEM Labs",
      "link": "https://arxiv.org/pdf/2512.23029",
      "code": null,
      "tags": [
        "llm inference",
        "quantization",
        "mixture-of-experts",
        "on-premise deployment",
        "consumer-grade hardware",
        "benchmark analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed34c10397ed5cae19c39a4a8e2a5a1f0fd64e2f76183b8ba093c74b9a79fe51_w640_q70.webp",
      "contributions": "1. A comprehensive benchmarking framework for evaluating both the intrinsic model capabilities and the server-side performance (latency, throughput, scalability) of a private LLM deployment. 2. A practical demonstration and performance analysis of deploying a quantized, large-scale (30B parameter) Mixture-of-Experts model (Qwen3) on next-generation consumer-grade hardware (NVIDIA RTX 5090). 3. Evidence that a carefully configured on-premises LLM server can achieve performance comparable to cloud services, offering SMBs a viable, cost-effective, and privacy-preserving alternative.",
      "summary": "This paper investigates the feasibility of deploying a private, high-performance LLM server for Small and Medium Businesses using consumer-grade hardware. It benchmarks a quantized Qwen3-30B model on an NVIDIA RTX 5090, evaluating both model capability and server performance under load. The results show that such an on-premises setup can achieve performance close to cloud services at a lower cost and with full data privacy.",
      "mindmap": "graph TB\n        A[Viability and Performance of a Private LLM Server for SMBs<br>SMB私有LLM服务器的可行性与性能] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Cloud reliance: cost, privacy, sovereignty for SMBs<br>云依赖：成本、隐私、SMB主权]\n        C[主要方法/Method<br>Benchmark quantized Qwen3-30B on consumer hardware (RTX 5090)<br>在消费级硬件上对量化Qwen3-30B进行基准测试]\n        D[关键结果/Results<br>On-premises performance rivals cloud, viable for SMBs<br>本地性能媲美云端，对SMB可行]"
    },
    {
      "title": "Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization",
      "authors": "Kerem Zaman, Shashank Srivastava",
      "institution": "UNC Chapel Hill",
      "link": "https://arxiv.org/pdf/2512.23032",
      "code": null,
      "tags": [
        "interpretability",
        "chain-of-thought",
        "faithfulness",
        "causal mediation analysis",
        "biasing features",
        "explainability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/527440e442abe55ce371c5ad3ce8f49609f0398a6001b33523b6a3aa4bbc6e44_w640_q70.webp",
      "contributions": "1. Argues that the Biasing Features metric conflates unfaithfulness with incompleteness in Chain-of-Thought explanations. 2. Introduces a new faithful@k metric showing increased token budgets improve hint verbalization. 3. Uses Causal Mediation Analysis to show non-verbalized hints can still causally mediate predictions through the CoT.",
      "summary": "This paper challenges the use of hint-verbalization metrics like Biasing Features for evaluating the faithfulness of Chain-of-Thought reasoning. It proposes that apparent unfaithfulness is often due to incompleteness from lossy compression and tight token limits, not a lack of alignment, and demonstrates this using new metrics and causal mediation analysis. The conclusion advocates for a broader interpretability toolkit beyond hint-based evaluations.",
      "mindmap": "graph TB\n        A[Is Chain-of-Thought Really Not Explainability?<br/>Chain-of-Thought Can Be Faithful without Hint Verbalization] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Biasing Features 指标将不完整性误判为不忠实性<br/>Biasing Features metric mislabels incompleteness as unfaithfulness]\n        C --> C1[提出 faithful@k 指标并增加推理令牌预算<br/>Propose faithful@k metric & increase inference token budget]\n        C --> C2[使用因果中介分析<br/>Use Causal Mediation Analysis]\n        D --> D1[许多被标记为不忠实的 CoT 被其他指标判定为忠实<br/>Many CoTs flagged unfaithful are judged faithful by other metrics]\n        D --> D2[更大的令牌预算显著提高提示词显化率<br/>Larger token budgets greatly increase hint verbalization]\n        D --> D3[未显化的提示词仍可通过 CoT 因果中介预测<br/>Non-verbalized hints can causally mediate predictions through CoT]"
    },
    {
      "title": "Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education",
      "authors": "Danial Hooshyar, Yeongwook Yang, Gustav Šíř, Tommi Kärkkäinen, Raija Hämäläinen, Mutlu Cukurova, Roger Azevedo",
      "institution": "Tallinn University, University of Jyväskylä, Gangneung-Wonju National University, Czech Technical University, University College London, University of Central Florida",
      "link": "https://arxiv.org/pdf/2512.23036",
      "code": null,
      "tags": [
        "educational data mining",
        "knowledge tracing",
        "learner modelling",
        "temporal coherence",
        "fine-tuning",
        "deep knowledge tracing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5735716766e72627a0d5d23b01771e8d0161795e3958d394eccf1045f5a797ec_w640_q70.webp",
      "contributions": "1. Provides a synthesis of evidence on the limitations of LLM-based tutors, framing them within the high-risk context of K-12 education and responsible AI design. 2. Empirically demonstrates that a Deep Knowledge Tracing (DKT) model significantly outperforms a widely-used LLM (both zero-shot and fine-tuned) in next-step correctness prediction and temporal coherence of mastery estimation. 3. Highlights the computational inefficiency of fine-tuning LLMs for this task compared to DKT, and argues for hybrid frameworks over LLM-only approaches for responsible tutoring.",
      "summary": "This paper investigates whether large language models (LLMs) can effectively replace traditional learner modelling for adaptive tutoring in K-12 education. By comparing a Deep Knowledge Tracing (DKT) model against a fine-tuned and zero-shot LLM on knowledge assessment tasks, it finds DKT is more accurate, reliable, and temporally coherent. The study concludes that LLMs alone are insufficient for responsible tutoring and advocates for hybrid systems that incorporate dedicated learner models.",
      "mindmap": "graph TB\n        A[”Problems With LLMs for Learner Modelling<br/>LLM在学情建模中的问题”] --> B\n        A --> C\n        A --> D\n        B[”核心问题/Problem<br/>LLMs may replace learner models<br/>LLM可能替代学情模型”] --> B1[”高风险领域/High-risk domain (K-12)”]\n        B --> B2[”需要评估准确性、可靠性、时序一致性/Need to assess accuracy, reliability, temporal coherence”]\n        C[”主要方法/Method<br/>Compare DKT vs. LLM<br/>对比DKT与LLM”] --> C1[”数据集/Dataset: large open-access”]\n        C --> C2[”模型/Models: DKT, LLM (zero-shot & fine-tuned)”]\n        D[”关键结果/Results<br/>DKT outperforms LLM<br/>DKT优于LLM”] --> D1[”更高AUC/Higher AUC (0.83)”]\n        D --> D2[”更好的时序一致性/Better temporal coherence”]\n        D --> D3[”结论: LLMs alone fall short, need hybrid frameworks<br/>Conclusion: LLM单独不足，需要混合框架”]"
    },
    {
      "title": "The Reward Model Selection Crisis in Personalized Alignment",
      "authors": "Fady Rezk, Yuangang Pan, Chuan-Sheng Foo, Xun Xu, Nancy Chen, Henry Gouk, Timothy Hospedales",
      "institution": "University of Edinburgh, Agency for Science, Technology and Research (A*STAR)",
      "link": "https://arxiv.org/pdf/2512.23067",
      "code": null,
      "tags": [
        "alignment & personalization",
        "reward-guided decoding",
        "policy accuracy",
        "Pref-LaMP benchmark"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1a0a1bd7d940db8c394f189ea84b7dbcfae7cd34b8e3662ea7c7a8babfdfefe_w640_q70.webp",
      "contributions": "1. Identifies and demonstrates the failure of standard reward model (RM) accuracy as a selection criterion for deployment-ready personalized alignment. 2. Introduces a new metric, policy accuracy, to evaluate the token-level discrimination ability of reward models under inference-time adaptation (reward-guided decoding). 3. Introduces Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation and revealing a decoupling between reward discrimination and actual generation quality.",
      "summary": "This paper identifies a crisis in personalized alignment, showing that optimizing reward models for preference ranking accuracy does not translate to effective behavioral adaptation under realistic deployment constraints like reward-guided decoding. The authors propose a new metric (policy accuracy) and a new benchmark (Pref-LaMP) to evaluate this gap, finding that reward model accuracy poorly predicts generation quality and that simple in-context learning often outperforms reward-guided methods for larger models.",
      "mindmap": "graph TB\n        A[The Reward Model Selection Crisis in Personalized Alignment] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Standard RM accuracy fails to predict deployment performance for personalized alignment]\n        C[主要方法/Method<br>Introduce policy accuracy metric and Pref-LaMP benchmark for direct evaluation]\n        D[关键结果/Results<br>Weak correlation between RM & policy accuracy; ICL outperforms reward-guided decoding]"
    },
    {
      "title": "Trust Region Masking for Long-Horizon LLM Reinforcement Learning",
      "authors": "Yingru Li, Jiacai Liu, Jiawei Xu, Yuxuan Tong, Ziniu Li, Baoxiang Wang",
      "institution": "(Institutions not explicitly listed in provided content; inferred from author names and common affiliations in the field, but not specified. Therefore, output is left blank.)",
      "link": "https://arxiv.org/pdf/2512.23075",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "trust region",
        "policy gradient",
        "off-policy mismatch",
        "KL divergence",
        "sequence-level masking"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74d8872516a568f2933a83e36b0cf25d414f3a25ffa113cd0cee809d2b39c6ac_w640_q70.webp",
      "contributions": "1. Deriving two novel, tighter theoretical bounds (Pinsker-Marginal and Mixed) on the approximation error in off-policy LLM-RL, scaling better with sequence length than classical O(T^2) bounds. 2. Identifying that these bounds depend on a sequence-level quantity (maximum token-level KL divergence) that cannot be controlled by token-independent methods like PPO clipping. 3. Proposing the Trust Region Masking (TRM) algorithm, which masks entire sequences from gradient updates to enforce the trust region, providing non-vacuous monotonic improvement guarantees for long-horizon tasks.",
      "summary": "The paper addresses the problem that classical trust region bounds become vacuous for long-horizon LLM reinforcement learning due to unavoidable off-policy mismatch. It proposes Trust Region Masking (TRM), a method that excludes entire sequences from gradient computation if any token violates a trust region constraint. This approach, supported by new tighter theoretical bounds, provides the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.",
      "mindmap": "graph TB\n        A[Trust Region Masking for Long-Horizon LLM RL] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Off-policy mismatch in LLM-RL<br/>导致经典信任域边界失效]\n        C --> C1[提出信任域掩码(TRM)<br/>Propose Trust Region Masking (TRM)]\n        C1 --> C2[序列级掩码<br/>Sequence-level Masking]\n        D --> D1[推导更紧的理论边界<br/>Derive Tighter Bounds (O(T), O(T^{3/2}))]\n        D --> D2[提供非平凡的单调改进保证<br/>Provide Non-vacuous Guarantees]"
    },
    {
      "title": "Multimodal Functional Maximum Correlation for Emotion Recognition",
      "authors": "Deyang Zheng, Tianyi Zhang, Wenming Zheng, Shujian Yu",
      "institution": "Southeast University, Westlake University, Vrije Universiteit Amsterdam",
      "link": "https://arxiv.org/pdf/2512.23076",
      "code": "https://github.com/DY9910/MFMC",
      "tags": [
        "multimodal learning",
        "self-supervised learning",
        "dual total correlation",
        "functional maximum correlation analysis",
        "affective computing",
        "physiological signals"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369b23e1c7a17085940402e88d2347afb3386819a237c48ac347be73127aea2a_w640_q70.webp",
      "contributions": "1. Proposes a novel self-supervised learning framework (MFMC) that maximizes higher-order multimodal dependence using a Dual Total Correlation objective. 2. Derives a tight sandwich bound and optimizes it using a functional maximum correlation analysis-based trace surrogate to capture joint interactions. 3. Demonstrates state-of-the-art or competitive performance on affective computing benchmarks, showing robustness to inter-subject variability.",
      "summary": "The paper addresses the challenge of learning joint dynamics from scarce and subjective emotion labels by proposing a self-supervised learning framework called MFMC. It captures higher-order multimodal dependencies beyond pairwise alignment, leading to improved emotion recognition performance on physiological signal benchmarks. The results show significant accuracy gains, particularly in subject-independent settings, highlighting the method's effectiveness.",
      "mindmap": "graph TB\n        A[MFMC for Emotion Recognition] --> B[核心问题/Problem: 情感状态表现为跨系统的协调但异质的生理反应，现有自监督方法难以捕捉多模态高阶交互。]\n        A --> C[主要方法/Method: 提出MFMC框架，通过Dual Total Correlation目标和Functional Maximum Correlation Analysis最大化高阶多模态依赖性。]\n        A --> D[关键结果/Results: 在多个基准测试中达到SOTA或竞争性性能，显著提升CEAP-360VR数据集上的准确率，对主体间变异性鲁棒。]"
    },
    {
      "title": "Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning",
      "authors": "Yingru Li, Jiawei Xu, Jiacai Liu, Yuxuan Tong, Ziniu Li, Tianle Cai, Ge Zhang, Qian Liu, Baoxiang Wang",
      "institution": "(Institutions not explicitly listed in provided content. Affiliation inference requires author list with affiliations or email domains, which are not present in the given text. Therefore, cannot be determined from the provided snippet.)",
      "link": "https://arxiv.org/pdf/2512.23087",
      "code": null,
      "tags": [
        "llm training",
        "reinforcement learning",
        "training-inference mismatch",
        "vocabulary pruning",
        "gradient estimation",
        "numerical stability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c83b750895381feb238b14991a4015088fa8d05eb24ab0374082f6c25fb3ddd7_w640_q70.webp",
      "contributions": "1. Proves that the training-inference mismatch in LLM RL has an asymmetric effect, where the bound on log-probability mismatch scales with (1-p), making low-probability \"tail\" tokens the primary source of instability. 2. Proposes a novel method to stabilize RL training by dynamically pruning the vocabulary to exclude the extreme tail tokens, trading large, biased mismatches for a small, bounded optimization bias. 3. Provides both empirical demonstration of stable training and a theoretical bound on the optimization bias introduced by the proposed vocabulary pruning.",
      "summary": "The paper identifies a fundamental training-inference mismatch in LLM reinforcement learning caused by differing numerical precision between high-throughput inference and stable training systems. To address this, the authors propose dynamically pruning low-probability \"tail\" tokens from the vocabulary during RL optimization, which stabilizes training by replacing large, biased errors with a small, bounded bias. Both theoretical analysis and empirical results support the effectiveness of this method.",
      "mindmap": "graph TB\n        A[Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[训练-推理不匹配 / Training-Inference Mismatch]\n        B1 --> B2[尾部token导致梯度不稳定 / Tail tokens destabilize gradient estimation]\n        C --> C1[动态剪枝词汇表 / Dynamic Vocabulary Pruning]\n        C1 --> C2[排除极端尾部token / Exclude extreme tail tokens]\n        D --> D1[实现稳定训练 / Achieves stable training]\n        D --> D2[理论界定优化偏差 / Theoretically bounds optimization bias]"
    },
    {
      "title": "MedSAM-based lung masking for multi-label chest X-ray classification",
      "authors": "Brayden Miao, Zain Rehman, Xin Miao, Siming Liu, Jianjie Wang",
      "institution": "Missouri State University",
      "link": "https://arxiv.org/pdf/2512.23089",
      "code": null,
      "tags": [
        "medical image analysis",
        "MedSAM",
        "lung segmentation",
        "multi-label classification",
        "chest X-ray",
        "spatial prior"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78073436289f27d236dc3f5c9f70b55eb6480c3a7ea02e8c30b8eb1b8e59faa9_w640_q70.webp",
      "contributions": "1. Proposes a segmentation-guided CXR classification pipeline that integrates a fine-tuned MedSAM model for lung region extraction. 2. Empirically demonstrates that the effect of lung masking is task-dependent and architecture-dependent, revealing a trade-off between abnormality classification and normal case screening. 3. Suggests that lung masking should be treated as a controllable spatial prior tailored to the model backbone and clinical objective, rather than a uniform preprocessing step.",
      "summary": "This paper proposes a method that uses a fine-tuned MedSAM model to extract lung masks from chest X-rays to guide multi-label abnormality classification. The study finds that the impact of masking depends on the task and model architecture, with loose masking improving normal case screening while tight masking aids training efficiency. The conclusion is that lung masking should be a tunable spatial prior aligned with the specific clinical goal and model, not a fixed step.",
      "mindmap": "graph TB\n        A[MedSAM-based lung masking for multi-label chest X-ray classification] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Automated CXR interpretation is challenging<br>自动CXR解读具有挑战性]\n        C --> C1[Fine-tune MedSAM for lung segmentation<br>微调MedSAM进行肺部分割]\n        C --> C2[Use masks to guide multi-label classification<br>使用掩码指导多标签分类]\n        D --> D1[Masking effect is task/architecture dependent<br>掩码效果依赖于任务和架构]\n        D --> D2[Trade-off: abnormality vs. normal screening<br>权衡：异常检测与正常筛查]\n        D --> D3[Masking is a controllable spatial prior<br>掩码是一种可控的空间先验]"
    },
    {
      "title": "A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms",
      "authors": "Yingru Li, Ziniu Li, Jiacai Liu",
      "institution": "Not explicitly stated in provided content.",
      "link": "https://arxiv.org/pdf/2512.23097",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "Imitation Learning",
        "Reinforcement Learning",
        "KL divergence",
        "Dense Gradient",
        "Sparse Gradient"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c49fbea41eedc7a9f58604cc114a9246db61882fc20a851c8ec68a24ff6b343_w640_q70.webp",
      "contributions": "1. Derives the exact gradient decomposition of a unified KL+reward objective into analytic Dense and sampled Sparse terms. 2. Provides an efficient logit-level gradient formula for GPU implementation. 3. Establishes mathematical equivalence to KL-regularized RLHF and discusses training curriculum implications.",
      "summary": "This paper proposes a unified framework for fine-tuning LLMs that integrates Imitation Learning and Reinforcement Learning. It analyzes the gradient of a combined objective to decompose it into a token-level Dense Gradient and a long-horizon Sparse Gradient, enabling efficient implementation. The work clarifies its relationship to existing methods like RLHF and discusses practical training considerations.",
      "mindmap": "graph TB\n        A[Hybrid Online RL and IL for LLMs] --> B[核心问题/Problem: Train-inference distribution mismatch in LLM fine-tuning]\n        A --> C[主要方法/Method: Unified framework combining Imitation Learning and Reinforcement Learning]\n        A --> D[关键结果/Results: Gradient decomposes into Dense Gradient (analytic) and Sparse Gradient (sampled)]"
    },
    {
      "title": "Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients",
      "authors": "Armin Berger, Manuela Bergau, Helen Schneider, Saad Ahmad, Tom Anglim Lagones, Gianluca Brugnara, Martha Foltyn-Dumitru, Kai Schlamp, Philipp Vollmuth, Rafet Sifa",
      "institution": "Fraunhofer IAIS, University of Bonn, Lamarr Institute, Department of Health Queensland, Griffith University, University Hospital Bonn",
      "link": "https://arxiv.org/pdf/2512.23090",
      "code": null,
      "tags": [
        "reinforcement learning",
        "reinforcement learning",
        "vision-language model",
        "supervised fine-tuning",
        "generalization paradox",
        "cross-dataset transferability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c87b0b1a571aa28f5dd9685e96b13ff3420ed36b0e1d569fff8b1394d564751f_w640_q70.webp",
      "contributions": "1. Introduced ChexReason, a resource-efficient vision-language model for medical imaging trained with an R1-style (SFT+GRPO) method using minimal data and compute. 2. Identified a fundamental tension where RL optimization (GRPO) improves in-distribution benchmark performance but significantly degrades cross-dataset generalization, a pattern also observed in high-resource models. 3. Discovered a generalization paradox where the SFT checkpoint uniquely improves cross-dataset performance, suggesting teacher-guided reasoning captures more institution-agnostic features than RL optimization.",
      "summary": "The paper investigates applying reinforcement learning (RL) to vision-language models for medical imaging, finding that while RL improves performance on the training benchmark, it harms the model's ability to generalize to new datasets. The authors conclude that for clinical robustness, curated supervised fine-tuning may be more effective than aggressive RL optimization.",
      "mindmap": "graph TB\n        A[Benchmark Success, Clinical Failure<br>基准成功，临床失败] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[RL优化提升基准性能但损害泛化<br>RL improves benchmarks but harms generalization]\n        C --> C1[使用SFT+GRPO训练ChexReason VLM<br>Train ChexReason VLM with SFT+GRPO]\n        D --> D1[GRPO提升CheXpert性能23%<br>GRPO improves CheXpert by 23%]\n        D --> D2[GRPO导致NIH性能下降19%<br>GRPO degrades NIH by 19%]\n        D --> D3[SFT检查点提升跨数据集泛化<br>SFT checkpoint improves cross-dataset generalization]"
    },
    {
      "title": "How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure",
      "authors": "Paul M. Thompson",
      "institution": "Stevens Institute of Neuroimaging and Informatics, University of Southern California",
      "link": "https://arxiv.org/pdf/2512.23109",
      "code": null,
      "tags": [
        "statistical learning theory",
        "uniform convergence",
        "calibration",
        "low-dimensional structure",
        "vision-language models",
        "sample complexity"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dca56fb1537f7d92cc10d66ba9adb9e3272fcc872fe5fdbf475ccf92cc17e24_w640_q70.webp",
      "contributions": "1. Provides finite-sample uniform convergence bounds for accuracy and calibration of VLM-induced classifiers under Lipschitz stability assumptions. 2. Derives sample complexity bounds that depend on the intrinsic/effective dimension of the embedding space, not the ambient dimension. 3. Offers spectrum-dependent bounds that explicitly link eigenvalue decay in embedding covariance to data requirements, explaining reliable generalization with fewer samples.",
      "summary": "This paper studies when generative and vision-language models can achieve uniformly accurate and calibrated predictions with practical sample sizes. By assuming model outputs depend smoothly on a low-dimensional semantic representation, it derives finite-sample uniform convergence bounds for VLM-induced classifiers. The main conclusion is that sample complexity depends on intrinsic dimension and eigenvalue decay, providing a framework to assess data sufficiency for reliable biomedical predictions.",
      "mindmap": "graph TB\n        A[How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现代生成和视觉语言模型在科学/医疗决策中需要准确且校准良好的概率预测 / Modern generative & VLMs need accurate, calibrated predictions for scientific/medical decisions]\n        B --> B2[平均性能良好时，罕见情况或特定子群仍可能出现大误差 / Large errors can persist for rare conditions/subgroups despite low average loss]\n        B --> B3[需要何种结构假设才能实现具有实用样本量的均匀泛化？ / What structural assumptions enable uniform generalization with practical sample sizes?]\n        C --> C1[分析由提示或语义嵌入在受限表示空间中诱导出的分类器族 / Analyze induced families of classifiers from varying prompts/embeddings in a restricted space]\n        C --> C2[假设模型输出对低维语义表示平滑依赖 / Assume model outputs depend smoothly on a low-dimensional semantic representation]\n        C --> C3[应用经典均匀收敛工具 / Apply classical uniform convergence tools]\n        D --> D1[在Lipschitz稳定性下，为VLM诱导分类器的准确性和校准功能提供有限样本均匀收敛界 / Provide finite-sample uniform convergence bounds for accuracy & calibration of VLM-induced classifiers under Lipschitz stability]\n        D --> D2[样本复杂度取决于内在/有效维度，而非环境维度 / Sample complexity depends on intrinsic/effective dimension, not ambient dimension]\n        D --> D3[谱相关边界阐明特征值衰减如何控制数据需求 / Spectrum-dependent bounds show how eigenvalue decay governs data requirements]"
    },
    {
      "title": "It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents",
      "authors": "Karolina Korgul, Yushi Yang, Arkadiusz Drohomirecki, Piotr Błaszczyk, Will Howard, Lukas Aichberger, Chris Russell, Philip H.S. Torr, Adam Mahdi, Adel Bibi",
      "institution": "University of Oxford, SoftServe, Johannes Kepler University Linz",
      "link": "https://arxiv.org/pdf/2512.23128",
      "code": null,
      "tags": [
        "prompt injection",
        "prompt injection",
        "web agents",
        "social-engineering",
        "benchmark",
        "autonomous agents"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c246d5b23e99374a1d754ec870b203d23f214abac92f8d20d849cc98d00e86ca_w640_q70.webp",
      "contributions": "1. Introduces the Task-Redirecting Agent Persuasion Benchmark (TRAP) for evaluating prompt injection vulnerabilities in web-based LLM agents. 2. Provides a modular social-engineering injection framework for controlled experiments on high-fidelity website clones. 3. Demonstrates systemic vulnerabilities, showing agents are susceptible to injection in 25% of tasks on average, with small interface changes often doubling success rates.",
      "summary": "The paper addresses the vulnerability of web-based LLM agents to prompt injection attacks, where hidden adversarial instructions can divert agents from their tasks. It introduces the TRAP benchmark, built on realistic website clones, to evaluate these vulnerabilities. The study finds significant susceptibility across models, revealing systemic, psychologically driven weaknesses in current agents.",
      "mindmap": "graph TB\n        Root[It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Web agents vulnerable to prompt injection attacks] --> Problem_Detail[问题详情/Problem Detail: Adversarial instructions in web content can divert agents from original tasks]\n        Method[主要方法/Method: Introduce TRAP benchmark & modular injection framework] --> Method_Detail[方法详情/Method Detail: Evaluation on high-fidelity website clones using social-engineering techniques]\n        Results[关键结果/Results: Agents susceptible in 25% of tasks on average] --> Results_Detail[结果详情/Results Detail: Small interface changes can double success rates, revealing systemic vulnerabilities]"
    },
    {
      "title": "InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization",
      "authors": "Yu Li, Tian Lan, Zhengling Qi",
      "institution": "George Washington University",
      "link": "https://arxiv.org/pdf/2512.23126",
      "code": null,
      "tags": [
        "reinforcement learning from human feedback (rlhf)",
        "preference optimization",
        "direct preference optimization",
        "self-reflection",
        "invariance",
        "bradley-terry model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4933654befdce9d244a4f36811432e84021ec775f760f24a3cb71dec1951db76_w640_q70.webp",
      "contributions": "1. Identifies two fundamental limitations of DPO: lack of invariance to modeling choices and theoretical suboptimality due to ignoring comparative information in pairwise data. 2. Proposes Intrinsic Self-reflective Preference Optimization (InSPO), a novel family of methods that derives a globally optimal policy conditioned on both context and alternative responses, formalizing self-reflection. 3. Theoretically demonstrates InSPO's superiority over DPO/RLHF and its invariance properties, and practically shows it as a plug-and-play enhancement that improves win rates and length-controlled metrics without inference overhead.",
      "summary": "The paper identifies limitations in Direct Preference Optimization (DPO), such as its sensitivity to modeling choices and failure to use comparative data fully. It proposes InSPO, a method that conditions the policy on both the context and the alternative response to enable intrinsic self-reflection. Experiments show InSPO consistently improves model alignment and robustness as a plug-and-play enhancement to DPO-family algorithms.",
      "mindmap": "graph TB\n        A[InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[DPO Limitations<br/>DPO的局限性]\n        B1 --> B2[Lacks Invariance<br/>缺乏不变性]\n        B1 --> B3[Suboptimal Use of Data<br/>数据利用次优]\n        C --> C1[Propose InSPO<br/>提出InSPO]\n        C1 --> C2[Globally Optimal Policy<br/>全局最优策略]\n        C2 --> C3[Conditions on Context & Alternative<br/>基于上下文与备选答案]\n        D --> D1[Theoretical Superiority<br/>理论优越性]\n        D --> D2[Practical Improvement<br/>实际提升]\n        D2 --> D3[Better Win Rates<br/>更高的胜率]\n        D2 --> D4[No Inference Overhead<br/>无推理开销]"
    },
    {
      "title": "PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion",
      "authors": "Jian Wang, Sixing Rong, Jiarui Xing, Yuling Xu, Weide Liu",
      "institution": "Harvard Medical School, Northeastern University, Yale University, Nanchang University, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.23130",
      "code": null,
      "tags": [
        "medical image synthesis",
        "diffusion model",
        "disentangled representation",
        "pathological residual",
        "anatomical manifold",
        "seam-aware fusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b87a518aaabc4319ec5eba26d8ed0e23b50b1f611b88f2d8241ebecec605cc8c_w640_q70.webp",
      "contributions": "1. Proposes a unified generative framework that reformulates MRI pathology synthesis as a disentangled additive deviation on a stable anatomical manifold. 2. Introduces a Deviation-Space Diffusion Model to learn the conditional distribution of pathological residuals, preserving global structure while modeling local variations. 3. Incorporates a seam-aware fusion strategy and an inference-time stabilization module to suppress boundary artifacts and ensure spatial coherence in synthesized lesions.",
      "summary": "The paper proposes PathoSyn, a novel framework for synthesizing pathological MRI images by decomposing the task into deterministic anatomical reconstruction and stochastic modeling of pathological deviations using a diffusion model. This approach preserves anatomical integrity while generating realistic lesion heterogeneity. Evaluations show it outperforms existing baselines in perceptual realism and anatomical fidelity.",
      "mindmap": "graph TB\n        Root[”PathoSyn: Imaging-Pathology MRI Synthesis<br>PathoSyn: 成像-病理MRI合成”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Feature entanglement in generative models<br>causes corrupted anatomy<br>生成模型中的特征纠缠导致解剖结构损坏”] --> P1[”现有范式/Existing Paradigms<br>Global pixel domain or binary masks<br>全局像素域或二进制掩码”]\n        Method[”主要方法/Method<br>Disentangled Deviation Diffusion<br>解耦偏差扩散”] --> M1[”分解任务/Decompose Task<br>1. Deterministic anatomical reconstruction<br>确定性解剖重建<br>2. Stochastic deviation modeling<br>随机偏差建模”]\n        Method --> M2[”核心模型/Core Model<br>Deviation-Space Diffusion Model<br>偏差空间扩散模型<br>Learns pathological residuals<br>学习病理残差”]\n        Method --> M3[”融合与稳定/Fusion & Stabilization<br>Seam-aware fusion & inference-time<br>stabilization module<br>接缝感知融合与推理时稳定模块”]\n        Results[”关键结果/Results<br>Outperforms baselines<br>超越基线模型”] --> R1[”评估/Evaluation<br>Quantitative & qualitative on tumor benchmarks<br>肿瘤基准上的定量与定性评估”]\n        Results --> R2[”优势/Advantages<br>Higher perceptual realism & anatomical fidelity<br>更高的感知真实性与解剖保真度”]"
    },
    {
      "title": "Reservoir Computing inspired Matrix Multiplication-free Language Model",
      "authors": "Takumi Shiratsuchi, Yuichiro Tanaka, Hakaru Tamukoh",
      "institution": "Kyushu Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.23145",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "MatMul-free LM",
        "reservoir computing",
        "weight sharing",
        "ternary quantization",
        "MLGRU"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b5c1771a82be40c7ba47d813ef33ce372e599bd79d60507444a618ff4e28d2c_w640_q70.webp",
      "contributions": "1. Proposes a novel language model architecture that integrates reservoir computing principles into a MatMul-free LM to reduce training costs. 2. Introduces techniques of partially fixing/sharing weights and inserting reservoir layers to obtain dynamic representations without extra training overhead. 3. Combines operations to reduce memory accesses, achieving reductions in parameters, training time, and inference time while maintaining performance.",
      "summary": "This paper addresses the high computational cost of large language models by proposing a matrix multiplication-free model enhanced with reservoir computing. The method fixes/shared weights in selected layers and inserts reservoir layers to reduce training overhead and memory accesses. Experiments show the approach reduces parameters by up to 19%, training time by 9.9%, and inference time by 8.0% while maintaining comparable performance to the baseline.",
      "mindmap": "graph TB\n        A[Reservoir Computing inspired Matrix Multiplication-free Language Model] --> B[核心问题/Problem: LLMs计算成本高/High computational cost of LLMs]\n        A --> C[主要方法/Method: 结合储层计算与无矩阵乘法模型/Combine RC with MatMul-free LM, 固定共享权重/Fix & share weights, 减少内存访问/Reduce memory access]\n        A --> D[关键结果/Results: 参数减少19%/Params reduced by 19%, 训练时间减少9.9%/Training time reduced by 9.9%, 推理时间减少8.0%/Inference time reduced by 8.0%, 性能相当/Performance maintained]"
    },
    {
      "title": "Why We Need a New Framework for Emotional Intelligence in AI",
      "authors": "Max Parks, Kheli Atluru, Meera Vinod, Mike Kuniavsky, Jud Brewer, Sean White, Sarah Adler, Wendy Ju",
      "institution": "Inflection AI",
      "link": "https://arxiv.org/pdf/2512.23163",
      "code": null,
      "tags": [
        "affective computing",
        "emotional intelligence",
        "benchmark evaluation",
        "affective AI",
        "emotion theory",
        "AI assessment"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afa72b09c5f0d3f095e827a81d95825630a6951253349696736df1091d38dd71_w640_q70.webp",
      "contributions": "1. A critical review of existing emotional intelligence (EI) theories and their applicability to artificial systems. 2. An analysis of current benchmark frameworks for evaluating EI in AI, identifying their foundational shortcomings. 3. A proposal for new evaluation strategies to better measure relevant aspects of EI in AI systems.",
      "summary": "This paper argues that current frameworks for assessing emotional intelligence (EI) in AI are inadequate because they lack a solid theoretical foundation on emotion and fail to distinguish between human-specific and AI-relevant EI components. The authors propose a new framework by first reviewing emotion theories to define EI applicable to AI, then critiquing existing benchmarks, and finally outlining improved evaluation strategies. The main conclusion is that a refined, theoretically-grounded framework is needed to properly evaluate EI capabilities in artificial systems.",
      "mindmap": "graph TB\n        Root[”Why We Need a New Framework for Emotional Intelligence in AI”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”现有EI评估框架不充分/Current EI evaluation frameworks are inadequate”]\n        Problem --> P2[”缺乏坚实的理论基础/Lack a solid theoretical foundation on emotion”]\n        Method --> M1[”回顾情绪与EI理论/Review emotion and EI theories”]\n        Method --> M2[”批判性评估现有基准/Critically evaluate existing benchmarks”]\n        Method --> M3[”提出改进策略/Outline improved evaluation strategies”]\n        Results --> R1[”需要新的评估框架/A new evaluation framework is needed”]\n        Results --> R2[”区分AI相关与无关的EI/Distinguish AI-relevant vs. irrelevant EI aspects”]"
    },
    {
      "title": "SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search",
      "authors": "Yifan Zhang, Giridhar Ganapavarapu, Srideepika Jayaraman, Bhavna Agrawal, Dhaval Patel, Achille Fokoue",
      "institution": "IBM T.J. Watson Research Center, Vanderbilt University",
      "link": "https://arxiv.org/pdf/2512.23167",
      "code": "https://github.com/IBM/SPIRAL",
      "tags": [
        "agent system",
        "LLM planning",
        "Monte Carlo Tree Search (MCTS)",
        "multi-agent architecture",
        "symbolic reasoning",
        "self-correction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c24b184565ce8e4c4a35d80f46a857779e111625dc4ea57e56333bef27bea7e6_w640_q70.webp",
      "contributions": "1. Introduces SPIRAL, a novel framework that embeds a cognitive architecture of three specialized LLM agents (Planner, Simulator, Critic) into an MCTS loop for planning. 2. Transforms MCTS from a brute-force search into a guided, self-correcting reasoning process by leveraging dense, semantic-aware feedback from the agents. 3. Demonstrates superior performance and token efficiency on benchmark datasets (e.g., DailyLifeAPIs) compared to Chain-of-Thought and other state-of-the-art planning agents.",
      "summary": "The paper addresses the problem of LLMs struggling with complex planning tasks due to linear reasoning and lack of self-correction. It proposes SPIRAL, a framework that integrates three specialized LLM agents into a Monte Carlo Tree Search loop to create a guided, reflective, and grounded planning process. The method significantly outperforms existing planning approaches in accuracy and efficiency on benchmark datasets.",
      "mindmap": "graph TB\n        A[SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search] --> B[核心问题/Problem: LLMs falter at complex planning, linear reasoning lacks self-correction]\n        A --> C[主要方法/Method: Integrates three LLM agents (Planner, Simulator, Critic) into MCTS loop]\n        A --> D[关键结果/Results: Outperforms SOTA agents, achieves 83.6% accuracy on DailyLifeAPIs, superior token efficiency]"
    },
    {
      "title": "EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion",
      "authors": "Zhen Liang, Hai Huang, Zhengkui Chen",
      "institution": "Zhejiang Sci-Tech University",
      "link": "https://arxiv.org/pdf/2512.23173",
      "code": "https://github.com/lzzzr123/Equacode",
      "tags": [
        "adversarial attacks",
        "jailbreak attacks",
        "large language models",
        "adversarial prompting",
        "equation solving",
        "code completion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3023ba644e0cdeddfd98604ca7c5871aceb213706aede21b77e0d35b95cf6d23_w640_q70.webp",
      "contributions": "1. Proposes a novel multi-strategy jailbreak approach that combines mathematical equation solving and code completion to bypass LLM safety constraints. 2. Demonstrates high attack success rates (e.g., 91.19% on GPT series) with only a single query, outperforming single-strategy attacks. 3. Shows through ablation studies a strong synergistic effect between the equation and code modules, proving the multi-strategy approach is more effective than the sum of its parts.",
      "summary": "This paper introduces EquaCode, a multi-strategy jailbreak attack that transforms malicious intent into a mathematical problem and forces the LLM to solve it via code, diverting its focus from safety. The method achieves high success rates on various LLMs with a single query, and ablation studies confirm the synergistic benefit of combining equation-solving and code completion strategies.",
      "mindmap": "graph TB\n        A[EquaCode: 多策略越狱方法 / Multi-Strategy Jailbreak Approach] --> B[核心问题: LLM安全性评估不足 / Problem: Insufficient LLM Safety Evaluation]\n        A --> C[主要方法: 方程求解与代码补全 / Method: Equation Solving & Code Completion]\n        A --> D[关键结果: 高成功率与协同效应 / Results: High Success Rate & Synergistic Effect]"
    },
    {
      "title": "From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research",
      "authors": "Hongshen Sun, Juanjuan Zhang",
      "institution": "MIT Sloan School of Management",
      "link": "https://arxiv.org/pdf/2512.23184",
      "code": null,
      "tags": [
        "generative ai evaluation",
        "model belief",
        "token-level probabilities",
        "statistical efficiency",
        "demand estimation",
        "synthetic data"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/997c7eb2e35882ae4411dc7956b1f70a51835fa22d25bcd7ec8b5d6d1cb72413_w640_q70.webp",
      "contributions": "1. Introduces and formalizes the concept of \"model belief,\" a novel measure derived from an LLM's token-level probabilities to capture its belief distribution over choices in a single generation. 2. Proves that model belief is asymptotically equivalent to the mean of model choices but is a more statistically efficient estimator with lower variance and faster convergence, with analogous properties for smooth functions used in downstream applications. 3. Empirically demonstrates that model belief outperforms model choice in explaining and predicting ground-truth choices in practical, limited-run settings (e.g., demand estimation), reducing required computation by roughly a factor of 20.",
      "summary": "This paper addresses the inefficiency of using single LLM outputs (\"model choice\") by proposing \"model belief,\" a measure based on token-level probabilities that captures the model's full belief distribution. The authors prove model belief is a more statistically efficient estimator than model choice and demonstrate its practical superiority in a demand estimation task, where it reduces the computation needed for accurate estimates by about 20 times.",
      "mindmap": "graph TB\n        A[From Model Choice to Model Belief] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLM数据使用效率低 / Inefficient use of LLM-generated data]\n        B --> B2[模型选择信息利用不足 / Underutilizes probabilistic information in model choice]\n        C --> C1[提出模型信念 / Propose model belief]\n        C --> C2[基于Token级概率 / Based on token-level probabilities]\n        C --> C3[捕获信念分布 / Captures belief distribution]\n        D --> D1[统计效率更高 / More statistically efficient estimator]\n        D --> D2[计算需求减少20倍 / Reduces computation by ~20x]\n        D --> D3[预测性能更优 / Better explains/predicts ground truth]"
    },
    {
      "title": "ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis",
      "authors": "Maisha Haque, Israt Jahan Ayshi, Sadaf M. Anis, Nahian Tasnim, Mithila Moontaha, Md. Sabbir Ahmed, Muhammad Iqbal Hossain, Mohammad Zavid Parvez, Subrata Chakraborty, Biswajeet Pradhan, Biswajit Banik",
      "institution": "BRAC University, Charles Sturt University, University of Technology Sydney",
      "link": "https://arxiv.org/pdf/2512.23196",
      "code": null,
      "tags": [
        "semantic segmentation",
        "Object-Based Image Analysis (OBIA)",
        "Deep Learning",
        "Sentinel-2",
        "Forest Cover Mapping",
        "UNet"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/477043abcfb8b4e851144d00c2ae33596765e1b2a27375709fcbcfc01f79e3e5_w640_q70.webp",
      "contributions": "1. Proposes \"ForCM\", a novel method that integrates Object-Based Image Analysis (OBIA) with various Deep Learning models for forest cover mapping. 2. Evaluates and compares the performance of multiple DL models (UNet, UNet++, ResUNet, AttentionUNet, ResNet50-Segnet) combined with OBIA against traditional OBIA. 3. Demonstrates the practical application of free tools like QGIS for accurate environmental mapping, achieving improved accuracy (up to 95.64%) over traditional methods.",
      "summary": "This paper proposes ForCM, a method for forest cover mapping that combines Object-Based Image Analysis with Deep Learning models like ResUNet and AttentionUNet using Sentinel-2 imagery. The results show that this integration significantly improves mapping accuracy compared to traditional OBIA alone, demonstrating the potential of accessible tools for environmental monitoring.",
      "mindmap": "graph TB\n        A[ForCM: Forest Cover Mapping] --> B[核心问题/Problem: Accurate forest cover mapping for environmental monitoring]\n        A --> C[主要方法/Method: Integrate OBIA with DL models (e.g., UNet, ResUNet) on Sentinel-2 imagery]\n        A --> D[关键结果/Results: Improved accuracy (95.64% with AttentionUNet-OBIA vs 92.91% traditional OBIA)]"
    },
    {
      "title": "Not too long do read: Evaluating LLM-generated extreme scientific summaries",
      "authors": "Zhuoqi Lyu, Qing Ke",
      "institution": "City University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.23206",
      "code": "https://github.com/netknowledge/LLM_summarization",
      "tags": [
        "text summarization",
        "extreme summarization",
        "TLDR",
        "abstractive summarization",
        "extractive summarization",
        "dataset creation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e22384bc3a440a2b34601b9d8ed0a9de58fdee4ff92f1d83db278778c4293a7_w640_q70.webp",
      "contributions": "1. Introduces BiomedTLDR, a novel high-quality dataset of researcher-authored scientific TLDRs, curated from author annotations in bibliographies. 2. Evaluates the performance of popular open-weight LLMs in generating scientific TLDRs from paper abstracts. 3. Provides an analysis revealing that LLM-generated summaries tend to be more extractive (closer to the source text's lexicon and structure) compared to more abstractive human-written summaries.",
      "summary": "This paper addresses the lack of high-quality datasets for evaluating LLMs in generating scientific extreme summaries (TLDRs) by introducing BiomedTLDR, a dataset of human-authored summaries. It then evaluates open-weight LLMs on this task and finds that, while some can produce human-like summaries, LLMs generally tend to be more extractive and less abstractive than human experts.",
      "mindmap": "graph TB\n        A[”Not too long do read: Evaluating LLM-generated extreme scientific summaries<br>论文标题”]\n        A --> B[”核心问题/Problem<br>Lack of high-quality scientific TLDR dataset hinders LLM evaluation”]\n        A --> C[”主要方法/Method<br>Propose BiomedTLDR dataset & test LLMs on TLDR generation”]\n        A --> D[”关键结果/Results<br>LLMs are more extractive; humans are more abstractive”]"
    },
    {
      "title": "Exploring Syn-to-Real Domain Adaptation for Military Target Detection",
      "authors": "Jongoh Jeong, Youngjin Oh, Gyeongrae Nam, Jeongeun Lee, Kuk-Jin Yoon",
      "institution": "Korea Advanced Institute of Science and Technology (KAIST), LIG Nex1",
      "link": "https://arxiv.org/pdf/2512.23208",
      "code": null,
      "tags": [
        "object detection",
        "domain adaptation",
        "synthetic-to-real",
        "Unreal Engine",
        "military target detection"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b67711a2133943d8083642ed36e63614aae288f161e8fc258230c5d03bacb3_w640_q70.webp",
      "contributions": "1. Proposed generating a synthetic RGB dataset for military target detection using Unreal Engine to address the lack of real-world data. 2. Conducted and benchmarked synthetic-to-real domain adaptation experiments on a new train-val dataset pair for military targets. 3. Found that domain adaptation methods using minimal supervision (e.g., object class hints) substantially outperform unsupervised or semi-supervised methods in this challenging cross-domain setting.",
      "summary": "This paper addresses the challenge of military target detection by generating synthetic RGB data using Unreal Engine to overcome the lack of real datasets and high costs of SAR data. It benchmarks state-of-the-art domain adaptation methods on this synthetic-to-real task and finds that methods using minimal supervision achieve the best performance, highlighting remaining challenges in this area.",
      "mindmap": "graph TB\n        A[Exploring Syn-to-Real Domain Adaptation for Military Target Detection] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[缺乏军事目标数据集/Lack of military target datasets]\n        B --> B2[SAR数据成本高/High cost of SAR data]\n        B --> B3[跨域适应挑战/Cross-domain adaptation challenge]\n        C --> C1[使用Unreal Engine生成合成RGB数据/Generate synthetic RGB data using Unreal Engine]\n        C --> C2[合成到真实域适应实验/Synthetic-to-real domain adaptation experiments]\n        C --> C3[基准测试SOTA方法/Benchmark SOTA DA methods]\n        D --> D1[最小监督方法表现最佳/Minimal supervision methods perform best]\n        D --> D2[识别当前挑战/Identify current challenges]"
    },
    {
      "title": "Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process",
      "authors": "Zhijun Chen, Zeyu Ji, Qianren Mao, Junhang Cheng, Bangjie Qin, Hao Wu, Zhuoran Li, Jingzheng Li, Kai Sun, Zizhe Wang, Yikun Ban, Zhu Sun, Xiangyang Ji, Hailong Sun",
      "institution": "Beihang University, Zhongguancun Laboratory, Xi'an Jiaotong University, Hong Kong University of Science and Technology, Tsinghua University, Singapore University of Technology and Design",
      "link": "https://arxiv.org/pdf/2512.23213",
      "code": null,
      "tags": [
        "llm inference",
        "LLM Ensemble",
        "LLM-as-a-Judge",
        "Peer-Review",
        "Unsupervised Selection",
        "Truth Inference"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/366f9e4fb3bf94aabbe40f3849a7637d6656821ec3dde88cd37d06effd3ed3f5_w640_q70.webp",
      "contributions": "1. Proposes LLM-PeerReview, a novel, peer-review-inspired, and interpretable framework for unsupervised LLM ensemble selection. 2. Introduces a three-stage process (scoring via LLM-as-a-Judge, reasoning via aggregation, and selection) that leverages multiple LLMs to evaluate each other's responses. 3. Demonstrates strong empirical performance, with two variants significantly outperforming a recent advanced baseline (Smoothie-Global) on multiple datasets.",
      "summary": "This paper proposes LLM-PeerReview, an unsupervised ensemble method that selects the best response from multiple LLM candidates. The method uses a peer-review process where LLMs score each other's outputs, then aggregates these scores to make a final selection. The approach is shown to be simple and powerful, outperforming a strong baseline by a significant margin across several datasets.",
      "mindmap": "graph TB\n        A[LLM-PeerReview: Ensembling LLMs via Peer-Review] --> B[核心问题/Problem: Single LLM limitations & diverse model strengths]\n        A --> C[主要方法/Method: Unsupervised 3-stage peer-review framework]\n        C --> C1[评分/Scoring: LLM-as-a-Judge]\n        C --> C2[推理/Reasoning: Score aggregation (graphical model or averaging)]\n        C --> C3[选择/Selection: Pick highest-scoring response]\n        A --> D[关键结果/Results: Outperforms Smoothie-Global by ~7% points]"
    },
    {
      "title": "TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI",
      "authors": "Jingming Li",
      "institution": "School of Civil Engineering and Architecture, Nanyang Normal University",
      "link": "https://arxiv.org/pdf/2512.23217",
      "code": null,
      "tags": [
        "ai evaluation",
        "thermal comfort",
        "cognitive turing test",
        "cross-modal reasoning",
        "causal association",
        "adaptive decision-making"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0080eb693040bd7b2c752a63de09a7937c8abe23c0a11752ee6d9fe7cdd04c7f_w640_q70.webp",
      "contributions": "1. Proposes TCEval, the first evaluation framework that uses thermal comfort scenarios to assess AI's core cognitive capacities (cross-modal reasoning, causal association, adaptive decision-making). 2. Introduces a methodology using LLM agents with virtual personalities to generate and validate clothing and comfort feedback against real human databases (ASHRAE, Chinese Thermal Comfort Database). 3. Demonstrates the framework's ecological validity as a Cognitive Turing Test, revealing that current LLMs have foundational cross-modal reasoning but lack precise causal understanding of nonlinear relationships in thermal comfort.",
      "summary": "This paper proposes TCEval, a novel evaluation framework that uses thermal comfort scenarios and LLM agents to assess AI's cognitive abilities. The method involves simulating agent decisions and comparing them to human data from established comfort databases. The results show that while LLMs exhibit basic cross-modal reasoning, they lack a precise causal understanding of the complex factors in thermal comfort, validating TCEval as an ecologically valid cognitive test.",
      "mindmap": "graph TB\n        A[TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(”LLM任务特定基准存在关键差距<br/>Critical gap in LLM task-specific benchmarks”)\n        C --> C1(”利用热舒适场景和LLM智能体<br/>Leverage thermal comfort scenarios & LLM agents”)\n        C --> C2(”评估三种核心认知能力<br/>Assess three core cognitive capacities”)\n        C2 --> C2a(”跨模态推理<br/>Cross-modal reasoning”)\n        C2 --> C2b(”因果关联<br/>Causal association”)\n        C2 --> C2c(”自适应决策<br/>Adaptive decision-making”)\n        D --> D1(”智能体反馈与人类有限对齐<br/>Agent feedback has limited exact alignment with humans”)\n        D --> D2(”具备基础跨模态推理能力<br/>Possess foundational cross-modal reasoning ability”)\n        D --> D3(”缺乏对非线性关系的精确因果理解<br/>Lack precise causal understanding of nonlinear relationships”)"
    },
    {
      "title": "Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information",
      "authors": "Youngchae Kwon, Jinyoung Choi, Injung Kim",
      "institution": "Handong Global University",
      "link": "https://arxiv.org/pdf/2512.23221",
      "code": null,
      "tags": [
        "object detection",
        "Detection Transformer (DETR)",
        "Contextual Information",
        "Holistic Detection",
        "Fashion Item Detection",
        "Co-occurrence Relationship"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dda1eab87d833571a5feac46be0fc3ba879ccabde53c04f72e0d4fcae137cb6e_w640_q70.webp",
      "contributions": "1. Proposes Holi-DETR, a novel holistic detection framework for fashion items that leverages contextual information to reduce detection ambiguities., 2. Introduces a novel architecture that integrates three distinct types of contextual information (co-occurrence, inter-item spatial arrangements, and item-body keypoint relationships) into DETR-based models., 3. Demonstrates performance improvements over baseline models (vanilla DETR and Co-DETR) in terms of average precision (AP).",
      "summary": "This paper addresses the challenge of fashion item detection, which is difficult due to diverse appearances and similar subcategories. The authors propose Holi-DETR, a holistic Detection Transformer that leverages three types of contextual information—co-occurrence, spatial arrangements, and body keypoints—to improve detection accuracy. The method shows improved performance over baseline DETR models.",
      "mindmap": "graph TB\n        A[Holi-DETR: Holistic Fashion Item Detection<br>Holi-DETR: 整体时尚物品检测] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Fashion item detection is challenging due to diverse appearances and similarities among subcategories.<br>时尚物品检测因外观多样和子类别相似而具有挑战性。]\n        C[主要方法/Method<br>Proposes Holi-DETR, a holistic detector leveraging three contextual cues: co-occurrence, spatial arrangements, and body keypoints.<br>提出Holi-DETR，利用共现、空间布局和身体关键点三种上下文线索的整体检测器。]\n        D[关键结果/Results<br>Improved performance over vanilla DETR (+3.6pp AP) and Co-DETR (+1.1pp AP).<br>性能超越原始DETR (+3.6pp AP) 和 Co-DETR (+1.1pp AP)。]"
    },
    {
      "title": "Anomaly Detection by Effectively Leveraging Synthetic Images",
      "authors": "Sungho Kang, Hyunkyu Park, Yeonho Lee, Hanbyul Lee, Mijoo Jeong, YeongHyeon Park, Injae Lee, Juneho Yi",
      "institution": "Sungkyunkwan University, The University of Texas MD Anderson Cancer Center",
      "link": "https://arxiv.org/pdf/2512.23227",
      "code": null,
      "tags": [
        "anomaly detection",
        "synthetic data",
        "image-to-image translation",
        "image retrieval",
        "two-stage training",
        "MVTec AD"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3546b17641c719167618772aa6e4da085e82a3b6d85cd2abc9940897d3e1f92_w640_q70.webp",
      "contributions": "1. A novel framework that efficiently generates synthetic defect images by leveraging a pre-trained text-guided image-to-image translation model and an image retrieval model for filtering. 2. A two-stage training strategy that pre-trains on a large volume of rule-based synthetic images and then fine-tunes on a smaller set of high-quality generated images. 3. Demonstration of the approach's effectiveness in reducing data collection costs while improving anomaly detection performance on the MVTec AD benchmark dataset.",
      "summary": "This paper addresses the trade-off in synthetic data generation for anomaly detection by proposing a framework that uses a pre-trained image-to-image translation model and an image retrieval filter to efficiently create realistic defect images. It also introduces a two-stage training strategy to leverage both cheap, low-quality and expensive, high-quality synthetic data effectively. Experiments on MVTec AD show this method reduces costs and improves detection performance.",
      "mindmap": "graph TB\n        A[Anomaly Detection by Effectively Leveraging Synthetic Images] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 真实缺陷图像稀缺，现有合成方法在成本与质量间难以权衡/Scarcity of real defect images, trade-off between cost and quality in synthesis]\n        C[主要方法/Method: 1. 使用预训练图像翻译与检索模型高效生成缺陷图/Use pre-trained image-to-image & retrieval models for generation. 2. 两阶段训练策略：先预训练再微调/Two-stage training: pre-train then fine-tune]\n        D[关键结果/Results: 在MVTec AD数据集上验证有效，降低成本并提升性能/Validated on MVTec AD, reduces cost and improves performance]"
    },
    {
      "title": "Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network",
      "authors": "Dongsheng Li, Chaobo Chen, Siling Wang, Song Gao",
      "institution": "(Inferred from author names and arXiv handle; specific institution not provided in the given text. Could be a Chinese research institution or university.)",
      "link": "https://arxiv.org/pdf/2512.23234",
      "code": null,
      "tags": [
        "object detection",
        "physics-inspired modeling",
        "edge detection",
        "content-adaptive routing",
        "multi-scale feature fusion",
        "infrared gas leak detection"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee948d9a0589e5467502d1d916e59d51137b1253413c1001ade729584fd4bf55_w640_q70.webp",
      "contributions": "1. Proposed a physics-inspired Gas Block module that models gas transport using a diffusion-convection unit with local and large-kernel branches, fused via an edge-gated module to enhance weak plume features. 2. Introduced a novel Adaptive Gradient and Phase Edge Operator (AGPEO) and a Multi-Scale Edge Perception Module (MSEPM) to compute and integrate reliable hierarchical edge priors for boundary reinforcement. 3. Designed a Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN) that uses adaptive modulation to selectively propagate informative features across scales based on content and edge cues, improving efficiency and discriminability.",
      "summary": "The paper proposes PEG-DRNet, a physics-inspired and edge-guided network for detecting faint infrared gas leaks. The method combines a gas transport model, a novel edge detection operator, and a content-adaptive routing mechanism for multi-scale feature fusion. Experiments show PEG-DRNet achieves superior accuracy and computational efficiency on benchmark datasets compared to existing detectors.",
      "mindmap": "graph TB\n        A[Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network] --> B(核心问题/Problem: 红外气体泄漏检测困难/Infrared gas leak detection is difficult due to faint, small, semitransparent plumes with weak boundaries.)\n        A --> C(主要方法/Method: 提出PEG-DRNet/Propose PEG-DRNet)\n        C --> C1(气体块建模气体传输/Gas Block models gas transport)\n        C --> C2(自适应梯度相位边缘算子/Adaptive Gradient and Phase Edge Operator (AGPEO))\n        C --> C3(内容自适应稀疏路由聚合网络/Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN))\n        A --> D(关键结果/Results: 在IIG和LangGas数据集上性能优越/Superior performance on IIG and LangGas datasets, achieving higher AP and AP50 with good efficiency.)"
    },
    {
      "title": "KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta",
      "authors": "Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang, Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner, Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Abdul Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu",
      "institution": "Meta Platforms",
      "link": "https://arxiv.org/pdf/2512.23236",
      "code": null,
      "tags": [
        "gpu kernels",
        "agentic kernel coding",
        "heterogeneous accelerators",
        "retrieval-augmented prompt synthesis",
        "graph-based search",
        "Triton/CuTe DSL"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp",
      "contributions": "1. Proposes KernelEvolve, an agentic framework that automates kernel generation and optimization for DLRMs across heterogeneous hardware (NVIDIA/AMD GPUs, Meta accelerators) by operating at multiple programming abstractions. 2. Introduces a kernel optimization process modeled as a graph-based search with dynamic adaptation to runtime context via retrieval-augmented prompt synthesis and a persistent hardware knowledge base. 3. Demonstrates the system's effectiveness by achieving 100% correctness on benchmark suites and substantial performance speedups (up to 17x) in production, reducing development time from weeks to hours and lowering the programmability barrier for new AI hardware.",
      "summary": "This paper presents KernelEvolve, an agentic framework that automates the generation and optimization of compute kernels for deep learning recommendation models to address challenges posed by model, kernel, and hardware heterogeneity. The method uses a graph-based search process enhanced with retrieval-augmented prompts and operates across multiple programming abstractions. The system was validated on production models and benchmarks, showing significant performance improvements and reduced development time, effectively mitigating the programmability barrier for new AI accelerators.",
      "mindmap": "graph TB\n        A[KernelEvolve: Scaling Agentic Kernel Coding] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[DLRM训练/推理效率<br/>DLRM Training/Inference Efficiency]\n        B --> B2[模型、内核、硬件异构性<br/>Model, Kernel, Hardware Heterogeneity]\n        C --> C1[智能内核编码框架<br/>Agentic Kernel Coding Framework]\n        C --> C2[多抽象层: Triton, CuTe DSL<br/>Multi-Abstraction: Triton, CuTe DSL]\n        C --> C3[图搜索与检索增强提示<br/>Graph Search & Retrieval-Augmented Prompt]\n        D --> D1[100%正确率, 17倍加速<br/>100% Correctness, 17x Speedup]\n        D --> D2[开发时间: 数周->数小时<br/>Dev Time: Weeks->Hours]\n        D --> D3[降低新硬件编程壁垒<br/>Reduces New Hardware Programmability Barrier]"
    },
    {
      "title": "ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing",
      "authors": "Xingwei Ma, Shiyang Feng, Bo Zhang, Bin Wang",
      "institution": "Fudan University, Shanghai Artificial Intelligence Laboratory",
      "link": "https://arxiv.org/pdf/2512.23244",
      "code": null,
      "tags": [
        "change detection",
        "vision-language model",
        "remote sensing",
        "semantic change detection",
        "supervised fine-tuning",
        "reinforcement learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5871856f6e1854dd58df304f783ce8ea57314887e0296e446d48afb30805307_w640_q70.webp",
      "contributions": "1. Proposes ViLaCD-R1, a novel two-stage vision-language framework for semantic change detection in remote sensing, comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). 2. Introduces a training strategy for the VLM using supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks to generate a coarse change mask. 3. Demonstrates that the framework significantly improves semantic change recognition and localization while suppressing non-semantic variations, achieving state-of-the-art performance on multiple benchmarks.",
      "summary": "This paper addresses the limitations of existing remote sensing change detection methods, such as poor semantic understanding and inaccurate localization, by proposing ViLaCD-R1. This two-stage vision-language framework first uses a fine-tuned VLM to generate a coarse change mask from dual-temporal images, then refines it with a decoder to produce a precise change map. The method shows superior performance in recognizing true semantic changes and suppressing irrelevant variations across several benchmarks.",
      "mindmap": "graph TB\n        A[ViLaCD-R1: 遥感语义变化检测的视觉语言框架] --> B1(核心问题/Problem)\n        A --> B2(主要方法/Method)\n        A --> B3(关键结果/Results)\n        B1 --> C1[传统方法语义理解不足/Traditional methods lack semantic understanding]\n        B1 --> C2[现有VLM方法定位不准确/Existing VLM methods have inaccurate localization]\n        B2 --> D1[两阶段框架/Two-stage framework]\n        D1 --> E1[多图像推理器/Multi-Image Reasoner]\n        E1 --> F1[SFT与RL训练/SFT and RL training]\n        E1 --> F2[生成粗变化掩码/Generate coarse change mask]\n        D1 --> E2[掩码引导解码器/Mask-Guided Decoder]\n        E2 --> F3[融合特征与掩码/Fuse features and mask]\n        E2 --> F4[预测精细变化图/Predict precise change map]\n        B3 --> G1[提升语义变化识别/Improves semantic change recognition]\n        B3 --> G2[抑制非语义变化/Suppresses non-semantic variations]\n        B3 --> G3[达到SOTA性能/Achieves SOTA performance]"
    },
    {
      "title": "Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation",
      "authors": "Dianyun Wang, Qingsen Ma, Yuhu Shang, Zhifeng Lu, Lechen Ning, Zhenbo Xu, Huijia Wu, Zhaofeng He",
      "institution": "Beijing University of Posts and Telecommunications",
      "link": "https://arxiv.org/pdf/2512.23260",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "Sparse Autoencoders (SAEs)",
        "Low-Rank Adaptation (LoRA)",
        "Safety Alignment",
        "Interpretability",
        "Parameter-efficient Fine-tuning (PEFT)"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e615e6d561cef5b79dc991ed964fd9b6fb069427af26a4b7b42cd33cea4315a_w640_q70.webp",
      "contributions": "1. Proposes a novel method that uses pre-trained Sparse Autoencoders (SAEs) to construct an explicit, interpretable low-rank subspace for adapter initialization, addressing the black-box nature of traditional LoRA. 2. Provides theoretical analysis proving that SAE-based subspace identification achieves arbitrarily small recovery error under monosemanticity, while direct identification suffers an irreducible error floor due to polysemanticity. 3. Demonstrates state-of-the-art performance on safety alignment, achieving up to 99.6% safety rate while updating only 0.19-0.24% of parameters, and provides interpretable insights into the learned alignment subspace.",
      "summary": "This paper addresses the lack of interpretability in standard Low-Rank Adaptation (LoRA) methods for fine-tuning large language models. The proposed method leverages Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled space and uses them to construct an explicit, interpretable low-rank subspace for adapter initialization. The approach achieves superior safety alignment performance and provides transparency into the learned adaptation process.",
      "mindmap": "graph TB\n        Root(”Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”LoRA缺乏可解释性/LoRA lacks interpretability”)\n        Problem --> P2(”子空间学习是黑盒的/Subspace learning is black-box”)\n        Method --> M1(”利用预训练SAE/Use pre-trained SAEs”)\n        Method --> M2(”构建显式低秩子空间/Construct explicit low-rank subspace”)\n        Results --> R1(”高安全率99.6%/High safety rate 99.6%”)\n        Results --> R2(”参数高效0.19%/Parameter-efficient 0.19%”)\n        Results --> R3(”提供可解释性/Provides interpretability”)"
    },
    {
      "title": "Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control",
      "authors": "Yoonpyo Lee, Kazuma Kobayashi, Sai Puppala, Sajedul Talukder, Seid Koric, Souvik Chakraborty, Syed Bahauddin Alam",
      "institution": "Hanyang University, University of Illinois Urbana-Champaign, Southern Illinois University, University of Texas at El Paso, National Center for Supercomputing Applications, Indian Institute of Technology Delhi",
      "link": "https://arxiv.org/pdf/2512.23292",
      "code": null,
      "tags": [
        "reinforcement learning",
        "domain-specific foundation model",
        "agentic physical ai",
        "variance collapse",
        "physics-based validation",
        "policy distillation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb921102dfde629395ab8293510cb369a00c1199cadd4c88269dc076f8774a1a_w640_q70.webp",
      "contributions": "1. Proposes a new paradigm of Agentic Physical AI, where policy optimization is driven by physics-based outcome validation instead of perceptual inference, addressing the structural limitation of general-purpose models in control tasks. 2. Demonstrates that scaling data for a compact (360M parameter) model induces a sharp phase transition and variance collapse (&gt;500x reduction), leading to stable, execution-level behavior for safety-critical control. 3. Shows the model autonomously distills a robust policy (concentrating on a single strategy) and its learned representations transfer across different physics and input modalities without architectural changes, exhibiting early foundation-model properties.",
      "summary": "The paper identifies a fundamental limitation of general-purpose AI models in safety-critical physical control tasks, where they prioritize semantic plausibility over physical correctness. To address this, it introduces Agentic Physical AI, a paradigm using compact language models trained with physics-based validation on synthetic nuclear reactor control data. The key finding is that sufficient data scaling induces a sharp variance collapse, stabilizing the model's behavior and enabling it to autonomously distill a reliable control policy that generalizes across tasks.",
      "mindmap": "graph TB\n        Root(”Agentic Physical AI for Nuclear Reactor Control”) --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题/Problem<br>General-purpose models fail at physical control<br>通用模型在物理控制中失败”]\n        Method[”主要方法/Method<br>Agentic Physical AI with physics-based validation<br>基于物理验证的智能体物理AI”]\n        Results[”关键结果/Results<br>Variance collapse & emergent policy distillation<br>方差崩溃与策略蒸馏涌现”]\n    \n        Problem --> P1[”Input unfaithfulness / 输入不忠实”]\n        Problem --> P2[”Semantic vs. physical correctness / 语义与物理正确性冲突”]\n    \n        Method --> M1[”Compact LM (360M params) / 紧凑语言模型”]\n        Method --> M2[”Physics-driven optimization / 物理驱动优化”]\n        Method --> M3[”Synthetic data scaling (10^3 to 10^5) / 合成数据缩放”]\n    \n        Results --> R1[”Phase transition & >500x variance collapse / 相变与方差崩溃”]\n        Results --> R2[”Autonomous policy distillation / 自主策略蒸馏”]\n        Results --> R3[”Transferable representations / 可迁移表征”]"
    },
    {
      "title": "MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images",
      "authors": "Md. Sazzadul Islam Prottasha, Nabil Walid Rafi",
      "institution": "Bangladesh University of Professionals",
      "link": "https://arxiv.org/pdf/2512.23304",
      "code": null,
      "tags": [
        "medical image classification",
        "MedGemma",
        "GPT-4",
        "LoRA",
        "zero-shot classification",
        "multimodal LLM"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58abac32272c08efbbe249ec33f3b4f4aa3a6f4d477b241718ddbde679cce96a_w640_q70.webp",
      "contributions": "1. Conducted a critical comparison between the open-source MedGemma and proprietary GPT-4 for zero-shot medical disease classification from images. 2. Demonstrated that the LoRA-fine-tuned MedGemma model significantly outperformed the untuned GPT-4 in accuracy and sensitivity for high-stakes clinical tasks. 3. Highlighted the essential role of domain-specific fine-tuning in minimizing hallucinations and enabling complex, evidence-based medical reasoning for clinical implementation.",
      "summary": "This study compares the performance of the open-source MedGemma model and the proprietary GPT-4 for zero-shot classification of six diseases from medical images. The MedGemma model, fine-tuned with LoRA, achieved higher mean accuracy and sensitivity than GPT-4. The results show that domain-specific fine-tuning is crucial for reliable clinical applications, positioning MedGemma as a sophisticated tool for medical diagnostics.",
      "mindmap": "graph TB\n        A[MedGemma vs GPT-4: 医学图像零样本疾病分类] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 比较开源与闭源多模态LLM在医学图像诊断中的性能]\n        C[主要方法/Method: 使用LoRA微调的MedGemma与未调优的GPT-4进行零样本分类对比]\n        D[关键结果/Results: MedGemma准确率(80.37%)和敏感性更高，领域微调对减少幻觉至关重要]"
    },
    {
      "title": "Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL",
      "authors": "Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer",
      "institution": "University of Innsbruck, Sharif University of Technology",
      "link": "https://arxiv.org/pdf/2512.23310",
      "code": null,
      "tags": [
        "llm inference",
        "Lyapunov Optimization",
        "Deep Reinforcement Learning",
        "Edge-Cloud Partitioning",
        "Transformer Decomposition",
        "Queue Stability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/121b71aa214f4a7c1671c9df76bf67a9cd64f3cb9e74186606a380ce86f7634f_w640_q70.webp",
      "contributions": "1. Proposes a fine-grained, adaptive partitioning framework (Splitwise) that decomposes transformer layers into attention heads and feed-forward sub-blocks, enabling exponentially more partition choices than layer-wise schemes. 2. Introduces a hierarchical DRL policy guided by Lyapunov optimization to jointly optimize latency, energy, and accuracy while guaranteeing queue stability under stochastic workloads and variable bandwidth. 3. Ensures robustness through partition checkpoints with exponential backoff recovery for communication failures, validated on real edge devices with large models.",
      "summary": "The paper proposes Splitwise, a Lyapunov-assisted DRL framework for dynamically partitioning LLM inference between edge and cloud at a fine-grained sub-layer level. It aims to minimize latency and energy while maintaining accuracy under fluctuating network conditions. Experiments show Splitwise significantly reduces latency and energy consumption compared to existing methods.",
      "mindmap": "graph TB\n        A[Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL] --> B[核心问题/Problem: LLMs are hard to deploy on edge devices; cloud-only is slow; static partitions fail with bandwidth changes.]\n        A --> C[主要方法/Method: Fine-grained partition of transformer layers; Lyapunov-assisted DRL for adaptive optimization; checkpointing for robustness.]\n        A --> D[关键结果/Results: Reduces latency 1.4x-2.8x; cuts energy up to 41%; lowers 95th-percentile latency by 53-61%.]"
    },
    {
      "title": "Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants",
      "authors": "Sheng-Kai Chen, Yi-Ling Tsai, Chun-Chih Chang, Yan-Chen Chen, Po-Chiang Lin",
      "institution": "Yuan Ze University",
      "link": "https://arxiv.org/pdf/2512.23312",
      "code": null,
      "tags": [
        "explainable ai (xai)",
        "inverse kinematics",
        "shapley additive explanations (SHAP)",
        "InterpretML",
        "obstacle avoidance",
        "neural network"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ab3055f2df7d03d7972b536c04fab2618a41cdd21ee682cf0f1ab9c0c6610f6_w640_q70.webp",
      "contributions": "1. Proposes an explainability-centered workflow integrating SHapley Additive exPlanations (SHAP) with physics-based obstacle avoidance evaluation for neural inverse kinematics. 2. Introduces and trains two lightweight variants of IKNet (Improved IKNet with residual connections and Focused IKNet with position-orientation decoupling) on a synthetic dataset. 3. Demonstrates through simulation that neural IK architectures with more balanced feature importance attribution tend to maintain wider safety margins without sacrificing accuracy, linking XAI insights to robotic safety.",
      "summary": "This study addresses the lack of transparency in neural network-based inverse kinematics (IK) solvers by proposing an explainable AI workflow. It integrates SHAP analysis with physics-based simulation to evaluate two new IKNet variants on obstacle avoidance tasks. The key finding is that architectures with more evenly distributed feature importance achieve better safety performance, showing how XAI can guide the development of trustworthy robotic manipulation systems.",
      "mindmap": "graph TB\n        A[”Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation<br>可解释神经逆运动学用于障碍物感知机器人操作”] --> B\n        A --> C\n        A --> D\n        B[”核心问题/Problem<br>Opaque neural IK models lack transparency and safety for responsible AI.<br>黑盒神经IK模型缺乏透明度与安全性”] --> B1[”挑战/Challenges<br>Debugging failures, safety certification”]\n        C[”主要方法/Method<br>XAI workflow integrating SHAP and physics simulation.<br>集成SHAP与物理仿真的XAI工作流”] --> C1[”模型/Variants<br>Improved IKNet, Focused IKNet”]\n        C --> C2[”工具/Tools<br>SHAP, InterpretML, Simulator”]\n        D[”关键结果/Results<br>Balanced feature attribution correlates with wider safety margins.<br>均衡的特征归因与更宽的安全裕度相关”] --> D1[”结论/Conclusion<br>XAI guides architectural refinement for trustworthy IK.<br>XAI指导可信IK的架构改进”]"
    },
    {
      "title": "On Conformant Planning and Model-Checking of $^*^*$ Hyperproperties",
      "authors": "Raven Beutner, Bernd Finkbeiner",
      "institution": "CISPA Helmholtz Center for Information Security",
      "link": "https://arxiv.org/pdf/2512.23324",
      "code": null,
      "tags": [
        "formal methods",
        "conformant planning",
        "hyperproperties",
        "model-checking",
        "HyperLTL",
        "∃∗∀∗"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47343ca7bc4bf16389257261dd21e0c1fa42c0f512178576ff4ba501df841e8b_w640_q70.webp",
      "contributions": "1. Establishes a formal connection between conformant planning and model-checking of ∃∗∀∗ hyperproperties, showing they share the same computational core. 2. Provides an efficient, sound, and complete reduction from a hyperproperty model-checking instance to a conformant planning instance. 3. Demonstrates that every conformant planning problem is itself a hyperproperty model-checking task, establishing the converse direction.",
      "summary": "This paper identifies and formalizes a deep connection between two seemingly unrelated problems: conformant planning (finding a robust sequential plan under uncertainty) and model-checking of ∃∗∀∗ hyperproperties (verifying system properties that relate multiple execution traces). The authors provide efficient, sound, and complete translations between instances of these two problems, showing they are essentially two sides of the same computational coin. This foundational link aims to enable cross-pollination of solution techniques between the planning and verification communities.",
      "mindmap": "graph TB\n        A[On Conformant Planning and Model-Checking of ∃∗∀∗ Hyperproperties] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[连接两个看似无关的问题 / Linking two seemingly unrelated problems]\n        B1 --> B2[Conformant Planning / 一致性规划]\n        B1 --> B3[Hyperproperty Model-Checking / 超属性模型检测]\n        C --> C1[构建双向高效规约 / Constructing bidirectional efficient reductions]\n        D --> D1[证明规约的可靠性与完备性 / Proving reductions are sound and complete]\n        D --> D2[确立问题的等价性 / Establishing the equivalence of the problems]"
    },
    {
      "title": "CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations",
      "authors": "Huan-ang Gao, Zikang Zhang, Tianwei Luo, Kaisen Yang, Xinzhe Juan, Jiahao Qiu, Tianxing Chen, Bingxiang He, Hao Zhao, Hao Zhou, Shilong Liu, Mengdi Wang",
      "institution": "Tsinghua University, Princeton University, Shanghai Jiao Tong University & University of Michigan, The University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.23328",
      "code": null,
      "tags": [
        "agent evaluation",
        "spatial reasoning",
        "long-horizon planning",
        "partial observability",
        "mental simulation",
        "diagnostic benchmark"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/210465a4bf9048c43ec900e17f922e63394d83664c6fe631fec0d54577fd9fb6_w640_q70.webp",
      "contributions": "1. Identifies three core cognitive challenges (spatial reasoning, long-horizon state tracking, active exploration under partial observation) hindering LLM agents in the physical world. 2. Introduces CubeBench, a novel generative benchmark based on the Rubik's Cube with a three-tiered diagnostic framework to isolate and evaluate these capabilities. 3. Provides a diagnostic framework using external solver tools to analyze failure modes and reveals critical limitations of leading LLMs, including a 0.00% pass rate on long-horizon tasks.",
      "summary": "The paper introduces CubeBench, a diagnostic benchmark using a Rubik's Cube to evaluate LLM agents' spatial reasoning and long-horizon planning under partial observation. It employs a three-tiered framework from full symbolic to partial visual states. Experiments show leading LLMs fail completely on long-horizon tasks, highlighting a fundamental gap for physical-world deployment.",
      "mindmap": "graph TB\n        A[CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[LLM智能体缺乏物理世界部署所需的稳健空间心智模型/LLM agents lack robust spatial mental models for physical-world deployment]\n        C --> C1[提出基于魔方的三层诊断基准/CubeBench: A three-tiered diagnostic benchmark using Rubik's Cube]\n        C --> C2[从完整符号状态到部分视觉状态逐步评估/Progressive evaluation from full symbolic to partial visual state]\n        D --> D1[领先LLM在长视野任务上通过率为0%/Leading LLMs have 0.00% pass rate on long-horizon tasks]\n        D --> D2[揭示了长期规划和主动探索的根本性失败/Exposes fundamental failure in long-term planning and active exploration]"
    },
    {
      "title": "The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models",
      "authors": "Dakuan Lu, Jiaqi Zhang, Cheng Yuan, Jiawei Shao, Chi Zhang, Xuelong Li",
      "institution": "Institute of Artificial Intelligence (TeleAI), China Telecom",
      "link": "https://arxiv.org/pdf/2512.23340",
      "code": null,
      "tags": [
        "scaling laws",
        "scaling laws",
        "model ensembling",
        "multi-model collaboration",
        "cross-entropy loss",
        "parameter budget"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68212ad5f9cd50ef959cdd80f4b7274178d9a6b124904010fc5b0cf0834b21a1_w640_q70.webp",
      "contributions": "1. Proposes the \"Law of Multi-model Collaboration,\" a novel scaling law for predicting the performance limits of LLM ensembles based on aggregated parameters. 2. Establishes a method-agnostic theoretical framework using an idealized integration oracle to quantify the intrinsic upper bound of multi-model collaboration. 3. Empirically demonstrates that multi-model systems follow a power-law scaling with better trends and lower loss floors than single models, and that heterogeneous ensembles outperform homogeneous ones.",
      "summary": "This paper addresses the lack of a theoretical framework for scaling in multi-model LLM systems. It proposes the \"Law of Multi-model Collaboration,\" a scaling law based on aggregated parameters, and finds that ensembles scale better and achieve lower loss than single models, with diversity being a key driver of gains.",
      "mindmap": "graph TB\n        Root[”The Law of Multi-Model Collaboration<br>多模型协作定律”] --> Problem[”核心问题/Problem<br>Lack of scaling theory for multi-model collaboration<br>缺乏多模型协作的扩展理论”]\n        Root --> Method[”主要方法/Method<br>Propose Law of Multi-model Collaboration<br>提出多模型协作定律”]\n        Root --> Results[”关键结果/Results<br>Ensembles scale better than single models<br>集成模型比单一模型扩展性更好”]"
    },
    {
      "title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents",
      "authors": "Jiafeng Liang, Hao Li, Chang Li, Jiaqi Zhou, Shixin Jiang, Zekun Wang, Changkai Ji, Zhihao Zhu, Runxuan Liu, Tao Ren, Jinlan Fu, See-Kiong Ng, Xia Liang, Ming Liu, Bing Qin",
      "institution": "Harbin Institute of Technology, Fudan University, Peking University, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.23343",
      "code": "https://github.com/AgentMemory/Huaman-Agent-Memory",
      "tags": [
        "agent system",
        "memory systems",
        "cognitive neuroscience",
        "LLM-driven agents",
        "memory security",
        "multimodal memory"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15773eb4c52c63f2641be869baf3af4b7f6bb74f6e36c67247957bfbd039e9b6_w640_q70.webp",
      "contributions": "1. Provides a systematic synthesis and comparative analysis of memory systems from cognitive neuroscience to LLM-driven autonomous agents. 2. Reviews mainstream benchmarks for evaluating agent memory and explores memory security from attack and defense perspectives. 3. Envisions future research directions, focusing on multimodal memory systems and skill acquisition.",
      "summary": "This survey paper bridges the interdisciplinary gap between cognitive neuroscience and AI by systematically analyzing memory systems for autonomous agents. It compares biological and artificial memory taxonomies, storage, and management, while also reviewing evaluation benchmarks and security issues. The work concludes by outlining future directions, including multimodal memory and skill learning.",
      "mindmap": "graph TB\n        Root[AI Meets Brain: Memory Systems / AI与大脑：记忆系统] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[核心问题/Problem] --> P1[Interdisciplinary Gap / 跨学科鸿沟]\n        P1 --> P2[Existing works struggle to assimilate human memory essence / 现有工作难以吸收人类记忆机制精髓]\n    \n        Method[主要方法/Method] --> M1[Systematic Synthesis / 系统综述]\n        M1 --> M2[Comparative Analysis / 对比分析]\n        M2 --> M3[Review Benchmarks & Security / 回顾基准与安全]\n    \n        Results[关键结果/Results] --> R1[Unified Memory Framework / 统一的记忆框架]\n        R1 --> R2[Future Directions / 未来方向]\n        R2 --> R3[Multimodal Memory & Skill Acquisition / 多模态记忆与技能获取]"
    },
    {
      "title": "ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling",
      "authors": "Hai Duong Nguyen, Xuan-The Tran",
      "institution": "HAI-Smartlink Research Lab (Anchi STE Company), Vietnam Maritime University",
      "link": "https://arxiv.org/pdf/2512.23347",
      "code": null,
      "tags": [
        "medical signal processing",
        "ECG classification",
        "morphology-rhythm disentanglement",
        "Mamba",
        "zero-shot generalization",
        "Power Mean pooling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f9b49242e586cadf1889fa40730ea1652c1b4ef5b15cf15697b58832c4dbf6_w640_q70.webp",
      "contributions": "1. Proposes ECG-RAMBA, a framework that explicitly disentangles ECG morphology (via MiniRocket) and rhythm (via HRV descriptors) before fusing them for robust classification. 2. Introduces a numerically stable Power Mean pooling operator (Q=3) for windowed inference to emphasize high-evidence segments. 3. Demonstrates strong zero-shot cross-dataset generalization for ECG classification using a bi-directional Mamba backbone for long-range contextual modeling.",
      "summary": "The paper addresses the problem of poor generalization of deep learning models for ECG classification across different datasets. It proposes ECG-RAMBA, a method that separates and then fuses morphological and rhythm features, using a Mamba backbone and a novel pooling operator. The results show that this approach achieves robust zero-shot performance on external datasets, outperforming a baseline model.",
      "mindmap": "graph TB\n        A[ECG-RAMBA: Zero-Shot ECG Generalization] --> B[核心问题/Problem: Poor cross-dataset generalization in ECG classification]\n        A --> C[主要方法/Method: Morphology-Rhythm Disentanglement & Long-Range Mamba Modeling]\n        A --> D[关键结果/Results: Strong zero-shot AUC on CPSC-2021 & PTB-XL]"
    },
    {
      "title": "AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis",
      "authors": "Cehua Yang, Dongyu Xiao, Junming Lin, Yuyang Song, Hanxu Yan, Shawn Guo, Wei Zhang, Jian Yang, Mingjie Tang, Bryan Dai",
      "institution": "Sichuan University, IQuest Research, Beihang University",
      "link": "https://arxiv.org/pdf/2512.23366",
      "code": null,
      "tags": [
        "text-to-sql",
        "Reinforcement Learning",
        "Data Synthesis",
        "Policy Optimization",
        "Semantic-Logic Alignment",
        "Group Relative Policy Optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6740c1fc529b82b509bd38c2a7b5fb405b969bc5c3e11e6e0b7690e7fa791c85_w640_q70.webp",
      "contributions": "1. Proposes an iterative data factory for synthesizing high-quality, RL-ready Text-to-SQL data with strict semantic-logic verification. 2. Introduces a novel Agentic Reinforcement Learning framework featuring a Diversity-Aware Cold Start stage and Group Relative Policy Optimization (GRPO). 3. Demonstrates state-of-the-art performance on the BIRD and Spider benchmarks through the synergistic combination of data-centric and model-centric approaches.",
      "summary": "This paper addresses the challenges of data scarcity and limited reasoning in Text-to-SQL systems. It proposes a holistic framework that combines a data-centric approach for synthesizing high-fidelity training data with a model-centric approach using a novel Agentic Reinforcement Learning method called Group Relative Policy Optimization. The method achieves state-of-the-art results on major benchmarks, showing the effectiveness of the synergistic approach.",
      "mindmap": "graph TB\n        A[AGRO-SQL] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[数据稀缺与质量/Data Scarcity & Quality]\n        B --> B2[模型推理限制/Model Reasoning Limitations]\n        C --> C1[数据中心方法/Data-Centric Approach]\n        C --> C2[模型中心方法/Model-Centric Approach]\n        C1 --> C1a[迭代数据工厂/Iterative Data Factory]\n        C1 --> C1b[语义逻辑对齐/Semantic-Logic Alignment]\n        C2 --> C2a[多样性感知冷启动/Diversity-Aware Cold Start]\n        C2 --> C2b[组相对策略优化/Group Relative Policy Optimization]\n        D --> D1[在BIRD和Spider上SOTA/SOTA on BIRD & Spider]"
    },
    {
      "title": "Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2",
      "authors": "Yilun Luo, HuaQing Zheng, Haoqian Meng, Wenyuan Liu, Peng Zhang",
      "institution": "Tianjin University",
      "link": "https://arxiv.org/pdf/2512.23367",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "post-training quantization",
        "W8A8",
        "W4A8",
        "Ascend NPU",
        "Chain-of-Thought (CoT)"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07e9cf3e7e8252aa7eae3fdf2e7647007d4786dd6ade9f5c4e940d1c74c4e2cd_w640_q70.webp",
      "contributions": "1. Introduces a unified low-bit inference framework for openPangu-Embedded models, supporting INT8 (W8A8) and W4A8 quantization optimized for the Atlas A2 Ascend NPU. 2. Provides a comprehensive evaluation of quantization across three distinct CoT reasoning modes (slow_think, auto_think, no_think) on code generation benchmarks (HumanEval, MBPP). 3. Demonstrates that INT8 quantization preserves over 90% of FP16 accuracy with a 1.5x prefill speedup, while W4A8 significantly reduces memory consumption, enabling efficient CoT reasoning on edge NPUs.",
      "summary": "This paper addresses the high memory and latency overhead of deploying Chain-of-Thought (CoT) enabled openPangu models on Ascend NPUs by applying post-training quantization (INT8 and W4A8). The proposed framework, optimized for the Atlas A2 hardware, maintains high accuracy for INT8 and reduces memory for W4A8, enabling efficient on-device CoT reasoning.",
      "mindmap": "graph TB\n        A[Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[CoT推理带来高内存与延迟 / CoT reasoning causes high memory & latency]\n        B --> B2[Ascend NPU部署挑战 / Deployment challenge on Ascend NPU]\n        C --> C1[低比特量化 / Low-bit Quantization]\n        C --> C2[统一推理框架 / Unified Inference Framework]\n        C --> C3[支持W8A8与W4A8 / Supports W8A8 & W4A8]\n        D --> D1[INT8保持>90%精度 / INT8 preserves >90% accuracy]\n        D --> D2[1.5倍预填充加速 / 1.5x prefill speedup]\n        D --> D3[W4A8显著减少内存 / W4A8 greatly reduces memory]"
    },
    {
      "title": "SoulX-LiveTalk Technical Report",
      "authors": "Le Shen, Qiao Qian, Tan Yu, Ke Zhou, Tianhang Yu, Yu Zhan, Zhenjie Wang, Ming Tao, Shunshun Yin, Siyuan Liu",
      "institution": "Soul AI Lab, Donghua University",
      "link": "https://arxiv.org/pdf/2512.23379",
      "code": "https://soul-ailab.github.io/soulx-livetalk/",
      "tags": [
        "diffusion models",
        "Self-correcting Bidirectional Distillation",
        "Multi-step Retrospective Self-Correction",
        "hybrid sequence parallelism",
        "Parallel VAE",
        "kernel-level optimizations"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d6b1bb994b3c3273da207d2f19494d99c1ff6bf6663f41b1d85b2f0c69b83bb_w640_q70.webp",
      "contributions": "1. Introduced a Self-correcting Bidirectional Distillation strategy that retains bidirectional attention within video chunks to preserve spatiotemporal correlations and enhance visual fidelity. 2. Proposed a Multi-step Retrospective Self-Correction Mechanism to ensure stability during infinite generation by enabling autonomous recovery from accumulated errors. 3. Engineered a full-stack inference acceleration suite with hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations to achieve real-time performance.",
      "summary": "The paper addresses the challenge of deploying large diffusion models for real-time, audio-driven avatar generation by introducing SoulX-LiveTalk, a 14B-parameter framework. It employs a bidirectional distillation strategy and a self-correction mechanism to maintain high visual quality and stability, while a suite of inference optimizations enables sub-second latency and 32 FPS throughput, setting a new standard for interactive digital humans.",
      "mindmap": "graph TB\n        A[SoulX-LiveTalk] --> B[核心问题/Problem: 实时无限时长音频驱动化身生成中计算负载与低延迟的冲突]\n        A --> C[主要方法/Method: 自校正双向蒸馏与多步回顾自校正机制]\n        A --> D[关键结果/Results: 0.87秒启动延迟，32 FPS实时吞吐]"
    },
    {
      "title": "A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers",
      "authors": "Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee",
      "institution": "Urmia University of Technology",
      "link": "https://arxiv.org/pdf/2512.23380",
      "code": "https://github.com/your-repo/CoLog",
      "tags": [
        "log anomaly detection",
        "collaborative transformers",
        "multi-head impressed attention",
        "modality adaptation layer"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp",
      "contributions": "1. Proposes CoLog, a unified framework for detecting both point and collective anomalies in OS logs by applying multimodal sentiment analysis concepts. 2. Introduces collaborative transformers and multi-head impressed attention to learn interactions between different log data modalities. 3. Incorporates a modality adaptation layer to handle heterogeneity and adapt representations from different log modalities.",
      "summary": "The paper addresses the challenge of log anomaly detection, where existing methods struggle with the multimodal nature of log data and the interactions between these modalities. It proposes CoLog, a framework that uses collaborative transformers and a modality adaptation layer to learn nuanced patterns across log modalities for comprehensive anomaly detection. Extensive experiments show CoLog achieves state-of-the-art performance, with mean precision, recall, and F1 scores over 99.5% across seven benchmark datasets.",
      "mindmap": "graph TB\n        Root[”A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers”] --> Problem[”核心问题/Problem: Unimodal & multimodal methods fail to handle log data modalities and their interactions”]\n        Root --> Method[”主要方法/Method: CoLog framework with collaborative transformers, multi-head impressed attention, and modality adaptation layer”]\n        Root --> Results[”关键结果/Results: Achieves ~99.6% mean precision, recall, F1 on 7 datasets; superior to SOTA”]"
    },
    {
      "title": "Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?",
      "authors": "Anh Nguyen, Triet Huynh Minh Le, M. Ali Babar",
      "institution": "University of Adelaide",
      "link": "https://arxiv.org/pdf/2512.23385",
      "code": null,
      "tags": [
        "AI Security",
        "AI supply chain",
        "security taxonomy",
        "distilBERT classifier"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c00453e76598d08a965d2a15fe6e7b197cf1f19518d88f46a334b638da6327dc_w640_q70.webp",
      "contributions": "1. Developed a pipeline combining keyword matching with a fine-tuned distilBERT classifier to identify 312,868 security discussions from Hugging Face and GitHub. 2. Conducted a thematic analysis to create a fine-grained taxonomy of 32 security issues and 24 solutions across four themes (System/Software, External Tools/Ecosystem, Model, Data). 3. Provided empirical insights revealing that security issues stem from complex dependencies and black-box AI components, with Model and Data challenges often lacking concrete solutions.",
      "summary": "This paper investigates security issues in the AI supply chain by analyzing developer discussions from Hugging Face and GitHub. The authors use a keyword and classifier pipeline to build a large dataset and perform a thematic analysis to create a taxonomy of issues and solutions. They conclude that many security problems arise from dependencies and the black-box nature of AI, with solutions for Model and Data issues being particularly scarce.",
      "mindmap": "graph TB\n        Root[Securing the AI Supply Chain] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[AI供应链安全格局复杂/Complex AI supply chain security landscape]\n        Problem --> P2[缺乏对常见问题与解决方案的了解/Lack of knowledge on common issues & solutions]\n        Method[主要方法/Method] --> M1[实证调查/Empirical investigation]\n        M1 --> M1_1[数据源: Hugging Face, GitHub/Data Sources: Hugging Face, GitHub]\n        M1 --> M1_2[构建分类管道/Build classification pipeline]\n        M1_2 --> M1_2_1[关键词匹配+微调distilBERT/Keyword matching + fine-tuned distilBERT]\n        Results[关键结果/Results] --> R1[数据集: 312,868个安全讨论/Dataset: 312,868 security discussions]\n        Results --> R2[分类法: 32个问题, 24个解决方案/Taxonomy: 32 issues, 24 solutions]\n        Results --> R3[洞察: 依赖复杂性和黑盒性导致问题/Insight: Issues from dependencies & black-box nature]"
    },
    {
      "title": "Theoretical Foundations of Scaling Law in Familial Models",
      "authors": "Huan Song, Qingfei Zhao, Ting Long, Shuyu Tian, Hongjun An, Jiawei Shao, Chi Zhang, Xuelong Li",
      "institution": "Institute of Artificial Intelligence (TeleAI), China Telecom",
      "link": "https://arxiv.org/pdf/2512.23407",
      "code": null,
      "tags": [
        "llm training",
        "familial models",
        "scaling law",
        "early exiting",
        "IsoFLOP design",
        "compute-optimal training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc66a2a88c82327d2e67ccabca47fcc7a15e81e139a0ed0135b0f3ea93534985_w640_q70.webp",
      "contributions": "1. Theoretically and empirically extends the neural scaling law to the \"familial models\" paradigm by introducing granularity (G) as a new fundamental scaling variable alongside model size (N) and tokens (D). 2. Proposes a rigorous IsoFLOP experimental design to decouple architectural impact from computational scale, enabling high-fidelity parameterization of the unified scaling law L(N, D, G). 3. Quantifies that the granularity penalty follows a multiplicative power law with an extremely small exponent (γ≈0.041), validating the \"train once, deploy many\" paradigm without compromising compute-optimality.",
      "summary": "This paper addresses the limitation of traditional neural scaling laws, which assume a single model, by extending them to familial models that generate multiple sub-models from one backbone. The authors propose a unified scaling law incorporating granularity (G) and validate it using a rigorous IsoFLOP experimental design. The key finding is that the performance penalty for increased granularity is very small, proving that deployment flexibility can be achieved efficiently.",
      "mindmap": "graph TB\n        A[Theoretical Foundations of Scaling Law in Familial Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统缩放定律忽略多模型范式/Traditional scaling laws overlook the multi-model paradigm]\n        C --> C1[引入粒度作为新变量/Introduce Granularity (G) as a new variable]\n        C --> C2[统一函数形式 L(N, D, G)/Unified functional form L(N, D, G)]\n        C --> C3[采用IsoFLOP实验设计/Employ rigorous IsoFLOP experimental design]\n        D --> D1[粒度惩罚遵循幂律/Granularity penalty follows a power law]\n        D --> D2[指数极小 (γ≈0.041)/Exponent is extremely small]\n        D --> D3[验证”一次训练，多次部署”/Validates ”train once, deploy many”]"
    },
    {
      "title": "MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning",
      "authors": "Jiawei Chen, Xintian Shen, Lihao Zheng, Zhenwei Shao, Hongyuan Zhang, Pengfei Yu, Xudong Rao, Ning Mao, Xiaobo Liu, Lian Wen, Chaoqun Du, Feng Gu, Wei He, Qizhen Li, Shanshan Li, Zide Liu, Jing Luo, Lifu Mu, Xuhao Pan, Chang Ren, Haoyi Sun, Qian Wang, Wei Wang, Hongfu Yang, Jiqing Zhan, Chunpeng Zhou, Zheng Zhou, Hao Ma, Tao Wei, Pan Zhou, Wei Chen",
      "institution": "Li Auto Inc",
      "link": "https://arxiv.org/pdf/2512.23412",
      "code": "https://github.com/TIMMY-CHAN/MindWatcher",
      "tags": [
        "agent system",
        "tool-integrated reasoning",
        "multimodal chain-of-thought",
        "interleaved thinking"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/36f0f229f533ecc1b9565a72c5c77b232eab32880c12545774f94ebd2a19e651_w640_q70.webp",
      "contributions": "1. Introduces MindWatcher, a TIR agent with interleaved thinking and multimodal CoT reasoning for autonomous tool invocation and coordination. 2. Constructs the MWE-Bench benchmark and releases high-quality datasets and distilled smaller models (2B, 3B, 4B). 3. Designs a more efficient training infrastructure to enhance training speed and hardware utilization.",
      "summary": "This paper introduces MindWatcher, a multimodal tool-integrated reasoning agent that uses interleaved thinking and chain-of-thought reasoning to autonomously decide when and how to invoke tools. It is equipped with auxiliary tools and a local image database to handle broad-domain problems. Experiments show it matches or exceeds larger models in performance and provides insights like the genetic inheritance phenomenon in agent training.",
      "mindmap": "graph TB\n        A[MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning] --> B[核心问题/Problem: Traditional workflow-based agents have limited intelligence for real-world tool-invocation problems.]\n        A --> C[主要方法/Method: Proposes MindWatcher agent with interleaved thinking and multimodal CoT reasoning for autonomous tool use.]\n        A --> D[关键结果/Results: Matches/exceeds larger models, introduces MWE-Bench, and provides efficient training infrastructure.]"
    },
    {
      "title": "Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks",
      "authors": "Yusuf Kalyoncuoglu",
      "institution": "RWTH Aachen University",
      "link": "https://arxiv.org/pdf/2512.23410",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "intrinsic dimension",
        "low-rank approximation",
        "subspace-native distillation",
        "weight matrices",
        "empirical spectral density"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d34f960cfbb8a9db281aca58a1b30934c42fc68964647e8066a3754aeb92d38_w640_q70.webp",
      "contributions": "1. Proposes a constructive method to decouple solution geometry from the ambient search space, bypassing the non-convex optimization bottleneck. 2. Empirically demonstrates significant compression (e.g., factor of 16) of classification heads in models like ResNet-50, ViT, and BERT with minimal performance loss. 3. Introduces \"Subspace-Native Distillation\" as a novel paradigm to provide a stable geometric coordinate system for student models, enabling \"Train Big, Deploy Small\".",
      "summary": "The paper addresses the redundancy in large neural networks by proposing a method to directly construct low-dimensional solution subspaces, decoupling the solution geometry from the high-dimensional optimization search space. It shows that classification heads can be heavily compressed without significant performance drops. This leads to a new distillation paradigm that allows student models to learn in a stable, low-dimensional subspace, potentially realizing efficient deployment of compact models.",
      "mindmap": "graph TB\n        Root[”Directly Constructing Low-Dimensional Solution Subspaces<br>直接构建低维解子空间”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Large models are redundant for representation but needed for optimization.<br>大模型对表示是冗余的，但对优化是必要的。”]\n        Method[”主要方法/Method<br>Construct low-dimensional subspaces, decouple solution geometry.<br>构建低维子空间，解耦解几何。”]\n        Results[”关键结果/Results<br>Head compression by 16x, Subspace-Native Distillation.<br>分类头压缩16倍，提出子空间原生蒸馏。”]"
    },
    {
      "title": "The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis",
      "authors": "Alex Lewandowski, Adtiya A. Ramesh, Edan Meyer, Dale Schuurmans, Marlos C. Machado",
      "institution": "University of Alberta, Amii, The Swiss AI Lab IDSIA, USI & SUPSI, Canada CIFAR AI Chair, Google DeepMind",
      "link": "https://arxiv.org/pdf/2512.23419",
      "code": null,
      "tags": [
        "continual learning",
        "big world hypothesis",
        "computationally-embedded agent",
        "interactivity",
        "partially observable Markov decision process",
        "model-based reinforcement learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1eff9d76987d2bc49a07c8de307183661687471b7fe4f21ca75040ba3e1de25a_w640_q70.webp",
      "contributions": "1. Introduced a computationally-embedded perspective, representing an agent as an automaton simulated within a universal computer, proving it's equivalent to interacting with a POMDP over an infinite state-space. 2. Proposed a new objective called \"interactivity\" to measure an agent's ability to continually adapt its behavior by learning new predictions. 3. Developed a model-based RL algorithm for interactivity-seeking and constructed a synthetic problem to evaluate continual learning, finding deep linear networks outperform nonlinear ones in sustaining interactivity as capacity scales.",
      "summary": "This paper proposes a computationally-embedded perspective to formalize the \"big world hypothesis\" in continual learning, where an agent is modeled as an automaton within the environment. It introduces \"interactivity\" as a new objective and a corresponding model-based RL algorithm to seek it. The main finding is that, in their synthetic evaluation, deep linear networks sustain higher interactivity as capacity increases, whereas deep nonlinear networks struggle.",
      "mindmap": "graph TB\n        Root[”The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis<br>论文标题”]\n        Root --> Problem[”核心问题/Problem<br>如何形式化智能体在'大世界'中的持续学习约束”]\n        Root --> Method[”主要方法/Method<br>提出计算嵌入视角与'交互性'目标，开发基于模型的强化学习算法”]\n        Root --> Results[”关键结果/Results<br>深度线性网络比非线性网络更能维持交互性”]"
    },
    {
      "title": "AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis",
      "authors": "Jinye Du, Quan Yuan, Zuyao Zhang, Yanzhi Yi, Jiahui Hu, Wangyi Chen, Yiyang Zhu, Qishui Zheng, Wenxiang Zou, Xiangyu Chang, Zuohe Zheng, Zichun Ye, Chao Liu, Shanni Li, Renwei Zhang, Yiping Deng, Xinwei Hu, Xuefeng Jin, Jie Zhao",
      "institution": "Huawei Technologies Co., Ltd., Hunan University",
      "link": "https://arxiv.org/pdf/2512.23424",
      "code": null,
      "tags": [
        "gpu kernels",
        "kernel generation",
        "multi-agent system",
        "domain-specific languages (DSLs)",
        "performance tuning",
        "Triton"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40d5942348375e7b86203ab7a7420bba7105494296f349fdd48174b020a1527e_w640_q70.webp",
      "contributions": "1. Proposed AKG kernel agent, a multi-agent framework that automates the generation, migration, and performance tuning of computational kernels for diverse hardware platforms. 2. Designed the system to support multiple Domain-Specific Languages (DSLs) like Triton, TileLang, CPP, and CUDA-C, enabling cross-platform portability and correctness. 3. Demonstrated the system's effectiveness through evaluation on KernelBench, achieving an average 1.46x speedup over PyTorch Eager baselines on GPU and NPU backends.",
      "summary": "This paper proposes AKG kernel agent, a multi-agent framework that automates the development and optimization of high-performance computational kernels for modern AI workloads across diverse hardware. The system supports multiple DSLs for portability and uses LLMs for code generation and tuning. Evaluation shows it achieves a 1.46x average speedup over baseline implementations, effectively accelerating kernel development.",
      "mindmap": "graph TB\n        A[AKG Kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[AI模型对高性能计算内核的需求 / AI Models Demand High-Performance Kernels]\n        B --> B2[硬件多样性与手动优化的瓶颈 / Hardware Diversity & Manual Optimization Bottleneck]\n        C --> C1[多智能体系统自动化内核生成与调优 / Multi-Agent System Automates Kernel Generation & Tuning]\n        C --> C2[支持多种DSL以面向不同硬件后端 / Supports Multiple DSLs for Different Hardware Backends]\n        D --> D1[在KernelBench上评估 / Evaluated on KernelBench]\n        D --> D2[平均加速1.46倍 / Average 1.46x Speedup Achieved]"
    },
    {
      "title": "Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification",
      "authors": "Mustafa Demetgul, Sanja Lazarova Molnar",
      "institution": "Karlsruhe Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.23436",
      "code": null,
      "tags": [
        "image classification",
        "convolutional neural networks",
        "fuzzy logic",
        "road surface classification",
        "intelligent transport systems",
        "data fusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d74437ab4a94c7ac8b2148bba4e1b6dd8d4706d55db8a2ae146a41ace94ef9f9_w640_q70.webp",
      "contributions": "1. Proposes a real-time system for road surface classification by fusing weather-conditional data and road condition data. 2. Compares the performance of multiple deep learning CNNs (AlexNet, LeNet, VGG, ResNet) on both image-based and acceleration-data-as-image classification tasks. 3. Introduces the use of fuzzy logic to classify road surfaces according to environmental factors like weather and time of day, using sensor data.",
      "summary": "This paper proposes a real-time system for road surface condition monitoring. It employs deep learning CNNs to classify road types from images and acceleration data, achieving over 95% accuracy, and suggests using fuzzy logic to incorporate weather and time-of-day factors. The work aims to enhance vehicle safety and autonomous driving systems.",
      "mindmap": "graph TB\n    A[Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification] --> B[核心问题/Problem: Classical road monitoring is expensive and unsystematic.]\n    A --> C[主要方法/Method: Use deep learning (CNN) on images/acceleration data and fuzzy logic for environmental context.]\n    A --> D[关键结果/Results: Over 95% classification accuracy achieved.]"
    },
    {
      "title": "Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study",
      "authors": "Saifelden M. Ismail",
      "institution": "University of Science and Technology, Zewail City",
      "link": "https://arxiv.org/pdf/2512.23435",
      "code": null,
      "tags": [
        "on-device ai",
        "DistilHuBERT",
        "8-bit quantization",
        "cross-corpus validation",
        "Leave-One-Session-Out (LOSO)",
        "model compression"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0945944056b62e3ffa17bd0a2e4e36ebfe24da404a4aea1692a6806374437b15_w640_q70.webp",
      "contributions": "1. Proposes a mobile-efficient SER system using a distilled and 8-bit quantized DistilHuBERT model, achieving a 92% parameter reduction and a 23 MB footprint. 2. Demonstrates that cross-corpus training with CREMA-D enhances generalization on IEMOCAP, improving accuracy and reducing variance. 3. Provides an analysis of cross-corpus evaluation on RAVDESS, revealing a \"theatricality effect\" where predictions cluster by arousal, and establishes a Pareto-optimal trade-off between model size and accuracy.",
      "summary": "This paper addresses the challenge of deploying Speech Emotion Recognition (SER) on mobile devices by proposing a system based on the compressed DistilHuBERT model. Through rigorous cross-validation and cross-corpus training, the method achieves a good balance between a small model size (23 MB) and competitive accuracy, enabling practical on-device affect recognition while analyzing generalization challenges across different emotional speech corpora.",
      "mindmap": "graph TB\n        A[”Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study”] --> B[”核心问题/Problem: SER部署受限于大模型的计算需求/SER deployment constrained by computational demands of large models”]\n        A --> C[”主要方法/Method: 使用蒸馏与8位量化的DistilHuBERT，并进行跨语料库训练/Use distilled & 8-bit quantized DistilHuBERT with cross-corpus training”]\n        A --> D[”关键结果/Results: 模型仅23MB，精度达基准91%，跨语料库训练提升泛化性/Model is 23MB, achieves ~91% of baseline accuracy, cross-corpus training improves generalization”]"
    },
    {
      "title": "CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models",
      "authors": "Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, Zepeng Wang",
      "institution": "Lenovo (PCIE), University of Minnesota (UMN)",
      "link": "https://arxiv.org/pdf/2512.23453",
      "code": "https://github.com/AI-Researcher-Team/CoFi-Dec",
      "tags": [
        "multi-modal inference",
        "hallucination mitigation",
        "coarse-to-fine conditioning",
        "Wasserstein fusion",
        "generative feedback",
        "training-free decoding"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cab1d37f48692f41be898afb160456b94e821d8b6ea16ba282cb4ee3ac5046_w640_q70.webp",
      "contributions": "1. Proposes CoFi-Dec, a training-free decoding framework that mitigates hallucinations in LVLMs by integrating generative self-feedback with coarse-to-fine visual conditioning. 2. Introduces a Wasserstein-based fusion mechanism to align predictive distributions from multiple visual conditions into a geometrically consistent decoding trajectory. 3. Demonstrates substantial reduction in both entity-level and semantic-level hallucinations across six benchmarks, showing the framework is model-agnostic and requires no additional training.",
      "summary": "The paper addresses the problem of hallucinated content in Large Vision-Language Models (LVLMs). It proposes CoFi-Dec, a training-free decoding framework that uses coarse-to-fine visual conditioning and generative feedback to create multi-level visual hypotheses, which are then unified via a Wasserstein-based fusion mechanism. The method significantly reduces hallucinations across multiple benchmarks and can be applied to various LVLMs without retraining.",
      "mindmap": "graph TB\n        A[CoFi-Dec: Hallucination-Resistant Decoding] --> B[核心问题/Problem: LVLMs产生与视觉输入不一致的幻觉内容]\n        A --> C[主要方法/Method: 基于粗到细视觉条件的生成式自反馈与Wasserstein融合]\n        A --> D[关键结果/Results: 在六个基准测试中显著减少幻觉，无需训练，模型无关]"
    },
    {
      "title": "Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following",
      "authors": "Kongcheng Zhang, Qi Yao, Shunyu Liu, Wenjian Zhang, Min Cen, Yang Zhou, Wenkai Fang, Yiru Zhao, Baisheng Lai, Mingli Song",
      "institution": "Zhejiang University, Cainiao Network, Nanyang Technological University, Dalian University of Technology, University of Science and Technology of China, Alibaba Cloud Computing, Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.23457",
      "code": "https://github.com/zhangkc97/HiR",
      "tags": [
        "reinforcement learning",
        "instruction following",
        "hindsight replay",
        "sample-efficient RL"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b72e272e5c14b0f640f80b3e8859a1fd3a0a8b8e8608dfacee9528d242698f15_w640_q70.webp",
      "contributions": "1. Proposes Hindsight instruction Replay (HiR), a novel RL framework that replays failed attempts as successes using a select-then-rewrite strategy to address sparse rewards. 2. Theoretically frames the RL objective as dual-preference learning at both instruction- and response-level, enabling efficient optimization with only binary rewards. 3. Demonstrates sample efficiency and promising results across various instruction following tasks with reduced computational budget.",
      "summary": "This paper addresses the problem of sparse rewards in RL for aligning LLMs to follow complex instructions. It proposes HiR, a sample-efficient framework that replays failed responses as successful ones based on partially satisfied constraints, framed as dual-preference learning. Experiments show HiR achieves strong performance on instruction-following tasks while being more computationally efficient.",
      "mindmap": "graph TB\n        Root[Replay Failures as Successes: Sample-Efficient RL for Instruction Following] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[稀疏/不可区分的奖励阻碍学习<br>Sparse/Indistinguishable Rewards Impede Learning]\n        Method[后见指令重放 (HiR)<br>Hindsight instruction Replay (HiR)]\n        Results[跨任务有效且计算高效<br>Effective Across Tasks & Computationally Efficient]"
    },
    {
      "title": "HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation",
      "authors": "Yuxin Wen, Qing Shuai, Di Kang, Jing Li, Cheng Wen, Yue Qian, Ningxin Jiao, Changhai Chen, Weijie Chen, Yiran Wang, Jinkun Guo, Dongyue An, Han Liu, Yanyu Tong, Chao Zhang, Qing Guo, Juan Chen, Qiao Zhang, Youyi Zhang, Zihao Yao, Cheng Zhang, Hong Duan, Xiaoping Wu, Qi Chen, Fei Cheng, Liang Dong, Peng He, Hao Zhang, Jiaxin Lin, Chao Zhang, Zhongyi Fan, Yifan Li, Zhichao Hu, Yuhong Liu, Linus, Jie Jiang, Xiaolong Li, Linchao Bao",
      "institution": "Tencent Hunyuan",
      "link": "https://arxiv.org/pdf/2512.23464",
      "code": "https://github.com/Tencent-Hunyuan/HY-Motion-1.0",
      "tags": [
        "motion generation",
        "flow matching",
        "diffusion transformer (DiT)",
        "reinforcement learning from human feedback (RLHF)"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3019219aea0683c229d44ce63a0fed59b5ebb795811dc1b1638ae995c9a8156_w640_q70.webp",
      "contributions": "1. The first successful scaling of DiT-based flow matching models to billion parameters for motion generation. 2. A comprehensive full-stage training paradigm including large-scale pretraining, fine-tuning, and RLHF. 3. A meticulous data processing pipeline enabling extensive coverage of over 200 motion categories.",
      "summary": "This paper introduces HY-Motion 1.0, a large-scale model for generating 3D human motions from text. It scales up Diffusion Transformer-based flow matching and uses a full-stage training pipeline with pretraining, fine-tuning, and RLHF. The model achieves state-of-the-art performance and broad motion coverage, and is released open-source.",
      "mindmap": "graph TB\n        Root[”HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation”]\n        Root --> Problem[”核心问题/Problem: Generating high-quality, text-aligned 3D human motions”]\n        Root --> Method[”主要方法/Method: Scale DiT-based flow matching, Full-stage training (pretrain, fine-tune, RLHF), Meticulous data pipeline”]\n        Root --> Results[”关键结果/Results: SOTA performance, Extensive motion coverage, Open-source release”]"
    },
    {
      "title": "Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance",
      "authors": "Zhuo Li, Pengyu Cheng, Zhechao Yu, Feifei Tong, Anningzhe Gao, Tsung-Hui Chang, Xiang Wan, Erchao Zhao, Xiaoxi Jiang, Guanjun Jiang",
      "institution": "Alibaba, The Chinese University of Hong Kong, Shenzhen Research Institute of Big Data",
      "link": "https://arxiv.org/pdf/2512.23461",
      "code": "https://github.com/Qwen-Applications/DIR",
      "tags": [
        "reinforcement learning from human feedback (RLHF)",
        "reward model",
        "inductive bias",
        "information bottleneck",
        "mutual information",
        "reward hacking"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/522f5bbb5a5776cd8df024fb1b24faf19bb1a1ea6e0408c7951f37bfd1657846_w640_q70.webp",
      "contributions": "1. Proposes DIR, a novel information-theoretic debiasing method for reward models that maximizes mutual information with human preference while minimizing it with biased attributes. 2. Theoretically justifies the method's ability to handle complex, non-linear inductive biases, extending beyond simple linear correlation models. 3. Empirically demonstrates DIR's effectiveness in mitigating three types of biases (length, sycophancy, format) and shows it enhances RLHF performance and generalization.",
      "summary": "This paper addresses the problem of inductive biases in reward models (RMs) for RLHF, which can lead to overfitting and reward hacking. It proposes DIR, an information-theoretic debiasing method inspired by the information bottleneck that optimizes mutual information to reduce bias. Experiments show DIR effectively mitigates multiple biases and improves RLHF performance and generalization.",
      "mindmap": "graph TB\n        A[Eliminating Inductive Bias in Reward Models<br>消除奖励模型中的归纳偏差] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Low-quality RM data with inductive biases<br>导致过拟合和奖励攻击] --> B1[举例/Example<br>Response length bias<br>响应长度偏差]\n        C[主要方法/Method<br>DIR: Information-theoretic debiasing<br>基于信息瓶颈优化互信息] --> C1[目标/Objective<br>Max MI with preference, Min MI with bias<br>最大化偏好互信息，最小化偏差互信息]\n        D[关键结果/Results<br>Mitigates multiple biases & enhances RLHF<br>减轻多种偏差并提升RLHF性能] --> D1[验证的偏差/Verified Biases<br>Length, Sycophancy, Format<br>长度、迎合性、格式]"
    },
    {
      "title": "Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings",
      "authors": "Thomas Haschka, Joseph Bakarji",
      "institution": "Technische Universität Wien, American University of Beirut",
      "link": "https://arxiv.org/pdf/2512.23471",
      "code": null,
      "tags": [
        "text clustering",
        "hierarchical clustering",
        "density-based clustering",
        "semantic embeddings",
        "large language models",
        "topic modeling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e39c485de109f72521e714f1d7489795a2f6737dcaff2fbddf6af40f9dc170ee_w640_q70.webp",
      "contributions": "1. Proposes a novel nested density clustering method to construct hierarchical semantic trees from text embeddings. 2. Demonstrates the method's application for data-driven discovery of research areas and subfields without predefined categories. 3. Validates the approach's robustness and general applicability across diverse domains using benchmark datasets like 20 Newsgroups and IMDB reviews.",
      "summary": "The paper addresses the problem of uncovering the global hierarchical semantic structure in text corpora, which remains opaque when using LLM embeddings only for similarity search. It proposes a method that applies nested density clustering on LLM embeddings, gradually relaxing a density criterion to merge clusters into a hierarchical tree. This approach enables the data-driven discovery of semantic relationships and topic hierarchies without predefined categories, as demonstrated on scientific abstracts and benchmark datasets.",
      "mindmap": "graph TB\n        Root[”Semantic Tree Inference on Text Corpa / 文本语料库的语义树推断”]\n        Root --> Problem[”核心问题/Problem: Opaque global semantic structure in text corpora / 文本语料库中不透明的全局语义结构”]\n        Root --> Method[”主要方法/Method: Nested density clustering on LLM embeddings / 基于大语言模型嵌入的嵌套密度聚类”]\n        Root --> Results[”关键结果/Results: Data-driven hierarchical semantic tree discovery / 数据驱动的层次化语义树发现”]"
    },
    {
      "title": "Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation",
      "authors": "Toqeer Ali Syed, Mohammad Riyaz Belgaum, Salman Jan, Asadullah Abdullah Khan, Saad Said Alqahtani",
      "institution": "Islamic University of Madinah, Arab Open University-Bahrain",
      "link": "https://arxiv.org/pdf/2512.23480",
      "code": null,
      "tags": [
        "software supply chain security",
        "agentic AI",
        "reinforcement learning",
        "large language model",
        "blockchain security ledger",
        "CI/CD"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3f037ac5977c7275a7c48edbcb676154bcff19330c107fe4c9769750efc5350_w640_q70.webp",
      "contributions": "1. Proposes an autonomous, agentic AI framework for software supply chain security that integrates LLM-based reasoning, RL, and multi-agent coordination for proactive vulnerability identification and mitigation. 2. Implements a system that interfaces with real CI/CD environments (e.g., GitHub Actions, Jenkins) via the Model Context Protocol (MCP) and logs actions to a blockchain for auditability. 3. Demonstrates through experiments that the framework outperforms rule-based and provenance-only baselines in detection accuracy and mitigation latency with acceptable operational overhead.",
      "summary": "This paper addresses the limitation of current software supply chain security frameworks (like SLSA) which focus on provenance but lack active vulnerability mitigation. It proposes an agentic AI system that combines LLMs for semantic analysis and RL for adaptive response, integrated with CI/CD pipelines via MCP and logged on a blockchain. Experiments show it achieves better detection and faster mitigation than baselines, enabling a shift from reactive to proactive defense.",
      "mindmap": "graph TB\n        A[Agentic AI for Autonomous Defense in Software Supply Chain Security] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统溯源框架无法主动缓解漏洞/Traditional provenance frameworks lack active vulnerability mitigation]\n        C --> C1[多智能体协调/Multi-Agent Coordination]\n        C --> C2[LLM推理与RL策略/LLM Reasoning & RL]\n        C --> C3[集成CI/CD与区块链日志/CI/CD Integration & Blockchain Ledger]\n        D --> D1[更高的检测准确率/Higher Detection Accuracy]\n        D --> D2[更短的缓解延迟/Lower Mitigation Latency]\n        D --> D3[合理的构建开销/Reasonable Build Overhead]"
    },
    {
      "title": "FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence",
      "authors": "Guoan Wan, Tianyu Chen, Fangzheng Feng, Haoyi Zhou, Runhua Xu",
      "institution": "Beihang University, Huazhong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.23485",
      "code": "https://github.com/Bane-Elvin/AAAI2026-FRoD",
      "tags": [
        "post-training (sft/rlhf)",
        "Parameter-efficient fine-tuning",
        "LoRA",
        "full-rank adaptation",
        "rotational degrees of freedom",
        "hierarchical joint decomposition"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e211fe6dc2d8e782c731fff29ba32c9fe2a1f958b475d5931d50f6e8fd04fdb8_w640_q70.webp",
      "contributions": "1. Proposes FRoD, a novel PEFT method that combines hierarchical joint decomposition with rotational degrees of freedom for full-rank updates. 2. Introduces a globally shared basis and sparse, learnable perturbations to enhance expressiveness and efficiency beyond low-rank constraints. 3. Demonstrates that FRoD matches full fine-tuning accuracy on 20 benchmarks while using only 1.72% of trainable parameters and achieves faster convergence.",
      "summary": "The paper addresses the slow convergence and limited capacity of low-rank PEFT methods like LoRA. It proposes FRoD, a method that enables full-rank updates via a shared basis and sparse perturbations, achieving faster convergence. The method matches full fine-tuning accuracy on diverse benchmarks while using only a tiny fraction of parameters.",
      "mindmap": "graph TB\n        A[FRoD: Full-Rank Efficient Fine-Tuning] --> B[核心问题/Problem: Low-rank PEFT methods suffer from slow convergence and limited adaptation capacity]\n        A --> C[主要方法/Method: Hierarchical joint decomposition with rotational degrees of freedom for full-rank updates]\n        A --> D[关键结果/Results: Matches full fine-tuning accuracy using only 1.72% parameters and achieves faster convergence]"
    },
    {
      "title": "Theory of Mind for Explainable Human-Robot Interaction",
      "authors": "Marie Bauer, Julia Gachot, Matthias Kerzel, Cornelius Weber, Stefan Wermter",
      "institution": "University of Hamburg",
      "link": "https://arxiv.org/pdf/2512.23482",
      "code": null,
      "tags": [
        "human-robot interaction",
        "Theory of Mind",
        "Explainable AI",
        "XAI evaluation",
        "human-centered explanation",
        "VXAI framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16cba35efa080097fd84afa40a6d270891cd44dfcf26d46fde5d29edb9bb1541_w640_q70.webp",
      "contributions": "1. Proposes to conceptualize Theory of Mind (ToM) in Human-Robot Interaction as a form of Explainable AI (XAI), 2. Identifies a critical gap in ToM-HRI research regarding the fidelity of explanations to the robot's actual internal reasoning, 3. Advocates for integrating ToM principles into XAI frameworks to shift focus towards user-centered explanations and enable evaluation using frameworks like VXAI.",
      "summary": "This paper identifies that Theory of Mind (ToM) in human-robot interaction and Explainable AI (XAI) share the goal of making AI reasoning understandable. It proposes to treat ToM as a form of XAI and argues for integrating ToM's user-centered perspective into XAI frameworks to address the lack of explanation fidelity and user-centered evaluation in current research.",
      "mindmap": "graph TB\n        Root(”Theory of Mind for Explainable Human-Robot Interaction”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”ToM解释与机器人内部推理不一致/ToM explanations may not match robot's internal reasoning”)\n        Problem --> P2(”XAI缺乏以用户为中心的解释/XAI lacks user-centered explanations”)\n        Method --> M1(”将ToM视为XAI的一种形式/Consider ToM as a form of XAI”)\n        Method --> M2(”在XAI框架内整合ToM原则/Integrate ToM principles within XAI frameworks”)\n        Results --> R1(”提出视角转变，优先考虑用户需求/Proposed shift in perspective to prioritize user's needs”)\n        Results --> R2(”为使用VXAI等框架评估ToM奠定基础/Laid foundation for evaluating ToM using frameworks like VXAI”)"
    },
    {
      "title": "ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment",
      "authors": "Vassilis Digalakis Jr, Ramayya Krishnan, Gonzalo Martin Fernandez, Agni Orfanoudaki",
      "institution": "Boston University, Carnegie Mellon University, Universitat Politècnica de Catalunya, Oxford University",
      "link": "https://arxiv.org/pdf/2512.23487",
      "code": null,
      "tags": [
        "llm inference",
        "model selection",
        "capability-cost frontier",
        "constrained optimization",
        "deployment-aware leaderboards",
        "compliance trade-offs"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c28f58754a80430c57b0351355d200ab9fbfde8dd5fafc1b0d5caf4dd85bbb_w640_q70.webp",
      "contributions": "1. Proposes ML Compass, a framework that treats AI model selection as constrained optimization over a capability-cost frontier to bridge the gap between capability leaderboards and deployment decisions. 2. Characterizes optimal model configurations theoretically, showing a three-regime structure in internal measures and deriving comparative statics for budget, regulation, and technology changes. 3. Implements a practical pipeline that extracts internal measures, estimates an empirical frontier, learns task-specific utility, and validates with case studies in conversational and healthcare settings.",
      "summary": "The paper addresses the gap between AI model capability rankings and real-world deployment decisions by introducing ML Compass, a framework that formulates model selection as constrained optimization over a capability-cost frontier. It combines theoretical analysis of optimal configurations with an implementation pipeline for recommendation, validated in conversational and healthcare case studies. The framework shows that deployment-aware rankings can differ significantly from capability-only leaderboards, clarifying trade-offs between capability, cost, and compliance.",
      "mindmap": "graph TB\n        Root(”ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”能力排行榜与部署决策脱节/Capability-Deployment Gap”)\n        Problem --> P2(”需平衡用户效用、成本、合规性/Balance Utility, Cost, Compliance”)\n        Method --> M1(”理论: 基于前沿的约束优化/Theoretical Constrained Optimization”)\n        Method --> M2(”实现: 提取、估计、学习、推荐/Pipeline: Extract, Estimate, Learn, Recommend”)\n        Results --> R1(”最优配置呈现三区结构/Optimal Configurations Show Three-Regime Structure”)\n        Results --> R2(”部署感知排名不同于能力排名/Deployment-Aware Rankings Differ from Capability-Only”)"
    },
    {
      "title": "The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction",
      "authors": "Haoyu Pei, Zhongyang Liu, Xiangyi Xiao, Xiaocong Du, Haipeng Zhang, Kunpeng Zhang, Suting Hong",
      "institution": "ShanghaiTech University, Xi’an Jiaotong-Liverpool University, University of Maryland",
      "link": "https://arxiv.org/pdf/2512.23489",
      "code": "https://anonymous.4open.science/r/MIRAGE-VC-323F",
      "tags": [
        "rag (retrieval-augmented generation)",
        "graph reasoning",
        "information gain",
        "multi-agent",
        "off-graph prediction",
        "path retrieval"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aed24a413df0661b461a6124bbf364c73762a6f25c3c90546bff33f8c4123ebf_w640_q70.webp",
      "contributions": "1. Proposed an information-gain-driven path retriever to tackle path explosion in graphs for LLM reasoning. 2. Introduced a multi-agent architecture with learnable gating to fuse heterogeneous evidence streams. 3. Addressed the off-graph prediction challenge in venture capital, demonstrating significant performance gains.",
      "summary": "The paper addresses the challenge of predicting venture capital success, an off-graph task requiring reasoning over complex relational evidence. It proposes MIRAGE-VC, a framework that uses information-gain-driven path retrieval and a multi-agent system to distill and reason over investment networks. The method achieves improved prediction performance and offers insights for other off-graph tasks like recommendation.",
      "mindmap": "graph TB\n        A[The Gaining Paths to Investment Success<br/>投资成功路径] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>VC预测是离图任务<br/>VC prediction is off-graph task]\n        C[主要方法/Method<br/>MIRAGE-VC框架<br/>信息增益路径检索与多智能体<br/>MIRAGE-VC: Info-gain path retrieval & multi-agent]\n        D[关键结果/Results<br/>性能显著提升<br/>+5.0% F1, +16.6% Precision@5<br/>Significant performance gains]"
    },
    {
      "title": "Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization",
      "authors": "Wei Gao, Paul Zheng, Peng Wu, Yulin Hu, Anke Schmeink",
      "institution": "Wuhan University, RWTH Aachen University",
      "link": "https://arxiv.org/pdf/2512.23493",
      "code": null,
      "tags": [
        "communication & networking",
        "URLLC",
        "Link Adaptation",
        "Device Scheduling",
        "Deep Reinforcement Learning",
        "Bayesian Optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa25685331ccee6042c70838d3438022f95490a2cc609beba627f444cba8cd7c_w640_q70.webp",
      "contributions": "1. Proposes a joint link adaptation and device scheduling design for multi-device URLLC IIoT networks under imperfect CSI, aiming to maximize total transmission rate under strict BLER constraints. 2. Introduces a novel Bayesian Optimization-driven Twin Delayed Deep Deterministic Policy Gradient (BO-TD3) method to adaptively determine device serving order and MCS based on outdated CQI. 3. Develops a BO-based training mechanism to address issues of error sample imbalance and TD3 parameter sensitivity, improving convergence speed and learning reliability.",
      "summary": "This paper addresses the challenge of joint link adaptation and device scheduling in URLLC IIoT networks with imperfect channel state information. It proposes a novel deep reinforcement learning method (BO-driven TD3) that adaptively selects the device serving order and modulation schemes, enhanced by Bayesian Optimization for faster and more reliable training. Simulation results show the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.",
      "mindmap": "graph TB\n        A[Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[URLLC IIoT网络的多设备动态调度与链路自适应/URLLC IIoT Multi-device Dynamic Scheduling & Link Adaptation]\n        B --> B2[不完美的信道状态信息/Imperfect Channel State Information]\n        B --> B3[严格的误块率约束/Strict Block Error Rate Constraints]\n        C --> C1[贝叶斯优化驱动的TD3方法/BO-driven TD3 Method]\n        C --> C2[自适应确定设备服务顺序与MCS/Adaptively Determine Device Order & MCS]\n        C --> C3[BO训练机制改进收敛/BO-based Training for Convergence]\n        D --> D1[更快的收敛速度/Faster Convergence]\n        D --> D2[更高的总速率性能/Higher Sum-rate Performance]"
    },
    {
      "title": "Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities",
      "authors": "Alessio Benavoli, Alessandro Facchini, Marco Zaffalon",
      "institution": "Trinity College Dublin, SUPSI (University of Applied Sciences and Arts of Southern Switzerland), Kozminski University",
      "link": "https://arxiv.org/pdf/2512.23508",
      "code": null,
      "tags": [
        "AI alignment",
        "AI assistance game",
        "AI shutdown",
        "Incomplete preferences",
        "Non-Archimedean utilities"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ea603db1434e19dcd83096c95297f9662c571581eb479d66e87432fcf6a9075_w640_q70.webp",
      "contributions": "1. Formally connects the AI assistance and AI shutdown problems to the need for reasoning under uncertainty. 2. Argues that handling incomplete human preferences is a fundamental requirement for safe AI. 3. Proposes that non-Archimedean utility structures are necessary to correctly model and prioritize safety constraints.",
      "summary": "This paper analyzes AI safety through the frameworks of the AI assistance and AI shutdown games. It argues that to address these challenges, AI agents must be designed to reason under uncertainty and handle incomplete and non-Archimedean human preferences. The main conclusion is that these capabilities are essential for ensuring AI systems remain aligned with human values and safe.",
      "mindmap": "graph TB\n        Root[”Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities”] --> Problem[”核心问题/Problem: AI Alignment and Safety”]\n        Root --> Method[”主要方法/Method: Analyze via AI Assistance & Shutdown Games”]\n        Root --> Results[”关键结果/Results: Requires Uncertainty, Incomplete & Non-Archimedean Preferences”]"
    },
    {
      "title": "UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?",
      "authors": "Fengjiao Chen, Minhao Jing, Weitao Lu, Yan Feng, Xiaoyu Li, Xuezhi Cao",
      "institution": "Meituan",
      "link": "https://arxiv.org/pdf/2512.23512",
      "code": null,
      "tags": [
        "multi-modal training",
        "vision-language model",
        "unified model",
        "semantic generation",
        "autoregression",
        "data scaling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75dffd20dae849b9b4d37288d6d557fa32f5f2c8bf947c87fc1e79319e9dbe8_w640_q70.webp",
      "contributions": "1. Demonstrates that generation enhances understanding in large-scale VLM training only when operating at the semantic level (e.g., autoregressing high-level visual representations), not at the pixel level. 2. Shows that unified generation-understanding models exhibit superior data scaling trends and higher data utilization efficiency compared to understanding-only models. 3. Proposes that autoregression on input embeddings is an effective and modality-independent method for capturing visual details, enabling pixel-level generation from learned semantics.",
      "summary": "This paper investigates whether visual generation tasks can enhance understanding in large-scale vision-language models. Through large-scale pretraining (&gt;200M samples) with a model called UniHetero, the authors find that semantic-level generation (not pixel-level) improves understanding, reveals better data scaling, and that autoregression on input embeddings effectively captures visual details.",
      "mindmap": "graph TB\n        A[UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?] --> B(核心问题/Problem: Does visual generation enhance understanding at large scale?);\n        A --> C(主要方法/Method: Large-scale pretraining of unified model UniHetero (>200M samples));\n        A --> D(关键结果/Results);\n        B --> D;\n        C --> D;\n        D --> E(结果1/Result 1: Generation helps, but Only if you generate Semantics, Not Pixels);\n        D --> F(结果2/Result 2: Superior Data Scaling trend and higher Data Utilization);\n        D --> G(结果3/Result 3: Autoregression on Input Embedding is effective);"
    },
    {
      "title": "AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization",
      "authors": "Binhe Yu, Zhen Wang, Kexin Li, Yuqian Yuan, Wenqiao Zhang, Long Chen, Juncheng Li, Jun Xiao, Yueting Zhuang",
      "institution": "Zhejiang University, HKUST (The Hong Kong University of Science and Technology)",
      "link": "https://arxiv.org/pdf/2512.23537",
      "code": null,
      "tags": [
        "diffusion models",
        "multi-subject customization",
        "layout guidance",
        "attention decoupling",
        "training-free",
        "image adapter"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e5ac220ed9f71a26f20317e74d4ddc9cd5a957b5751dba85f593ddfeaf39640_w640_q70.webp",
      "contributions": "1. Proposes AnyMS, a novel training-free framework for layout-guided multi-subject image customization. 2. Introduces a bottom-up dual-level attention decoupling mechanism (global and local) to balance text alignment, identity preservation, and layout control. 3. Employs pre-trained image adapters to extract subject features without requiring subject-specific training or adapter tuning.",
      "summary": "This paper addresses the challenge of generating coherent images containing multiple user-specified subjects while balancing text alignment, subject identity, and layout control. It proposes AnyMS, a training-free framework that uses a bottom-up attention decoupling mechanism and pre-trained adapters to integrate text, subject images, and layout constraints. The method achieves state-of-the-art performance, supporting complex compositions and scaling to many subjects.",
      "mindmap": "graph TB\n        A[AnyMS: 布局引导免训练多主体定制] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[多主体定制中文本对齐、身份保持与布局控制的平衡问题/Balancing text alignment, identity preservation, and layout control in multi-subject customization]\n        C --> C1[提出免训练框架AnyMS/Proposes training-free framework AnyMS]\n        C1 --> C2[引入自底向上双级注意力解耦机制/Introduces bottom-up dual-level attention decoupling]\n        C2 --> C3[全局解耦确保文本对齐/Global decoupling ensures text alignment]\n        C2 --> C4[局部解耦防止主体冲突/Local decoupling prevents subject conflicts]\n        C --> C5[使用预训练图像适配器提取特征/Uses pre-trained image adapters for feature extraction]\n        D --> D1[实现SOTA性能/Achieves SOTA performance]\n        D --> D2[支持复杂组合与更多主体/Supports complex compositions and scales to more subjects]"
    },
    {
      "title": "PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis",
      "authors": "Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li, Zhe Wang, Xiaofan Zhang",
      "institution": "Shanghai Jiao Tong University, Fourth Military Medical University, University of Science and Technology of China, Fudan University, Nanjing Medical University",
      "link": "https://arxiv.org/pdf/2512.23545",
      "code": null,
      "tags": [
        "computational pathology",
        "agentic multimodal model",
        "evidence-seeking inference",
        "reinforcement learning",
        "whole-slide images",
        "vision-language model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db008475f544af0752cf146b5b6ba57eaf58c0e376cb94807dd3df594fa09037_w640_q70.webp",
      "contributions": "1. Proposed PathFound, an agentic multimodal model that introduces an evidence-seeking inference paradigm for pathological diagnosis, moving beyond static, single-pass analysis. 2. Integrated pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to enable proactive information acquisition and multi-stage diagnosis refinement. 3. Demonstrated that the evidence-seeking strategy consistently improves diagnostic accuracy across models and that PathFound achieves state-of-the-art performance, showing strong potential for discovering subtle pathological details.",
      "summary": "The paper proposes PathFound, an agentic multimodal model that mimics clinical workflows by actively seeking evidence for ambiguous pathological diagnoses through multi-turn interactions. It integrates visual foundation models, vision-language models, and reinforcement learning-based reasoning to refine its initial diagnosis. The method achieves state-of-the-art diagnostic accuracy and demonstrates the effectiveness of evidence-seeking workflows in computational pathology.",
      "mindmap": "graph TB\n        A[PathFound: Agentic Multimodal Model] --> B[核心问题/Problem: Static inference vs. clinical workflow]\n        A --> C[主要方法/Method: Agentic model with VFM, VLM, RL]\n        A --> D[关键结果/Results: SOTA accuracy, discovers subtle details]\n        B --> B1[静态推理范式/Static inference paradigm]\n        B --> B2[缺乏证据再获取/Lacks reassessment & evidence acquisition]\n        C --> C1[多阶段诊断/Multi-stage diagnosis]\n        C --> C2[主动信息获取/Proactive information acquisition]\n        D --> D1[诊断准确性提升/Improved diagnostic accuracy]\n        D --> D2[发现细微特征/Discover subtle pathological features]"
    },
    {
      "title": "Act2Goal: From World Model To General Goal-conditioned Policy",
      "authors": "Pengfei Zhou, Liliang Chen, Shengcong Chen, Di Chen, Wenzhi Zhao, Rongjun Jin, Guanghui Ren, Jianlan Luo",
      "institution": "Agibot Research",
      "link": "https://arxiv.org/pdf/2512.23541",
      "code": "https://act2goal.github.io/",
      "tags": [
        "reinforcement learning",
        "goal-conditioned policy",
        "world model",
        "multi-scale temporal hashing",
        "hindsight goal relabeling",
        "LoRA"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c80e64501db97d2a806134a5544d87a826563f38be2334e8bdec4a9d7f9cf78_w640_q70.webp",
      "contributions": "1. Proposes Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control for long-horizon tasks. 2. Introduces Multi-Scale Temporal Hashing (MSTH) to decompose imagined visual trajectories into dense proximal and sparse distal frames for fine-grained control and global consistency. 3. Enables reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision.",
      "summary": "This paper addresses the challenge of long-horizon robotic manipulation by proposing Act2Goal, a policy that uses a goal-conditioned world model to generate visual plans and a multi-scale temporal control mechanism for robust execution. The method achieves strong zero-shot generalization and allows for rapid online adaptation. Real-robot experiments show it significantly improves success rates on out-of-distribution tasks.",
      "mindmap": "graph TB\n        A[Act2Goal: From World Model To General Goal-conditioned Policy] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有目标条件策略在长视野操作中表现不佳/Existing goal-conditioned policies struggle with long-horizon manipulation]\n        C --> C1[集成目标条件视觉世界模型与多尺度时序控制/Integrates goal-conditioned visual world model with multi-scale temporal control]\n        C --> C2[引入多尺度时序哈希(MSTH)分解轨迹/Introduces Multi-Scale Temporal Hashing (MSTH) to decompose trajectory]\n        D --> D1[零样本泛化能力强/Strong zero-shot generalization]\n        D --> D2[通过在线自适应显著提升成功率/Improves success rates significantly via online adaptation]"
    },
    {
      "title": "Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs",
      "authors": "Sahil Kale, Antonio Luca Alfeo",
      "institution": "Knowledge Verse AI, eCampus University",
      "link": "https://arxiv.org/pdf/2512.23547",
      "code": "https://github.com/knowledge-verse-ai/kg-hallu-eval",
      "tags": [
        "hallucination detection",
        "knowledge graphs",
        "self-detection",
        "structured verification",
        "GPT-4o",
        "Gemini-2.5-Flash"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a76b4a88e64d73392aa8d986a7f3dab5da424782ba41d701ca8db2d4ab4a12d_w640_q70.webp",
      "contributions": "1. Proposes a novel hallucination self-detection method that converts LLM responses into knowledge graphs for structured analysis., 2. Introduces a manually curated and enhanced hallucination detection dataset to support more reliable future benchmarking., 3. Demonstrates significant performance improvements (up to 16% accuracy, 20% F1) over standard self-detection and a state-of-the-art baseline (SelfCheckGPT).",
      "summary": "This paper addresses the problem of hallucinations in LLMs by proposing a self-detection method that converts model responses into knowledge graphs to better analyze atomic facts and estimate hallucination likelihood. The method, evaluated on GPT-4o and Gemini-2.5-Flash, shows substantial improvements in accuracy and F1-score over existing approaches. The work concludes that structuring facts as knowledge graphs enables more robust hallucination detection, offering a low-cost, model-agnostic path toward safer language models.",
      "mindmap": "graph TB\n        A[”Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs”] --> B[”核心问题/Problem: LLM Hallucinations hinder safe deployment”]\n        A --> C[”主要方法/Method: Convert responses to Knowledge Graphs for structured self-verification”]\n        A --> D[”关键结果/Results: Up to 16% accuracy & 20% F1 improvement over baselines”]"
    },
    {
      "title": "Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks",
      "authors": "Toqeer Ali Syed, Mishal Ateeq Almutairi, Mahmoud Abdel Moaty",
      "institution": "Islamic University of Madinah, Arab Open University-Bahrain",
      "link": "https://arxiv.org/pdf/2512.23557",
      "code": null,
      "tags": [
        "ai security",
        "prompt injection",
        "multi-agent systems",
        "provenance tracking",
        "trust validation",
        "multimodal sanitization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05cd3aa22e07307bda78f06b6f87c2195c61c758b4fe44dc5bffbc281d72a8e4_w640_q70.webp",
      "contributions": "1. Proposes a Cross-Agent Multimodal Provenance-Aware Defense Framework to secure agentic AI workflows against prompt injection attacks. 2. Introduces a coordinated defense with specialized sanitizer agents (text, visual) and an output validator, managed by a provenance ledger for tracking trust metadata. 3. Demonstrates through experiments that the framework significantly improves multimodal injection detection accuracy and minimizes trust leakage across agents.",
      "summary": "This paper addresses the security threat of multimodal prompt injection attacks in agentic AI systems like LangChain. It proposes a defense framework that sanitizes inputs and validates outputs using specialized agents coordinated by a provenance ledger. The experimental results show the framework enhances detection accuracy and stabilizes agentic execution pathways.",
      "mindmap": "graph TB\n        A[Toward Trustworthy Agentic AI<br>构建可信的智能体AI] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Multimodal Prompt Injection Attacks<br>多模态提示注入攻击]\n        C --> C1[Cross-Agent Provenance-Aware Framework<br>跨智能体溯源感知框架]\n        C1 --> C2[Sanitizer & Validator Agents<br>净化与验证智能体]\n        C1 --> C3[Provenance Ledger<br>溯源账本]\n        D --> D1[Enhanced Detection Accuracy<br>提升检测准确率]\n        D --> D2[Minimized Trust Leakage<br>最小化信任泄漏]\n        D --> D3[Stable Execution Pathways<br>稳定的执行路径]"
    },
    {
      "title": "VL-RouterBench: A Benchmark for Vision-Language Model Routing",
      "authors": "Zhehao Huang, Baijiong Lin, Jingyuan Zhang, Jingying Wang, Yuhang Liu, Ning Lu, Tao Li, Xiaolin Huang",
      "institution": "Shanghai Jiao Tong University, The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.23562",
      "code": "https://github.com/K1nght/VL-RouterBench",
      "tags": [
        "multi-modal inference",
        "vision-language model routing",
        "benchmark",
        "cost-accuracy trade-off",
        "model selection",
        "evaluation protocol"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/885d9087464eedead5301ad4cd041923ddee6d5773371e117abbd30fc4ae4f09_w640_q70.webp",
      "contributions": "1. Proposes VL-RouterBench, the first systematic and reproducible benchmark for evaluating vision-language model (VLM) routing systems. 2. Constructs a large-scale evaluation foundation with quality and cost matrices over 519,180 sample-model pairs from 17 models and 14 datasets. 3. Introduces a comprehensive evaluation protocol that jointly measures accuracy, cost, and throughput, and uses a ranking score based on the harmonic mean for fair comparison across router configurations.",
      "summary": "This paper introduces VL-RouterBench, a benchmark to systematically evaluate routing systems for vision-language models. It constructs matrices of quality and cost from extensive inference logs and uses a ranking score to compare routers. The evaluation shows current routers achieve significant gains but still fall short of an ideal Oracle, indicating room for improvement in router design.",
      "mindmap": "graph TB\n        Root[VL-RouterBench: A Benchmark for Vision-Language Model Routing] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>缺乏系统化、可复现的<br>VLM路由评估基准<br>Lack of systematic, reproducible<br>benchmark for VLM routing]\n        Method[主要方法/Method<br>基于原始推理日志构建<br>质量与成本矩阵<br>Construct quality & cost matrices<br>from raw inference logs]\n        Results[关键结果/Results<br>观察到显著的路由增益<br>但与理想性能仍有差距<br>Observe significant routability gain<br>but clear gap to ideal Oracle]"
    },
    {
      "title": "RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature",
      "authors": "Hanzheng Li, Xi Fang, Yixuan Li, Chaozheng Huang, Junjie Wang, Xi Wang, Hongzhe Bai, Bojun Hao, Shenyu Lin, Huiqi Liang, Linfeng Zhang, Guolin Ke",
      "institution": "DP Technology, Shanghai Jiao Tong University, Tsinghua University, New York University, Fudan University, Xiamen University, ShanghaiTech University",
      "link": "https://arxiv.org/pdf/2512.23565",
      "code": null,
      "tags": [
        "multimodal large language models",
        "chemical reaction understanding",
        "multimodal benchmark",
        "scientific literature",
        "visual perception",
        "cross-modal integration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e7b73b049eb3232e03516a09f66b0fddafff9357b89527de70340faa28603c6_w640_q70.webp",
      "contributions": "1. Introduces RxnBench, a multi-tiered benchmark for evaluating MLLMs on chemical reaction understanding from scientific PDFs, featuring two tasks (SF-QA and FD-QA). 2. Provides a comprehensive evaluation revealing a critical capability gap in MLLMs, showing they struggle with deep chemical logic and precise structural recognition despite excelling at text extraction. 3. Highlights the importance of inference-time reasoning and underscores the urgent need for domain-specific visual encoders and stronger reasoning engines for autonomous AI chemists.",
      "summary": "This paper introduces RxnBench, a multimodal benchmark to evaluate Large Language Models on understanding chemical reactions from scientific literature. The evaluation reveals that while models are good at extracting text, they struggle with chemical logic and structural recognition, showing the need for better domain-specific visual and reasoning components.",
      "mindmap": "graph TB\n        Root[RxnBench: A Multimodal Benchmark for Evaluating LLMs on Chemical Reaction Understanding from Scientific Literature] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: MLLMs' ability to comprehend dense, graphical reaction language in literature is underexplored.]\n        Method[主要方法/Method: A multi-tiered benchmark with two tasks: Single-Figure QA and Full-Document QA.]\n        Results[关键结果/Results: Models struggle with chemical logic and structure; inference-time reasoning helps but accuracy remains low, highlighting need for domain-specific encoders and reasoning engines.]"
    },
    {
      "title": "Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation",
      "authors": "Manh Hung Nguyen, Adish Singla",
      "institution": "MPI-SWS (Max Planck Institute for Software Systems)",
      "link": "https://arxiv.org/pdf/2512.23601",
      "code": null,
      "tags": [
        "creative text generation",
        "divergent-convergent thinking",
        "prompting method",
        "creative problem generation",
        "artificial hivemind",
        "constraint satisfaction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86ff3954389096abbd357b8cdc09c7b6f3087b3cf9c2edee2cf008bb8924d692_w640_q70.webp",
      "contributions": "1. Proposes CreativeDC, a novel two-phase prompting method inspired by human creative thinking to decouple creative exploration from constraint satisfaction in LLMs. 2. Introduces a comprehensive evaluation framework for creative problem generation, measuring diversity, novelty, and utility. 3. Demonstrates that CreativeDC significantly improves the diversity and novelty of generated educational problems compared to baseline methods while maintaining utility.",
      "summary": "The paper addresses the \"Artificial Hivemind\" effect in LLMs, which leads to homogeneous and repetitive outputs, particularly harmful for generating diverse educational problems. It proposes CreativeDC, a prompting method that scaffolds LLM reasoning into divergent (idea exploration) and convergent (constraint satisfaction) phases. The results show that CreativeDC generates problems with significantly higher diversity and novelty than baselines without sacrificing utility.",
      "mindmap": "graph TB\n        Root[”Divergent-Convergent Thinking in LLMs for Creative Problem Generation<br/>大语言模型中的发散-收敛思维用于创意问题生成”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem: LLMs suffer from 'Artificial Hivemind' effect, generating homogeneous and repetitive educational problems.<br/>大语言模型存在'人工蜂群思维'效应，生成同质化、重复的教育问题。”]\n        Method[”主要方法/Method: CreativeDC, a two-phase prompting method decoupling divergent (exploration) and convergent (constraint) thinking.<br/>CreativeDC，一种将发散（探索）与收敛（约束）思维解耦的两阶段提示方法。”]\n        Results[”关键结果/Results: Achieves higher diversity & novelty while maintaining utility; scales better in generating distinct problems.<br/>在保持实用性的同时实现更高的多样性与新颖性；在生成独特问题方面扩展性更好。”]"
    },
    {
      "title": "Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning",
      "authors": "Deniz Akdemir",
      "institution": "None (Institution not specified in provided content)",
      "link": "https://arxiv.org/pdf/2512.23617",
      "code": null,
      "tags": [
        "transfer learning",
        "Le Cam Distortion",
        "Deficiency Distance",
        "Directional Simulability",
        "Unsupervised Domain Adaptation",
        "Negative Transfer"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bd736028263c7eaaaaecae78f0df59f633374608f59295b1185c8385eea1e5_w640_q70.webp",
      "contributions": "1. Proposes a decision-theoretic framework for robust transfer learning based on Le Cam's theory, replacing symmetric invariance with directional simulability. 2. Introduces Le Cam Distortion, quantified by the Deficiency Distance, as a rigorous upper bound for transfer risk. 3. Demonstrates the framework's effectiveness across diverse experiments (genomics, vision, RL), showing it prevents source degradation and catastrophic negative transfer where traditional methods fail.",
      "summary": "The paper identifies a flaw in standard Unsupervised Domain Adaptation, which can cause harmful \"negative transfer\" by forcing invariance between unequally informative domains. It proposes a new framework based on Le Cam's theory, using directional simulability and a metric called Le Cam Distortion to enable safe transfer without degrading the source domain. Experiments show this method successfully prevents information loss and catastrophic failure in safety-critical applications.",
      "mindmap": "graph TB\n        Root[Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[标准UDA的缺陷/Flaw of Standard UDA]\n        Problem --> P2[负迁移与信息破坏/Negative Transfer & Information Destruction]\n        Method --> M1[Le Cam理论/Le Cam's Theory]\n        Method --> M2[方向可模拟性/Directional Simulability]\n        Method --> M3[Le Cam Distortion度量/Le Cam Distortion Metric]\n        Results --> R1[基因组学完美估计/Perfect Genomics Estimation]\n        Results --> R2[零源域损失/Zero Source Utility Loss]\n        Results --> R3[安全RL策略转移/Safe RL Policy Transfer]"
    },
    {
      "title": "Regret-Based Federated Causal Discovery with Unknown Interventions",
      "authors": "Federico Baldo, Charles K. Assaad",
      "institution": "Sorbonne Université, INSERM, Institut Pierre Louis d'Epidémiologie et de Santé Publique",
      "link": "https://arxiv.org/pdf/2512.23626",
      "code": null,
      "tags": [
        "federated learning",
        "causal discovery",
        "unknown interventions",
        "differential privacy",
        "Φ-CPDAG",
        "regret-based"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/373e2e9b4041d9efb71fbe2d1095901813d7f2551cdfe37d40d79c04d7aa235d_w640_q70.webp",
      "contributions": "1. Proposes I-PERI, a novel federated causal discovery algorithm that handles unknown client-level interventions, 2. Introduces the Φ-Markov Equivalence Class (Φ-CPDAG), a tighter equivalence class derived from structural differences across clients, 3. Provides theoretical guarantees on convergence and privacy-preserving properties (differential privacy).",
      "summary": "The paper addresses federated causal discovery where client data are subject to unknown, heterogeneous interventions, a common real-world scenario overlooked by prior work. It proposes the I-PERI algorithm, which first recovers the union CPDAG and then orients additional edges by exploiting intervention-induced structural differences across clients, resulting in a tighter equivalence class called the Φ-CPDAG. Theoretical and empirical results demonstrate the algorithm's effectiveness and privacy guarantees.",
      "mindmap": "graph TB\n        A[Regret-Based Federated Causal Discovery with Unknown Interventions] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 联邦因果发现中客户端存在未知异质干预/Federated causal discovery with unknown, heterogeneous client interventions]\n        C[主要方法/Method: 提出I-PERI算法，利用干预差异定向边/Propose I-PERI algorithm, orienting edges using intervention differences]\n        D[关键结果/Results: 定义Φ-CPDAG，提供理论与隐私保证/Define Φ-CPDAG, provide theoretical and privacy guarantees]"
    },
    {
      "title": "Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE",
      "authors": "Chien-Ting Tung, Chenming Hu",
      "institution": "University of California at Berkeley",
      "link": "https://arxiv.org/pdf/2512.23624",
      "code": null,
      "tags": [
        "others",
        "physics-informed neural networks",
        "circuit simulation",
        "differential-algebraic equations",
        "surrogate modeling",
        "NeuroSPICE"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7d88b0c7c41bb8bbaf5959a08185913698993a5cc330641c604219b2a1c0622_w640_q70.webp",
      "contributions": "1. Proposes NeuroSPICE, a novel PINN-based framework for solving circuit differential-algebraic equations directly, bypassing traditional time-discretized numerical solvers. 2. Demonstrates the framework's flexibility for modeling emerging devices and multi-physics systems within a single Python environment, lowering the barrier for rapid prototyping. 3. Highlights the potential of the differentiable PINN model as a surrogate for circuit design optimization and inverse problems, despite not outperforming SPICE in raw training speed or accuracy.",
      "summary": "This paper introduces NeuroSPICE, a framework that uses Physics-Informed Neural Networks (PINNs) to simulate electronic circuits by solving their governing differential-algebraic equations through backpropagation. It represents circuit waveforms as continuous, differentiable functions of time, enabling the simulation of novel devices like ferroelectric memories. The main conclusion is that while not faster than SPICE for training, NeuroSPICE offers unique advantages as a flexible, differentiable surrogate model for design optimization and complex multi-physics systems.",
      "mindmap": "graph TB\n        A[”Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE<br>论文标题”] --> B[”Problem: Conventional SPICE struggles with emerging, highly nonlinear devices and multi-physics coupling.<br>核心问题: 传统SPICE难以处理新兴的非线性器件和多物理场耦合。”]\n        A --> C[”Method: NeuroSPICE, a PINN framework that solves circuit DAEs by minimizing equation residuals via backpropagation.<br>主要方法: NeuroSPICE，一个通过反向传播最小化方程残差来求解电路DAE的PINN框架。”]\n        A --> D[”Results: Provides a flexible, differentiable surrogate model for design optimization, enabling simulation of novel devices like ferroelectric memories.<br>关键结果: 提供了一个灵活的、可微分的代理模型用于设计优化，能够模拟如铁电存储器等新型器件。”]"
    },
    {
      "title": "BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization",
      "authors": "Iris Xu, Guangtao Zeng, Zexue He, Charles Jin, Aldo Pareja, Dan Gutfreund, Chuang Gan, Zhang-Wei Hong",
      "institution": "Massachusetts Institute of Technology, MIT-IBM Watson AI Lab, Stanford University, University of Massachusetts Amherst",
      "link": "https://arxiv.org/pdf/2512.23631",
      "code": "https://github.com/iamxjy/BOAD-SWE-Agent",
      "tags": [
        "agent system",
        "multi-agent systems",
        "hierarchical agents",
        "bandit optimization",
        "software engineering agents",
        "SWE-bench"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67084d40c24e3869f47efa4b52c7ec1479016d613997e911c6730d7e7684254f_w640_q70.webp",
      "contributions": "1. Formulates the automatic discovery of effective hierarchical multi-agent systems for software engineering as a multi-armed bandit (MAB) problem, enabling efficient exploration under limited budgets. 2. Proposes the BOAD framework, which uses bandit optimization to coordinate specialized sub-agents (e.g., for localization, editing, validation) and attribute credit within a team. 3. Demonstrates that automatically discovered hierarchical agents outperform single-agent and manually designed multi-agent systems on challenging, out-of-distribution SWE benchmarks, including achieving second place on SWE-bench-Live with a 36B model.",
      "summary": "The paper addresses the poor generalization of single-agent LLMs on long-horizon, out-of-distribution software engineering tasks by proposing a hierarchical multi-agent system. The core method, BOAD, automatically discovers effective agent hierarchies by formulating the search as a multi-armed bandit optimization problem. The results show that this approach significantly improves performance on SWE benchmarks, surpassing larger models like GPT-4 and Claude.",
      "mindmap": "graph TB\n        A[BOAD: 发现分层软件工程代理 / BOAD: Discovering Hierarchical Software Engineering Agents] --> B\n        A --> C\n        A --> D\n        B[核心问题 / Problem: 单一LLM代理在长视野、分布外软件工程任务上泛化能力差 / Single-agent LLMs generalize poorly on long-horizon, out-of-distribution SWE tasks]\n        C[主要方法 / Method: 将分层发现建模为多臂老虎机问题，优化子代理协作 / Formulate hierarchy discovery as a multi-armed bandit problem to optimize sub-agent collaboration]\n        D[关键结果 / Results: 在SWE-bench上超越单代理和手动设计的多代理系统，36B模型排名第二 / Outperforms single-agent and manual multi-agent systems on SWE-bench, 36B model ranks second]"
    },
    {
      "title": "AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms",
      "authors": "LearnLM Team Google, Eedi, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Brian Veprek, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Irina Jurenka, Iris Hulls, James Stalley-Moores, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Lucy Dalton, Markus Kunesch, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Sara Wiltberger, Shakir Mohamed, Simon Woodhead, Vasco Brazão",
      "institution": "Google, Eedi",
      "link": "https://arxiv.org/pdf/2512.23633",
      "code": null,
      "tags": [
        "educational ai",
        "generative AI",
        "fine-tuning",
        "randomized controlled trial",
        "Socratic questioning",
        "pedagogical instruction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp",
      "contributions": "1. Conducted a rigorous, in-classroom exploratory RCT to evaluate the safety and efficacy of a generative AI tutor (LearnLM) in a real educational setting. 2. Demonstrated that a pedagogically fine-tuned AI model can reliably draft instructional content, with human tutors approving 76.4% of its messages with minimal or no edits. 3. Showed that AI-supported tutoring led to student performance at least equivalent to human-only tutoring, with a significant 5.5 percentage point improvement in solving novel problems on subsequent topics.",
      "summary": "This paper investigates whether generative AI can scale effective one-to-one tutoring. The authors integrated LearnLM, a pedagogically fine-tuned AI model, into a math tutoring platform and conducted a randomized controlled trial where human tutors supervised its outputs. The results show that LearnLM was a reliable tutor, and students using it performed as well as or better than those with human tutors alone, suggesting AI can deliver effective, individualized learning support at scale.",
      "mindmap": "graph TB\n        Root[AI Tutoring RCT in UK Classrooms] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[个性化辅导成本高/High cost of 1-to-1 tutoring]\n        Problem --> P2[AI辅导的有效性与安全性未知/Unproven efficacy & safety of AI tutoring]\n        Method --> M1[整合LearnLM模型/Integrate LearnLM (pedagogically fine-tuned AI)]\n        Method --> M2[在Eedi平台进行RCT/Conduct RCT on Eedi platform]\n        Method --> M3[专家导师监督输出/Human tutors supervise AI drafts]\n        Results --> R1[76.4%消息被直接批准/76.4% messages approved with minimal edits]\n        Results --> R2[学生表现相当或更好/Student performance equal or better]\n        Results --> R3[解决新问题能力提升5.5%/5.5% improvement on novel problems]"
    },
    {
      "title": "Nested Browser-Use Learning for Agentic Information Seeking",
      "authors": "Baixuan Li, Jialong Wu, Wenbiao Yin, Kuan Li, Zhongwang Zhang, Huifeng Yin, Zhengwei Tao, Liwen Zhang, Pengjun Xie, Jingren Zhou, Yong Jiang",
      "institution": "Tongyi Lab, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.23647",
      "code": "https://github.com/Alibaba-NLP/DeepResearch",
      "tags": [
        "agent system",
        "information-seeking agents",
        "browser interaction",
        "ReAct-style agents",
        "nested framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab4c5e1fc52fc83cedffe542098b6777a8df396f1f3d30f2a130aebdd36e0dc_w640_q70.webp",
      "contributions": "1. Proposes a minimal and complete browser-action framework for agents, 2. Introduces a nested structure to decouple interaction control from page exploration, 3. Demonstrates improved performance on deep information-seeking benchmarks.",
      "summary": "The paper addresses the limitation of current information-seeking agents, which rely on simple API calls and cannot perform real browsing. It proposes NestBrowse, a framework that uses a nested structure to enable fine-grained browser control for agents, simplifying reasoning and improving performance on deep search tasks.",
      "mindmap": "graph TB\n        Root[”Nested Browser-Use Learning for Agentic Information Seeking<br>面向智能信息搜索的嵌套浏览器使用学习”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Agents lack real browsing, limited to APIs.”]\n        Method[”主要方法/Method<br>NestBrowse: nested browser-action framework.”]\n        Results[”关键结果/Results<br>Better performance on deep IS benchmarks.”]"
    },
    {
      "title": "Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing",
      "authors": "Panagiotis Theocharopoulos, Ajinkya Kulkarni, Mathew Magimai.-Doss",
      "institution": "International School of Athens, Idiap Research Institute",
      "link": "https://arxiv.org/pdf/2512.23684",
      "code": null,
      "tags": [
        "adversarial attacks",
        "prompt injection",
        "large language models",
        "academic peer review",
        "multilingual",
        "adversarial robustness"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6285e0b940378fdc27628286ec6510afd35bf7a004b7ad95ad776e49035c6e1c_w640_q70.webp",
      "contributions": "1. Constructed a dataset of ~500 real ICML papers to empirically evaluate hidden prompt injection attacks in a realistic academic reviewing context. 2. Demonstrated that embedding semantically equivalent adversarial instructions in multiple languages (English, Japanese, Chinese, Arabic) can significantly alter LLM-generated review scores and decisions. 3. Revealed notable cross-lingual differences in attack effectiveness, with Arabic injections having minimal impact compared to others.",
      "summary": "This paper investigates the vulnerability of LLM-based academic peer review systems to hidden prompt injection attacks. By injecting adversarial instructions in four languages into a dataset of real papers and having an LLM review them, the authors found that such attacks can substantially change review outcomes for English, Japanese, and Chinese, but not Arabic. The results highlight a critical security risk and language-dependent susceptibility in automated reviewing pipelines.",
      "mindmap": "graph TB\n        A[Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>LLM-based reviewing systems are vulnerable to hidden prompt injection attacks.]\n        C[主要方法/Method<br>Inject semantically equivalent adversarial prompts in 4 languages into ~500 real papers and review with an LLM.]\n        D[关键结果/Results<br>English, Japanese, Chinese injections change scores/decisions; Arabic injections have little effect.]"
    },
    {
      "title": "Web World Models",
      "authors": "Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, Mengdi Wang",
      "institution": "Princeton University, University of California, Los Angeles, University of Pennsylvania",
      "link": "https://arxiv.org/pdf/2512.23676",
      "code": "https://princeton-ai2-lab.github.io/Web-World-Models/",
      "tags": [
        "agent system",
        "world model",
        "language agent",
        "web framework",
        "structured latent state",
        "deterministic generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8017bbc1bd6722a0d7bd0f84c67f735c0f1b24747518e2aa15905d07d1b03c_w640_q70.webp",
      "contributions": "1. Introduced the Web World Model (WWM), a hybrid architecture that uses ordinary web code to enforce logical consistency and LLMs to generate open-ended content. 2. Built a suite of practical WWM demonstrations across diverse domains (travel, fiction, encyclopedia, games) on a realistic web stack. 3. Identified key design principles for WWMs, such as separating code-defined rules from model-driven imagination and representing latent state as typed web interfaces.",
      "summary": "This paper proposes Web World Models (WWMs), a framework that combines the reliability of web code for world \"physics\" with the generative power of LLMs for content and narratives. This hybrid approach aims to provide language agents with controllable, logically consistent, yet open-ended persistent environments. The work demonstrates that standard web stacks can serve as a scalable substrate for building such world models.",
      "mindmap": "graph TB\n        A[Web World Models] --> B[”核心问题/Problem: Language agents need persistent worlds; existing solutions are either too rigid (web frameworks) or too uncontrolled (fully generative models).”]\n        A --> C[”主要方法/Method: Hybrid Web World Model (WWM): Web code defines rules & state; LLMs generate context & narratives on top.”]\n        A --> D[”关键结果/Results: Demonstrates scalable, controllable, open-ended environments; proposes design principles for WWMs.”]"
    },
    {
      "title": "The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence",
      "authors": "Xiao Zhou, Yuze Sun, Jie Wu, Xiaomeng Huang",
      "institution": "Tsinghua University, National Climate Centre, China Meteorological Administration",
      "link": "https://arxiv.org/pdf/2512.22144",
      "code": null,
      "tags": [
        "climate informatics",
        "similarity-preserving representation",
        "latent space clustering",
        "physics-coherent monitoring"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b09901ad992d27215117f20984faf17d508a192bf9010cc39bd8b74553ff543_w640_q70.webp",
      "contributions": "1. Introduced an \"AI-for-theory\" paradigm using a deep learning model (PhysAnchor-MJO-AE) to learn a latent representation where distance corresponds to physical-feature similarity for the MJO. 2. Objectively discovered the first complete six-phase anatomical map of the MJO life cycle, isolating two long-hypothesized transitional phases. 3. Constructed a new physics-coherent monitoring framework that decouples location and intensity, drastically reducing spurious propagation and convective misplacement compared to classical methods.",
      "summary": "This paper addresses the challenge of objectively defining the life cycle of the Madden-Julian Oscillation (MJO) by introducing an \"AI-for-theory\" paradigm. It develops a deep learning model to learn a similarity-preserving latent representation, enabling clustering that reveals a complete six-phase anatomy of the MJO. The derived new monitoring framework significantly outperforms the classical index, demonstrating AI's role as a discovery tool for complex systems.",
      "mindmap": "graph TB\n        A[The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Defining MJO lifecycle is challenging due to propagation; classical methods conflate artifacts with physics.]\n        C[主要方法/Method<br>AI-for-theory paradigm; Deep learning model (PhysAnchor-MJO-AE) learns similarity-preserving latent representation for objective clustering.]\n        D[关键结果/Results<br>First complete six-phase MJO anatomy; New physics-coherent monitoring framework reduces errors by an order of magnitude.]"
    },
    {
      "title": "Neural ocean forecasting from sparse satellite-derived observations: a case-study for SSH dynamics and altimetry data",
      "authors": "Daria Botvynko, Pierre Haslée, Lucile Gaultier, Bertrand Chapron, Clement de Boyer Montégut, Anass El Aouni, Julien Le Sommer, Ronan Fablet",
      "institution": "IMT Atlantique, Ifremer, CNRS, Mercator Ocean International",
      "link": "https://arxiv.org/pdf/2512.22152",
      "code": null,
      "tags": [
        "spatiotemporal forecasting",
        "4DVarNet",
        "U-Net",
        "sequence-to-sequence",
        "sea level anomaly",
        "neural forecast"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b91a25a42a5dc1b5739ae5689c604765502ed07062eadaa473e043ec5a0d288f_w640_q70.webp",
      "contributions": "1. Adapts U-Net and 4DVarNet architectures for short-term forecasting of ocean dynamics from sparse satellite data. 2. Formulates the forecasting task as a sequence-to-sequence mapping using partial SLA snapshots to predict future full-field maps. 3. Demonstrates that the end-to-end neural framework outperforms an operational baseline, especially in high-variability regions.",
      "summary": "This paper proposes an end-to-end deep learning framework for 7-day forecasting of sea surface dynamics using sparse satellite altimetry data. It adapts U-Net and 4DVarNet models to perform sequence-to-sequence mapping from partial observations to full-field forecasts. The results show the neural model outperforms an operational ocean forecast product, demonstrating the feasibility of neural forecasting for operational oceanography under data-sparse conditions.",
      "mindmap": "graph TB\n        A[Neural Ocean Forecasting from Sparse Observations] --> B(核心问题/Problem: 稀疏卫星数据下的短期海洋预报/Short-term ocean forecasting from sparse satellite data)\n        A --> C(主要方法/Method: 基于U-Net和4DVarNet的端到端序列预测/End-to-end sequence forecasting using U-Net & 4DVarNet)\n        A --> D(关键结果/Results: 神经模型超越业务化基线，在多变区域改进显著/Neural model outperforms operational baseline, notable improvements in high-variability regions)"
    },
    {
      "title": "Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images",
      "authors": "Haozhe Jia",
      "institution": "Boston University",
      "link": "https://arxiv.org/pdf/2512.22209",
      "code": null,
      "tags": [
        "medical image super-resolution",
        "diffusion models",
        "SR3",
        "DDPM",
        "capsule endoscopy",
        "HyperKvasir"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70b95a1328af0d9ae7f16ab6cb15a216b2ad914761eed6496beb2ee934202e23_w640_q70.webp",
      "contributions": "1. Applied the SR3 diffusion model framework to the specific domain of capsule endoscopy image super-resolution, addressing hardware-imposed low-resolution constraints. 2. Demonstrated that the diffusion-based approach outperforms traditional interpolation and GAN-based methods (e.g., ESRGAN) in both quantitative metrics (PSNR, SSIM) and qualitative anatomical fidelity. 3. Showed that architectural enhancements like attention mechanisms further improve performance, achieving a PSNR of 29.3 dB and SSIM of 0.71.",
      "summary": "This paper proposes using a diffusion model (SR3/DDPM) for super-resolution enhancement of low-resolution capsule endoscopy images. The method learns a probabilistic mapping from low-resolution to high-resolution images and is evaluated on the HyperKvasir dataset. Results show it outperforms traditional and GAN-based methods, better preserving critical anatomical details for clinical diagnosis.",
      "mindmap": "graph TB\n        Root[”Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Capsule endoscopy images have low resolution, limiting clinical diagnosis.”]\n        Method[”主要方法/Method<br>Use SR3 diffusion model to learn mapping from LR to HR images.”]\n        Results[”关键结果/Results<br>Outperforms bicubic & GAN methods, improves PSNR/SSIM, preserves anatomy.”]"
    },
    {
      "title": "Literature Mining System for Nutraceutical Biosynthesis: From AI Framework to Biological Insight",
      "authors": "Xinyang Sun, Nipon Sarmah, Miao Guo",
      "institution": "King's College London",
      "link": "https://arxiv.org/pdf/2512.22225",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "literature mining",
        "large language models",
        "prompt engineering",
        "few-shot prompting",
        "domain adaptation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6c767d67875d1e2e25e95c99a9b826daf52082c66775d52728bda6802558969_w640_q70.webp",
      "contributions": "1. Developed a domain-adapted AI system using LLMs and advanced prompt engineering to automate the extraction of nutraceutical-producing microbial strains from unstructured text. 2. Created and validated a structured dataset of 35 nutraceutical-strain associations, spanning multiple compound categories. 3. Demonstrated the system's performance and provided biological insights, identifying dominant microbial contributors and the framework's utility for synthetic biology and precision fermentation.",
      "summary": "This paper presents an AI-driven literature mining system that uses large language models and prompt engineering to automatically identify microbes that produce nutraceuticals from scientific text. The system, which performed best with the DeepSeek-V3 model and domain-specific prompts, generated a validated dataset and revealed key microbial strains for biosynthesis. This framework enhances the scalability of literature mining and provides actionable insights for strain selection and fermentation strategies.",
      "mindmap": "graph TB\n        A[Literature Mining System for Nutraceutical Biosynthesis<br/>营养保健品生物合成的文献挖掘系统] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>Extracting structured knowledge on microbial strains from literature is a bottleneck.<br/>从文献中提取关于微生物菌株的结构化知识是一个瓶颈。]\n        C[主要方法/Method<br/>Domain-adapted system using LLMs and prompt engineering.<br/>使用LLM和提示工程的领域适应系统。]\n        D[关键结果/Results<br/>Created dataset of 35 associations; DeepSeek-V3 outperformed LLaMA-2; identified dominant microbial strains.<br/>创建了35个关联的数据集；DeepSeek-V3优于LLaMA-2；识别了主要微生物菌株。]"
    },
    {
      "title": "Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging",
      "authors": "Muhammad Ibtsaam Qadir, Duane Schonlau, Ulrike Dydak, Fiona R. Kolbinger",
      "institution": "Purdue University, Indiana University School of Medicine, TUD Dresden University of Technology",
      "link": "https://arxiv.org/pdf/2512.22176",
      "code": null,
      "tags": [
        "medical image segmentation",
        "nnU-Net",
        "MRI field strength",
        "radiomic analysis",
        "UMAP clustering",
        "model generalizability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff8ab0f68e15ce620cdfd03bebfec56b322101c32b9cea5d7a2881045b311bc2_w640_q70.webp",
      "contributions": "1. A systematic quantitative evaluation framework to assess the impact of MRI scanner magnetic field strength (1.5T vs. 3.0T) on the performance and generalizability of deep learning segmentation models. 2. Empirical demonstration that training data field strength significantly influences model performance, especially for soft-tissue segmentation tasks, with models trained on 3.0T data often outperforming others. 3. The use of radiomic analysis and UMAP clustering to provide an interpretable, feature-based explanation for the observed performance differences, linking them to field-strength-dependent image characteristics.",
      "summary": "This study investigates how MRI scanner magnetic field strength affects deep learning-based segmentation models. Using nnU-Net models trained on data from 1.5T, 3.0T, or combined field strengths across three anatomical datasets, the authors found that field strength in training data significantly impacts model performance, particularly for soft tissues. The conclusion is that magnetic field strength should be considered a confounding factor in AI studies for MRI analysis.",
      "mindmap": "graph TB\n        Root(”Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging<br>磁共振成像深度学习分析中场强依赖的性能变异性”) --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem(”核心问题/Problem<br>Impact of MRI field strength on DL model performance & generalizability<br>MRI场强对深度学习模型性能与泛化能力的影响”)\n        Method(”主要方法/Method<br>Train/evaluate nnU-Net models on 1.5T, 3.0T, and combined data; Analyze with UMAP & radiomics<br>在1.5T、3.0T及混合数据上训练/评估nnU-Net模型；使用UMAP和影像组学分析”)\n        Results(”关键结果/Results<br>Field strength in training data substantially influences performance, especially for soft tissues<br>训练数据中的场强显著影响性能，尤其对软组织”)"
    },
    {
      "title": "Space AI: Leveraging Artificial Intelligence for Space to Improve Life on Earth",
      "authors": "Ziyang Wang",
      "institution": "IEEE",
      "link": "https://arxiv.org/pdf/2512.22399",
      "code": null,
      "tags": [
        "autonomous systems",
        "autonomous operations",
        "mission planning",
        "in-situ resource utilisation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6ab175271eb74821199e8998bba499198a030f7f6b329c43a525f851b07aabe_w640_q70.webp",
      "contributions": "1. Proposes and defines \"Space AI\" as a unified interdisciplinary field at the intersection of AI and space science. 2. Consolidates historical and contemporary progress into a systematic four-context framework (AI on Earth, in Orbit, in Deep Space, for Multi-Planetary Life). 3. Identifies key application areas where AI advances can translate to societal benefits on Earth, such as in sensing, robotics, and trustworthy AI.",
      "summary": "This paper introduces \"Space AI\" as a new interdisciplinary field and proposes a systematic framework to organize its applications across four mission contexts, from Earth-based planning to multi-planetary life support. It argues that AI is critical for enabling autonomous and resilient space operations under extreme conditions, and that advances in this domain will also yield significant benefits for life on Earth.",
      "mindmap": "graph TB\n        A[Space AI: Leveraging AI for Space to Improve Life on Earth] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>如何在极端不确定和有限监督下<br>实现太空自主弹性操作<br>How to enable autonomous, resilient space operations under extreme uncertainty and limited oversight]\n        C[主要方法/Method<br>提出系统化四维框架<br>Propose a systematic four-context framework<br>(AI on Earth, in Orbit, in Deep Space, for Multi-Planetary Life)]\n        D[关键结果/Results<br>统一了跨学科领域并识别关键应用<br>加速太空探索能力并产生广泛地球影响<br>Unifies interdisciplinary field and identifies key applications<br>Accelerates space exploration capability and yields broad Earth impact]"
    },
    {
      "title": "Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds",
      "authors": "Naman Aggarwal, Siddhartha R. Dalal, Vishal Misra",
      "institution": "Dream Sports, Columbia University",
      "link": "https://arxiv.org/pdf/2512.22473",
      "code": null,
      "tags": [
        "transformer interpretability",
        "cross-entropy",
        "gradient dynamics",
        "attention mechanism",
        "expectation-maximization",
        "Bayesian inference"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed145ec9892ca4b4f8b91e5c78948ac6b81399c20bcafdccd4cb429e92da2aed_w640_q70.webp",
      "contributions": "1. Derived an advantage-based routing law and a responsibility-weighted update rule for attention scores and values under cross-entropy training. 2. Showed that the coupled gradient dynamics induce a positive feedback loop that behaves like a two-timescale Expectation-Maximization (EM) procedure. 3. Demonstrated that these gradient dynamics sculpt the low-dimensional manifolds necessary for Bayesian inference, linking optimization to geometry and function.",
      "summary": "This paper analyzes how cross-entropy training shapes the internal geometry of transformer attention heads. By deriving first-order gradient dynamics, it shows that attention score and value updates form a positive feedback loop analogous to an EM algorithm. The core conclusion is that this gradient flow sculpts the Bayesian manifolds that enable in-context probabilistic reasoning.",
      "mindmap": "graph TB\n    A[Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[Transformer内部几何结构如何形成?/How is transformer internal geometry formed?]\n    C --> C1[推导注意力梯度动态/Derive attention gradient dynamics]\n    C --> C2[建立EM算法类比/Establish EM algorithm analogy]\n    D --> D1[发现优势路由与责任更新/Discover advantage-based routing & responsibility-weighted update]\n    D --> D2[梯度流塑造贝叶斯流形/Gradient flow sculpts Bayesian manifolds]"
    },
    {
      "title": "Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers",
      "authors": "Atakan Işık, Selin Vulga Işık, Ahmet Feridun Işık, Mahşuk Taylan",
      "institution": "Başkent University, Gaziantep University",
      "link": "https://arxiv.org/pdf/2512.22564",
      "code": null,
      "tags": [
        "medical audio classification",
        "Audio Spectrogram Transformer",
        "Sharpness-Aware Minimization",
        "ICBHI 2017",
        "class imbalance",
        "loss landscape"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0861fcb0d50b762d97367d04dce9ca920f2ffca74cd5112df95b11652972a9b_w640_q70.webp",
      "contributions": "1. Proposes a novel framework integrating the Audio Spectrogram Transformer (AST) with Sharpness-Aware Minimization (SAM) for robust respiratory sound classification. 2. Implements a weighted sampling strategy to effectively handle the severe class imbalance present in medical datasets like ICBHI 2017. 3. Achieves state-of-the-art performance on the ICBHI 2017 dataset, with a particular focus on improving sensitivity for reliable clinical screening.",
      "summary": "This paper addresses the challenges of respiratory sound classification, such as data scarcity and class imbalance, by enhancing the Audio Spectrogram Transformer with Sharpness-Aware Minimization (SAM) to find flatter minima for better generalization. The method also employs weighted sampling and achieves a new state-of-the-art score of 68.10% and a sensitivity of 68.31% on the ICBHI 2017 dataset, demonstrating improved robustness for clinical applications.",
      "mindmap": "graph TB\n        A[Geometry-Aware Optimization for Respiratory Sound Classification<br/>呼吸声音分类的几何感知优化] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n    \n        B --> B1[数据限制与过拟合<br/>Data Constraints & Overfitting]\n        B1 --> B2[数据集小、噪声大、类别不平衡<br/>Small, Noisy, Imbalanced Dataset]\n    \n        C --> C1[使用SAM优化AST<br/>Enhance AST with SAM]\n        C1 --> C2[优化损失曲面几何<br/>Optimize Loss Surface Geometry]\n        C --> C3[加权采样策略<br/>Weighted Sampling Strategy]\n    \n        D --> D1[SOTA分数: 68.10%<br/>SOTA Score: 68.10%]\n        D --> D2[高敏感度: 68.31%<br/>High Sensitivity: 68.31%]"
    },
    {
      "title": "JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference",
      "authors": "Niels Bracher, Lars Kühmichel, Desi R. Ivanova, Xavier Intes, Paul-Christian Bürkner, Stefan T. Radev",
      "institution": "Rensselaer Polytechnic Institute, TU Dortmund University, University of Oxford",
      "link": "https://arxiv.org/pdf/2512.22999",
      "code": null,
      "tags": [
        "simulation-based inference",
        "Bayesian adaptive design",
        "amortized inference",
        "diffusion models",
        "sequential experimental design",
        "policy learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a357d49f23f5fdffd9539d05904d86150c3c7775033f4847b56afac3375ad8e6_w640_q70.webp",
      "contributions": "1. Introduces JADAI, a framework that jointly amortizes Bayesian adaptive design and inference by training a policy, history, and inference network end-to-end., 2. Proposes a generic loss function that aggregates incremental reductions in posterior error across sequential experiments., 3. Instantiates the inference network with diffusion-based posterior estimators to handle high-dimensional and multimodal posteriors at each experimental step.",
      "summary": "This paper introduces JADAI, a framework that jointly learns to optimize experimental designs and perform Bayesian inference in a sequential setting. It trains a policy network, a history network, and a diffusion-based inference network end-to-end to minimize posterior error. The method achieves superior or competitive performance on standard adaptive design benchmarks.",
      "mindmap": "graph TB\n        Root[JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Actively optimizing design variables for parameter estimation] --> Problem_Sub[子问题/Sub-problem: Sequential design and inference are typically treated separately]\n        Method[主要方法/Method: Jointly amortize design and inference via end-to-end training] --> Method_Sub1[网络/Networks: Policy, History, and Inference (Diffusion-based) networks]\n        Method --> Method_Sub2[损失函数/Loss: Aggregates incremental posterior error reduction]\n        Results[关键结果/Results: Superior/competitive performance on standard benchmarks]"
    },
    {
      "title": "Deep Learning for Art Market Valuation",
      "authors": "Jianping Mei, Michael Moses, Jan Waelty, Yucheng Yang",
      "institution": "Cheung Kong Graduate School of Business (CKGSB), University of Zurich, Swiss Finance Institute, Art Market Consultancy",
      "link": "https://arxiv.org/pdf/2512.23078",
      "code": null,
      "tags": [
        "multi-modal learning",
        "multi-modal deep learning",
        "visual embeddings",
        "Grad-CAM",
        "hedonic regression",
        "repeated-sales dataset"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp",
      "contributions": "1. Introduces and benchmarks multi-modal deep learning models that fuse tabular data (artist, history) with visual embeddings from artwork images for art market valuation. 2. Demonstrates that visual content provides a distinct and economically significant predictive contribution, especially for fresh-to-market works lacking prior transaction history. 3. Provides interpretability analyses using Grad-CAM and embedding visualizations to show models attend to compositional and stylistic visual cues.",
      "summary": "This paper proposes using multi-modal deep learning to improve art market valuation by incorporating visual content from artwork images alongside traditional tabular data. It finds that while artist identity and history are most predictive overall, visual features provide crucial value for first-time sales where historical data is absent. The results show deep learning offers new insights for valuation, particularly in the most challenging scenarios.",
      "mindmap": "graph TB\n        A[Deep Learning for Art Market Valuation<br/>艺术市场估值的深度学习] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>How to improve art market valuation?<br/>如何改进艺术市场估值？]\n        C[主要方法/Method<br/>Multi-modal deep learning fusing tabular & image data<br/>融合表格与图像数据的多模态深度学习]\n        D[关键结果/Results<br/>Visual features help most for fresh-to-market works<br/>视觉特征对首次上市作品最有帮助]"
    },
    {
      "title": "Constraint programming model and biased random-key genetic algorithm for the single-machine coupled task scheduling problem with exact delays to minimize the makespan",
      "authors": "Vítor A. Barbosa, Rafael A. Melo",
      "institution": "Institute of Computing, Universidade Federal da Bahia",
      "link": "https://arxiv.org/pdf/2512.23150",
      "code": null,
      "tags": [
        "scheduling",
        "constraint programming",
        "biased random-key genetic algorithm",
        "makespan",
        "exact delays",
        "local search"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f80cc4c12e23f8eb80b390efdd4a8b62ea37fb03c01008317951d1079c35c319_w640_q70.webp",
      "contributions": "1. A Constraint Programming (CP) model for the single-machine coupled task scheduling problem with exact delays, utilizing well-established global constraints. 2. A novel Biased Random-Key Genetic Algorithm (BRKGA) that incorporates an efficient decoder, periodical restarts, shakes, and a local search algorithm for enhanced exploration. 3. An empirical evaluation demonstrating that the BRKGA provides high-quality solutions quickly, while the CP model with extended resources can find best-known solutions for a majority of benchmark instances.",
      "summary": "This paper addresses the NP-hard single-machine coupled task scheduling problem with exact delays to minimize makespan. It proposes both a Constraint Programming model and a Biased Random-Key Genetic Algorithm (BRKGA) enhanced with local search and shake components. Computational results show the BRKGA finds good solutions quickly, while the CP model with more resources achieves state-of-the-art results on most benchmark instances.",
      "mindmap": "graph TB\n        Root[论文标题/Paper Title: Constraint Programming and BRKGA for Coupled Task Scheduling] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: 单机精确延迟耦合任务调度，最小化完工时间/Single-machine coupled task scheduling with exact delays to minimize makespan]\n        Method[主要方法/Method: 约束规划模型与带偏置随机密钥遗传算法/Constraint Programming model and Biased Random-Key Genetic Algorithm (BRKGA)]\n        Results[关键结果/Results: BRKGA快速提供高质量解，CP模型在充分资源下达到当前最优解/BRKGA provides high-quality solutions quickly; CP model reaches best-known solutions with sufficient resources]"
    },
    {
      "title": "An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making",
      "authors": "Wendyam Eric Lionel Ilboudo, Saori C Tanaka",
      "institution": "Nara Institute of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.23144",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Kullback-Leibler divergence",
        "decision paralysis",
        "intent selection",
        "affordance selection",
        "hierarchical decision process"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae0c1bd96570e940c6fa7d72cbfcec334b1a94d288ce774a384e5c60b2bfe206_w640_q70.webp",
      "contributions": "1. Proposes a computational account of decision paralysis as convergence failure in a hierarchical decision process, separating intent and affordance selection. 2. Formalizes decision commitment as inference under a mixture of reverse-KL (mode-seeking) and forward-KL (mode-covering) objectives. 3. Demonstrates through simulations that forward-KL-biased inference reproduces key features of decision inertia and shutdown, framing autism as an extreme regime of this general decision-making continuum.",
      "summary": "This paper addresses decision paralysis by proposing a hierarchical inference-based model that separates intent and affordance selection. Commitment is formalized using a mixture of reverse-KL and forward-KL divergence objectives, where a bias towards forward-KL leads to slow, heavy-tailed response times and distinct failure modes. The model reproduces features of decision inertia and suggests autism represents an extreme case on this decision-making continuum.",
      "mindmap": "graph TB\n        Root[An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>Decision Paralysis] --> P1[挑战/Challenge<br>Choice models assume ready-to-compare options]\n        Problem --> P2[现象/Phenomenon<br>Hesitation, freezing, failure to act]\n        Method[主要方法/Method<br>Computational Account] --> M1[架构/Architecture<br>Hierarchical decision process]\n        Method --> M2[形式化/Formalization<br>Intent vs. Affordance selection]\n        Method --> M3[目标/Objective<br>Mixture of reverse-KL & forward-KL]\n        Results[关键结果/Results<br>Simulation Outcomes] --> R1[行为/Behavior<br>Slow, heavy-tailed response times]\n        Results --> R2[失败模式/Failure Modes<br>Intent & Affordance saturation]\n        Results --> R3[解释/Interpretation<br>Autism as an extreme regime]"
    },
    {
      "title": "EIR: Enhanced Image Representations for Medical Report Generation",
      "authors": "Qiang Sun, Zongcheng Ji, Yinlong Xiao, Peng Chang, Jun Yu",
      "institution": "University of Science and Technology of China, PAII Inc., Beijing University of Technology",
      "link": "https://arxiv.org/pdf/2512.23185",
      "code": null,
      "tags": [
        "medical image captioning",
        "cross-modal transformer",
        "metadata fusion",
        "domain-specific pre-training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e359c65576903a65bf75a0600c08943744d6bd91d7b1f2e69d13330e289ce1_w640_q70.webp",
      "contributions": "1. Proposes a novel Enhanced Image Representations (EIR) method for medical report generation. 2. Introduces cross-modal transformers to effectively fuse medical metadata with image features, addressing the information asymmetry problem. 3. Leverages medical domain pre-trained models to encode chest X-ray images, bridging the domain gap between general and medical images.",
      "summary": "This paper addresses the problem of generating medical reports from chest X-ray images. It proposes the EIR method, which uses cross-modal transformers to fuse metadata with visual features and employs medical domain pre-trained models for better image representation. Experiments on MIMIC and Open-I datasets demonstrate the method's effectiveness.",
      "mindmap": "graph TB\n        Root[EIR: Enhanced Image Representations for Medical Report Generation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[报告生成耗时耗力/Report generation is time-consuming]\n        Problem --> P2[信息不对称与领域鸿沟/Information asymmetry & domain gap]\n        Method[主要方法/Method] --> M1[跨模态Transformer融合元数据/Cross-modal transformer for metadata fusion]\n        Method --> M2[医学领域预训练模型/Medical domain pre-trained model]\n        Results[关键结果/Results] --> R1[在MIMIC和Open-I数据集上验证/Validated on MIMIC & Open-I datasets]"
    },
    {
      "title": "Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning",
      "authors": "Zuoyou Jiang, Li Zhao, Rui Sun, Ruohan Sun, Zhongjian Li, Jing Li, Daxin Jiang, Zuo Bai, Cheng Hua",
      "institution": "Shanghai Jiao Tong University, StepFun, FinStep",
      "link": "https://arxiv.org/pdf/2512.23515",
      "code": "https://github.com/FinStep-AI/Alpha-R1",
      "tags": [
        "reinforcement learning",
        "alpha screening",
        "large language models",
        "reinforcement learning",
        "factor investing",
        "economic reasoning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d921912985e0858276fe1088914641df9c33c30f5de309733b2244c86c21e75e_w640_q70.webp",
      "contributions": "1. Proposes Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. 2. Introduces a method for LLMs to reason over factor logic and real-time news to evaluate alpha relevance under changing market conditions. 3. Demonstrates that the model consistently outperforms benchmarks and shows improved robustness to alpha decay across multiple asset pools.",
      "summary": "The paper addresses the challenge of alpha decay in non-stationary financial markets by proposing Alpha-R1, a reasoning model trained with reinforcement learning. It uses a large language model to process factor logic and news, selectively activating factors based on contextual economic relevance. Empirical results show it outperforms benchmark strategies and is more robust to signal decay.",
      "mindmap": "graph TB\n        A[Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning] --> B[核心问题/Problem: Signal decay and regime shifts in non-stationary markets; existing methods overlook semantic rationale for factor relevance.]\n        A --> C[主要方法/Method: Alpha-R1, an 8B-parameter LLM trained via RL, reasons over factor logic and real-time news for context-aware alpha screening.]\n        A --> D[关键结果/Results: Outperforms benchmark strategies; exhibits improved robustness to alpha decay across multiple asset pools.]"
    },
    {
      "title": "PINNs for Electromagnetic Wave Propagation",
      "authors": "Nilufer K. Bulut",
      "institution": "Izmir, Turkiye (Inferred from author location; no specific institution mentioned in provided content)",
      "link": "https://arxiv.org/pdf/2512.23396",
      "code": null,
      "tags": [
        "Scientific Computing / Computational Physics",
        "Physics-Informed Neural Networks (PINNs)",
        "Maxwell's Equations",
        "FDTD",
        "Time Marching",
        "Poynting Regularizer"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c5894c195aba4dd14bb55cc020098ad00e472ec3df24245a9a20ad4ca74b81c_w640_q70.webp",
      "contributions": "1. Introduces a hybrid training strategy combining time marching and causality-aware weighting to address the causality collapse problem in time-dependent PINNs. 2. Proposes a two-stage interface continuity loss to mitigate discontinuities introduced by time marching. 3. Develops a local Poynting-based regularizer to suppress cumulative energy drift and improve energy conservation.",
      "summary": "This paper addresses accuracy and energy conservation deficiencies in Physics-Informed Neural Networks (PINNs) for electromagnetic wave propagation. It proposes a hybrid training methodology incorporating time marching, causality-aware weighting, and a Poynting-based regularizer. The results show that the enhanced PINNs achieve competitive field accuracy and energy conservation compared to traditional FDTD methods, demonstrating their viability for canonical electromagnetic problems.",
      "mindmap": "graph TB\n        A[PINNs for Electromagnetic Wave Propagation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[PINNs在精度和能量守恒上落后于FDTD/PINNs lag behind FDTD in accuracy & energy]\n        C --> C1[混合训练策略/Hybrid Training Strategy]\n        C1 --> C1_1[时间推进与因果感知加权/Time Marching & Causality-Aware Weighting]\n        C1 --> C1_2[两阶段界面连续性损失/Two-Stage Interface Continuity Loss]\n        C1 --> C1_3[局部坡印廷正则化器/Local Poynting Regularizer]\n        D --> D1[高场精度/High Field Accuracy (0.09% NRMSE)]\n        D --> D2[能量守恒/Energy Conservation (0.024% mismatch)]\n        D --> D3[与FDTD结果竞争/Competitive with FDTD]"
    },
    {
      "title": "CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation",
      "authors": "Santhosh Kumar Ravindran",
      "institution": "Microsoft Corporation",
      "link": "https://arxiv.org/pdf/2512.21351",
      "code": null,
      "tags": [
        "reinforcement learning",
        "dream-replay reinforcement learning",
        "evolutionary algorithms",
        "adaptive code generation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/318e081ebd83b7b451c47feed4db9ca1fa830f70f86844ea65dc8e8551ea3656_w640_q70.webp",
      "contributions": "1. Introduces CosmoCore-Evo, an extension of CosmoCore that integrates evolutionary algorithms into the dream-replay reinforcement learning framework for code generation, 2. Proposes treating RL trajectories as \"genomes\" that undergo mutation and selection during nocturnal replay to enhance adaptability and novelty, 3. Develops enterprise-tuned fitness functions incorporating efficiency, compliance, and scalability metrics, and demonstrates improved performance on benchmarks with distribution shifts.",
      "summary": "CosmoCore-Evo enhances the affective dream-replay reinforcement learning framework by incorporating evolutionary algorithms to improve adaptability in code generation. It treats RL trajectories as genomes for mutation and selection, enabling agents to break free from trained patterns and adapt to changing environments like API updates. The method achieves higher novelty and faster adaptation compared to baselines, as validated on benchmarks including HumanEval variants and BigCodeBench.",
      "mindmap": "graph TB\n        A[CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLM代码生成缺乏适应性，难以应对API变化/LLM code generation lacks adaptability to API changes]\n        C --> C1[将RL轨迹视为基因组进行进化操作/Treat RL trajectories as genomes for evolutionary operations]\n        C --> C2[在夜间回放阶段进行突变与选择/Mutation and selection during nocturnal replay]\n        D --> D1[解决方案新颖性提升35%/35% higher novelty in solutions]\n        D --> D2[适应速度加快25%/25% faster adaptation]"
    },
    {
      "title": "EcoNet: Multiagent Planning and Control Of Household Energy Resources Using Active Inference",
      "authors": "John C. Boik, Kobus Esterhuysen, Jacqueline B. Hynes, Axel Constant, Ines Hipolito, Mahault Albarracin, Alex B. Kiefer, Karl Friston",
      "institution": "VERSES, University of Sussex, Macquarie University, UCL (University College London)",
      "link": "https://arxiv.org/pdf/2512.21343",
      "code": null,
      "tags": [
        "agent system",
        "active inference",
        "multi-agent systems",
        "home energy management systems (HEMS)",
        "distributed energy resources (DER)",
        "Bayesian inference"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b885d3c6c9f392a494063522c79cde9a59fead8ab6b04010259b6485f007cec8_w640_q70.webp",
      "contributions": "1. Proposes EcoNet, a novel Bayesian framework for household and neighborhood energy management based on active inference. 2. Addresses the challenge of planning under uncertainty (e.g., weather, solar forecasts) while handling complex, conditional, and conflicting household goals. 3. Demonstrates the approach through simulations for multiagent planning and control of distributed energy resources.",
      "summary": "This paper introduces EcoNet, a multiagent planning and control system for household energy resources using active inference, a Bayesian approach, to manage uncertainty and conflicting goals. The method aims to optimize energy use, costs, and emissions while maintaining comfort. Simulation results demonstrate its potential for improved energy management and coordination.",
      "mindmap": "graph TB\n        A[EcoNet: 多智能体家庭能源规划与控制 / EcoNet: Multiagent Household Energy Planning & Control] --> B[核心问题 / Problem]\n        A --> C[主要方法 / Method]\n        A --> D[关键结果 / Results]\n        B --> B1[复杂且冲突的家庭目标 / Complex & Conflicting Household Goals]\n        B --> B2[决策存在不确定性 / Decision-making Under Uncertainty]\n        C --> C1[基于主动推理的贝叶斯方法 / Active Inference-based Bayesian Approach]\n        C --> C2[多智能体规划与控制 / Multiagent Planning & Control]\n        D --> D1[模拟结果展示 / Simulation Results Presented]\n        D --> D2[改善能源管理与协调 / Improved Energy Management & Coordination]"
    },
    {
      "title": "Multi-Agent LLM Committees for Autonomous Software Beta Testing",
      "authors": "Sumanth Bharadwaj Hachalli Karanam, Dhiwahar Adhithya Kennady",
      "institution": "New York University",
      "link": "https://arxiv.org/pdf/2512.21352",
      "code": null,
      "tags": [
        "automated software testing",
        "multi-agent system",
        "large language model",
        "vision-language model",
        "consensus voting",
        "beta testing"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40573d0b1209c41e9825c09111398107cc51ee9d86c5234b50bea2515d0ab37f_w640_q70.webp",
      "contributions": "1. A novel multi-agent committee framework that uses a three-round voting protocol for consensus-based decision-making in software testing. 2. Integration of vision-enabled LLMs and diverse testing personas to systematically explore and understand web application user interfaces. 3. Demonstrated significant performance improvements over single-agent baselines in task success, bug detection (F1 score), and security vulnerability coverage on established benchmarks.",
      "summary": "The paper addresses the high cost of manual software beta testing and the limitations of single-agent LLM approaches by proposing a multi-agent committee framework. The method employs diverse, vision-enabled LLMs that collaborate through a structured voting protocol and persona-driven behavior to autonomously test web applications. The results show that this multi-agent approach significantly outperforms single-agent baselines in task success rates, bug detection, and security testing coverage, making it suitable for real-time CI/CD integration.",
      "mindmap": "graph TB\n        A[Multi-Agent LLM Committees for Autonomous Software Beta Testing] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[手动测试成本高，单智能体LLM存在幻觉/Manual testing costly, single-agent LLM hallucinates]\n        C --> C1[多智能体委员会与三轮投票协议/Multi-agent committee & three-round voting]\n        C --> C2[视觉LLM与角色多样性/Vision LLMs & persona diversity]\n        D --> D1[任务成功率89.5%，超越基线/Task success 89.5%, beats baseline]\n        D --> D2[动作延迟0.71秒，适合CI/CD/Action latency 0.71s, suitable for CI/CD]\n        D --> D3[覆盖8/10 OWASP漏洞类别/Covers 8/10 OWASP Top 10]"
    },
    {
      "title": "Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software",
      "authors": "Ying Xiao, Shangwen Wang, Sicen Liu, Dingyuan Xue, Xian Zhan, Yepang Liu, Jie M. Zhang",
      "institution": "King’s College London, National University of Defense Technology, Southern University of Science and Technology, The Hong Kong Polytechnic University",
      "link": "https://arxiv.org/pdf/2512.21348",
      "code": null,
      "tags": [
        "software fairness",
        "correlation tuning",
        "phi-coefficient",
        "multi-objective optimization",
        "pre-processing",
        "bias mitigation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4f80681a9ac6a6c3ad7d2bd938623a06836acba00279d9cec368a5ebbe44df3_w640_q70.webp",
      "contributions": "1. Proposes a novel pre-processing bias mitigation method called Correlation Tuning (CoT) that adjusts data correlations. 2. Introduces the Phi-coefficient as an intuitive measure to quantify correlation between sensitive attributes and labels. 3. Employs multi-objective optimization to address proxy biases, demonstrating superior effectiveness over state-of-the-art methods in single and multiple attribute scenarios.",
      "summary": "This paper proposes Correlation Tuning (CoT), a novel pre-processing method to mitigate bias in ML software by adjusting data correlations using the Phi-coefficient and multi-objective optimization. It frames fairness as a core software quality issue. Extensive evaluation shows CoT significantly improves performance for unprivileged groups and reduces key bias metrics, outperforming existing methods.",
      "mindmap": "graph TB\n        A[Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[传统公平研究忽视软件质量维度/Traditional fairness research neglects software quality dimension]\n        B --> B2[预处理方法效果不足/Pre-processing methods lack effectiveness]\n        C --> C1[提出相关性调优 (CoT)/Propose Correlation Tuning (CoT)]\n        C --> C2[使用Phi系数量化相关性/Use Phi-coefficient to quantify correlation]\n        C --> C3[采用多目标优化/Employ multi-objective optimization]\n        D --> D1[提高弱势群体TPR 17.5%/Increase unprivileged group TPR by 17.5%]\n        D --> D2[关键偏差指标降低 >50%/Key bias metrics reduced by >50%]\n        D --> D3[超越SOTA方法 3-10个百分点/Outperform SOTA by 3-10 percentage points]"
    },
    {
      "title": "Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks",
      "authors": "Jasmin Saxer, Isabella Maria Aigner, Luise Linzmeier, Andreas Weiler, Kurt Stockinger",
      "institution": "Zurich University of Applied Sciences, University of Zurich",
      "link": "https://arxiv.org/pdf/2512.21345",
      "code": null,
      "tags": [
        "text-to-SQL",
        "unanswerable question detection",
        "few-shot prompting",
        "biomedical databases"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d95c00b7fa86810771a1c8fb0ff6fd8768baaa0419f172cc5c7a3068ac67a64_w640_q70.webp",
      "contributions": "1. Proposed Query Carefully, a pipeline integrating LLM-based SQL generation with explicit detection of unanswerable inputs. 2. Constructed OncoMX-NAQ, a benchmark dataset of 80 no-answer questions for biomedical text-to-SQL. 3. Demonstrated that balanced few-shot prompting with both answerable and unanswerable examples achieves high unanswerable-detection accuracy without degrading performance on answerable queries.",
      "summary": "This paper addresses the risk of text-to-SQL systems generating executable but incorrect SQL for ambiguous or unanswerable queries, especially in biomedical contexts. The authors propose the Query Carefully pipeline, which uses an LLM with schema-aware prompts and few-shot examples to detect and abstain from unanswerable inputs. Their evaluation shows the method achieves high detection accuracy for structurally unanswerable queries, though challenges remain for semantic ambiguities like missing values.",
      "mindmap": "graph TB\n        Root(”Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks”) --> Problem\n        Root --> Method\n        Root --> Results\n        Problem(”核心问题/Problem”) --> P1(”Text-to-SQL对不可回答查询生成可执行SQL/Text-to-SQL generates executable SQL for unanswerable queries”)\n        P1 --> P2(”生物医学领域风险高/High risk in biomedical contexts”)\n        Method(”主要方法/Method”) --> M1(”Query Carefully 管道/Query Carefully pipeline”)\n        M1 --> M2(”LLM (llama3.3:70b) + 模式感知提示 + 少样本/LLM (llama3.3:70b) + schema-aware prompts + few-shot”)\n        M2 --> M3(”包含可回答与不可回答示例/Includes answerable and unanswerable examples”)\n        Results(”关键结果/Results”) --> R1(”构建OncoMX-NAQ基准/Built OncoMX-NAQ benchmark”)\n        R1 --> R2(”不可回答检测准确率0.8/Unanswerable-detection accuracy 0.8”)\n        R2 --> R3(”结构性问题检测好，语义模糊挑战大/Good for structural, challenging for semantic ambiguity”)"
    },
    {
      "title": "A Study of Solving Life-and-Death Problems in Go Using Relevance-Zone Based Solvers",
      "authors": "Chung-Chin Shih, Ti-Rong Wu, Ting Han Wei, Yu-Shan Hsu, Hung Guei, I-Chen Wu",
      "institution": "Academia Sinica, National Yang Ming Chiao Tung University, Kochi University of Technology",
      "link": "https://arxiv.org/pdf/2512.21365",
      "code": "https://rlg.iis.sinica.edu.tw/papers/study-LD-RZ",
      "tags": [
        "reinforcement learning",
        "Relevance-Zone Based Search",
        "AlphaZero",
        "Life-and-Death problems",
        "heuristic search",
        "pattern table"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6376caf4e3dced23991e86eb4d5b0f512ce3623019adeaf18ee253d5fd00c507_w640_q70.webp",
      "contributions": "1. Applied and analyzed Relevance-Zone Based Search (RZS) and relevance-zone pattern tables to solve Go Life-and-Death problems, identifying critical relevance-zones. 2. Discovered that solvers can find rare patterns and even alternative solutions differing from established human grandmaster answers. 3. Identified and analyzed key limitations of current solvers, such as misjudging rare patterns and prioritizing direct survival over territory maximization.",
      "summary": "This paper analyzes the performance of state-of-the-art computer Go solvers using Relevance-Zone Based Search on classic Life-and-Death problems. The study finds that while these solvers can identify critical areas and discover rare patterns, they exhibit limitations like misjudging pattern values and having a non-human preference for direct survival over territory. The authors suggest future approaches to address these solver issues.",
      "mindmap": "graph TB\n        A[”A Study of Solving Life-and-Death Problems in Go Using Relevance-Zone Based Solvers<br/>使用基于相关区域求解器解决围棋死活问题的研究”] --> B[”核心问题/Problem<br/>Analyzing solver behavior on Go Life-and-Death problems<br/>分析求解器在围棋死活问题上的行为”]\n        A --> C[”主要方法/Method<br/>Using Relevance-Zone Based Search (RZS) and pattern tables<br/>使用基于相关区域的搜索和模式表”]\n        A --> D[”关键结果/Results<br/>Identifies relevance-zones, finds rare/alternative solutions, reveals solver limitations<br/>识别相关区域，发现罕见/替代解法，揭示求解器局限”]"
    },
    {
      "title": "From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration",
      "authors": "Shuide Wen, Yu Sun, Beier Ku, Zhi Gao, Lijun Ma, Yang Yang, Can Jiao",
      "institution": "Tsinghua University, Shenzhen University, University of Oxford, Guangzhou University of Chinese Medicine, Harbin Institute of Technology, Shenzhen Institute of Education Sciences",
      "link": "https://arxiv.org/pdf/2512.21360",
      "code": null,
      "tags": [
        "computational psychology",
        "multimodal large language model",
        "multi-agent collaboration",
        "cosine similarity",
        "projective assessment",
        "psychological report generation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/681955eb18a63880e0327c5debb6e992188de12ca52cef0c9c34258f09c3a91d_w640_q70.webp",
      "contributions": "1. Proposed a novel multi-agent collaboration framework to automate the interpretation of House-Tree-Person drawings, decoupling visual feature recognition from psychological inference. 2. Demonstrated that multimodal large language models (MLLMs) can achieve expert-level baseline comprehension in interpreting projective drawings, with high semantic similarity to human expert interpretations. 3. Introduced a destigmatizing narrative and social-psychological perspective integration to correct visual hallucinations and enhance the ecological validity and coherence of automated psychological reports.",
      "summary": "This paper proposes an automated assessment framework for the House-Tree-Person drawing test using multimodal LLMs and multi-agent collaboration to address issues of subjective scoring and lack of standardization. The framework effectively interprets drawings with high similarity to expert analysis and generates coherent psychological reports. The results confirm the potential of multimodal models as standardized tools for projective psychological assessment.",
      "mindmap": "graph TB\n        A[From Visual Perception to Deep Empathy: An Automated Assessment Framework for House-Tree-Person Drawings Using Multimodal LLMs and Multi-Agent Collaboration<br>从视觉感知到深度共情：基于多模态大模型与多智能体协作的房树人绘画自动评估框架] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>HTP测试评分标准不一，依赖主观经验，缺乏统一量化系统] --> B1\n        C[主要方法/Method<br>多模态大语言模型与多智能体协作框架] --> C1\n        D[关键结果/Results<br>模型解释与专家解释语义相似度高，多智能体系统生成有效心理报告] --> D1\n        B1[HTP test has heterogeneous scoring, relies on subjective experience, lacks unified quantitative coding]\n        C1[Multimodal LLMs and multi-agent collaboration framework]\n        D1[High semantic similarity to expert interpretations; multi-agent system produces reports with high ecological validity]"
    },
    {
      "title": "Reflection-Driven Control for Trustworthy Code Agents",
      "authors": "Bin Wang, Jiazheng Quan, Xingrui Yu, Hansen Hu, Yuhao, Ivor Tsang",
      "institution": "Peking University, Xiamen University, Agency for Science, Technology and Research (A*STAR)",
      "link": "https://arxiv.org/pdf/2512.21354",
      "code": null,
      "tags": [
        "agent system",
        "reflection-driven control",
        "secure code generation",
        "trustworthy agents",
        "reflective memory",
        "safety control"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5126773543627efe84c972810f76eb0631192d8d90ed930bbc91d54b6664007b_w640_q70.webp",
      "contributions": "1. Introduces Reflection-Driven Control, a standardized and pluggable control module that integrates self-reflection as an explicit, internal step in an agent's reasoning process. 2. Instantiates the method for secure code generation, using a reflection loop to monitor decisions and retrieve repair examples/guidelines from an evolving reflective memory to inject constraints. 3. Empirically demonstrates that the approach substantially improves security and policy compliance of generated code while preserving functional correctness, with minimal overhead.",
      "summary": "The paper addresses the lack of reliable safety controls in LLM agents by proposing Reflection-Driven Control, a module that makes self-reflection an explicit, continuous part of the agent's reasoning to monitor and constrain its decisions using evidence from a reflective memory. Evaluated on security-critical code generation tasks, the method significantly improves code security and compliance while maintaining functionality, offering a practical path toward trustworthy AI coding agents.",
      "mindmap": "graph TB\n        Root[Reflection-Driven Control for Trustworthy Code Agents] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[LLM代理缺乏可靠的安全控制/LLM agents lack reliable safety controls]\n        Problem --> P2[可能产生有害输出/Can produce harmful outputs]\n        Method --> M1[将自我反思作为推理的显式步骤/Elevates self-reflection to an explicit reasoning step]\n        Method --> M2[内部反思循环监控决策路径/Internal reflection loop monitors decision path]\n        Method --> M3[从反思记忆中检索修复示例/Retrieves repair examples from reflective memory]\n        Results --> R1[显著提高生成代码的安全性和合规性/Substantially improves security & policy compliance]\n        Results --> R2[基本保持功能正确性/Largely preserves functional correctness]\n        Results --> R3[运行时和token开销最小/Minimal runtime & token overhead]"
    },
    {
      "title": "AInsteinBench: Benchmarking Coding Agents on Scientific Repositories",
      "authors": "Titouan Duston, Shuo Xin, Yang Sun, Daoguang Zan, Aoyan Li, Shulin Xin, Kai Shen, Yixiao Chen, Qiming Sun, Ge Zhang, Jiashuo Liu, Huan Zhou, Jingkai Liu, Zhichen Pu, Yuanheng Wang, Bo-Xuan Ge, Xin Tong, Fei Ye, Zhi-Chao Zhao, Wen-Biao Han, Zhoujian Cao, Yueran Zhao, Weiluo Ren, Qingshen Long, Yuxiao Liu, Anni Huang, Yidi Du, Yuanyuan Rong, Jiahao Peng",
      "institution": "ByteDance Seed, Princeton University",
      "link": "https://arxiv.org/pdf/2512.21373",
      "code": "https://github.com/ByteDance-Seed/AInsteinBench",
      "tags": [
        "software engineering",
        "benchmark",
        "scientific computing",
        "code generation",
        "pull requests",
        "test-driven verification"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aadf07b453d8d5a061a247b4c4e5e4fc27a43f5b1ffca131e81738bd3728f348_w640_q70.webp",
      "contributions": "1. Introduces a novel benchmark (AInsteinBench) for evaluating LLM agents in end-to-end scientific development using real-world, production-grade codebases. 2. Curates tasks from maintainer-authored pull requests across six diverse scientific domains, ensuring scientific challenge and calibrated difficulty. 3. Employs executable environments and test-driven verification to measure core competencies beyond surface-level code generation.",
      "summary": "The paper introduces AInsteinBench, a benchmark designed to evaluate LLM agents' ability to function as scientific computing developers by solving tasks derived from real pull requests in scientific repositories. It uses executable environments and test-driven verification to assess deeper competencies. The benchmark provides a new standard for measuring AI's role in computational scientific research.",
      "mindmap": "graph TB\n        A[AInsteinBench: Benchmarking Coding Agents on Scientific Repositories] --> B[核心问题/Problem: Can LLM agents operate as scientific computing development agents?]\n        A --> C[主要方法/Method: End-to-end evaluation using tasks from real scientific pull requests]\n        A --> D[关键结果/Results: Measures ability beyond surface-level code generation]"
    },
    {
      "title": "Safe Path Planning and Observation Quality Enhancement Strategy for Unmanned Aerial Vehicles in Water Quality Monitoring Tasks",
      "authors": "Yuanshuang Fu, Qianyao Wang, Qihao Wang, Bonan Zhang, Jiaxin Zhao, Yiming Cao, Zhijun Li",
      "institution": "University of Electronic Science and Technology of China, North China University of Technology",
      "link": "https://arxiv.org/pdf/2512.21375",
      "code": null,
      "tags": [
        "robotic perception and planning",
        "Interfered Fluid Dynamical System (IFDS)",
        "Model Predictive Control (MPC)",
        "Dynamic Flight Altitude Adjustment (DFAA)"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5582bc8dd27c45953b75b142df4da9d25f5164a9ed81f8842a846572fbb8a2f_w640_q70.webp",
      "contributions": "1. Proposes a dynamic prediction model that transforms time-varying light and shadow disturbances (e.g., sun glint) into 3D virtual obstacles for path planning. 2. Introduces an improved IFDS algorithm combined with an MPC framework to generate smooth, safe, and dynamically feasible real-time trajectories for UAVs. 3. Designs a Dynamic Flight Altitude Adjustment (DFAA) mechanism to actively lower flight altitude in narrow observable areas, enhancing spatial resolution and data quality.",
      "summary": "This paper addresses the problem of UAV water quality monitoring being hindered by dynamic illumination disturbances like shadows and sun glint, which degrade spectral data. The proposed method actively plans safe flight paths by modeling disturbances as obstacles, using an improved IFDS and MPC for real-time trajectory optimization, and dynamically adjusting altitude to improve data quality. Simulation results show the method achieves a 98% obstacle avoidance success rate and increases effective observation data volume by approximately 27%.",
      "mindmap": "graph TB\n    A[Safe Path Planning and Observation Quality Enhancement Strategy for UAVs in Water Quality Monitoring Tasks] --> B\n    A --> C\n    A --> D\n    B[核心问题/Problem<br>Dynamic illumination disturbances (shadows, sun glint) cause spectral distortion, reducing data quality and safety.]\n    C[主要方法/Method<br>1. Model disturbances as 3D virtual obstacles.<br>2. Improved IFDS + MPC for real-time path planning.<br>3. Dynamic Flight Altitude Adjustment (DFAA).]\n    D[关键结果/Results<br>98% obstacle avoidance success rate, improved path smoothness, ~27% increase in effective observation data.]"
    },
    {
      "title": "LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors",
      "authors": "Tianwei Lan, Farid Naït-Abdesselam",
      "institution": "Université Paris Cité",
      "link": "https://arxiv.org/pdf/2512.21404",
      "code": null,
      "tags": [
        "adversarial attacks",
        "adversarial attack",
        "large language model",
        "retrieval-augmented generation",
        "Android malware detection",
        "adversarial training"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6061210b194ba5cf79f70b8959faa3abe6b3e91ffad512d9cfd319de948593bb_w640_q70.webp",
      "contributions": "1. Proposes LAMLAD, a novel adversarial attack framework that uses a dual-agent LLM architecture (manipulator and analyzer) to generate feature-level perturbations for evading Android malware detectors., 2. Integrates Retrieval-Augmented Generation (RAG) into the LLM pipeline to improve the efficiency and contextual awareness of the attack., 3. Proposes and evaluates an adversarial training-based defense strategy to enhance model robustness against the proposed LAMLAD-style attacks.",
      "summary": "This paper proposes LAMLAD, a novel adversarial attack framework that leverages the generative and reasoning capabilities of Large Language Models (LLMs) to bypass ML-based Android malware classifiers. The method uses a dual-agent LLM architecture with RAG to generate realistic, functionality-preserving feature perturbations, achieving a high attack success rate. The paper also demonstrates that adversarial training can significantly reduce the effectiveness of such attacks, enhancing model robustness.",
      "mindmap": "graph TB\n        A[LLM-Driven Feature-Level Adversarial Attacks on Android Malware Detectors] --> B[核心问题/Problem: ML-based Android malware detectors are vulnerable to adversarial attacks.]\n        A --> C[主要方法/Method: Proposes LAMLAD, a dual-agent LLM framework with RAG for generating stealthy perturbations.]\n        A --> D[关键结果/Results: Achieves up to 97% attack success rate; adversarial training defense reduces ASR by >30%.]"
    },
    {
      "title": "Feasible strategies in three-way conflict analysis with three-valued ratings",
      "authors": "Jing Liu, Mengjun Hu, Guangming Lang",
      "institution": "Changsha University of Science and Technology, Saint Mary's University",
      "link": "https://arxiv.org/pdf/2512.21420",
      "code": null,
      "tags": [
        "conflict analysis",
        "three-way conflict analysis",
        "feasible strategy",
        "consistency measure",
        "non-consistency measure",
        "weighted agent-issue evaluation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4b2e48e756e98608f4388b841aa7940f0ba7b237737fa314abc4db83fbf680f_w640_q70.webp",
      "contributions": "1. Proposes a novel framework for identifying feasible strategies in conflict resolution from the perspectives of consistency and non-consistency. 2. Introduces weighted consistency and non-consistency measures that incorporate the importance of both agents and issues. 3. Develops algorithms to systematically identify feasible strategies, L-order feasible strategies, and optimal solutions, demonstrating superior performance over existing approaches.",
      "summary": "This paper addresses the gap in formulating actionable strategies for conflict resolution within three-way conflict analysis. It proposes a method that computes agent clique ratings and uses novel weighted consistency and non-consistency measures to identify feasible and optimal strategies. The approach is validated through case studies and shown to outperform conventional conflict analysis models.",
      "mindmap": "graph TB\n        A[Feasible strategies in three-way conflict analysis<br>三向冲突分析中的可行策略] --> B(Problem: Lack of focus on conflict resolution strategies<br>问题: 缺乏对冲突解决策略的关注)\n        A --> C(Method: Weighted consistency/non-consistency measures & algorithms<br>方法: 加权一致/非一致性度量与算法)\n        A --> D(Results: Outperforms conventional approaches, identifies optimal solutions<br>结果: 优于传统方法，识别最优解)"
    },
    {
      "title": "Three-way conflict analysis based on alliance and conflict functions",
      "authors": "Junfang Luo, Mengjun Hu, Guangming Lang, Xin Yang, Keyun Qin",
      "institution": "Southwestern University of Finance and Economics, University of Regina, Changsha University of Science and Technology, Southwest Jiaotong University",
      "link": "https://arxiv.org/pdf/2512.21419",
      "code": null,
      "tags": [
        "decision theory",
        "three-way decision",
        "conflict analysis",
        "alliance function",
        "conflict function",
        "alliance set"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54277009925f58600c765060d6cbc575e96e562e3c9748aa2f54e97b83024e0b_w640_q70.webp",
      "contributions": "1. Proposes a novel separation of the traditional auxiliary function into distinct alliance and conflict functions to clarify semantic interpretation in conflict analysis. 2. Introduces a framework for trisecting agents, issues, and agent pairs based on the new alliance and conflict functions. 3. Explores and applies new concepts such as alliance sets and strategies to solve crucial questions in conflict analysis, demonstrating the model with a real-world application.",
      "summary": "This paper addresses the semantic ambiguity in aggregating traditional three-way conflict analysis functions by proposing a separation into distinct alliance and conflict functions. The method enables clearer trisection of agents, issues, and agent pairs, leading to the exploration of alliance sets and strategies. The main conclusion is that this separation provides a more interpretable and applicable framework for conflict analysis, as illustrated by a real-world example.",
      "mindmap": "graph TB\n        A[Three-way Conflict Analysis Based on Alliance and Conflict Functions] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统辅助函数聚合语义模糊/Semantic ambiguity in aggregating traditional auxiliary functions]\n        C --> C1[分离为联盟与冲突函数/Separate into alliance and conflict functions]\n        C --> C2[基于新函数进行三分/Trisec based on new functions]\n        D --> D1[提出联盟集与策略概念/Propose alliance sets and strategies]\n        D --> D2[提供真实应用案例/Provide a real-world application]"
    },
    {
      "title": "Teaching People LLM's Errors and Getting it Right",
      "authors": "Nathan Stringham, Fateme Hashemi Chaleshtori, Xinyuan Yan, Zhichao Xu, Bei Wang, Ana Marasović",
      "institution": "University of Utah",
      "link": "https://arxiv.org/pdf/2512.21422",
      "code": null,
      "tags": [
        "human-ai interaction",
        "overreliance",
        "failure patterns",
        "mental models",
        "user study",
        "meta-labels"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83a262b8daf44fdf951904b9202074fd9db4ef9e9666cd5769ec1d8514053804_w640_q70.webp",
      "contributions": "1. Empirically demonstrated that failure patterns for LLMs do exist by identifying sizable, error-prone meta-label groups in datasets, countering the hypothesis that their absence caused prior teaching failures. 2. Evaluated automated methods for discovering these failure patterns (prompting and embedding-based) and found mixed results, identifying a key bottleneck in the teaching pipeline. 3. Proposed and validated a new metric for teaching effectiveness—assessing a user's ability to anticipate LLM errors using taught patterns—which showed a positive effect, unlike traditional human-AI team accuracy.",
      "summary": "This paper investigates why prior attempts to teach users about LLM failure patterns to reduce overreliance have failed. It finds that failure patterns do exist, but automated methods to discover them are unreliable, and proposes a new user-centric evaluation metric that shows teaching can be effective. The conclusion is that teaching failure patterns is viable but requires better failure-discovery methods and appropriate metrics.",
      "mindmap": "graph TB\n        A[Teaching People LLM’s Errors and Getting it Right] --> B[核心问题/Problem: Users overrely on LLMs due to inaccurate mental models]\n        A --> C[主要方法/Method: Analyze failure pattern existence, test discovery methods, propose new evaluation metric]\n        A --> D[关键结果/Results: Patterns exist, discovery methods are mixed, new metric shows teaching is effective]"
    },
    {
      "title": "Three-way decision with incomplete information based on similarity and satisfiability",
      "authors": "Junfang Luo, Mengjun Hu, Keyun Qin",
      "institution": "Southwest Jiaotong University, University of Regina",
      "link": "https://arxiv.org/pdf/2512.21421",
      "code": null,
      "tags": [
        "rough set theory",
        "three-way decision",
        "incomplete information",
        "similarity degree",
        "satisfiability degree",
        "approximability"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d87784acc1b1cc397b822153c118be23974be9721c3d19c9dfc95fbbaef158e_w640_q70.webp",
      "contributions": "1. Proposes a new measure of similarity degree of objects as a generalization of equivalence relations for handling incomplete information in the computational formulation of three-way decision. 2. Introduces a measure of satisfiability degree of formulas as a quantitative generalization of satisfiability for the conceptual formulation of three-way decision under incomplete information. 3. Proposes novel approaches for three-way decision using approximability of objects and confidence of formulas, pointing out new research directions beyond the common method of similarity classes.",
      "summary": "This paper generalizes the computational and conceptual formulations of three-way decision to handle incomplete information, which is common in real-world applications. For the computational side, it introduces a similarity degree measure and explores decision-making via α-similarity classes and approximability; for the conceptual side, it proposes a satisfiability degree measure and studies approaches using α-meaning sets and confidence. The work extends rough set theory and identifies promising new directions for three-way decision under uncertainty.",
      "mindmap": "graph TB\n        A[Three-Way Decision with Incomplete Information Based on Similarity and Satisfiability] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[处理不完全信息/Handling Incomplete Information]\n        C --> C1[计算式: 相似度/Computational: Similarity Degree]\n        C --> C2[概念式: 可满足度/Conceptual: Satisfiability Degree]\n        C1 --> C1a[α-相似类/α-Similarity Classes]\n        C1 --> C1b[可逼近性/Approximability]\n        C2 --> C2a[α-意义集/α-Meaning Sets]\n        C2 --> C2b[置信度/Confidence]\n        D --> D1[推广两种表述/Generalizes Both Formulations]\n        D --> D2[指出新方向/Points to New Directions]"
    },
    {
      "title": "Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models",
      "authors": "Geoffroy Morlat, Marceau Nahon, Augustin Chartouny, Raja Chatila, Ismael T. Freire, Mehdi Khamassi",
      "institution": "Institute of Intelligent Systems and Robotics, Sorbonne University",
      "link": "https://arxiv.org/pdf/2512.21439",
      "code": null,
      "tags": [
        "computational ethics",
        "moral context",
        "probabilistic clustering",
        "LLM semantics",
        "interpretable prediction",
        "human judgment"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25f4abc9f666c2d29dadd77869bddf3f159d0bbc8839c7c0f65bbdb4c29ad40c_w640_q70.webp",
      "contributions": "1. An empirically grounded dataset of 300 moral scenarios with human ternary judgments. 2. A reproducible pipeline (COMETH) combining human judgments, probabilistic context learning, and LLM-based semantic abstraction. 3. An interpretable, context-sensitive moral prediction model that outperforms end-to-end LLM prompting.",
      "summary": "The paper addresses the problem that moral judgments depend heavily on context. It proposes the COMETH framework, which uses probabilistic clustering on human judgment data and LLM-based semantic abstraction to learn and explain action-specific moral contexts. The main conclusion is that COMETH significantly outperforms direct LLM prompting in aligning with human majority judgments while providing interpretable predictions.",
      "mindmap": "graph TB\n        A[COMETH: Learning Interpretable Moral Contexts] --> B[核心问题/Problem: Moral judgments are context-dependent]\n        A --> C[主要方法/Method: Probabilistic clustering + LLM semantics + Human judgments]\n        A --> D[关键结果/Results: Doubles alignment with human judgments vs. LLM prompting]"
    },
    {
      "title": "dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning",
      "authors": "Shirui Chen, Jiantao Jiao, Lillian J. Ratliff, Banghua Zhu",
      "institution": "University of Washington, University of California, Berkeley",
      "link": "https://arxiv.org/pdf/2512.21446",
      "code": null,
      "tags": [
        "diffusion models",
        "masked diffusion language models",
        "reinforcement learning",
        "parallel decoding",
        "on-policy optimization",
        "unmasking planner"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5f1e67e2dde4b6b9e98e4e3c5574326f0e2d63114afe47049e17c2ae04bb41b_w640_q70.webp",
      "contributions": "1. Proposes dUltra, an on-policy RL framework (GRPO-based) for learning efficient unmasking strategies in MDLMs. 2. Introduces a joint optimization scheme for the base diffusion model and a new unmasking planner head using a composite reward. 3. Demonstrates improved accuracy-efficiency trade-off over heuristic and distillation baselines in reasoning and code generation tasks.",
      "summary": "The paper addresses the slow sampling speed of masked diffusion language models (MDLMs) by proposing dUltra, a reinforcement learning framework that learns an optimal strategy for parallel token unmasking. The method jointly optimizes the diffusion model and a planner head using rewards for correctness, distillation, and step count. The results show dUltra achieves a better trade-off between accuracy and efficiency than existing methods, advancing towards \"diffusion supremacy\" over autoregressive models.",
      "mindmap": "graph TB\n        A[dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[MDLMs解码慢，速度优势有限/MDLMs decode slowly, limiting speed advantage]\n        C --> C1[基于GRPO的在线强化学习框架/On-policy RL framework based on GRPO]\n        C --> C2[联合优化扩散模型与解掩码规划器/Jointly optimize diffusion model & unmasking planner]\n        D --> D1[提升精度-效率权衡/Improves accuracy-efficiency trade-off]\n        D --> D2[迈向”扩散霸权”/Moving towards ”diffusion supremacy”]"
    },
    {
      "title": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism",
      "authors": "Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang",
      "institution": "Harbin Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.21452",
      "code": null,
      "tags": [
        "object detection",
        "Ground Penetrating Radar (GPR)",
        "Multi-modal Chain Feature Fusion (MCFF)",
        "Global Attention Mechanism (GAM)",
        "DCGAN",
        "transfer learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp",
      "contributions": "1. Proposed a DCGAN-based data augmentation strategy to synthesize high-fidelity GPR images, mitigating data scarcity. 2. Designed a novel Multi-modal Chain and Global Attention Network (MCGA-Net) integrating Multi-modal Chain Feature Fusion (MCFF) and a Global Attention Mechanism (GAM) for enhanced defect representation. 3. Utilized MS COCO transfer learning to fine-tune the backbone network, accelerating convergence and improving model generalization.",
      "summary": "This paper addresses the subjective and inefficient interpretation of Ground Penetrating Radar (GPR) images for road defect detection by proposing a comprehensive framework. The method combines DCGAN-based data augmentation, a novel MCGA-Net architecture with feature fusion and attention mechanisms, and transfer learning. The proposed model achieves high precision, recall, and robustness, establishing a new paradigm for automated GPR-based defect detection.",
      "mindmap": "graph TB\n        Root(”Intelligent recognition of GPR road hidden defect images <br/> GPR道路隐蔽病害图像智能识别”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n    \n        Problem --> P1(”Subjective & inefficient GPR interpretation <br/> GPR图像解释主观且低效”)\n        Problem --> P2(”Data scarcity <br/> 数据稀缺”)\n    \n        Method --> M1(”DCGAN-based Data Augmentation <br/> 基于DCGAN的数据增强”)\n        Method --> M2(”MCGA-Net (MCFF + GAM) <br/> MCGA-Net网络”)\n        Method --> M3(”MS COCO Transfer Learning <br/> MS COCO迁移学习”)\n    \n        Results --> R1(”High Performance (Precision 92.8%, mAP@50 95.9%) <br/> 高性能”)\n        Results --> R2(”Robust to noise & weak signals <br/> 对噪声和弱信号鲁棒”)\n        Results --> R3(”New paradigm for automated detection <br/> 自动化检测新范式”)"
    },
    {
      "title": "GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification",
      "authors": "Suncheng Xiang, Xiaoyang Wang, Junjie Jiang, Hejia Wang, Dahong Qian",
      "institution": "Shanghai Jiao Tong University, Peking University, Shanghai Fifth People's Hospital",
      "link": "https://arxiv.org/pdf/2512.21476",
      "code": "https://github.com/JeremyXSC/GPF-Net",
      "tags": [
        "medical image retrieval",
        "polyp re-identification",
        "gated progressive fusion",
        "multimodal feature fusion"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp",
      "contributions": "1) Proposes a novel multimodal feature fusion framework named GPF-Net for polyp re-identification. 2) Introduces a gated progressive fusion strategy for layer-wise refinement of semantic information through multi-level feature interactions. 3) Demonstrates state-of-the-art performance on standard benchmarks, showing the benefit of multimodal fusion over unimodal methods.",
      "summary": "This paper addresses the challenge of colonoscopic polyp re-identification, where coarse high-level features harm small object matching. The authors propose GPF-Net, a Gated Progressive Fusion network that selectively fuses multi-level features using gates. Experiments show this multimodal approach outperforms state-of-the-art unimodal ReID models.",
      "mindmap": "graph TB\n        A[GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification] --> B[核心问题/Problem: Coarse high-level features lead to inferior results for small polyps]\n        A --> C[主要方法/Method: Gated Progressive Fusion network for selective, multi-level feature fusion]\n        A --> D[关键结果/Results: Outperforms unimodal models, benefits of multimodal fusion strategy]"
    },
    {
      "title": "Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism",
      "authors": "Xinglin Pan, Shaohuai Shi, Wenxiang Lin, Yuxin Wang, Zhenheng Tang, Wei Wang, Xiaowen Chu",
      "institution": "The Hong Kong University of Science and Technology (Guangzhou), Harbin Institute of Technology (Shenzhen), Hong Kong Baptist University, The Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.21487",
      "code": null,
      "tags": [
        "llm inference",
        "mixture-of-experts (MoE)",
        "disaggregated expert parallelism (DEP)",
        "task scheduling",
        "inference throughput",
        "fine-grained pipelining"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44cc55e59c66470ffb4e47c93ad8e48f60e8377f30eff6289fbad1cfcb862c96_w640_q70.webp",
      "contributions": "1) Partitioning intensive computation and communication tasks into smaller, fine-grained tasks to enable pipelining, including support for shared experts. 2) Formulating a fine-grained task scheduling optimization problem that supports variable task granularity and ordering. 3) Developing an efficient solver to navigate the large solution space and derive a near-optimal task schedule.",
      "summary": "This paper addresses the memory-intensive inference problem in Mixture-of-Experts (MoE) models by proposing FinDEP, a fine-grained task scheduling algorithm for Disaggregated Expert Parallelism (DEP). FinDEP improves inference throughput by maximizing task overlap through computational partitioning and optimized scheduling. Experiments on systems with up to 32 GPUs show throughput improvements of up to 1.61x over prior methods.",
      "mindmap": "graph TB\n        A[FinDEP: Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[MoE推理内存密集，现有DEP调度效率低/MoE inference is memory-intensive, existing DEP scheduling is inefficient]\n        C --> C1[细粒度任务划分与调度优化/Fine-grained task partitioning and scheduling optimization]\n        D --> D1[吞吐量最高提升1.61倍/Throughput improved by up to 1.61x]"
    },
    {
      "title": "Oogiri-Master: Benchmarking Humor Understanding via Oogiri",
      "authors": "Soichiro Murakami, Hidetaka Kamigaito, Hiroya Takamura, Manabu Okumura",
      "institution": "CyberAgent, Nara Institute of Science and Technology, Institute of Science Tokyo",
      "link": "https://arxiv.org/pdf/2512.21494",
      "code": null,
      "tags": [
        "humor understanding",
        "Oogiri",
        "benchmark",
        "linguistic analysis",
        "incongruity resolution",
        "insight-augmented prompting"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41486d2e77493633c6cf66d7f5134ccf646d1df0d17e6d258bc98cc3132ef02b_w640_q70.webp",
      "contributions": "1. Introduces Oogiri-Master, a benchmark for rigorous evaluation of humor understanding in LLMs, and Oogiri-Corpus, a dataset with ~100 diverse responses per prompt and independent human ratings to reduce bias. 2. Conducts quantitative analysis of linguistic factors (e.g., text length, ambiguity, incongruity resolution) to derive objective metrics for predicting human funniness judgments. 3. Benchmarks LLMs and human baselines, showing state-of-the-art models approach human performance and that insight-augmented prompting improves model humor understanding.",
      "summary": "This paper addresses the challenge of evaluating humor understanding in LLMs by introducing the Oogiri-Master benchmark and Oogiri-Corpus dataset, which enable rigorous analysis of funniness through diverse responses and independent human ratings. It quantitatively analyzes linguistic factors to derive objective metrics and benchmarks LLMs, demonstrating that advanced models approach human-level performance and benefit from insight-augmented prompting. The work provides a principled basis for advancing humor understanding in AI.",
      "mindmap": "graph TB\n        A[Oogiri-Master: Benchmarking Humor Understanding via Oogiri] --> B[核心问题/Problem: What makes Oogiri responses funny to humans?]\n        A --> C[主要方法/Method: Introduce Oogiri-Master benchmark and Oogiri-Corpus dataset with diverse responses and independent ratings]\n        A --> D[关键结果/Results: LLMs approach human performance; insight-augmented prompting improves results]"
    },
    {
      "title": "LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis",
      "authors": "Fanwei Zeng, Changtao Miao, Jing Huang, Zhiya Tan, Shutao Gong, Xiaoming Yu, Yang Wang, Huazhe Tan, Weibin Yao, Jianshu Li",
      "institution": "Ant Group, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.21482",
      "code": null,
      "tags": [
        "multimodal forgery detection",
        "visual-textual co-reasoning",
        "cross-cues-aware chain of thought (CCT)",
        "GRPO-based optimization"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/138f8fcb4727c77950b23146e1184851aa8b8cea95056b1d6b161c37e231ad80_w640_q70.webp",
      "contributions": "1. Proposed LogicLens, a unified framework for visual-textual co-reasoning that jointly performs detection, grounding, and explanation for text-centric forgery analysis. 2. Introduced a Cross-Cues-aware Chain of Thought (CCT) mechanism for iterative cross-validation of visual and textual cues, and a weighted multi-task reward function for GRPO-based optimization. 3. Created the RealText dataset with 5,397 images and fine-grained annotations using a novel PR² (Perceiver, Reasoner, Reviewer) multi-agent annotation pipeline.",
      "summary": "This paper introduces LogicLens, a unified visual-textual co-reasoning framework for analyzing text-centric forgeries. It uses a novel Cross-Cues-aware Chain of Thought mechanism and multi-task optimization to jointly handle detection, grounding, and explanation. Experiments show LogicLens achieves state-of-the-art performance, significantly outperforming specialized frameworks and other MLLMs in zero-shot and dense-text scenarios.",
      "mindmap": "graph TB\n        A[LogicLens: Visual-Logical Co-Reasoning for Text-Centric Forgery Analysis] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[文本中心伪造威胁/Sophisticated text-centric forgeries]\n        B --> B2[现有方法缺乏推理/Current methods lack reasoning]\n        B --> B3[任务割裂/Tasks treated as discrete]\n        C --> C1[统一框架/Unified Visual-Textual Co-reasoning framework]\n        C --> C2[跨线索思维链/Cross-Cues-aware Chain of Thought (CCT)]\n        C --> C3[多任务奖励函数/Weighted multi-task reward function]\n        C --> C4[PR²标注管道/PR² annotation pipeline]\n        C --> C5[RealText数据集/RealText dataset]\n        D --> D1[零样本评估领先/Superior zero-shot performance]\n        D --> D2[密集文本数据集领先/Lead on dense-text dataset]\n        D --> D3[公开资源/Public dataset, model, code]"
    },
    {
      "title": "MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs for Health and Behavioral Understanding",
      "authors": "Aiwei Zhang, Arvind Pillai, Andrew Campbell, Nicholas C. Jacobson",
      "institution": "Dartmouth College",
      "link": "https://arxiv.org/pdf/2512.21506",
      "code": null,
      "tags": [
        "multi-modal training",
        "wearable sensing",
        "actigraphy encoder",
        "projection module",
        "frozen LLM",
        "behavioral summarization"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a567cc66ec70f31b5dc9bb11a80d73d42749b10088a54744f9b87f208526ccd_w640_q70.webp",
      "contributions": "1. Introduces MotionTeller, a generative framework that natively integrates minute-level wearable activity data with large language models (LLMs) for free-text generation of daily behavioral summaries. 2. Constructs a novel, large-scale dataset of 54,383 (actigraphy, text) pairs derived from real-world NHANES recordings. 3. Demonstrates superior performance over prompt-based baselines in semantic fidelity and lexical accuracy, with qualitative analysis showing the model captures circadian structure and behavioral transitions.",
      "summary": "The paper addresses the challenge of generating natural language summaries from raw physiological signals like actigraphy. It proposes MotionTeller, a framework that integrates a pretrained actigraphy encoder with a frozen LLM via a projection module. The model, trained on a novel dataset, outperforms baselines in generating fluent, human-centered descriptions of daily behavior.",
      "mindmap": "graph TB\n        A[MotionTeller: Multi-modal Integration of Wearable Time-Series with LLMs] --> B[核心问题/Problem: How to generate natural language summaries from raw physiological signals like actigraphy?]\n        A --> C[主要方法/Method: Combines a pretrained actigraphy encoder and a projection module to map behavioral embeddings into a frozen LLM's token space.]\n        A --> D[关键结果/Results: Achieves high semantic fidelity (BERTScore-F1=0.924) and lexical accuracy (ROUGE-1=0.722), outperforming baselines by 7%.]"
    },
    {
      "title": "DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO",
      "authors": "Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji",
      "institution": "Tsinghua University, Kuaishou Technology (Kling Team), Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.21514",
      "code": null,
      "tags": [
        "diffusion models",
        "GRPO",
        "mode collapse",
        "diversity-aware reward",
        "spectral clustering",
        "structure-aware regularization"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp",
      "contributions": "1. Identifies and analyzes the mode collapse problem in GRPO-based image generation from both reward modeling and generation dynamics perspectives. 2. Proposes a distributional creativity bonus reward based on semantic grouping via spectral clustering to encourage novel visual modes. 3. Introduces a structure-aware regularization that applies stronger constraints during early-stage denoising to preserve diversity without sacrificing quality optimization.",
      "summary": "This paper addresses the mode collapse problem in GRPO-based image generation, where models produce homogenized outputs. The proposed DiverseGRPO method introduces a diversity-aware reward based on semantic clustering and a structure-aware regularization to preserve generation diversity. Experiments show the method significantly improves semantic diversity while maintaining image quality, establishing a better quality-diversity trade-off.",
      "mindmap": "graph TB\n        A[DiverseGRPO: Mitigating Mode Collapse] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[GRPO导致模式崩溃/GRPO causes mode collapse]\n        B1 --> B2[缺乏视觉多样性/Lacks visual diversity]\n        C --> C1[奖励层面: 分布创造力奖励/Reward Level: Distributional Creativity Bonus]\n        C --> C2[生成层面: 结构感知正则化/Generation Level: Structure-Aware Regularization]\n        C1 --> C3[基于语义分组的谱聚类/Spectral Clustering for Semantic Grouping]\n        D --> D1[语义多样性提升13%-18%/13%-18% Semantic Diversity Improvement]\n        D --> D2[建立新的帕累托前沿/Establishes New Pareto Frontier]"
    },
    {
      "title": "Selective LLM-Guided Regularization for Enhancing Recommendation Models",
      "authors": "Shanglin Yang, Zhan Shi",
      "institution": "Sichuan University",
      "link": "https://arxiv.org/pdf/2512.21526",
      "code": null,
      "tags": [
        "recommender systems",
        "selective regularization",
        "knowledge distillation",
        "cold-start",
        "long-tail",
        "gating mechanism"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76336a3d123794e83843c14c4b799afd0817948ee9dfeb2f6f19ce776f183796_w640_q70.webp",
      "contributions": "1. Proposes a selective LLM-guided regularization framework (S-LLMR) that activates LLM supervision only when a gating mechanism predicts the LLM to be reliable, addressing the issue of inaccurate global distillation. 2. Introduces a trainable gating mechanism informed by user history length, item popularity, and model uncertainty to dynamically decide when to apply LLM-based pairwise ranking supervision. 3. Demonstrates through experiments that the method improves overall accuracy and yields substantial gains in cold-start and long-tail recommendation scenarios, outperforming global distillation baselines.",
      "summary": "The paper addresses the problem of leveraging large language models (LLMs) for recommendation without suffering from their high cost and unreliability in certain scenarios. It proposes Selective LLM-Guided Regularization (S-LLMR), a model-agnostic framework that uses a gating mechanism to selectively apply LLM-based supervision only when the LLM is predicted to be reliable. Experiments show this approach improves recommendation accuracy, especially for cold-start users and long-tail items, outperforming methods that uniformly distill LLM knowledge.",
      "mindmap": "graph TB\n        A[Selective LLM-Guided Regularization<br>选择性LLM引导正则化] --> B(Problem/核心问题<br>LLMs as standalone recommenders are costly/unreliable;<br>Global distillation forces imitation of inaccurate LLM guidance.)\n        A --> C(Method/主要方法<br>Selective LLM-Guided Regularization (S-LLMR):<br>Trainable gating mechanism activates LLM supervision only when reliable.)\n        A --> D(Results/关键结果<br>Improves overall accuracy;<br>Substantial gains in cold-start & long-tail regimes.)"
    },
    {
      "title": "Hierarchy-Aware Fine-Tuning of Vision-Language Models",
      "authors": "Jiayu Li, Rajesh Gangireddy, Samet Akcay, Wei Cheng, Juhua Hu",
      "institution": "University of Washington, Intel",
      "link": "https://arxiv.org/pdf/2512.21529",
      "code": null,
      "tags": [
        "multimodal learning",
        "hierarchical classification",
        "vision-language models",
        "efficient fine-tuning",
        "LoRA",
        "Tree-Path KL Divergence"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp",
      "contributions": "1. Proposes an efficient hierarchy-aware fine-tuning framework for Vision-Language Models (VLMs) that updates only a few parameters. 2. Introduces two novel loss functions: Tree-Path KL Divergence (TP-KL) for vertical consistency along label paths and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency among sibling classes. 3. Demonstrates consistent improvements in Full-Path Accuracy and reduced Tree-based Inconsistency Error across multiple hierarchical benchmarks with minimal parameter overhead.",
      "summary": "This paper addresses the problem of adapting large Vision-Language Models (VLMs) to hierarchical classification tasks efficiently. The proposed method combines two novel hierarchy-aware loss functions (TP-KL and HiSCE) with lightweight LoRA adaptation to enforce structural consistency in predictions. Experiments show the approach improves accuracy and reduces inconsistency across taxonomy levels with minimal computational cost.",
      "mindmap": "graph TB\n        Root(”Hierarchy-Aware Fine-Tuning of Vision-Language Models”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”VLMs适应层级分类效率低/VLMs inefficient for hierarchical classification”)\n        Problem --> P2(”标准方法预测不一致/Standard methods produce inconsistent predictions”)\n        Method --> M1(”提出层级感知微调框架/Propose hierarchy-aware fine-tuning framework”)\n        Method --> M2(”结合TP-KL与HiSCE损失/Combine TP-KL and HiSCE losses”)\n        Method --> M3(”集成轻量级LoRA适配/Integrate lightweight LoRA adaptation”)\n        Results --> R1(”提升全路径精度/Improves Full-Path Accuracy”)\n        Results --> R2(”降低不一致性错误/Reduces Tree-based Inconsistency Error”)\n        Results --> R3(”参数开销最小/Minimal parameter overhead”)"
    },
    {
      "title": "Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model",
      "authors": "Yanhao Li, Lu Ma, Jiaran Zhang, Lexiang Tang, Wentao Zhang, Guibo Luo",
      "institution": "Peking University, Harbin Institute of Technology, Shenzhen",
      "link": "https://arxiv.org/pdf/2512.21540",
      "code": null,
      "tags": [
        "llm inference",
        "adaptive length penalty",
        "reinforcement learning",
        "constrained optimization",
        "Lagrangian primal-dual",
        "reasoning efficiency"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ec5b3e2930213678e6d04b060a50d89faaaacded209387c96170a775f9db310_w640_q70.webp",
      "contributions": "1. Proposes Leash, a reinforcement learning framework that formulates length control as a constrained optimization problem and uses a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. 2. Introduces an adaptive mechanism that intensifies the penalty when generations exceed the target length and relaxes it when they are shorter, guiding models toward concise reasoning without sacrificing performance. 3. Demonstrates experimentally that Leash reduces average reasoning length by 60% across diverse tasks while maintaining competitive performance, offering a practical paradigm for efficient LLMs.",
      "summary": "The paper addresses the problem of LLMs producing overly long reasoning traces, which increases computational cost. It proposes Leash, an adaptive reinforcement learning framework that dynamically adjusts length penalties using a Lagrangian method to balance conciseness and accuracy. Experiments show it reduces reasoning length by 60% while maintaining performance, providing an effective approach for efficient LLM reasoning.",
      "mindmap": "graph TB\n        A[LEASH: Adaptive Length Penalty and Reward Shaping] --> B[核心问题/Problem: LLMs生成过长推理链，计算成本高/Fixed penalties fail to adapt, leading to suboptimal accuracy-conciseness trade-offs]\n        A --> C[主要方法/Method: 自适应强化学习框架，使用拉格朗日对偶方法动态调整惩罚系数/Adaptive RL framework with Lagrangian primal-dual for dynamic penalty adjustment]\n        A --> D[关键结果/Results: 平均推理长度减少60%，性能保持竞争力/Average reasoning length reduced by 60% while maintaining competitive performance across tasks]"
    },
    {
      "title": "Human-AI Interaction Alignment: Designing, Evaluating, and Evolving Value-Centered AI For Reciprocal Human-AI Futures",
      "authors": "Hua Shen, Tiffany Knearem, Divy Thakkar, Pat Pataranutaporn, Anoop Sinha, Yike, Jenny T. Liang, Lama Ahmad, Tanu Mitra, Brad A. Myers, Yang Li",
      "institution": "NYU Shanghai, MBZUAI, Google, Massachusetts Institute of Technology, Carnegie Mellon University, OpenAI, University of Washington, Google DeepMind",
      "link": "https://arxiv.org/pdf/2512.21551",
      "code": null,
      "tags": [
        "human-ai interaction",
        "bidirectional alignment",
        "value-centered design",
        "interactive alignment"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/acbc6d9188f5aaa4289d9a01fb321cc29a9a54b03061c38e31010c7988a9ca12_w640_q70.webp",
      "contributions": "1. Proposes a shift from unidirectional to bidirectional human-AI alignment, framing it as a dynamic, reciprocal co-adaptation process. 2. Emphasizes embedding human and societal values into AI alignment research through value-centered design. 3. Aims to establish an interdisciplinary research agenda for responsible, reciprocal human-AI futures through collaborative workshop activities.",
      "summary": "This workshop paper identifies the inadequacy of traditional, one-way AI alignment and proposes a bidirectional human-AI alignment framework where humans and AI co-adapt through interaction and value-centered design. It aims to bring together interdisciplinary researchers to explore methods for interactive alignment and societal impact evaluation. The main conclusion is the need for a shared agenda to advance responsible, reciprocal collaboration between humans and AI systems.",
      "mindmap": "graph TB\n        A[Human-AI Interaction Alignment] --> B[核心问题/Problem: Unidirectional AI alignment is inadequate for dynamic human-AI interaction]\n        A --> C[主要方法/Method: Bidirectional alignment via value-centered design, interaction, and evaluation]\n        A --> D[关键结果/Results: Establishes agenda for reciprocal, responsible human-AI futures]"
    },
    {
      "title": "Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments",
      "authors": "Hua Shen",
      "institution": "NYU Shanghai, New York University",
      "link": "https://arxiv.org/pdf/2512.21552",
      "code": null,
      "tags": [
        "ai for education",
        "human-ai alignment",
        "trustworthy ai",
        "adaptive learning",
        "educational technology",
        "ai ethics"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7f40fe114da7bae34b09e44db18fa32bb4f64e57bd01e75e96afc80c2ddc136_w640_q70.webp",
      "contributions": "1. Proposes the novel concept of \"bidirectional human-AI alignment\" for education, emphasizing mutual adaptation between humans and AI systems. 2. Explores the evolution of AI's role in education from a support tool to a collaborative partner, analyzing its impact on teacher roles and student agency. 3. Provides actionable strategies for policymakers, developers, and educators to ensure AI advances equity, transparency, and human flourishing in learning environments.",
      "summary": "This paper addresses the risks of AI in education, such as bias and loss of autonomy, by proposing the concept of bidirectional human-AI alignment. The method involves not only embedding human values into AI but also equipping educators and students to guide these technologies. It concludes that reframing AI adoption as a process of mutual adaptation is key to creating trustworthy learning environments where humans and AI can grow together.",
      "mindmap": "graph TB\n        A[论文标题: Bidirectional Human-AI Alignment in Education] --> B[核心问题/Problem: AI in education introduces risks to equity, privacy, and autonomy.]\n        A --> C[主要方法/Method: Proposes bidirectional alignment: embedding human values into AI and equipping humans to interpret/guide AI.]\n        A --> D[关键结果/Results: Envisions a future of mutual adaptation where AI advances equity, transparency, and human flourishing.]"
    },
    {
      "title": "Exploration of Reproducible Generated Image Detection",
      "authors": "Yihang Duan",
      "institution": "Not explicitly stated in the provided content. (Author name only, no affiliation or email domain provided)",
      "link": "https://arxiv.org/pdf/2512.21562",
      "code": null,
      "tags": [
        "image forensics",
        "AIGC detection",
        "reproducibility",
        "generalizability",
        "diffusion models",
        "binary classification"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp",
      "contributions": "1. Identifies and analyzes the root causes of poor reproducibility in AIGC image detection research, citing omitted experimental details and overfitting to generator-specific features. 2. Provides empirical evidence for the reproducibility issue by constructing a test dataset and reproducing a representative detection method, demonstrating performance drops under cross-generator testing. 3. Proposes reference directions for the research community to improve reproducibility and generalizability, such as more comprehensive disclosure of experimental details.",
      "summary": "This paper investigates the reproducibility and generalizability challenges in AI-Generated Content (AIGC) image detection. By reviewing key literature, building a test dataset, and reproducing a detection method, it identifies causes like omitted experimental details and model overfitting. The study concludes that while basic performance can be reproduced, detection fails when preprocessing disrupts key features or when testing across different generators, highlighting the need for better methodological disclosure and robustness.",
      "mindmap": "graph TB\n        A[Exploration of Reproducible Generated Image Detection] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Poor Reproducibility & Generalizability]\n        C[主要方法/Method<br>Literature Review, Dataset Construction, Method Reproduction]\n        D[关键结果/Results<br>Performance Drops with Preprocessing/Cross-Generator Tests]"
    },
    {
      "title": "Towards Long-window Anchoring in Vision-Language Model Distillation",
      "authors": "Haoyi Zhou, Shuo Li, Tianyu Chen, Qi Song, Chonghan Gao, Jianxin Li",
      "institution": "Beihang University, Zhongguancun Laboratory",
      "link": "https://arxiv.org/pdf/2512.21576",
      "code": null,
      "tags": [
        "multi-modal training",
        "knowledge distillation",
        "long-context",
        "rotary position embeddings (RoPE)",
        "attention mechanism",
        "vision-language models"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp",
      "contributions": "1. Identifies the problem of limited effective context windows in small, distilled vision-language models despite using identical positional embeddings and architectures as their larger counterparts. 2. Proposes LAid, a novel distillation method featuring progressive distance-weighted attention matching and learnable RoPE response gain modulation to transfer long-range attention mechanisms. 3. Demonstrates that LAid-distilled models achieve significantly longer effective context windows (up to 3.2x) while maintaining performance on standard benchmarks, and provides spectral analysis showing successful transfer of low-frequency attention components.",
      "summary": "This paper addresses the problem that small, distilled vision-language models have much shorter effective context windows than their large teacher models. The authors propose LAid, a new distillation method that transfers long-range attention capabilities via progressive attention matching and learnable RoPE modulation. Their method successfully extends the context window of small models by up to 3.2 times while preserving performance on standard benchmarks.",
      "mindmap": "graph TB\n        A[Towards Long-window Anchoring in Vision-Language Model Distillation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Small distilled VLMs have limited effective context windows]\n        C[主要方法/Method: LAid - Progressive attention matching & learnable RoPE modulation]\n        D[关键结果/Results: Achieves up to 3.2x longer context, maintains benchmark performance]"
    },
    {
      "title": "NEMO-4-PAYPAL: Leveraging NVIDIA's Nemo Framework for empowering PayPal's Commerce Agent",
      "authors": "Ali Sahami, Sudhanshu Garg, Andrew Wang, Chaitanya Kulkarni, Farhad Farahani, Sean Yun-Shiuan Chuang, Jian Wan, Srinivasan Manoharan, Uma Kona, Nitin Sharma, Linsey Pang, Prakhar Mehrotra, Jessica Clark, Mark Moyou",
      "institution": "PayPal AI, NVIDIA",
      "link": "https://arxiv.org/pdf/2512.21578",
      "code": null,
      "tags": [
        "agent system",
        "NeMo Framework",
        "LoRA",
        "Nemotron SLM",
        "hyperparameter sweep",
        "multi-agent system"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90dda98c8c5c5f9d75ede0c681c9024dc5973d432f246b90eed23aad0a03c916_w640_q70.webp",
      "contributions": "1. The first application of NVIDIA's NeMo Framework to optimize commerce-specific agents. 2. An LLM-powered fine-tuning strategy for retrieval-focused commerce tasks. 3. A demonstration of significant latency and cost improvements while maintaining agent quality, providing a scalable framework for multi-agent system optimization in production e-commerce.",
      "summary": "This paper presents the optimization of PayPal's Commerce Agent, a multi-agent system, by fine-tuning a Nemotron small language model using NVIDIA's NeMo Framework and LoRA. The method involved systematic hyperparameter sweeps to improve the performance-critical search component. The results show that the fine-tuned model effectively resolves the key latency issue in the retrieval component, which accounted for over 50% of response time, while maintaining or enhancing overall system performance.",
      "mindmap": "graph TB\n        Root[”NEMO-4-PAYPAL: Empowering PayPal's Commerce Agent”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”Search Latency/搜索延迟”]\n        P1 --> P2[”>50% Response Time/超过50%响应时间”]\n        Method --> M1[”Fine-tuning with NeMo/使用NeMo微调”]\n        M1 --> M2[”LoRA on Nemotron SLM/在Nemotron SLM上使用LoRA”]\n        M2 --> M3[”Hyperparameter Sweep/超参数扫描”]\n        Results --> R1[”Latency & Cost Improvement/延迟与成本改进”]\n        Results --> R2[”Maintained Agent Quality/保持代理质量”]\n        Results --> R3[”Scalable Framework/可扩展框架”]"
    },
    {
      "title": "A Unified Definition of Hallucination, Or: It's the World Model, Stupid",
      "authors": "Emmy Liu, Varun Gangal, Chelsea Zou, Xiaoqi Huang, Michael Yu, Alex Chang, Zhuofu Tao, Sachin Kumar, Steven Y. Feng",
      "institution": "Carnegie Mellon University, Stanford University, The Ohio State University, Patronus AI, DegenAI Labs, Independent Researchers",
      "link": "https://arxiv.org/pdf/2512.21577",
      "code": null,
      "tags": [
        "hallucination detection & evaluation",
        "hallucination",
        "world modeling",
        "knowledge conflict",
        "benchmark",
        "language model evaluation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e2cc31121f769cca7464239d4aa27b26c8c4bc903970a4833fbebac56dc9b85_w640_q70.webp",
      "contributions": "1. Proposes a unified definition of hallucination as inaccurate internal world modeling that is observable to the user, synthesizing prior definitions. 2. Provides a framework for analyzing hallucinations by varying the reference world model and knowledge conflict policy, clarifying what constitutes a hallucination versus other error types. 3. Outlines plans for a family of benchmarks based on synthetic, fully-specified world models to stress-test and improve the world modeling components of language models.",
      "summary": "This paper argues that the persistent problem of hallucination in language models stems from inaccurate internal world modeling. It unifies various historical definitions under this core concept and proposes a framework for clearer evaluation. The authors conclude by sketching plans for new benchmarks to rigorously test and improve language models' world modeling capabilities.",
      "mindmap": "graph TB\n        Root[”A Unified Definition of Hallucination / 幻觉的统一定义”] --> Problem[”核心问题/Problem”]\n        Root[”A Unified Definition of Hallucination / 幻觉的统一定义”] --> Method[”主要方法/Method”]\n        Root[”A Unified Definition of Hallucination / 幻觉的统一定义”] --> Results[”关键结果/Results”]\n        Problem --> P1[”Hallucination persists in LLMs / 幻觉在LLM中持续存在”]\n        Method --> M1[”Unified definition: inaccurate world modeling / 统一定义：不准确的世界建模”]\n        Method --> M2[”Framework: reference world & conflict policy / 框架：参考世界与冲突策略”]\n        Results --> R1[”Clarifies evaluation & terminology / 澄清评估与术语”]\n        Results --> R2[”Proposes new benchmark plans / 提出新基准计划”]"
    },
    {
      "title": "A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning",
      "authors": "Zelin Zang, Wenyi Gu, Siqi Ma, Dan Yang, Yue Shen, Zhu Zhang, Guohui Fan, Wing-Kuen Ling, Fuji Yang",
      "institution": "Tsientang Institute of Advanced Study (TIAS), Westlake University, Ant Group, China-Japan Friendship Hospital",
      "link": "https://arxiv.org/pdf/2512.21583",
      "code": null,
      "tags": [
        "multi-modal inference",
        "vision-language model",
        "logic tree reasoning",
        "medical multimodal diagnosis",
        "explainable AI",
        "LLaVA"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b6b8d6d34a614d3cb042f13334dede18494914ca29d6f0fd6f4467f789871f82_w640_q70.webp",
      "contributions": "1. Proposes a diagnostic framework integrating vision-language alignment with logic-regularized reasoning to enhance reliability. 2. Introduces a reasoning controller and logic tree generator to decompose tasks and assemble verifiable conclusions, improving interpretability. 3. Demonstrates improved diagnostic accuracy and more interpretable reasoning traces on multimodal medical benchmarks like MedXpertQA.",
      "summary": "The paper addresses the problem of unreliable reasoning and hallucinations in existing multimodal medical AI models. It proposes a diagnostic framework built on LLaVA that combines vision-language alignment with logic-regularized reasoning to generate verifiable conclusions via logic trees. Evaluations show the method improves diagnostic accuracy and yields more interpretable reasoning traces, advancing trustworthy multimodal medical AI.",
      "mindmap": "graph TB\n        A[医学多模态诊断框架<br/>Medical Multimodal Diagnostic Framework] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有模型幻觉与推理不一致<br/>Existing Models: Hallucinations & Inconsistent Reasoning]\n        C --> C1[结合视觉语言对齐与逻辑树推理<br/>Vision-Language Alignment + Logic Tree Reasoning]\n        D --> D1[提升诊断准确性与可解释性<br/>Improved Diagnostic Accuracy & Interpretability]"
    },
    {
      "title": "LLM-I2I: Boost Your Small Item2Item Recommendation Model with Large Language Model",
      "authors": "Yinfu Feng, Yanjing Wu, Rong Xiao, Xiaoyi Zen",
      "institution": "Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.21595",
      "code": null,
      "tags": [
        "llm inference",
        "item-to-item recommendation",
        "data-centric",
        "long-tail items",
        "data augmentation",
        "data filtering"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cb08ea9b26b612493e4d48e7db88c46a869c2050c94d47b75488adcaf6ddfa9_w640_q70.webp",
      "contributions": "1. Proposes LLM-I2I, a data-centric framework that leverages Large Language Models to enhance I2I recommendation models without altering their architecture. 2. Introduces an LLM-based data generator to synthesize user-item interactions, specifically targeting long-tail items to alleviate data sparsity. 3. Designs an LLM-based data discriminator to filter out noisy interactions from both real and synthetic data, improving overall data quality for training.",
      "summary": "The paper addresses data sparsity and noise problems in Item-to-Item (I2I) recommendation systems by proposing LLM-I2I, a data-centric framework that uses an LLM to generate synthetic interactions for long-tail items and filter noisy data. The refined data is then used to train existing I2I models. Experimental results on industrial and academic datasets show significant improvements in recommendation accuracy, especially for long-tail items, and deployment on a large e-commerce platform led to measurable gains in recall and gross merchandise value.",
      "mindmap": "graph TB\n        A[LLM-I2I: Boost Your Small Item2Item Recommendation Model with Large Language Model] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[数据稀疏与噪声/Data Sparsity & Noise]\n        C --> C1[LLM数据生成器/LLM-based Data Generator]\n        C --> C2[LLM数据判别器/LLM-based Data Discriminator]\n        C1 --> C3[合成交互数据/Synthesize Interaction Data]\n        C2 --> C4[过滤噪声数据/Filter Noisy Data]\n        C3 & C4 --> C5[融合数据训练I2I模型/Fuse Data to Train I2I Model]\n        D --> D1[提升推荐准确率/Improves Recommendation Accuracy]\n        D --> D2[提升长尾物品性能/Better for Long-tail Items]\n        D --> D3[线上指标提升/Online Metric Improvements (RN+6.02%, GMV+1.22%)]"
    },
    {
      "title": "AMS-IO-Bench and AMS-IO-Agent: Benchmarking and Structured Reasoning for Analog and Mixed-Signal Integrated Circuit Input/Output Design",
      "authors": "Zhishuai Zhang, Xintian Li, Shilong Liu, Aodong Zhang, Lu Jie, Nan Sun",
      "institution": "Tsinghua University, Princeton University",
      "link": "https://arxiv.org/pdf/2512.21613",
      "code": "https://github.com/Arcadia-1/AMS-IO-Agent",
      "tags": [
        "agent system",
        "AMS IC design",
        "LLM-based agent",
        "structured reasoning",
        "design automation",
        "I/O ring generation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7205181105fdcb012bb4c8c5b3cce6565751edc220003d3784f6dbf648ee893a_w640_q70.webp",
      "contributions": "1. Proposed AMS-IO-Agent, a domain-specialized LLM-based agent for structure-aware I/O subsystem generation in AMS ICs. 2. Introduced AMS-IO-Bench, a benchmark for wirebond-packaged AMS I/O ring automation. 3. Demonstrated the first reported human-agent collaborative AMS IC design where an LLM agent's output was directly used in a silicon tape-out, achieving over 70% DRC+LVS pass rate and reducing design time from hours to minutes.",
      "summary": "This paper addresses the labor-intensive and non-reusable nature of analog and mixed-signal (AMS) integrated circuit I/O design by proposing AMS-IO-Agent, an LLM-based agent that uses structured domain knowledge and intent structuring to automate the process. The method connects natural language design intent to industrial deliverables and is evaluated on a new benchmark, AMS-IO-Bench. The agent significantly outperforms baseline LLMs, achieves a high verification pass rate, and its generated I/O ring was successfully fabricated, demonstrating practical effectiveness in real design flows.",
      "mindmap": "graph TB\n        A[AMS-IO-Bench and AMS-IO-Agent<br>论文标题/Paper Title] --> B[手动AMS I/O设计费时且不可复用<br>核心问题/Problem: Manual AMS I/O design is time-consuming and non-reusable]\n        A --> C[提出基于LLM的智能体与结构化推理框架<br>主要方法/Method: Proposes an LLM-based agent and structured reasoning framework]\n        A --> D[验证通过率>70%，设计时间从小时减至分钟，成功流片<br>关键结果/Results: >70% pass rate, design time reduced from hours to minutes, successful tape-out]"
    },
    {
      "title": "Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design",
      "authors": "Takahide Suzuki, Kazuki Nakanishi, Takashi Fujiwara, Hideyuki Shimizu",
      "institution": "Institute of Science Tokyo, Kyoto University",
      "link": "https://arxiv.org/pdf/2512.21623",
      "code": null,
      "tags": [
        "agent system",
        "multi-agent platform",
        "knowledge graph",
        "physiologically based pharmacokinetic (PBPK) simulations",
        "autonomous execution",
        "human-in-the-loop"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbd4a841925fb9129111928c81fe29ac7016c26df168e9b4ca87c8782a692d5e_w640_q70.webp",
      "contributions": "1. Introduces OrchestRA, a human-in-the-loop multi-agent platform that unifies biology, chemistry, and pharmacology into an autonomous discovery engine for drug design. 2. Features an architecture with specialized agents (Biologist, Chemist, Pharmacologist) governed by an Orchestrator, which actively execute simulations and reason over results to create a dynamic feedback loop for iterative optimization. 3. Democratizes therapeutic design by transforming drug discovery from a stochastic search into a programmable, evidence-based engineering discipline through the integration of autonomous execution with human guidance.",
      "summary": "The paper addresses the challenge of fragmented and passive tools in therapeutic discovery by proposing OrchestRA, a multi-agent platform where specialized AI agents autonomously execute and reason over biological, chemical, and pharmacological tasks. This creates a dynamic feedback loop for iterative drug candidate optimization, guided by human input. The conclusion is that this approach transforms drug discovery into a more programmable and evidence-based engineering process.",
      "mindmap": "graph TB\n        A[Democratizing Drug Discovery with an Orchestrated, Knowledge-Driven Multi-Agent Team for User-Guided Therapeutic Design] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Fragmented domains & execution gap<br>AI as passive assistants]\n        C[主要方法/Method<br>OrchestRA Multi-Agent Platform<br>Agents execute & reason<br>Human-in-the-loop]\n        D[关键结果/Results<br>Autonomous discovery engine<br>Dynamic feedback loop<br>Programmable evidence-based design]"
    },
    {
      "title": "Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing",
      "authors": "Hong Xie, Haoran Gu, Yanying Huang, Tao Tan, Defu Lian",
      "institution": "University of Science and Technology of China, Chongqing University",
      "link": "https://arxiv.org/pdf/2512.21626",
      "code": null,
      "tags": [
        "multi-armed bandits",
        "multiple-play bandits",
        "prioritized resource sharing",
        "regret analysis",
        "combinatorial optimization",
        "UCB"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8562089dc3d9fa0a65e8caf8921b51648e1718efd62395a7af2fadba8cf952d9_w640_q70.webp",
      "contributions": "1. Proposes a new variant of the multiple-play stochastic bandit model (MSB-PRS) that incorporates prioritized capacity sharing among plays, tailored for resource allocation in LLM and edge intelligence applications. 2. Establishes instance-independent and instance-dependent regret lower bounds for the proposed model, characterizing its fundamental learning difficulty. 3. Designs an offline optimal policy solver (MSB-PRS-OffOpt) and an online UCB-based learning algorithm with theoretical regret guarantees that nearly match the derived lower bounds.",
      "summary": "This paper introduces a new multi-armed bandit model where multiple plays with priorities compete for the stochastic capacity of arms. The authors design an algorithm that first computes an optimal allocation offline and then uses it within an online UCB-based strategy, proving that its regret nearly matches the fundamental lower bounds they establish for this problem.",
      "mindmap": "graph TB\n        Root[”Multiple-play Stochastic Bandits with Prioritized Arm Capacity Sharing<br/>多臂老虎机优先容量共享”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br/>Prioritized resource allocation in LLM/edge intelligence<br/>LLM/边缘智能中的优先资源分配”] --> P1[”模型/Model<br/>M arms, K plays, stochastic capacity, priority weights<br/>M个臂，K个玩家，随机容量，优先级权重”]\n        Method[”主要方法/Method<br/>Algorithm Design<br/>算法设计”] --> M1[”离线最优求解器/MSB-PRS-OffOpt<br/>Computes optimal policy<br/>计算最优策略”]\n        Method --> M2[”在线UCB算法/Online UCB Algorithm<br/>Uses offline solver as subroutine<br/>以离线求解器为子程序”]\n        Results[”关键结果/Results<br/>Theoretical Analysis<br/>理论分析”] --> R1[”下界/Regret Lower Bounds<br/>Ω(α₁σ√KMT), Ω(α₁σ²(M/Δ)lnT)”]\n        Results --> R2[”上界/Regret Upper Bounds<br/>Matching lower bounds up to factors<br/>与下界匹配（差因子）”]"
    },
    {
      "title": "Variance-Aware Prior-Based Tree Policies for Monte Carlo Tree Search",
      "authors": "Maximilian Weichart",
      "institution": "University of Regensburg",
      "link": "https://arxiv.org/pdf/2512.21648",
      "code": "https://github.com/Max-We/inverse-rpo",
      "tags": [
        "reinforcement learning",
        "Monte Carlo Tree Search",
        "Upper Confidence Bound",
        "Variance-Aware",
        "Prior-Based Tree Policy",
        "Inverse-RPO"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c9098504a8a9013ab805fffa4a23f04af76e28f5d8b8a0353e1e7d5583f589_w640_q70.webp",
      "contributions": "1. Introduces Inverse-RPO, a general methodology to systematically derive prior-based UCTs from any prior-free UCB., 2. Applies Inverse-RPO to UCB-V to create two new variance-aware prior-based tree policies., 3. Provides an extension to the mctx library for variance-aware UCTs, showing minimal code changes and improved performance over PUCT in benchmarks.",
      "summary": "This paper addresses the challenge of extending prior-based tree policies in Monte Carlo Tree Search beyond the empirically derived PUCT. The authors propose Inverse-RPO, a principled method to derive prior-based UCTs from any prior-free UCB, and apply it to create variance-aware policies. Their new policies outperform the standard PUCT across multiple benchmarks without added computational cost.",
      "mindmap": "graph TB\n        A[Variance-Aware Prior-Based Tree Policies for MCTS] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Extending prior-based UCTs from other UCBs is challenging]\n        C[主要方法/Method: Propose Inverse-RPO to derive prior-based UCTs; apply to UCB-V]\n        D[关键结果/Results: New policies outperform PUCT without extra cost]"
    },
    {
      "title": "TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References",
      "authors": "Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng",
      "institution": "Zhejiang University, Fudan University, Huawei Technologies Ltd.",
      "link": "https://arxiv.org/pdf/2512.21641",
      "code": null,
      "tags": [
        "3D object grounding",
        "temporal multimodal grounding",
        "LiDAR-image fusion",
        "language-conditioned decoding",
        "UniScene representation",
        "NuPrompt benchmark"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp",
      "contributions": "1. Proposes TrackTeller, a unified temporal multimodal framework for 3D grounding that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning. 2. Introduces a shared UniScene representation aligned with textual semantics to generate language-aware 3D proposals. 3. Demonstrates significant performance improvements on the NuPrompt benchmark, including a 70% relative gain in Average Multi-Object Tracking Accuracy and a 3.15-3.4x reduction in False Alarm Frequency.",
      "summary": "This paper addresses the problem of grounding natural language references to objects in dynamic 3D driving scenes, which often depend on recent motion or behavior. The authors propose TrackTeller, a framework that fuses LiDAR and camera data with language, builds a unified scene representation, and uses temporal reasoning to refine object identification. Experiments show that TrackTeller significantly outperforms existing baselines in language-grounded tracking accuracy.",
      "mindmap": "graph TB\n        A[TrackTeller: Temporal Multimodal 3D Grounding] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[动态3D场景中的行为依赖语言指代/Dynamic 3D Behavior-Dependent Language Grounding]\n        C --> C1[统一多模态时序框架/Unified Temporal Multimodal Framework]\n        C1 --> C2[LiDAR-图像融合与语言解码/LiDAR-Image Fusion & Language Decoding]\n        C1 --> C3[构建UniScene表示/Build UniScene Representation]\n        C1 --> C4[利用运动历史推理/Reason with Motion History]\n        D --> D1[在NuPrompt上显著提升性能/Significant Improvement on NuPrompt]\n        D1 --> D2[AMOTA提升70%/70% AMOTA Gain]\n        D1 --> D3[误报率降低3.15-3.4倍/3.15-3.4x FA Reduction]"
    },
    {
      "title": "Near-Optimal Coalition Structures in Polynomial Time",
      "authors": "Angshul Majumdar",
      "institution": "Indraprastha Institute of Information Technology, Delhi",
      "link": "https://arxiv.org/pdf/2512.21657",
      "code": null,
      "tags": [
        "cooperative game theory",
        "coalition structure generation",
        "anytime algorithms",
        "sparse relaxations",
        "dynamic programming",
        "MILP"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb8e459bba93eeaf4c9237729ad06cf13b47ef6a941811135ed634157dd979c7_w640_q70.webp",
      "contributions": "1. Proves that under a \"sparse synergy\" model, sparse relaxation methods can find near-optimal coalition structures in polynomial time with high probability. 2. Demonstrates that broad classes of dynamic programming and MILP algorithms require exponential time to achieve comparable solution quality. 3. Establishes a rigorous probabilistic anytime performance separation favoring sparse relaxations over exact methods for the CSG problem.",
      "summary": "This paper studies the coalition structure generation (CSG) problem. It compares three algorithmic paradigms and proves that, under a random sparse synergy model, sparse relaxation methods can find near-optimal solutions in polynomial time, while exact methods like DP and MILP require exponential time to reach similar quality, establishing a clear anytime performance advantage for the sparse approach.",
      "mindmap": "graph TB\n        A[Near-Optimal Coalition Structures in Polynomial Time] --> B[核心问题/Problem: Coalition Structure Generation (CSG)]\n        A --> C[主要方法/Method: Compare DP, MILP, and Sparse Relaxations]\n        A --> D[关键结果/Results: Sparse relaxations achieve near-optimal welfare in polynomial time; DP/MILP require exponential time]"
    },
    {
      "title": "Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning",
      "authors": "Zikun Guo, Adeyinka P. Adedigba, Rammohan Mallipeddi, Heoncheol Lee",
      "institution": "Kyungpook National University, Kumoh National Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.21654",
      "code": null,
      "tags": [
        "swarm intelligence",
        "Ant Colony Optimization",
        "structural prior",
        "load-aware objective",
        "overlap suppression",
        "multi-robot path planning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca4635b64758650f78e762004770f2a7ac8eb62cd36aa2db98d90af82d3f6eae_w640_q70.webp",
      "contributions": "1. Proposes a structure-induced exploration framework that integrates structural priors into ACO initialization to constrain the search space. 2. Designs a pheromone update rule that emphasizes structurally meaningful connections and incorporates a load-aware objective to balance total travel distance with individual robot workload. 3. Introduces an explicit overlap suppression strategy to ensure distinct and balanced task allocation across the robot team.",
      "summary": "This paper addresses the challenge of scalable and balanced multi-robot path planning. It proposes a new framework that integrates structural priors into Ant Colony Optimization, along with a load-aware objective and overlap suppression, to improve route compactness, stability, and workload distribution. The method demonstrates consistent improvements over metaheuristic baselines and offers a scalable solution for applications like logistics and search-and-rescue.",
      "mindmap": "graph TB\n        A[Structural Induced Exploration for Balanced and Scalable Multi-Robot Path Planning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Multi-robot path planning is combinatorially complex and requires balancing global efficiency with fair task allocation. 传统方法难以扩展/Traditional methods struggle to scale.]\n        C[主要方法/Method: A structure-induced ACO framework. 利用结构先验、负载感知目标和重叠抑制/Uses structural prior, load-aware objective, and overlap suppression.]\n        D[关键结果/Results: Improves route compactness, stability, and workload distribution. 提供可扩展的框架/Provides a scalable framework.]"
    },
    {
      "title": "Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles",
      "authors": "Jalal Khan",
      "institution": "United Arab Emirates University",
      "link": "https://arxiv.org/pdf/2512.21673",
      "code": null,
      "tags": [
        "object detection",
        "YOLO-NAS",
        "YOLOv8",
        "perception",
        "autonomous vehicles",
        "custom dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp",
      "contributions": "1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.",
      "summary": "This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.",
      "mindmap": "graph TB\n    A[Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[评估自动驾驶感知中深度学习模型的性能/Evaluate DL model performance for AV perception]\n    C --> C1[使用自定义数据集比较YOLO-NAS与YOLOv8/Compare YOLO-NAS and YOLOv8 using a custom dataset]\n    D --> D1[YOLOv8s训练时间减少75%/YOLOv8s saves 75% training time]\n    D --> D2[YOLOv8s准确率更高(83% vs 81%)/YOLOv8s has higher accuracy (83% vs 81%)]"
    },
    {
      "title": "RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting",
      "authors": "Haochen Lv, Yan Lin, Shengnan Guo, Xiaowei Mao, Hong Nie, Letian Gong, Youfang Lin, Huaiyu Wan",
      "institution": "Beijing Jiaotong University, Aalborg University",
      "link": "https://arxiv.org/pdf/2512.21685",
      "code": null,
      "tags": [
        "spatiotemporal forecasting",
        "probabilistic forecasting",
        "uncertainty estimation",
        "principal component analysis",
        "road impedance",
        "traffic flow"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f835b2051524d67481fbf9f4479086a541b29307c2876046f2c3ee4eec60f92_w640_q70.webp",
      "contributions": "1. Proposes a dynamic impedance evolution network to model directional traffic transfer patterns driven by congestion and flow variability, revealing causes of uncertainty and enhancing reliability and interpretability. 2. Designs a principal component network to forecast the dominant eigenvectors of future flow covariance, enabling the capture of spatiotemporal uncertainty correlations. 3. Integrates domain-specific transportation theory with spatiotemporal principal component learning for probabilistic traffic flow forecasting, achieving superior performance over existing methods.",
      "summary": "The paper proposes RIPCN, a Road Impedance Principal Component Network for probabilistic traffic flow forecasting. It integrates transportation theory with principal component learning to model the causes of uncertainty and capture spatiotemporal uncertainty correlations. Experimental results show it outperforms existing probabilistic forecasting methods.",
      "mindmap": "graph TB\n        A[RIPCN: A Road Impedance Principal Component Network for Probabilistic Traffic Flow Forecasting] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(如何建模交通流不确定性的成因? / How to model the causes of traffic flow uncertainty?)\n        B --> B2(如何捕捉不确定性的时空相关性? / How to capture spatiotemporal correlations of uncertainty?)\n        C --> C1(动态阻抗演化网络 / Dynamic Impedance Evolution Network)\n        C --> C2(主成分网络 / Principal Component Network)\n        D --> D1(超越现有概率预测方法 / Outperforms existing probabilistic forecasting methods)"
    },
    {
      "title": "BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks",
      "authors": "Md. Rakibul Islam, Md. Kamrozzaman Bhuiyan, Safwan Muntasir, Arifur Rahman Jawad, Most. Sharmin Sultana Samu",
      "institution": "Not explicitly stated in provided text. Affiliation/domain cannot be reliably inferred.",
      "link": "https://arxiv.org/pdf/2512.21694",
      "code": null,
      "tags": [
        "handwritten text generation",
        "Generative Adversarial Networks",
        "Handwritten Text Generation",
        "Bengali",
        "Dataset",
        "Pre-processing"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp",
      "contributions": "1. Proposed a method for generating Bengali handwritten words from plain text using GANs, addressing a significant research gap. 2. Developed and used a novel, self-collected dataset of Bengali handwriting from approximately 500 diverse individuals. 3. Demonstrated the ability to produce diverse and realistic handwritten outputs through the described approach.",
      "summary": "This paper addresses the lack of research on Bengali handwritten text generation by proposing a GAN-based method to generate words from plain text. The authors created a new dataset of Bengali handwriting samples from hundreds of contributors. The work successfully generates diverse handwritten outputs and contributes to advancing research in this area for the Bengali language.",
      "mindmap": "graph TB\n        A[BeHGAN: Bengali Handwritten Word Generation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[HTG is challenging & understudied for Bengali<br/>孟加拉语手写文本生成研究不足且困难]\n        C --> C1[Propose GAN-based method<br/>提出基于GAN的方法]\n        C --> C2[Use self-collected dataset<br/>使用自收集数据集]\n        C --> C3[Pre-process images<br/>预处理图像]\n        D --> D1[Generates diverse handwritten words<br/>生成多样化手写词]\n        D --> D2[Contributes to Bengali HTG research<br/>推动孟加拉语手写文本生成研究]"
    },
    {
      "title": "Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning",
      "authors": "Eranga Bandara, Tharaka Hewa, Ross Gore, Sachin Shetty, Ravi Mukkamala, Peter Foytik, Abdul Rahman, Safdar H. Bouk, Xueping Liang, Amin Hass, Sachini Rajapakse, Ng Wee Keong, Kasun De Zoysa, Aruna Withanage, Nilaan Loganathan",
      "institution": "Old Dominion University, University of Oulu, Deloitte & Touche LLP, Florida International University, Nanyang Technological University, University of Colombo, IcicleLabs.AI, Accenture Technology Labs, Effectz.AI",
      "link": "https://arxiv.org/pdf/2512.21699",
      "code": null,
      "tags": [
        "agent system",
        "agentic AI",
        "consensus-driven reasoning",
        "explainable AI",
        "responsible AI",
        "multi-model governance"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed22161f8f004c98e3fb6124bac7991548de5e54f47adb42f0d3eb1095409e6e_w640_q70.webp",
      "contributions": "1. Proposes a novel RAI/XAI agent architecture for production workflows based on multi-model consensus and reasoning-layer governance. 2. Introduces a mechanism where a consortium of heterogeneous LLM/VLM agents generate independent outputs, exposing uncertainty and alternatives for structured consolidation. 3. Demonstrates that the consensus-driven approach improves robustness, transparency, and operational trust across diverse real-world agentic AI workflows.",
      "summary": "This paper addresses the challenges of explainability and responsibility in increasingly autonomous agentic AI systems. It proposes a new architecture where multiple AI agents generate candidate outputs, and a dedicated reasoning agent consolidates them while enforcing safety constraints, thereby improving decision robustness and auditability. The work provides a practical framework for building agentic systems that are both scalable and responsible by design.",
      "mindmap": "graph TB\n        A[Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Agentic AI lacks explainability & responsibility] --> B1[挑战/Challenges<br>Explainability, Accountability, Robustness, Governance]\n        C[主要方法/Method<br>Multi-model Consensus & Reasoning-layer Governance] --> C1[架构/Architecture<br>Consortium of LLM/VLM Agents]\n        C --> C2[过程/Process<br>Structured Consolidation by Dedicated Reasoning Agent]\n        D[关键结果/Results<br>Improved Robustness, Transparency & Operational Trust]"
    },
    {
      "title": "Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning",
      "authors": "Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Zahid Hossain, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman",
      "institution": "Not explicitly stated in the provided content.",
      "link": "https://arxiv.org/pdf/2512.21702",
      "code": null,
      "tags": [
        "audio deepfake detection",
        "transfer learning",
        "zero-shot inference",
        "fine-tuning",
        "Bengali audio",
        "BanglaFake dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66eab5318a9b87f6facd46827f1723103def2a66467b77d3a3f1b6ea7a41d92f_w640_q70.webp",
      "contributions": "1. Conducts the first systematic benchmark for Bengali deepfake audio detection using the BanglaFake dataset. 2. Evaluates and demonstrates the limited performance of multiple pre-trained models in a zero-shot setting for this task. 3. Shows that fine-tuning deep learning models (e.g., ResNet18) significantly improves detection performance, establishing an effective approach for low-resource languages.",
      "summary": "This paper addresses the underexplored problem of Bengali deepfake audio detection. It first evaluates several pre-trained models using zero-shot inference, finding limited performance, and then fine-tunes various architectures, with ResNet18 achieving the best results. The study concludes that fine-tuning is crucial for effective deepfake detection in low-resource languages like Bengali.",
      "mindmap": "graph TB\n        A[Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning] --> B(核心问题/Problem: Bengali Deepfake Audio Detection is unexplored)\n        A --> C(主要方法/Method: Zero-shot inference & Fine-tuning of pre-trained models)\n        A --> D(关键结果/Results: Fine-tuned ResNet18 achieves best performance (79.17% accuracy))"
    },
    {
      "title": "Enabling Conversational Behavior Reasoning Capabilities in Full-Duplex Speech",
      "authors": "Shuchang Pan, Siddharth Banerjee, Dhruv Hebbar, Siddhant Patel, Akshaj Gupta, Kan Jen Cheng, Hanjo Kim, Zeyi Austin Li, Martin Q. Ma, Tingle Li, Gopala Anumanchipalli, Jiachen Lian",
      "institution": "Zhejiang University, University of California, Berkeley, Carnegie Mellon University",
      "link": "https://arxiv.org/pdf/2512.21706",
      "code": "https://got-duplex.github.io/",
      "tags": [
        "spoken dialogue systems",
        "Graph-of-Thoughts",
        "full-duplex",
        "speech acts",
        "causal inference",
        "multimodal transformer"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaad0398d39f15391b728b9e3c53af71ff071dcfd269c61b0a277091d58ee7f3_w640_q70.webp",
      "contributions": "1. A framework that models conversational behavior reasoning as causal inference within a Graph-of-Thoughts (GoT) to enable interpretable decision-making in full-duplex dialogue. 2. A hierarchical labeling scheme and hybrid training corpus combining simulated dialogues with human rationales and real speech to learn causal and temporal dependencies between intents and speech acts. 3. A system that structures streaming predictions as an evolving graph, allowing a multimodal transformer to forecast the next speech act, generate justifications, and dynamically refine its reasoning.",
      "summary": "This paper addresses the lack of explicit reasoning in full-duplex spoken dialogue systems by proposing a framework that models the perception-reasoning-generation loop as causal inference within a Graph-of-Thoughts (GoT). The method uses a hierarchical behavior detection model and a hybrid corpus to learn dependencies, enabling an agent to predict the next speech act and generate interpretable justifications. Experiments show the framework provides robust behavior detection and interpretable reasoning, establishing a foundation for benchmarking conversational reasoning.",
      "mindmap": "graph TB\n        Root[”Enabling Conversational Behavior Reasoning in Full-Duplex Speech<br/>实现全双工语音对话行为推理”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br/>Current systems lack explicit reasoning for conversational behaviors.”]\n        Method[”主要方法/Method<br/>Model reasoning as causal inference in a Graph-of-Thoughts (GoT).”]\n        Results[”关键结果/Results<br/>Robust behavior detection and interpretable reasoning chains.”]"
    },
    {
      "title": "Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers",
      "authors": "Md. Rakibul Islam, Most. Sharmin Sultana Samu, Md. Zahid Hossain, Farhad Uz Zaman, Md. Kamrozzaman Bhuiyan",
      "institution": "Not specified in provided content.",
      "link": "https://arxiv.org/pdf/2512.21709",
      "code": null,
      "tags": [
        "ai-generated text detection",
        "transformer",
        "fine-tuning",
        "zero-shot",
        "Bengali",
        "paraphrase detection"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b32597f75301412c6dbf1765506d21eb52c7e7727c1e3eefdfaa406f8c4ae44_w640_q70.webp",
      "contributions": "1. Conducts the first comparative study of transformer models for detecting AI-generated paraphrases specifically in the Bengali language. 2. Demonstrates that zero-shot evaluation of pre-trained models yields near-chance performance, highlighting the necessity of task-specific fine-tuning for this problem. 3. Shows that fine-tuning significantly boosts performance, with XLM-RoBERTa, mDeBERTa, and MultilingualBERT achieving high accuracy (~91%), establishing a strong baseline for future research.",
      "summary": "This paper addresses the challenge of detecting AI-generated paraphrased text in Bengali, a low-resource language. It evaluates five transformer models in zero-shot and fine-tuned settings, finding that fine-tuning is essential and leads to high detection accuracy (~91%) for several models. The work establishes a foundation for robust AI-generated content detection systems in Bengali.",
      "mindmap": "graph TB\n        A[Detecting AI-Generated Paraphrases in Bengali] --> B[核心问题/Problem: LLM misuse & lack of Bengali detection research]\n        A --> C[主要方法/Method: Compare 5 transformers (Zero-Shot vs. Fine-Tuned)]\n        A --> D[关键结果/Results: Fine-tuning needed; XLM-R, mDeBERTa, mBERT achieve ~91% accuracy]"
    },
    {
      "title": "Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought",
      "authors": "Yuyi Zhang, Boyu Tang, Tianjie Ju, Sufeng Duan, Gongshen Liu",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.21711",
      "code": null,
      "tags": [
        "interpretability & analysis",
        "latent tokens",
        "chain-of-thought",
        "model reliability",
        "causal analysis",
        "shortcut learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99abfa3b8406909febaa5ee077a1feab3c1d8b8cda1eebe350774e19cb82eb77_w640_q70.webp",
      "contributions": "1. Introduces \"Steering Experiments\" to causally test the impact of perturbing latent reasoning tokens, revealing COCONUT tokens are insensitive to perturbation unlike explicit CoT tokens. 2. Conducts \"Shortcut Experiments\" to evaluate models under biased and out-of-distribution settings, demonstrating COCONUT exploits dataset artifacts rather than performing genuine reasoning. 3. Repositions COCONUT as a \"pseudo-reasoning\" mechanism that generates plausible traces to conceal shortcut dependence, challenging its claimed reasoning capabilities.",
      "summary": "This paper investigates the reliability of latent reasoning tokens in LLMs, specifically Chain-of-Continuous-Thought (COCONUT). Through causal steering and adversarial shortcut experiments, it finds that COCONUT tokens are uninterpretable placeholders insensitive to perturbation and that the method relies on dataset shortcuts. The main conclusion is that COCONUT is a pseudo-reasoning mechanism that inflates benchmark performance without faithful reasoning.",
      "mindmap": "graph TB\n        A[Do Latent Tokens Think? A Causal and Adversarial Analysis of Chain-of-Continuous-Thought] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Latent token mechanisms unclear, reliability concerns] --> B1[潜在令牌机制不明确/Unclear latent token mechanisms]\n        B --> B2[可靠性问题/Reliability concerns]\n        C[主要方法/Method: Causal & adversarial analysis] --> C1[引导实验/Steering experiments]\n        C --> C2[捷径实验/Shortcut experiments]\n        D[关键结果/Results: COCONUT is pseudo-reasoning] --> D1[令牌对扰动不敏感/Tokens insensitive to perturbation]\n        D --> D2[利用数据集捷径/Exploits dataset shortcuts]\n        D --> D3[性能提升不基于真实推理/Performance gains not from true reasoning]"
    },
    {
      "title": "Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities",
      "authors": "Abd Ullah Khan, Adnan Shahid, Haejoon Jung, Hyundong Shin",
      "institution": "Kyung Hee University, Ghent University",
      "link": "https://arxiv.org/pdf/2512.21717",
      "code": null,
      "tags": [
        "communication & networking",
        "multiconnectivity",
        "SAGIN",
        "resource allocation",
        "agentic reinforcement learning",
        "heterogeneous networks"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31a130dffac74abb8ad0616817d555bf857333050b690554853596eda30c2fa7_w640_q70.webp",
      "contributions": "1. Provides a comprehensive review of current developments and key implementation challenges in SAGIN-enabled multiconnectivity. 2. Highlights the transformative potential of AI-driven approaches, particularly agentic reinforcement learning, for resource optimization in heterogeneous SAGIN environments. 3. Presents a case study demonstrating that learning-based methods can effectively enhance network performance (latency, capacity) with a moderate trade-off in power consumption.",
      "summary": "This paper reviews the challenges of implementing multiconnectivity in heterogeneous Space-Air-Ground Integrated Networks (SAGIN) and proposes AI-driven solutions, specifically agentic reinforcement learning, for optimal resource allocation. A case study shows these methods significantly improve latency and capacity, albeit with a moderate increase in power consumption as a trade-off. The work concludes by outlining open research problems for realizing efficient SAGIN-enabled multiconnectivity.",
      "mindmap": "graph TB\n        Root[”Multiconnectivity for SAGIN: Current Trends, Challenges, AI-driven Solutions, and Opportunities”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem: Heterogeneous SAGIN complicates multiconnectivity and resource allocation”]\n        Method[”主要方法/Method: Use AI-driven approaches, specifically agentic reinforcement learning”]\n        Results[”关键结果/Results: Enhanced network performance (latency, capacity) with moderate power trade-off”]"
    },
    {
      "title": "CATCH: A Controllable Theme Detection Framework with Contextualized Clustering and Hierarchical Generation",
      "authors": "Rui Ke, Jiahui Xu, Shenghao Yang, Kuang Wang, Feng Jiang, Haizhou Li",
      "institution": "The Chinese University of Hong Kong, Shenzhen; Shenzhen University of Advanced Technology; National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.21715",
      "code": null,
      "tags": [
        "dialogue systems",
        "theme detection",
        "topic clustering",
        "hierarchical generation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da012bcf7b19d126b0f1a64e4fc67ee4a82a999c3d110d6b449ab0c750d9458e_w640_q70.webp",
      "contributions": "1. A context-aware topic representation method that enriches utterance semantics using surrounding topic segments. 2. A preference-guided topic clustering mechanism that jointly models semantic proximity and personalized feedback for cross-dialogue theme alignment. 3. A hierarchical theme generation mechanism designed to suppress noise and produce robust, coherent topic labels.",
      "summary": "The paper proposes CATCH, a framework for controllable theme detection in dialogues, which integrates contextualized clustering and hierarchical generation to address sparse utterances and user preference alignment. It demonstrates effectiveness on the DSTC-12 benchmark using an 8B LLM for both clustering and label generation quality.",
      "mindmap": "graph TB\n        Root[CATCH: 可控主题检测框架 / Controllable Theme Detection Framework] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题 / Problem] --> P1[短话语稀疏语义 / Sparse, short utterances]\n        Problem --> P2[跨对话主题对齐 / Cross-dialogue theme alignment]\n        Problem --> P3[用户偏好整合 / Personalized user preferences]\n        Method[主要方法 / Method] --> M1[上下文感知主题表示 / Context-aware topic representation]\n        Method --> M2[偏好引导主题聚类 / Preference-guided topic clustering]\n        Method --> M3[分层主题生成 / Hierarchical theme generation]\n        Results[关键结果 / Results] --> R1[在DSTC-12基准测试有效 / Effective on DSTC-12 benchmark]\n        Results --> R2[提升聚类与生成质量 / Improved clustering & generation quality with 8B LLM]"
    },
    {
      "title": "An Information Theoretic Perspective on Agentic System Design",
      "authors": "Shizhe He, Avanika Narayan, Ishan S. Khare, Scott W. Linderman, Christopher Ré, Dan Biderman",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.21720",
      "code": null,
      "tags": [
        "agent system",
        "mutual information",
        "noisy channel",
        "compressor-predictor",
        "on-device AI",
        "information-theoretic"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/124535642e159e8f7a123525ffbf3cb5f163a7ae4a7876a4d1e71e7e6c885ace_w640_q70.webp",
      "contributions": "1. Proposes an information-theoretic framework for analyzing agentic LM systems, viewing the compressor as a noisy channel. 2. Introduces a task-independent estimator of mutual information between context and compression to quantify compression quality. 3. Empirically demonstrates that scaling compressor models is more effective than scaling predictors for performance and cost, enabling efficient on-device compression.",
      "summary": "The paper addresses the ad-hoc design of agentic LM systems that use a compressor LM to summarize context for a predictor LM. It proposes an information-theoretic framework using mutual information to evaluate compressors, finding that larger compressors are more accurate, concise, and information-dense, making scaling compressors more effective than scaling predictors for cost-efficient performance.",
      "mindmap": "graph TB\n        A[An Information Theoretic Perspective on Agentic System Design] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(”Agentic系统设计缺乏理论指导<br/>Agentic system design lacks theoretical guidance”)\n        C --> C1(”提出信息论框架与互信息估计器<br/>Propose information-theoretic framework & mutual information estimator”)\n        D --> D1(”更大压缩器更高效、更准确<br/>Larger compressors are more efficient and accurate”)\n        D --> D2(”扩展压缩器优于扩展预测器<br/>Scaling compressors outperforms scaling predictors”)"
    },
    {
      "title": "HELP: Hierarchical Embodied Language Planner for Household Tasks",
      "authors": "Alexandr V. Korchemnyi, Anatoly O. Onishchenko, Eva A. Bakaeva, Alexey K. Kovalev, Aleksandr I. Panov",
      "institution": "MIRAI, Cognitive AI Systems Lab",
      "link": "https://arxiv.org/pdf/2512.21723",
      "code": null,
      "tags": [
        "agent system",
        "embodied agent",
        "hierarchical planning",
        "large language model",
        "household tasks",
        "open source LLM"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d5e8ef0910254268525eec44918d2562afe5a6df81ece96ba720311313fef5b_w640_q70.webp",
      "contributions": "1. Proposes a Hierarchical Embodied Language Planner (HELP) architecture using multiple LLM-based agents for decomposing and grounding natural language instructions. 2. Demonstrates the approach on a real-world household task using an embodied agent. 3. Focuses on the use of relatively small, open-source LLMs to enable autonomous deployment.",
      "summary": "The paper addresses the challenge of planning for embodied agents following ambiguous natural language instructions in complex environments. It proposes HELP, a hierarchical planner using multiple LLM-based agents to decompose high-level instructions into grounded, executable subtasks. The method is evaluated on a household task with a real robot, showing the feasibility of using smaller, open-source LLMs for autonomous operation.",
      "mindmap": "graph TB\n        A[HELP: Hierarchical Embodied Language Planner for Household Tasks] --> B[核心问题/Problem: Embodied agents need robust planning for ambiguous natural language instructions in complex environments.]\n        A --> C[主要方法/Method: Hierarchical planner with multiple LLM-based agents to decompose and ground instructions into executable steps.]\n        A --> D[关键结果/Results: Evaluated on real-world household task; demonstrates use of smaller open-source LLMs for autonomous deployment.]"
    },
    {
      "title": "A Model of Causal Explanation on Neural Networks for Tabular Data",
      "authors": "Takashi Isozaki, Masahiro Yamamoto, Atsushi Noda",
      "institution": "Sony Computer Science Laboratories, Inc., Sony Corporation of America",
      "link": "https://arxiv.org/pdf/2512.21746",
      "code": null,
      "tags": [
        "explainable ai",
        "CENNET",
        "structural causal models",
        "entropy",
        "causal explanation",
        "tabular data"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02e8ba8968728eba560984773ed8ba2b124e42c46e3c839dec8a1ca2a5975ce1_w640_q70.webp",
      "contributions": "1. Proposes CENNET, a novel causal explanation method for neural network predictions on tabular data. 2. Introduces a new explanation power index based on entropy for evaluating the proposed method. 3. Demonstrates the method's effectiveness by combining structural causal models with neural networks for causal explanations, validated on synthetic and quasi-real data.",
      "summary": "This paper addresses the challenge of providing causal explanations for neural network predictions on tabular data, where pseudo-correlations can mislead. The authors propose a method called CENNET, which integrates structural causal models with neural networks to generate causal explanations and introduces an entropy-based index to measure explanation power. Experiments on synthetic and quasi-real data show that CENNET effectively provides causal explanations compared to existing methods.",
      "mindmap": "graph TB\n        A[A Model of Causal Explanation on Neural Networks for Tabular Data] --> B[核心问题/Problem: Explaining NN predictions on tabular data, addressing pseudo-correlation and causality]\n        A --> C[主要方法/Method: Propose CENNET, a causal explanation method using SCMs and an entropy-based index]\n        A --> D[关键结果/Results: CENNET provides causal explanations, validated via comparative experiments]"
    },
    {
      "title": "How Do Agents Perform Code Optimization? An Empirical Study",
      "authors": "Huiyun Peng, Antonio Zhong, Ricardo Andrés Calvo Méndez, Kelechi G. Kalu, James C. Davis",
      "institution": "Purdue University",
      "link": "https://arxiv.org/pdf/2512.21757",
      "code": null,
      "tags": [
        "code optimization",
        "AI coding agents",
        "performance optimization",
        "empirical study",
        "pull request analysis",
        "AIDev dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e44d9c47004517dbb7baa5f42b9023e94e10fbf2a09070a4a953b43ded2bf802_w640_q70.webp",
      "contributions": "1. Conducts the first empirical study comparing AI-agent-authored and human-authored performance optimization commits using real-world PR data. 2. Identifies a significant gap in explicit performance validation between AI-authored (45.7%) and human-authored (63.6%) PRs. 3. Finds that AI agents largely employ the same optimization patterns as humans, suggesting they learn from existing code but lack rigorous validation practices.",
      "summary": "This paper presents an empirical study comparing how AI coding agents and humans perform code optimization by analyzing performance-related pull requests from the AIDev dataset. The study finds that while AI agents use similar optimization patterns as humans, they are significantly less likely to include explicit performance validation in their commits. This highlights a key limitation in current agentic code optimization and an opportunity for improvement.",
      "mindmap": "graph TB\n        A[How Do Agents Perform Code Optimization? An Empirical Study] --> B[核心问题/Problem: AI coding agents' effectiveness on real-world performance optimization tasks is unknown.]\n        A --> C[主要方法/Method: Empirical comparison of 324 agent-generated and 83 human-authored performance PRs from AIDev dataset.]\n        A --> D[关键结果/Results: AI-authored PRs use similar patterns but include less explicit performance validation (45.7% vs 63.6%).]"
    },
    {
      "title": "A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets",
      "authors": "Arunkumar V, Firos V M, Senthilkumar S, Gangadharan G R",
      "institution": "Anna University, National Institute of Technology Tiruchirappalli",
      "link": "https://arxiv.org/pdf/2512.21760",
      "code": null,
      "tags": [
        "medical image segmentation",
        "Quaternion Neural Networks",
        "Cross-Attention",
        "Unpaired Data",
        "Multimodal Learning",
        "Explainable AI"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp",
      "contributions": "1. Proposes an Adaptive Quaternion Cross-Fusion (A-QCF) block for bidirectional knowledge transfer between unpaired CT and MRI data streams., 2. Introduces a unified segmentation model (A-QCF-Net) that leverages Quaternion Neural Networks to build a shared feature space from separate datasets., 3. Demonstrates significant performance gains over strong unimodal baselines and validates clinical relevance through explainability analysis.",
      "summary": "This paper addresses the challenge of training multimodal segmentation models with unpaired datasets. It proposes A-QCF-Net, which uses Quaternion Neural Networks and an adaptive cross-fusion block to enable knowledge transfer between separate CT and MRI data. The method significantly outperforms unimodal baselines and provides a viable paradigm for leveraging large, unpaired medical image archives.",
      "mindmap": "graph TB\n        A[A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network<br>for Multimodal Liver Tumor Segmentation from Unpaired Datasets] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Scarcity of paired & aligned multimodal medical datasets]\n        C[主要方法/Method<br>Adaptive Quaternion Cross-Fusion (A-QCF) block<br>Quaternion Neural Networks for shared feature space]\n        D[关键结果/Results<br>Significant Dice score improvement over nnU-Net<br>Validated by explainability analysis]"
    },
    {
      "title": "Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets",
      "authors": "Matyas Bohacek, Ignacio Vilanova Echavarri",
      "institution": "Stanford University, Imperial College London",
      "link": "https://arxiv.org/pdf/2512.21775",
      "code": null,
      "tags": [
        "Data Provenance",
        "Data Provenance",
        "Compliance Rating",
        "Generative AI",
        "Dataset Ethics",
        "Transparency"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa33a1fedd52ef1c87e9bf7d9a25dad61aae942ba50660263862470b9b677745_w640_q70.webp",
      "contributions": "1. Proposes the Compliance Rating Scheme (CRS), a framework for evaluating dataset compliance with transparency, accountability, and security principles. 2. Develops and releases an open-source Python library that implements the CRS framework using data provenance technology. 3. Creates a tool that is both reactive (evaluating existing datasets) and proactive (guiding the responsible construction of new datasets).",
      "summary": "The paper addresses the lack of ethical and legal oversight in the creation and sharing of datasets for Generative AI. It proposes the Compliance Rating Scheme (CRS) framework and an accompanying open-source library to assess and ensure dataset compliance with key principles. The work aims to improve traceability and accountability in the AI data supply chain.",
      "mindmap": "graph TB\n        Root(”Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”数据集创建缺乏伦理与法律监督/Lack of ethical & legal oversight in dataset creation”)\n        Problem --> P2(”数据来源与合法性信息丢失/Loss of data origin & legitimacy info”)\n        Method --> M1(”提出合规评级方案(CRS)框架/Propose Compliance Rating Scheme (CRS) framework”)\n        Method --> M2(”开发基于数据溯源技术的开源库/Develop open-source library using data provenance”)\n        Results --> R1(”评估现有数据集的合规性/Evaluate compliance of existing datasets”)\n        Results --> R2(”指导负责任的新数据集构建/Guide responsible construction of new datasets”)"
    },
    {
      "title": "Inference-based GAN Video Generation",
      "authors": "Jingbo Yang, Adrian G. Bors",
      "institution": "University of York",
      "link": "https://arxiv.org/pdf/2512.21776",
      "code": null,
      "tags": [
        "video generation",
        "VAE-GAN",
        "Markov chain",
        "long video generation",
        "temporal consistency",
        "encoder-decoder"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp",
      "contributions": "1. Proposes a new video generator, Encoder GAN3 (EncGAN3), which is a VAE-GAN hybrid structure that incorporates inference capabilities into an adversarial-based unconditional video generator. 2. Introduces a novel, memory-efficient approach to generate long videos (hundreds/thousands of frames) by extending the base VAE-GAN model. 3. Leverages a Markov chain framework with a recall mechanism, where each state is a short VAE-GAN generator, to sequentially connect video sub-sequences and ensure temporal continuity.",
      "summary": "This paper addresses the challenge of generating long, high-quality videos, a task where existing models suffer from quality degradation. The proposed method first introduces a VAE-GAN hybrid video generator (EncGAN3) and then extends it using a Markov chain framework to sequentially generate short video clips, enabling the creation of temporally consistent long videos. The main conclusion is that this approach overcomes the temporal scaling limitation and allows for memory-efficient generation of long video sequences.",
      "mindmap": "graph TB\n        Root[Inference-based GAN Video Generation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有模型难以生成长视频/Existing models struggle with long video generation]\n        P1 --> P2[视频长度增加导致质量下降/Increased length degrades quality]\n        Method[主要方法/Method] --> M1[提出VAE-GAN混合视频生成器/Propose VAE-GAN hybrid video generator]\n        M1 --> M2[使用马尔可夫链框架扩展/Extend with Markov chain framework]\n        M2 --> M3[状态代表短视频生成器/Each state is a short video generator]\n        Results[关键结果/Results] --> R1[能够生成长视频序列/Can generate long video sequences]\n        R1 --> R2[确保时序连续性与一致性/Ensures temporal continuity and consistency]"
    },
    {
      "title": "Accelerating Scientific Discovery with Autonomous Goal-evolving Agents",
      "authors": "Yuanqi Du, Botao Yu, Tianyu Liu, Tony Shen, Junwu Chen, Jan G. Rittig, Kunyang Sun, Yikun Zhang, Zhangde Song, Bo Zhou, Cassandra Masschelein, Yingze Wang, Haorui Wang, Haojun Jia, Chao Zhang, Hongyu Zhao, Martin Ester, Teresa Head-Gordon, Carla P. Gomes, Huan Sun, Chenru Duan, Philippe Schwaller, Wengong Jin",
      "institution": "Cornell University, The Ohio State University, Yale University, Simon Fraser University, École Polytechnique Fédérale de Lausanne, University of California Berkeley, Northeastern University, Deep Principle, University of Illinois Chicago, Georgia Institute of Technology, Broad Institute of MIT and Harvard",
      "link": "https://arxiv.org/pdf/2512.21782",
      "code": null,
      "tags": [
        "scientific discovery agents",
        "autonomous goal evolution",
        "bi-level optimization",
        "LLM agents",
        "objective function design",
        "scientific discovery"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9035dcf8fd16c34c235e39c8960c63fa826c45df283663a01722181f7ab419d8_w640_q70.webp",
      "contributions": "1. Identifies and addresses the unmet requirement of automating objective function design for scientific discovery agents, moving beyond fixed, imperfect proxies. 2. Proposes the SAGA framework, a novel bi-level architecture where an outer loop of LLM agents evolves objectives and an inner loop optimizes solutions under them. 3. Demonstrates the framework's effectiveness across diverse scientific domains (antibiotic, materials, DNA, chemical process design), showing improved discovery outcomes.",
      "summary": "This paper introduces SAGA, a framework for scientific discovery where LLM agents autonomously evolve and refine the objective functions used to guide optimization, rather than relying on fixed human-specified goals. This bi-level architecture enables systematic exploration of objective spaces and their trade-offs. The method is shown to substantially improve the effectiveness of discovery agents across multiple application domains.",
      "mindmap": "graph TB\n        Root[Accelerating Scientific Discovery with Autonomous Goal-evolving Agents] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[Fixed objectives are imperfect proxies for grand scientific challenges / 固定的目标函数是科学重大挑战的不完美代理]\n        Method[主要方法/Method] --> M1[Proposes SAGA: Scientific Autonomous Goal-evolving Agent / 提出SAGA: 科学自主目标演化智能体]\n        M1 --> M2[Bi-level architecture: LLM outer loop evolves objectives, inner loop optimizes solutions / 双层架构: LLM外循环演化目标，内循环优化解]\n        Results[关键结果/Results] --> R1[Applied to antibiotic, materials, DNA, chemical process design / 应用于抗生素、材料、DNA、化工过程设计]\n        R1 --> R2[Automating objective formulation improves discovery effectiveness / 自动化目标制定提升了发现效能]"
    },
    {
      "title": "Multi-agent Adaptive Mechanism Design",
      "authors": "Qiushi Han, David Simchi-Levi, Renfei Tan, Zishuo Zhao",
      "institution": "Massachusetts Institute of Technology, University of Illinois Urbana-Champaign",
      "link": "https://arxiv.org/pdf/2512.21794",
      "code": null,
      "tags": [
        "mechanism design",
        "distributionally robust optimization",
        "online learning",
        "incentive compatibility",
        "adaptive mechanism",
        "regret analysis"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ece7d8b60855735e1eee48c51256eacf5f3997d56ced3396434f12a30ad44_w640_q70.webp",
      "contributions": "1. Introduces DRAM, a novel framework combining mechanism design and online learning to handle unknown agent beliefs. 2. Provides theoretical guarantees of high-probability truthfulness and achieves optimal $\\tilde\\{O\\}(\\sqrt\\{T\\})$ cumulative regret with a matching lower bound. 3. Generalizes the framework (DRAM+) to support plug-in estimators, structured priors, and delayed feedback.",
      "summary": "This paper addresses the problem of designing a truthful mechanism when the principal has no prior knowledge of agents' beliefs. It proposes the Distributionally Robust Adaptive Mechanism (DRAM), which iteratively learns beliefs and updates a robust optimization problem to minimize cost while ensuring truthfulness. The mechanism is proven to achieve optimal regret, and the framework is the first to maintain truthfulness under these general learning conditions.",
      "mindmap": "graph TB\n        A[Multi-agent Adaptive Mechanism Design] --> B[核心问题/Problem: Elicit truthful reports with no prior knowledge of agent beliefs]\n        A --> C[主要方法/Method: Distributionally Robust Adaptive Mechanism (DRAM)]\n        A --> D[关键结果/Results: Guaranteed truthfulness & optimal $\\tilde{O}(\\sqrt{T})$ regret]"
    },
    {
      "title": "Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning",
      "authors": "Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles",
      "institution": "The Pennsylvania State University, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.21789",
      "code": null,
      "tags": [
        "image captioning",
        "scientific figure captioning",
        "large-scale dataset",
        "domain-specific training",
        "human evaluation",
        "large language models (LLMs)"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp",
      "contributions": "1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions.",
      "summary": "This paper reviews the SciCap project's first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field.",
      "mindmap": "graph TB\n        Root[Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[科学图表说明质量差/Poor quality of scientific figure captions]\n        Problem --> P2[缺乏大规模真实数据集/Lack of large-scale real-world dataset]\n        Method --> M1[构建arXiv图表-说明对数据集/Construct arXiv figure-caption dataset]\n        Method --> M2[领域特定训练与评估/Domain-specific training & evaluation]\n        Method --> M3[应对大语言模型兴起/Navigate rise of LLMs]\n        Results --> R1[总结技术方法经验/Summarize technical & methodological lessons]\n        Results --> R2[提出未来挑战与方向/Outline future challenges & directions]"
    },
    {
      "title": "InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation",
      "authors": "Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang, Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan",
      "institution": "ByteDance Inc., Rutgers University",
      "link": "https://arxiv.org/pdf/2512.21788",
      "code": "https://github.com/yanq095/InstructMoLE",
      "tags": [
        "diffusion models",
        "Parameter-Efficient Fine-Tuning",
        "Mixture of Low-rank Experts",
        "Instruction-Guided Routing",
        "Multi-Conditional Generation",
        "Diffusion Transformers"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp",
      "contributions": "1. Introduces InstructMoLE, a framework using an Instruction-Guided Mixture of Low-Rank Experts for multi-conditional image generation., 2. Proposes Instruction-Guided Routing (IGR), a global routing signal derived from user instructions to select a coherent expert council for all tokens, addressing spatial fragmentation and semantic drift., 3. Introduces an output-space orthogonality loss to promote expert functional diversity and prevent representational collapse.",
      "summary": "This paper addresses the problem of task interference in multi-conditional image generation when using monolithic PEFT adapters like LoRA. It proposes InstructMoLE, a novel framework that uses global instruction-guided routing to select a consistent mixture of low-rank experts, combined with an orthogonality loss for diversity, which outperforms existing methods on benchmarks.",
      "mindmap": "graph TB\n        A[InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation] --> B[核心问题/Problem: Task interference & spatial fragmentation in multi-conditional DiT fine-tuning]\n        A --> C[主要方法/Method: Global Instruction-Guided Routing (IGR) & output-space orthogonality loss]\n        A --> D[关键结果/Results: Outperforms LoRA & MoLE variants on benchmarks]"
    },
    {
      "title": "CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection",
      "authors": "Ruochen Liu, Yi Tian, Jiahao Wang, Hongbin Liu, Xianxu Hou, Jingxin Liu",
      "institution": "University of Liverpool, National University of Singapore, Xi'an Jiaotong-Liverpool University",
      "link": "https://arxiv.org/pdf/2512.21803",
      "code": null,
      "tags": [
        "object detection",
        "Mamba",
        "Triple-Mapping Adaptive Coupling (TMAC)",
        "Adaptive Mamba Head",
        "biomedical instance detection",
        "VSSD backbone"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp",
      "contributions": "1. Proposes a novel Triple-Mapping Adaptive Coupling (TMAC) module that splits channels into parallel branches with dual idiosyncratic and one consensus attention map for enhanced spatial discriminability. 2. Designs an Adaptive Mamba Head that fuses multi-scale features via learnable weights to handle varying object sizes robustly. 3. Introduces CellMamba, a lightweight one-stage detector built on a VSSD backbone with CellMamba Blocks, achieving superior accuracy and efficiency on biomedical cell detection datasets.",
      "summary": "This paper proposes CellMamba, a lightweight one-stage detector for cell detection in pathological images. It introduces a novel Triple-Mapping Adaptive Coupling (TMAC) module and an Adaptive Mamba Head to improve spatial discriminability and multi-scale feature fusion. Experiments show CellMamba outperforms CNN, Transformer, and Mamba baselines in accuracy while being more efficient in model size and inference speed.",
      "mindmap": "graph TB\n        A[CellMamba: Adaptive Mamba for Cell Detection] --> B[核心问题/Problem: Cell detection challenges in pathological images]\n        A --> C[主要方法/Method: CellMamba with TMAC module & Adaptive Mamba Head]\n        A --> D[关键结果/Results: Outperforms baselines, lightweight & efficient]"
    },
    {
      "title": "S&P 500 Stock's Movement Prediction using CNN",
      "authors": "Rahul Gupta",
      "institution": "None (No affiliation or email domain provided in the given content)",
      "link": "https://arxiv.org/pdf/2512.21804",
      "code": null,
      "tags": [
        "financial time series forecasting",
        "Convolutional Neural Network (CNN)",
        "multivariate raw data",
        "stock movement prediction",
        "historical data matrices",
        "S&P 500"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp",
      "contributions": "1. Proposes the application of Convolutional Neural Networks (CNNs), typically used for image classification, to the problem of stock movement prediction by treating multivariate historical stock data as image-like matrices. 2. Utilizes raw, unprocessed market data including events like stock splits and dividends, instead of relying on pre-engineered financial features. 3. Demonstrates a flexible prediction framework that can be applied at different levels: individual stocks, sectors, or entire portfolios.",
      "summary": "This paper tackles the problem of predicting stock price movements for the S&P 500 index. The core method involves using a Convolutional Neural Network (CNN) to analyze multivariate historical stock data, which is structured as image-like matrices, without extensive feature engineering. The approach shows promising results and offers a flexible model for stock, sector, or portfolio-level predictions.",
      "mindmap": "graph TB\n        Root[”S&P 500 Stock's Movement Prediction using CNN<br>使用CNN预测标普500股票走势”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Predicting stock price movement<br>预测股票价格走势”] --> P1[”传统方法依赖特征工程<br>Traditional methods rely on engineered features”]\n        Problem --> P2[”现有研究多使用单维数据<br>Existing research often uses single-dimension data”]\n        Method[”主要方法/Method<br>Use CNN on raw multivariate data<br>对原始多变量数据使用CNN”] --> M1[”将历史数据矩阵视为图像<br>Treat historical data matrices as images”]\n        Method --> M2[”包含原始市场事件(如拆股)<br>Include raw market events (e.g., splits)”]\n        Results[”关键结果/Results<br>Model achieves promising results<br>模型取得有希望的结果”] --> R1[”支持股票/行业/组合级别预测<br>Supports stock/sector/portfolio prediction”]"
    },
    {
      "title": "MoonBot: Modular and On-Demand Reconfigurable Robot Toward Moon Base Construction",
      "authors": "Kentaro Uno, Elian Neppel, Gustavo H. Diaz, Ashutosh Mishra, Shamistan Karimov, A. Sejal Jain, Ayesha Habib, Pascal Pama, Hazal Gozbasi, Shreya Santra, Kazuya Yoshida",
      "institution": "Space Robotics Laboratory (SRL), Department of Aerospace Engineering, Graduate School of Engineering, Tohoku University",
      "link": "https://arxiv.org/pdf/2512.21853",
      "code": null,
      "tags": [
        "space robotics",
        "modular robot",
        "reconfigurable robot",
        "lunar construction",
        "field demonstration",
        "connector design"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7ceec836e29be790b6ca39392714dc64e19923b0842210e75988ad1c5fdeeb2_w640_q70.webp",
      "contributions": "1. Introduces MoonBot, a modular and reconfigurable robotic system designed for lunar payload constraints and task adaptability. 2. Details the system's design and development, including a field demonstration simulating lunar infrastructure tasks like civil engineering and component deployment. 3. Systematically summarizes lessons learned, particularly on connector design, to inform future modular robotic systems for lunar missions.",
      "summary": "This paper introduces MoonBot, a modular and reconfigurable robot designed for constructing lunar bases under strict mass constraints. It details the robot's design and validates its concept through field demonstrations of simulated construction tasks. The work concludes with lessons learned, especially regarding connector design, to guide future lunar robotic systems.",
      "mindmap": "graph TB\n        A[MoonBot: 面向月球基地建设的模块化按需可重构机器人] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[月球探索与基地建设需求 / Lunar Exploration & Base Construction Needs]\n        C --> C1[模块化可重构机器人系统 / Modular & Reconfigurable Robotic System]\n        C --> C2[概念验证与现场演示 / Proof-of-Concept & Field Demonstration]\n        D --> D1[成功执行模拟任务 / Successfully Executed Simulated Tasks]\n        D --> D2[总结了连接器设计等经验教训 / Summarized Lessons (e.g., Connector Design)]"
    },
    {
      "title": "HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs",
      "authors": "Jiaxin Liu, Peiyi Tu, Wenyu Chen, Yihong Zhuang, Xinxia Ling, Anji Zhou, Chenxi Wang, Zhuo Han, Zhengkai Yang, Junbo Zhao, Zenan Huang, Yuanyuan Wang",
      "institution": "Ant Group, Xiamen University, Beijing Normal University, Zhejiang University",
      "link": "https://arxiv.org/pdf/2512.21849",
      "code": "https://github.com/inclusionAI/HeartBench",
      "tags": [
        "evaluation",
        "anthropomorphic intelligence",
        "benchmark",
        "psychological counseling",
        "rubric-based evaluation",
        "reasoning-before-scoring"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0dc9f1570e111d07840de8240e6e5f545f05ae646e05a5121a0a6c4037e3637a_w640_q70.webp",
      "contributions": "1. Introduces HeartBench, a novel benchmark framework for evaluating the integrated emotional, cultural, and ethical dimensions (anthropomorphic intelligence) of Chinese LLMs. 2. Proposes a theory-driven taxonomy and a case-specific, rubric-based \"reasoning-before-scoring\" evaluation protocol to translate abstract human-like traits into measurable criteria. 3. Provides an analysis revealing a significant performance gap in current LLMs, especially in scenarios with subtle emotional subtexts and complex ethical trade-offs, establishing a standardized metric and a blueprint for creating human-aligned training data.",
      "summary": "The paper addresses the gap in evaluating the social and emotional intelligence (anthropomorphic intelligence) of LLMs, particularly in the Chinese context. It proposes HeartBench, a benchmark framework grounded in psychological counseling scenarios, which uses a rubric-based evaluation method. The assessment of 13 LLMs shows a substantial performance ceiling, with even top models achieving only 60% of the expert ideal, highlighting significant decay in handling complex emotional and ethical nuances.",
      "mindmap": "graph TB\n        A[HeartBench: Probing Core Dimensions of Anthropomorphic Intelligence in LLMs] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[LLMs缺乏拟人化智能 / LLMs lack anthropomorphic intelligence]\n        B --> B2[中文语境缺乏评估框架 / Lack of evaluation frameworks in Chinese context]\n        C --> C1[基于心理咨询场景的基准 / Benchmark based on psychological counseling scenarios]\n        C --> C2[理论驱动的分类法 / Theory-driven taxonomy]\n        C --> C3[基于量规的推理评分法 / Rubric-based reasoning-before-scoring]\n        D --> D1[模型性能存在上限 / Performance ceiling in models]\n        D --> D2[复杂场景表现显著下降 / Significant decay in complex scenarios]"
    },
    {
      "title": "A Comedy of Estimators: On KL Regularization in RL Training of LLMs",
      "authors": "Vedant Shah, Johan Obando-Ceron, Vineet Jain, Brian Bartoldson, Bhavya Kailkhura, Sarthak Mittal, Glen Berseth, Pablo Samuel Castro, Yoshua Bengio, Nikolay Malkin, Moksh Jain, Siddarth Venkatraman, Aaron Courville",
      "institution": "Mila – Quebec AI Institute, Université de Montréal, McGill University, LLNL, University of Edinburgh, CIFAR",
      "link": "https://arxiv.org/pdf/2512.21852",
      "code": null,
      "tags": [
        "reinforcement learning",
        "KL divergence",
        "policy gradient",
        "on-policy sampling",
        "off-policy training",
        "gradient bias"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96999f081bc421143202d98f560b5d13a6fb0c09613b8c160b121158bce3811a_w640_q70.webp",
      "contributions": "1. Systematic analysis of KL divergence estimator configurations in RL for LLMs, revealing how design choices introduce gradient bias. 2. Empirical demonstration that estimator configurations with unbiased gradients lead to better and more stable performance on both in-domain and out-of-domain tasks. 3. Investigation showing KL regularization can stabilize off-policy RL training in asynchronous setups.",
      "summary": "This paper analyzes the use of various estimators for the KL divergence regularization term in RL fine-tuning of LLMs, finding that common practices introduce biased gradients. Through experiments on models like Qwen2.5-7B, the study shows that using estimator configurations with unbiased gradients improves training stability and downstream task performance. The work also finds that KL regularization helps stabilize off-policy RL training.",
      "mindmap": "graph TB\n        Root[”A Comedy of Estimators: On KL Regularization in RL Training of LLMs<br>论文标题”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>KL正则化估计器配置缺乏系统研究，梯度存在偏差”] --> P1[”实践问题/Practical Issue<br>广泛使用但实现与目标不一致”]\n        Problem --> P2[”理论问题/Theoretical Issue<br>梯度偏差影响训练稳定性”]\n        Method[”主要方法/Method<br>分析梯度偏差并进行实证验证”] --> M1[”分析/Analysis<br>研究多种估计器配置的梯度”]\n        Method --> M2[”实验/Experiments<br>RL微调多个LLM并评估性能”]\n        Results[”关键结果/Results<br>无偏梯度配置带来更好性能”] --> R1[”在线策略/On-Policy<br>无偏梯度配置提升稳定性和性能”]\n        Results --> R2[”离线策略/Off-Policy<br>KL正则化有助于稳定异步训练”]"
    },
    {
      "title": "Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening",
      "authors": "Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan",
      "institution": "North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh",
      "link": "https://arxiv.org/pdf/2512.21861",
      "code": null,
      "tags": [
        "medical image classification",
        "feature-level fusion",
        "convolutional neural networks",
        "diabetic retinopathy screening",
        "EfficientNet",
        "DenseNet"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp",
      "contributions": "1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.",
      "summary": "This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.",
      "mindmap": "graph TB\n        A[Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Large-scale DR screening is constrained by limited specialists and variable image quality.]\n        C[主要方法/Method<br>Feature-level fusion of complementary CNN backbones (e.g., EfficientNet, DenseNet) on pooled fundus images.]\n        D[关键结果/Results<br>Fusion outperforms single models. Eff+Den fusion offers best accuracy-latency balance.]"
    },
    {
      "title": "Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?",
      "authors": "Naen Xu, Jinghuai Zhang, Changjiang Li, Hengyu An, Chunyi Zhou, Jun Wang, Boyu Xu, Yuyuan Li, Tianyu Du, Shouling Ji",
      "institution": "Zhejiang University, University of California, Los Angeles, Palo Alto Networks",
      "link": "https://arxiv.org/pdf/2512.21871",
      "code": "https://github.com/bluedream02/CopyGuard",
      "tags": [
        "multi-modal inference",
        "copyright compliance",
        "vision-language models",
        "tool-augmented defense",
        "benchmark dataset",
        "multimodal query"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a933ea78af16685ceab38b447862e9c50b08de435c2e6b662d59551bf5552fdc_w640_q70.webp",
      "contributions": "1. Introduced a large-scale benchmark dataset of 50,000 multimodal query-content pairs to evaluate copyright compliance in LVLMs. 2. Conducted a comprehensive evaluation revealing significant deficiencies in state-of-the-art LVLMs' ability to recognize and respect copyrighted content. 3. Proposed a novel tool-augmented defense framework to reduce copyright infringement risks in LVLM inference.",
      "summary": "This paper evaluates how large vision-language models (LVLMs) handle copyrighted visual content and finds they often fail to comply with copyright regulations. To address this, the authors propose a tool-augmented defense framework for copyright compliance. The work highlights the need for developing copyright-aware LVLMs to ensure responsible use.",
      "mindmap": "graph TB\n        Root[”Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?”]\n        Root --> Problem[”核心问题/Problem: LVLMs may infringe copyright when processing visual inputs”]\n        Root --> Method[”主要方法/Method: Benchmark dataset & Tool-augmented defense framework”]\n        Root --> Results[”关键结果/Results: Current LVLMs are deficient; Proposed framework reduces risk”]"
    },
    {
      "title": "Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation",
      "authors": "Yiming Qian, Thorsten Neumann, Xueyining Huang, David Hardoon, Fei Gao, Yong Liu, Siow Mong Rick Goh",
      "institution": "Institute of High Performance Computing (A*STAR), Standard Chartered Bank, Xi’an Jiaotong–Liverpool University",
      "link": "https://arxiv.org/pdf/2512.21866",
      "code": null,
      "tags": [
        "Privacy-preserving machine learning",
        "dataset distillation",
        "random forest",
        "synthetic data generation",
        "explainable AI",
        "membership-inference attack"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6994d363f1e76d7cc78ab02d48685c11199b353aab61b3eb5297064b1df9b722_w640_q70.webp",
      "contributions": "1. Proposes a privacy-preserving dataset distillation framework that converts a trained random forest into transparent rule regions and generates synthetic data by uniform sampling within these regions, creating a compact, auditable surrogate dataset. 2. Enables explainable AI by providing both global pattern summaries (e.g., support, lift) from aggregated rules and local, human-readable rationales with calibrated uncertainty for individual cases based on tree-vote disagreement. 3. Demonstrates strong utility-privacy trade-offs, showing the distilled data maintains competitive model performance (e.g., precision, F1) with significant data reduction (85-93%), improves cross-institution learning, and resists membership-inference attacks (AUC ~0.5), indicating low memorization risk.",
      "summary": "This paper proposes a dataset distillation method for financial fraud detection that converts a random forest model into interpretable rule regions to generate synthetic data, preserving privacy and model utility. The approach produces a compact, explainable dataset that supports collaborative learning across institutions while resisting privacy attacks. Experiments show it reduces data volume by over 85% with minimal performance loss and enhances cross-cluster fraud detection.",
      "mindmap": "graph TB\n        Root(”Secure and Explainable Fraud Detection in Finance via Hierarchical Multi-source Dataset Distillation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”需要隐私保护的协作式欺诈检测/Need for privacy-preserving collaborative fraud detection”)\n        Problem --> P2(”模型需要可解释性/Model needs explainability”)\n        Method --> M1(”将随机森林转换为规则区域/Convert random forest to rule regions”)\n        Method --> M2(”在区域内均匀采样生成合成数据/Uniformly sample within regions to generate synthetic data”)\n        Results --> R1(”数据量减少85-93%/Data volume reduced by 85-93%”)\n        Results --> R2(”保持竞争性性能/Maintains competitive performance”)\n        Results --> R3(”抵抗成员推理攻击/Resists membership-inference attacks”)"
    },
    {
      "title": "MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting",
      "authors": "Marc S. Montalvo, Hamed Yaghoobian",
      "institution": "Rochester Institute of Technology, Muhlenberg College",
      "link": "https://arxiv.org/pdf/2512.21878",
      "code": null,
      "tags": [
        "agent system",
        "multi-agent framework",
        "bias mitigation",
        "financial forecasting",
        "LLM integration",
        "modular design"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/351cdc3ccfe2b2d987cf53c8380e153fe4b93de0def6253cbc7b2feb4af093fe_w640_q70.webp",
      "contributions": "1. Introduces MASFIN, a modular multi-agent framework that integrates LLMs with structured financial metrics and unstructured news for decomposed financial reasoning. 2. Embeds explicit bias-mitigation protocols (e.g., against survivorship and hindsight bias) to enhance transparency and robustness. 3. Demonstrates practical effectiveness through an eight-week evaluation showing outperformance of major market benchmarks, highlighting the promise of bias-aware generative AI in finance.",
      "summary": "The paper introduces MASFIN, a multi-agent system that combines LLMs with financial data and news to perform decomposed reasoning and forecasting while mitigating biases. In an eight-week evaluation, it achieved a 7.33% cumulative return, outperforming benchmarks like the S&P 500 in most weeks, though with higher volatility. The results show the potential of modular, bias-aware AI frameworks for transparent and reproducible quantitative finance.",
      "mindmap": "graph TB\n        A[MASFIN: A Multi-Agent System for Decomposed Financial Reasoning and Forecasting] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统量化方法易受生存偏差影响/Traditional quantitative methods vulnerable to survivorship bias]\n        B --> B2[AI方法在信号集成和可复现性上存在挑战/AI approaches struggle with signal integration and reproducibility]\n        C --> C1[模块化多智能体框架/Modular multi-agent framework]\n        C --> C2[集成LLM与结构化指标和非结构化新闻/Integrates LLMs with structured metrics and unstructured news]\n        C --> C3[嵌入偏差缓解协议/Embeds bias-mitigation protocols]\n        D --> D1[8周累计回报7.33%/7.33% cumulative return over eight weeks]\n        D --> D2[在6/8周中超越基准/Outperformed benchmarks in six of eight weeks]\n        D --> D3[波动性较高/Higher volatility]"
    },
    {
      "title": "CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics",
      "authors": "Vaibhav Devraj, Dhruv Kumar, Jagat Sesh Challa",
      "institution": "Birla Institute of Technology and Science (BITS), Pilani",
      "link": "https://arxiv.org/pdf/2512.21877",
      "code": null,
      "tags": [
        "text-to-sql",
        "benchmark",
        "multilingual",
        "domain-specific",
        "large language models",
        "sports analytics"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd335127a490c2b4b59330fd1867a57551c792f1b695f15e48789a3992b7c05a_w640_q70.webp",
      "contributions": "1. Introduces CricBench, a novel benchmark for evaluating LLMs on Text-to-SQL tasks in the specialized domain of cricket analytics. 2. Establishes a multilingual framework, providing a \"Gold Standard\" dataset in both English and Hindi, with extensibility to other languages. 3. Demonstrates a significant performance gap for LLMs between general and specialized domains and challenges the assumption of English as the optimal prompt language for such tasks.",
      "summary": "This paper introduces CricBench, a multilingual benchmark for evaluating Large Language Models on Text-to-SQL tasks in the specialized domain of cricket analytics. The benchmark features a manually curated dataset in English and Hindi and is used to evaluate six state-of-the-art models. The results show that high performance on general benchmarks does not transfer well to this specialized domain, and surprisingly, code-mixed Hindi queries can perform as well as or better than English ones.",
      "mindmap": "graph TB\n        A[CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLMs在专业领域Text-to-SQL能力未充分探索/LLMs' Text-to-SQL capability in specialized domains is under-explored]\n        B --> B2[现有基准缺乏多语言和体育分析特性/Existing benchmarks lack multilingual and sports analytics features]\n        C --> C1[构建板球领域专业多语言基准/Build a specialized multilingual benchmark for cricket]\n        C --> C2[与专家合作创建”黄金标准”查询/Collaborate with experts to create ”Gold Standard” queries]\n        C --> C3[评估六个最先进的LLMs/Evaluate six state-of-the-art LLMs]\n        D --> D1[专业领域性能显著下降/Significant performance drop in specialized domain]\n        D --> D2[DeepSeek R1表现最佳/DeepSeek R1 achieves SOTA]\n        D --> D3[印地语查询准确率可比或更高/Hindi queries yield parity or higher accuracy]"
    },
    {
      "title": "Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models",
      "authors": "Tingyang Sun, Ting He, Bo Ji, Parimal Parag",
      "institution": "Pennsylvania State University, Virginia Tech, Indian Institute of Science",
      "link": "https://arxiv.org/pdf/2512.21884",
      "code": null,
      "tags": [
        "llm inference",
        "distributed inference",
        "block placement",
        "request routing",
        "performance modeling",
        "resource allocation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25938e1be55cbd072ba066aea4bb0e492f8b8c2a83e48eaa7e09e800b8697383_w640_q70.webp",
      "contributions": "1. Developed experimentally validated performance models for distributed LLM inference under given block placement and request routing decisions. 2. Formulated the offline optimization problem as a MILP, proved its NP-hardness, and designed a polynomial-complexity algorithm with performance guarantees. 3. Adapted the offline algorithm for the online setting with the same performance guarantee under bounded load.",
      "summary": "This paper addresses the resource allocation problem for geographically-distributed LLM inference, focusing on optimizing block placement and request routing. It proposes performance models, offline and online algorithms with theoretical guarantees, and a lightweight CPU-only simulator. The solution significantly reduces inference time compared to the state-of-the-art in diverse distributed settings.",
      "mindmap": "graph TB\n        A[Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 分布式LLM推理的资源分配优化/Optimizing resource allocation for distributed LLM inference]\n        C[主要方法/Method: 性能建模与优化算法/Performance modeling and optimization algorithms]\n        D[关键结果/Results: 显著降低推理时间/Substantially reduces inference time]"
    },
    {
      "title": "Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space",
      "authors": "Weichen Zhang, Peizhi Tang, Xin Zeng, Fanhang Man, Shiquan Yu, Zichao Dai, Baining Zhao, Hongjin Chen, Yu Shang, Wei Wu, Chen Gao, Xinlei Chen, Xin Wang, Yong Li, Wenwu Zhu",
      "institution": "Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.21887",
      "code": null,
      "tags": [
        "visual navigation",
        "world model",
        "future frame projection",
        "4-dof uav",
        "long-horizon visual generation",
        "aerial navigation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02f69e3df37002aba4354016667950073739b574c3c8044ad15f65d9721e62db_w640_q70.webp",
      "contributions": "1. Proposes ANWM, an aerial navigation world model for predicting future visual observations to incorporate high-level semantics into UAV path planning. 2. Introduces a physics-inspired Future Frame Projection (FFP) module to provide coarse geometric priors and mitigate uncertainty in long-distance visual generation. 3. Demonstrates superior performance in long-distance visual forecasting and improves UAV navigation success rates in large-scale 3D environments.",
      "summary": "This paper proposes ANWM, an aerial navigation world model that predicts future visual observations for UAVs using a novel Future Frame Projection module. It addresses the challenges of complex 4-DoF action spaces and long-horizon visual generation. The model outperforms existing methods in visual forecasting and enhances navigation success in large-scale environments.",
      "mindmap": "graph TB\n        A[Aerial World Model for Long-horizon Visual Generation and Navigation in 3D Space] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[UAV导航缺乏高层语义规划能力/UAV navigation lacks high-level semantic planning]\n        B --> B2[现有模型难以处理复杂动作空间与长距离视觉生成/Existing models struggle with complex action space & long-horizon visual generation]\n        C --> C1[提出ANWM世界模型/Propose ANWM world model]\n        C --> C2[引入未来帧投影模块/Introduce Future Frame Projection module]\n        D --> D1[长距离视觉预测性能显著提升/Significantly outperforms in long-distance visual forecasting]\n        D --> D2[提高大规模环境导航成功率/Improves UAV navigation success rates in large-scale environments]"
    },
    {
      "title": "Flexible Multitask Learning with Factorized Diffusion Policy",
      "authors": "Chaoqi Liu, Haonan Chen, Sigmund H. Høeg, Shaoxiong Yao, Yunzhu Li, Kris Hauser, Yilun Du",
      "institution": "University of Illinois at Urbana-Champaign, Harvard University, Norwegian University of Science and Technology, Columbia University",
      "link": "https://arxiv.org/pdf/2512.21898",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion policy",
        "modular architecture",
        "multitask learning",
        "imitation learning",
        "mixture-of-experts"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b261f496908cd892964760d9f52edb76c57a6126f2c6a9969b7e36d9d43b048e_w640_q70.webp",
      "contributions": "1. Introduces a novel modular diffusion policy framework (FDP) that factorizes complex action distributions into a composition of specialized diffusion models. 2. Proposes continuous score aggregation via an observation-conditioned router for stable training and clear component specialization, addressing issues in standard MoE. 3. Demonstrates that the modular structure enables flexible policy adaptation to new tasks and mitigates catastrophic forgetting.",
      "summary": "This paper addresses the challenge of multitask imitation learning in robotics, where complex action distributions are difficult to model. It proposes a Factorized Diffusion Policy (FDP) that decomposes the policy into specialized diffusion components and composes them via a router. The method outperforms baselines in simulation and real-world manipulation and supports flexible adaptation.",
      "mindmap": "graph TB\n        A[Flexible Multitask Learning with Factorized Diffusion Policy] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[机器人多任务学习/Robot Multitask Learning]\n        B1 --> B2[动作分布复杂多模态/Action Distribution Highly Multimodal]\n        B2 --> B3[单体模型欠拟合与不灵活/Monolithic Models Underfit & Inflexible]\n        C --> C1[因子化扩散策略/Factorized Diffusion Policy (FDP)]\n        C1 --> C2[模块化扩散专家/Modular Diffusion Experts]\n        C2 --> C3[基于观察的路由器/Observation-Conditioned Router]\n        C3 --> C4[连续分数聚合/Continuous Score Aggregation]\n        D --> D1[性能超越基线/Outperforms Baselines]\n        D1 --> D2[仿真与真实机器人验证/Simulation & Real-World Validation]\n        D2 --> D3[支持灵活策略适应/Enables Flexible Policy Adaptation]"
    },
    {
      "title": "MMCTOP: A Multimodal Textualization and Mixture-of-Experts Framework for Clinical Trial Outcome Prediction",
      "authors": "Carolina Aparício, Qi Shi, Bo Wen, Tesfaye Yadete, Qiwei Han",
      "institution": "Nova School of Business and Economics, Hogarthian Technologies, IBM Research, Cleveland Clinic, Oregon Health & Science University",
      "link": "https://arxiv.org/pdf/2512.21897",
      "code": null,
      "tags": [
        "multi-modal training",
        "multimodal fusion",
        "sparse mixture-of-experts",
        "schema-guided textualization",
        "clinical trial prediction",
        "temperature scaling"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb552211ea905d5cbf8e190ead9078caf65c5d200327a02c7a6dab156a030645_w640_q70.webp",
      "contributions": "1. A multimodal framework (MMCTOP) that integrates molecular structures, protocol metadata, eligibility narratives, and disease ontologies for clinical trial outcome prediction. 2. A novel architecture combining schema-guided textualization for data normalization and a drug-disease-conditioned sparse Mixture-of-Experts (SMoE) for context-aware, specialized multimodal fusion. 3. Demonstrates improved prediction performance (precision, F1, AUC) over baselines and incorporates operational safeguards like temperature scaling for calibrated probabilities to enhance auditability and reproducibility.",
      "summary": "The paper proposes MMCTOP, a multimodal framework for predicting clinical trial outcomes. It uses schema-guided textualization to normalize heterogeneous data and a sparse Mixture-of-Experts model for specialized fusion, achieving better performance than existing methods and providing calibrated risk estimates.",
      "mindmap": "graph TB\n        Root[MMCTOP: 多模态文本化与专家混合框架<br>MMCTOP: Multimodal Textualization and Mixture-of-Experts Framework] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>多模态数据融合挑战<br>Multimodal Data Fusion Challenge] --> P1[高维生物医学信息学<br>High-Dim Biomedical Informatics]\n        Method[主要方法/Method<br>多模态框架<br>Multimodal Framework] --> M1[模式感知表征学习<br>Modality-Aware Representation Learning]\n        Method --> M2[架构设计/Architecture Design]\n        M1 --> M1_1[领域特定编码器<br>Domain-Specific Encoders]\n        M2 --> M2_1[模式感知表征学习<br>Modality-Aware Representation Learning]\n        M2 --> M2_2[稀疏专家混合<br>Sparse Mixture-of-Experts (SMoE)]\n        M2 --> M2_3[模式感知表征学习<br>Modality-Aware Representation Learning]\n        Results[关键结果/Results<br>性能提升与校准<br>Performance & Calibration] --> R1[指标改进<br>Metric Improvements]\n        Results --> R2[消融研究<br>Ablation Studies]\n        Results --> R3[概率校准<br>Probability Calibration]"
    },
    {
      "title": "SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?",
      "authors": "Kenny Workman, Zhen Yang, Harihara Muralidharan, Hannah Le",
      "institution": "LatchBio",
      "link": "https://arxiv.org/pdf/2512.21907",
      "code": null,
      "tags": [
        "agent system",
        "spatial transcriptomics",
        "AI agents",
        "benchmark",
        "deterministic grader",
        "harness design"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3912de9f7e0f0d2acbf8bcd709b023f2ed3b9ccd56886885ca3f8e9e9e81880_w640_q70.webp",
      "contributions": "1. Introduces SpatialBench, a benchmark of 146 verifiable problems derived from real-world spatial biology analysis workflows, covering five technologies and seven task categories. 2. Provides a deterministic grader for each problem to evaluate the recovery of key biological results from messy spatial datasets. 3. Demonstrates through benchmark data that frontier AI agents have low accuracy (20-38%) on these tasks and reveals the significant impact of harness design (tools, prompts, control flow, execution environment) on performance.",
      "summary": "The paper introduces SpatialBench, a benchmark to evaluate whether AI agents can analyze messy, real-world spatial biology data. It tests frontier models on 146 practical problems and finds low accuracy, highlighting that performance heavily depends on the agent's harness design. The benchmark serves as a tool to measure and diagnose agent capabilities for faithful and reproducible data analysis.",
      "mindmap": "graph TB\n        A[SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[AI代理能否从混乱的真实空间数据中提取生物学见解?/Can AI agents extract biological insight from messy, real-world spatial datasets?]\n        C --> C1[引入包含146个可验证问题的基准SpatialBench/Introduce SpatialBench benchmark with 146 verifiable problems]\n        C --> C2[提供确定性评分器评估关键生物学结果恢复/Provide deterministic grader to evaluate recovery of key biological result]\n        D --> D1[基础模型准确率低 (20-38%)/Base model accuracy remains low (20-38%)]\n        D --> D2[工具链设计对性能有重大影响/Harness design has large empirical effect on performance]"
    },
    {
      "title": "Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model",
      "authors": "Nathan Kallus",
      "institution": "Netflix, Cornell University",
      "link": "https://arxiv.org/pdf/2512.21917",
      "code": null,
      "tags": [
        "reinforcement learning from human feedback (rlhf)",
        "preference optimization",
        "single-index model",
        "semiparametric",
        "link function",
        "policy learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/378875cc0ef0eb142c184d960e479724ea3df83c84cf81e185efea5469a4387e_w640_q70.webp",
      "contributions": "1. Formulates policy alignment as a semiparametric single-index model problem, relaxing the need for a known link function between preferences and rewards. 2. Develops novel policy learners based on profiling, orthogonalizing, and link-agnostic ranking objectives, providing theoretical error bounds. 3. Proposes practical first-order optimization implementations that are robust to unknown preference noise and scale, enabling direct policy optimization without explicit reward fitting.",
      "summary": "The paper addresses the problem of bias in aligning language models when the assumed link function between human preferences and latent rewards is misspecified. It proposes a semiparametric framework that treats the link function as unknown, develops several robust policy learning algorithms, and provides theoretical guarantees. The main conclusion is that this approach enables more reliable policy alignment without needing to correctly specify the preference noise distribution.",
      "mindmap": "graph TB\n        Root[”Semiparametric Preference Optimization<br>你的语言模型是一个单指标模型”] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题/Problem<br>已知链接函数错误导致策略偏差<br>Misspecified link function causes policy misalignment”]\n        Method[”主要方法/Method<br>将链接函数视为未知的半参数单指标模型<br>Treat link as unknown semiparametric single-index model”]\n        Results[”关键结果/Results<br>开发鲁棒的策略学习器并提供理论保证<br>Develop robust policy learners with theoretical guarantees”]"
    },
    {
      "title": "Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning",
      "authors": "Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.21924",
      "code": null,
      "tags": [
        "medical image analysis",
        "unsupervised anomaly detection",
        "disentangled representation",
        "pseudo-healthy image reconstruction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp",
      "contributions": "1. Proposed a disentangled representation module to decouple brain MRI into imaging-invariant anatomical information and imaging-specific information, improving model generalizability across multi-modality and multi-center data. 2. Designed an edge-to-image restoration module that reconstructs high-quality pseudo-healthy images from edge information, suppressing the propagation of abnormal residuals. 3. Introduced brain anatomical priors and a differentiable one-hot encoding operator to constrain and stabilize the disentanglement learning process.",
      "summary": "This paper addresses the limitations of generalizability and performance in unsupervised anomaly detection for brain MRI. It proposes a new framework that disentangles anatomical from imaging information and reconstructs pseudo-healthy images from edges, achieving state-of-the-art results on multi-center datasets.",
      "mindmap": "graph TB\n        A[Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[泛化性差与异常残留/Generalizability & Residuals]\n        C --> C1[解耦表示模块/Disentangled Representation Module]\n        C --> C2[边缘到图像恢复模块/Edge-to-Image Restoration Module]\n        D --> D1[性能超越17种SOTA方法/Outperforms 17 SOTA Methods]"
    },
    {
      "title": "LVLM-Aided Alignment of Task-Specific Vision Models",
      "authors": "Alexander Koebler, Lukas Kuhn, Ingo Thon, Florian Buettner",
      "institution": "Goethe University Frankfurt, Siemens AG, German Cancer Research Center (DKFZ)",
      "link": "https://arxiv.org/pdf/2512.21985",
      "code": null,
      "tags": [
        "model alignment and interpretability",
        "LVLM-VA",
        "spurious correlations",
        "explainable AI (XAI)",
        "vision-language model",
        "human-in-the-loop"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp",
      "contributions": "1. Introduces LVLM-Aided Visual Alignment (LVLM-VA), a novel method for aligning small task-specific vision models with human domain knowledge using a Large Vision Language Model (LVLM). 2. Proposes a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling efficient expert-model interaction. 3. Demonstrates that the method effectively reduces the model's dependence on spurious features and group-specific biases without requiring fine-grained, instance-level feedback.",
      "summary": "This paper addresses the problem of small task-specific vision models relying on spurious correlations, which leads to brittle real-world performance. The authors propose LVLM-Aided Visual Alignment (LVLM-VA), a method that uses a Large Vision Language Model to create a bidirectional interface between human domain knowledge and the model, translating explanations and critiques. The method is shown to significantly improve model alignment with human specifications and reduce dependence on spurious features across synthetic and real-world datasets.",
      "mindmap": "graph TB\n        A[LVLM-Aided Alignment of Task-Specific Vision Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[小规模任务专用视觉模型依赖虚假相关性/Small task-specific vision models rely on spurious correlations]\n        B --> B2[导致部署时行为脆弱/Leads to brittle behavior when deployed]\n        C --> C1[利用LVLM进行视觉对齐/Leverage LVLM for visual alignment]\n        C --> C2[双向接口: 行为转语言, 规范转评估/Bidirectional interface: behavior to language, specs to critiques]\n        D --> D1[模型行为与人类规范更好对齐/Better alignment of model behavior with human specifications]\n        D --> D2[减少对虚假特征和偏见的依赖/Reduced dependence on spurious features and biases]"
    },
    {
      "title": "LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration",
      "authors": "Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, Xiangyang Ji",
      "institution": "Beijing Institute of Technology, Tsinghua University, Dalian University of Technology, Xidian University",
      "link": "https://arxiv.org/pdf/2512.22010",
      "code": null,
      "tags": [
        "vision-and-language navigation",
        "spatiotemporal context modeling",
        "slot-based compression",
        "prompt-guided multimodal integration"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp",
      "contributions": "1. A slot-based historical image compression module to distill multi-view historical observations into fixed-length contextual representations. 2. A spatiotemporal trajectory encoding module to capture the temporal dynamics and spatial structure of UAV trajectories. 3. A prompt-guided multimodal integration module to fuse spatiotemporal context with current observations for robust waypoint prediction.",
      "summary": "This paper proposes LongFly, a framework for long-horizon UAV vision-and-language navigation that addresses the challenge of modeling spatiotemporal context. The method integrates a history-aware modeling strategy with modules for compressing past observations, encoding trajectories, and fusing multimodal information. Experimental results show it outperforms state-of-the-art baselines in navigation success metrics across seen and unseen environments.",
      "mindmap": "graph TB\n        A[LongFly: Long-Horizon UAV Vision-and-Language Navigation] --> B[核心问题/Problem: Current UAV VLN methods struggle with long-horizon spatiotemporal context, leading to inaccurate alignment and unstable planning.]\n        A --> C[主要方法/Method: History-aware spatiotemporal modeling with slot-based image compression, trajectory encoding, and prompt-guided multimodal integration.]\n        A --> D[关键结果/Results: Outperforms SOTA baselines by 7.89% in success rate and 6.33% in SPL.]"
    },
    {
      "title": "From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation",
      "authors": "Nagham Osman, Vittorio Lembo, Giovanni Bottegoni, Laura Toni",
      "institution": "University College London, University of Urbino Carlo Bo",
      "link": "https://arxiv.org/pdf/2512.22031",
      "code": null,
      "tags": [
        "generative models for drug discovery",
        "hit-like molecule generation",
        "autoregressive models",
        "diffusion models",
        "docking scores",
        "multi-stage filtering"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2a0d0d188f2e13e0f8561de5950d5637586c871a0ee36935ebfbd5bb2bad540_w640_q70.webp",
      "contributions": "1. Framing hit-like molecule generation as a standalone task for generative models. 2. Proposing a tailored evaluation framework integrating physicochemical, structural, and bioactivity criteria. 3. Benchmarking autoregressive and diffusion models, with synthesized GSK-3β hits confirmed active in vitro.",
      "summary": "This paper investigates whether generative models can replace the hit identification step in drug discovery. It proposes a multi-stage evaluation framework and benchmarks autoregressive and diffusion models, showing they can generate valid, diverse, and biologically relevant compounds, with some synthesized hits confirmed active in vitro.",
      "mindmap": "graph TB\n        Root(”From In Silico to In Vitro: Evaluating Molecule Generative Models for Hit Generation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”Hit identification is resource-intensive/命中识别资源密集”)\n        Method --> M1(”Propose tailored evaluation framework/提出定制评估框架”)\n        Method --> M2(”Benchmark autoregressive & diffusion models/基准测试自回归和扩散模型”)\n        Results --> R1(”Models generate valid, diverse, bioactive compounds/模型生成有效、多样、有生物活性的化合物”)\n        Results --> R2(”Selected hits synthesized & confirmed active/选定命中物被合成并确认有效”)"
    },
    {
      "title": "Meta-Learning-Based Handover Management in NextG O-RAN",
      "authors": "Michail Kalntis, George Iosifidis, José Suárez-Varela, Andra Lutu, Fernando A. Kuipers",
      "institution": "Delft University of Technology, Telefónica Research",
      "link": "https://arxiv.org/pdf/2512.22022",
      "code": null,
      "tags": [
        "communication & networking",
        "Conditional Handovers",
        "O-RAN",
        "Meta-Learning",
        "Mobility Management",
        "xApp"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d76dc23b355711f7c28f6efbb425c914a47fa4ebefd3807c4f00b61b58aedb3e_w640_q70.webp",
      "contributions": "1. Introduces CONTRA, the first framework to jointly optimize Traditional and Conditional Handovers within the O-RAN architecture. 2. Proposes a practical meta-learning algorithm for adaptive, on-the-fly handover type selection, guaranteeing universal no-regret performance. 3. Provides and analyzes unique, countrywide mobility management datasets from a top-tier mobile network operator, offering fresh insights into handover trade-offs.",
      "summary": "This paper addresses the limitations of traditional and conditional handovers in mobile networks by proposing CONTRA, a meta-learning-based framework for O-RAN that dynamically selects and optimizes handover types. It is designed as a near-real-time xApp and is evaluated using real-world datasets. The results show that CONTRA improves user throughput and reduces switching costs, outperforming standard and RL-based baselines.",
      "mindmap": "graph TB\n        A[Meta-Learning-Based Handover Management in NextG O-RAN] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[传统切换延迟与失败/Traditional HO delays & failures]\n        B --> B2[切换类型间的权衡/Trade-offs between HO types]\n        C --> C1[CONTRA框架: 联合优化THO与CHO/CONTRA: Jointly optimizes THOs & CHOs]\n        C --> C2[元学习算法动态选择/Meta-learning for dynamic selection]\n        C --> C3[O-RAN xApp部署/O-RAN xApp deployment]\n        D --> D1[提升用户吞吐量/Improves user throughput]\n        D --> D2[降低切换成本/Reduces HO switching costs]\n        D --> D3[优于3GPP与RL基线/Outperforms 3GPP & RL baselines]"
    },
    {
      "title": "LibContinual: A Comprehensive Library towards Realistic Continual Learning",
      "authors": "Wenbin Li, Shangge Liu, Borui Kang, Yiyang Chen, KaXuan Lew, Yang Chen, Yinghuan Shi, Lei Wang, Yang Gao, Jiebo Luo",
      "institution": "Nanjing University, University of Wollongong, University of Rochester",
      "link": "https://arxiv.org/pdf/2512.22029",
      "code": "https://github.com/RL-VIG/LibContinual",
      "tags": [
        "others",
        "catastrophic forgetting",
        "stability-plasticity dilemma",
        "modular architecture",
        "memory budget",
        "online continual learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a733a967f663a216bbd5fb0cee657ed8bb70a4e6f0205d669f983cbf9bb6fd_w640_q70.webp",
      "contributions": "1. Proposed LibContinual, a unified, modular, and reproducible library for Continual Learning (CL) that integrates 19 representative algorithms across five methodological categories. 2. Systematically identified and investigated three unrealistic implicit assumptions (offline data accessibility, unregulated memory, intra-task semantic homogeneity) prevalent in mainstream CL evaluation. 3. Conducted a comprehensive analysis under stricter, more realistic settings (strict online CL, unified memory budget, category-randomized tasks), revealing significant performance drops in many existing methods and highlighting the need for resource-aware and semantically robust CL strategies.",
      "summary": "This paper introduces LibContinual, a comprehensive library designed to unify and standardize research in Continual Learning (CL). By using this framework to evaluate existing methods under more realistic constraints, the study shows that many current CL algorithms suffer significant performance drops, underscoring the gap between common evaluation practices and real-world applicability.",
      "mindmap": "graph TB\n        A[LibContinual: A Comprehensive Library towards Realistic Continual Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[研究碎片化，缺乏统一框架/Fragmented research landscape, lack of unified framework]\n        B --> B2[评估存在不现实的隐含假设/Unrealistic implicit assumptions in evaluation]\n        C --> C1[构建模块化、可复现的库/Build a modular, reproducible library]\n        C --> C2[集成19种代表性算法/Integrate 19 representative algorithms]\n        C --> C3[在更现实的设定下系统评估/Systematically evaluate under more realistic settings]\n        D --> D1[现有方法在现实约束下性能显著下降/Existing methods show significant performance drop under realistic constraints]\n        D --> D2[强调资源感知和语义鲁棒策略的必要性/Highlight the necessity of resource-aware and semantically robust strategies]"
    },
    {
      "title": "StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars",
      "authors": "Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu",
      "institution": "Tsinghua University, Renmin University of China, Tencent Hunyuan, Nanjing University",
      "link": "https://arxiv.org/pdf/2512.22065",
      "code": "https://streamavatar.github.io",
      "tags": [
        "diffusion models",
        "autoregressive distillation",
        "adversarial refinement",
        "real-time streaming",
        "reference-anchored positional re-encoding",
        "consistency-aware discriminator"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp",
      "contributions": "1. A two-stage autoregressive adaptation and acceleration framework (autoregressive distillation + adversarial refinement) to adapt a non-causal human video diffusion model for real-time, interactive streaming. 2. Three novel components to ensure long-term stability and consistency: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. 3. A one-shot, interactive human avatar model capable of generating both natural talking and listening behaviors with coherent full-body gestures, surpassing existing methods in quality, efficiency, and interaction naturalness.",
      "summary": "This paper addresses the challenge of making diffusion-based human avatar generation suitable for real-time, interactive streaming. It proposes StreamAvatar, a two-stage framework that adapts a high-fidelity human video diffusion model using autoregressive distillation and adversarial refinement, incorporating novel components for long-term consistency. The method achieves state-of-the-art performance in generating high-resolution, full-body interactive avatars with natural talking/listening behaviors in real-time.",
      "mindmap": "graph TB\n        A[StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Non-causal, high-cost diffusion models unsuitable for real-time streaming; Limited to head-and-shoulder, lacking gestures.]\n        C[主要方法/Method<br>Two-stage autoregressive adaptation (distillation + refinement) with Reference Sink, RAPR, Consistency-Aware Discriminator.]\n        D[关键结果/Results<br>State-of-the-art real-time, interactive full-body avatar with natural talking/listening and gestures.]"
    },
    {
      "title": "Unifying Learning Dynamics and Generalization in Transformers Scaling Law",
      "authors": "Chiwun Yang",
      "institution": "Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.22088",
      "code": null,
      "tags": [
        "learning theory",
        "scaling law",
        "learning dynamics",
        "generalization error",
        "transformer",
        "stochastic gradient descent"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9b067b56202cd4607e684058e78ac331373bf12bf6848ca444276e2dcafe9f9_w640_q70.webp",
      "contributions": "1. Formalizes the learning dynamics of transformers as an ODE system and approximates it to kernel behaviors, moving beyond toy models to analyze SGD on multi-layer transformers with arbitrary data distributions. 2. Establishes a theoretical upper bound on excess risk with a distinct phase transition: exponential decay in the optimization phase and a power-law decay of Θ(C^\\{-1/6\\}) in the statistical phase. 3. Derives isolated scaling laws for model size, training time, and dataset size, explaining how each variable independently governs generalization bounds.",
      "summary": "This paper provides a theoretical foundation for the empirical scaling laws of large language models. It models transformer learning dynamics as an ODE system and analyzes SGD training on realistic data. The main result is a unified theory showing a phase transition in generalization error, from exponential to power-law decay, as computational resources scale.",
      "mindmap": "graph TB\n        A[Unifying Learning Dynamics and Generalization in Transformers Scaling Law] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Scaling Law理论原理不清 / Poorly understood theoretical underpinnings of scaling laws]\n        C --> C1[形式化学习动态为ODE系统 / Formalize learning dynamics as ODE system]\n        C --> C2[近似为核行为 / Approximate to kernel behaviors]\n        C --> C3[分析SGD训练真实Transformer / Analyze SGD training for real transformers]\n        D --> D1[泛化误差上界与相变 / Upper bound on excess risk with phase transition]\n        D --> D2[优化相:指数衰减 / Optimization phase: Exponential decay]\n        D --> D3[统计相:幂律衰减 Θ(C^{-1/6}) / Statistical phase: Power-law decay Θ(C^{-1/6})]\n        D --> D4[分离的规模定律 / Isolated scaling laws for model size, time, data]"
    },
    {
      "title": "Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis",
      "authors": "Duygu Altinok",
      "institution": "Independent Researcher",
      "link": "https://arxiv.org/pdf/2512.22100",
      "code": null,
      "tags": [
        "benchmark construction",
        "Turkish NLU benchmark",
        "semi-automated annotation",
        "sentiment analysis dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aae4aef01bf4bd7a32414041836c4d9d7383c50872f47bdb0dee4d45af35adb_w640_q70.webp",
      "contributions": "1. Introduces TrGLUE, the first comprehensive GLUE-style benchmark for Turkish Natural Language Understanding, filling a critical gap. 2. Presents SentiTurca, a specialized benchmark for Turkish sentiment analysis. 3. Provides a scalable, reproducible semi-automated dataset creation pipeline combining LLM annotation, cross-model checks, and human validation.",
      "summary": "This paper addresses the lack of a comprehensive benchmark for evaluating Turkish language understanding by introducing TrGLUE and SentiTurca. The benchmarks are created using a semi-automated pipeline with LLM annotation and human validation to ensure quality and linguistic naturalness. The work establishes a robust evaluation framework and provides resources to empower Turkish NLP research.",
      "mindmap": "graph TB\n        A[Introducing TrGLUE and SentiTurca] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[缺乏土耳其语综合基准/Lack of Turkish NLU Benchmark]\n        C --> C1[半自动标注流程/Semi-automated Pipeline]\n        C1 --> C2[LLM标注 + 交叉验证 + 人工校验/LLM Annotation + Cross-check + Human Validation]\n        D --> D1[发布TrGLUE & SentiTurca/Release TrGLUE & SentiTurca]\n        D --> D2[提供代码与资源/Provide Code & Resources]"
    },
    {
      "title": "A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting",
      "authors": "Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang",
      "institution": "University of Minnesota",
      "link": "https://arxiv.org/pdf/2512.22101",
      "code": null,
      "tags": [
        "agent system",
        "multi-agent pipeline",
        "automated data analysis",
        "insight generation",
        "report synthesis",
        "visual analytics"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92651ded84480402816f8db1df902e28dd62cce1b0958cece60e0f518bdd7e1c_w640_q70.webp",
      "contributions": "1. A Data Analyzer agent that orchestrates data profiling, generates diverse visualizations, filters low-quality charts, and automatically scores candidate insights for depth, correctness, and actionability. 2. A Presenter agent that sequences topics, composes chart-grounded narratives from top insights, writes transitions, and revises the document to produce a coherent, publication-ready report. 3. An end-to-end Analyzer-to-Presenter (A2P) pipeline that operationalizes co-analysis by coupling quality-assured analysis with narrative synthesis, improving the real-world usefulness of automated data analysis.",
      "summary": "This paper presents A2P-Vis, a two-part multi-agent pipeline designed to automate the generation of data visualization reports. The system uses a Data Analyzer to create and vet visual insights and a Presenter to assemble them into a coherent narrative. The authors claim this end-to-end approach improves the practical utility of automated data analysis for practitioners.",
      "mindmap": "graph TB\n        A[A2P-Vis] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[自动化数据科学流程的瓶颈/Gaps in automating data science]\n        B1 --> B2[生成有洞察力的可视化/Generating insightful visual evidence]\n        B1 --> B3[组装成专业报告/Assembling coherent professional report]\n        C --> C1[两部分多智能体管道/Two-part multi-agent pipeline]\n        C1 --> C2[数据分析器/Data Analyzer]\n        C2 --> C3[生成并评估图表与洞察/Generates & evaluates charts & insights]\n        C1 --> C4[报告呈现器/Presenter]\n        C4 --> C5[编排主题并撰写叙述/Orders topics & composes narrative]\n        D --> D1[端到端协同分析/End-to-end co-analysis]\n        D1 --> D2[提高自动化数据分析的实用性/Improves usefulness of automated analysis]"
    },
    {
      "title": "Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks",
      "authors": "Zubair Shah, Noaman Khan",
      "institution": "Hamad Bin Khalifa University",
      "link": "https://arxiv.org/pdf/2512.22106",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "neural network pruning",
        "game theory",
        "equilibrium",
        "non-cooperative game",
        "sparsification"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4fd762b6b064bb7810151beeb40f55c93bfc05054b2d4a98cd925aed8bea43b2_w640_q70.webp",
      "contributions": "1. Proposes a novel game-theoretic perspective on neural network pruning, modeling parameter groups as players in a non-cooperative game where sparsity emerges as an equilibrium outcome. 2. Provides a theoretical analysis showing that dominated players (redundant parameters) collapse to zero participation under mild conditions, offering a principled explanation for pruning. 3. Derives a simple equilibrium-driven pruning algorithm that jointly updates network parameters and participation variables without relying on explicit, heuristic importance scores.",
      "summary": "This paper proposes a novel game-theoretic framework for neural network pruning, where sparsity emerges naturally from the equilibrium of a non-cooperative game among model components. The method jointly updates network parameters and participation variables without external importance scores. Experiments show it achieves competitive sparsity-accuracy trade-offs with a more interpretable, theory-grounded foundation.",
      "mindmap": "graph TB\n        Root(”Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks”) --> Problem(”核心问题/Problem: Sparsity is imposed externally via heuristics, lacking a principled model of parameter interaction.”)\n        Root --> Method(”主要方法/Method: Model pruning as a non-cooperative game among parameters; sparsity emerges at equilibrium.”)\n        Root --> Results(”关键结果/Results: Competitive sparsity-accuracy trade-offs with an interpretable, theory-grounded algorithm.”)"
    },
    {
      "title": "Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications",
      "authors": "Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer",
      "institution": "University of Illinois at Urbana-Champaign, IBM Research",
      "link": "https://arxiv.org/pdf/2512.22113",
      "code": null,
      "tags": [
        "agent system",
        "root cause analysis",
        "service dependency graph",
        "program dependence graph",
        "LLM agent",
        "cloud incident"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62ebd8a01fd966235e0d8d40581cb8352024a391331fada8ea23868c2235ada9_w640_q70.webp",
      "contributions": "1. PRAXIS, an agentic approach for cloud incident RCA with structured, LLM-driven graph reasoning and traversal over microservice and program dependency graphs. 2. An application of the hammock block program dependence graph for agentic RCA, leveraging its hierarchical structure for multi-granular code analysis. 3. A Code-Cloud-RCA Benchmark consisting of 30 real-world incident scenarios injected in a live Kubernetes environment.",
      "summary": "This paper introduces PRAXIS, an orchestrator that uses an LLM-driven agent to traverse service dependency graphs and program dependence graphs to diagnose the root cause of code- and configuration-related cloud incidents. Compared to ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x, as demonstrated on a benchmark of 30 real-world incidents.",
      "mindmap": "graph TB\n        A[Agentic Structured Graph Traversal for Root Cause Analysis<br/>基于智能体结构化图遍历的云应用根因分析] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>High cost of unresolved cloud incidents; Need for effective root cause analysis]\n        C[主要方法/Method<br/>PRAXIS: LLM-driven traversal over Service Dependency Graph and Program Dependence Graph]\n        D[关键结果/Results<br/>3.1x higher RCA accuracy, 3.8x lower token consumption vs. ReAct baselines]"
    },
    {
      "title": "Atomistic Simulation Guided Convolutional Neural Networks for Thermal Modeling of Friction Stir Welding",
      "authors": "Akshansh Mishra",
      "institution": "Politecnico di Milano, AI Fab Lab",
      "link": "https://arxiv.org/pdf/2512.21344",
      "code": null,
      "tags": [
        "physics-informed machine learning",
        "molecular dynamics",
        "convolutional neural network",
        "friction stir welding",
        "explainable AI",
        "LAMMPS"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34d6f71500319f52aecac1e95e1951a537fdc504134a8ba43851f31acc2e41c6_w640_q70.webp",
      "contributions": "1. Developed a novel method to transform atomistic simulation data (atomic positions and velocities) into physics-based 2D spatial grids for deep learning input. 2. Created and optimized a 2D CNN model to directly predict temperature evolution from spatially resolved atomistic data, achieving high accuracy (R²=0.94). 3. Used Class Activation Map analysis to provide explainability, showing the model's focus aligns with physical mechanisms (e.g., tool-material interface).",
      "summary": "This paper presents a method that combines molecular dynamics simulations with convolutional neural networks for thermal modeling in friction stir welding. The method transforms atomic-scale simulation data into spatial grids and uses a CNN to accurately predict temperature, with results validated against physical mechanisms. The approach demonstrates that deep learning can effectively learn from atomistic data to model complex thermomechanical processes.",
      "mindmap": "graph TB\n        Root[”Atomistic Simulation Guided CNNs for Thermal Modeling of FSW / 原子模拟引导的CNN用于搅拌摩擦焊热建模”]\n        Root --> Problem[”准确预测温度演化对于理解搅拌摩擦焊的热机械行为至关重要 / Accurate prediction of temperature evolution is essential for understanding thermomechanical behavior in FSW”]\n        Root --> Method[”使用LAMMPS进行分子动力学模拟，将原子数据转换为物理二维空间网格，并开发2D CNN进行预测 / Use LAMMPS for MD simulations, transform atomic data into physics-based 2D spatial grids, and develop a 2D CNN for prediction”]\n        Root --> Results[”模型预测精度高（R²=0.94），CAM分析表明模型关注与剧烈变形和生热相关的区域 / Model achieves high predictive accuracy (R²=0.94), CAM analysis shows model focuses on regions associated with intense deformation and heat generation”]"
    },
    {
      "title": "Applications of synthetic financial data in portfolio and risk modeling",
      "authors": "Christophe D. Hounwanou, Yae Ulrich Gaba",
      "institution": "African Institute for Mathematical Sciences (AIMS Rwanda), Sefako Makgatho Health Sciences University (SMU), AI Research and Innovation Nexus for Africa (AIRINA Labs)",
      "link": "https://arxiv.org/pdf/2512.21798",
      "code": null,
      "tags": [
        "generative models for time series",
        "TimeGAN",
        "Variational Autoencoder (VAE)",
        "synthetic financial data",
        "portfolio optimization",
        "risk modeling"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a7ecf00c1350b58057e2f03c93bf0e8845d1441745fc22505c46475eceeeb65_w640_q70.webp",
      "contributions": "1. Evaluated and compared the performance of TimeGAN and VAEs for generating realistic synthetic financial time series data. 2. Demonstrated that TimeGAN-generated data closely matches real data in distributional, volatility, and autocorrelation properties. 3. Showed the practical utility of synthetic data in downstream financial tasks like mean-variance portfolio optimization, yielding similar portfolio weights and risk metrics to real data.",
      "summary": "This paper addresses the scarcity and privacy issues of real financial data by using generative models like TimeGAN and VAEs to create synthetic return series. It evaluates the synthetic data on statistical similarity and financial tasks, concluding that TimeGAN effectively captures temporal dynamics and can serve as a privacy-preserving, cost-effective substitute for real data in portfolio and risk analysis.",
      "mindmap": "graph TB\n        Root(”Applications of synthetic financial data in portfolio and risk modeling”) --> Problem(”核心问题/Problem: Privacy and accessibility limit financial research”)\n        Root --> Method(”主要方法/Method: Use TimeGAN and VAEs to generate synthetic financial time series”)\n        Root --> Results(”关键结果/Results: TimeGAN data is realistic and useful for portfolio/risk tasks”)"
    },
    {
      "title": "Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models",
      "authors": "Takuro Kutsuna",
      "institution": "Toyota Central R&D Labs., Inc.",
      "link": "https://arxiv.org/pdf/2512.21593",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion models",
        "generative modeling",
        "evidence lower bound",
        "residual learning",
        "two-stage framework"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp",
      "contributions": "1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.",
      "summary": "The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.",
      "mindmap": "graph TB\n        Root[”Residual Prior Diffusion (RPD) / 残差先验扩散模型”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”单一扩散模型难以同时捕捉全局结构和局部细节 / Single diffusion model struggles with global structure and local details”]\n        Method --> M1[”两阶段框架: 粗粒度先验 + 残差扩散模型 / Two-stage framework: coarse prior + residual diffusion model”]\n        Method --> M2[”概率模型与可处理ELBO / Probabilistic model with tractable ELBO”]\n        Results --> R1[”在合成数据上准确捕捉细节 / Accurately captures details on synthetic data”]\n        Results --> R2[”自然图像生成匹配或超越基线 / Natural image generation matches or exceeds baselines”]\n        Results --> R3[”少步推理保持性能 / Maintains performance with few inference steps”]"
    },
    {
      "title": "Enabling Ultra-Fast Cardiovascular Imaging Across Heterogeneous Clinical Environments with a Generalist Foundation Model and Multimodal Database",
      "authors": "Zi Wang, Mingkai Huang, Zhang Shi, Hongjie Hu, Lan Lan, Hui Zhang, Yan Li, Xi Hu, Qing Lu, Zongming Zhu, Qiong Yao, Yuxiang Dai, Fanwen Wang, Yinzhe Wu, Jun Lyu, Qianqian Gao, Guangming Xu, Zhenxuan Zhang, Haosen Zhang, Qing Li, Guangming Wang, Tianxing He, Lizhen Lan, Siyue Li, Le Xue, Mengting Sun, Yuntong Lyu, Junpu Hu, Jiayu Zhu, Rizwan Ahmad, Zhengyu Bu, Xianling Qian, Guanke Cai, Ruiyu Cao, Weirui Cai, Chang Xu, Yuyang Ren, Feidan Yu, Siying Ma, Ziqiang Xu, Xinran Chen, Sha Hua, Daniel Kim, Yajing Zhang, Chen Ouyang, Wenjia Bai, Jing Qin, Yucheng Yang, Daniel Rueckert, He Wang, Qian Tao, Claudia Prieto, Michael Markl, Alistair Young, Lianming Wu, Shuo Wang, Chen Qin, Mengsu Zeng, Xihong Hu, Haibo Xu, Xiaobo Qu, Hao Li, Guang Yang, Chengyan Wang",
      "institution": "Imperial College London, Fudan University, Xiamen University",
      "link": "https://arxiv.org/pdf/2512.21652",
      "code": null,
      "tags": [
        "medical image reconstruction",
        "foundation model",
        "k-space",
        "multimodal database",
        "zero-shot generalization",
        "accelerated imaging"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6001c34604e8bff7b6e1efa31a4faa67e7e43e6efb18e01ea880e43095344349_w640_q70.webp",
      "contributions": "1. The curation of MMCMR-427K, the largest and most comprehensive multimodal cardiovascular magnetic resonance (CMR) k-space database. 2. The introduction of CardioMM, a generalist reconstruction foundation model that unifies semantic understanding with physics-informed data consistency for robust, accelerated imaging. 3. Demonstrating state-of-the-art performance and strong zero-shot generalization across heterogeneous clinical settings, enabling up to 24x acceleration without compromising clinical integrity.",
      "summary": "This paper addresses the slow scan times and environmental heterogeneity limiting clinical cardiovascular MRI. It proposes CardioMM, a generalist foundation model trained on a large multimodal k-space database (MMCMR-427K), which achieves robust, ultra-fast reconstructions across diverse scanners and protocols. The results show that CardioMM enables high acceleration (up to 24x) while preserving diagnostic quality and generalizing to unseen clinical environments.",
      "mindmap": "graph TB\n    A[Enabling Ultra-Fast Cardiovascular Imaging...] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[CMR扫描时间长/CMR Scan Time Long]\n    B --> B2[临床环境异质性高/High Clinical Heterogeneity]\n    C --> C1[构建多模态数据库MMCMR-427K/Build Multimodal DB MMCMR-427K]\n    C --> C2[提出通用基础模型CardioMM/Propose Generalist Foundation Model CardioMM]\n    D --> D1[实现24倍加速成像/Achieve 24x Accelerated Imaging]\n    D --> D2[零样本泛化至新环境/Zero-shot Generalization to New Settings]\n    D --> D3[保持诊断质量/Preserve Diagnostic Quality]"
    },
    {
      "title": "Parameter-Efficient Neural CDEs via Implicit Function Jacobians",
      "authors": "Ilya Kuleshov, Alexey Zaytsev",
      "institution": "Applied AI Institute",
      "link": "https://arxiv.org/pdf/2512.20625",
      "code": null,
      "tags": [
        "time series analysis",
        "Neural Controlled Differential Equations",
        "parameter efficiency",
        "implicit function Jacobians",
        "continuous RNN"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f69d35dc890877df610e96a1c984c641596f7fcac2c4ff1dbf30f641c90d5d77_w640_q70.webp",
      "contributions": "1. Proposes a novel, parameter-efficient formulation of Neural Controlled Differential Equations (NCDEs) that drastically reduces the number of required parameters. 2. Introduces a logical interpretation of the method as a \"Continuous RNN,\" aligning with the original inspiration of NCDEs. 3. Presents a method leveraging implicit function Jacobians to achieve this efficiency.",
      "summary": "This paper addresses the high parameter cost of Neural Controlled Differential Equations (NCDEs) for temporal sequence analysis. It proposes a new, parameter-efficient formulation that reinterprets NCDEs as a \"Continuous RNN\" and uses implicit function Jacobians to reduce the parameter count. The main conclusion is that this approach maintains the modeling power of NCDEs while being significantly more parameter-efficient.",
      "mindmap": "graph LR\n    A[Parameter-Efficient Neural CDEs via Implicit Function Jacobians] --> B[核心问题/Problem: NCDEs require many parameters]\n    A --> C[主要方法/Method: Parameter-efficient formulation via implicit Jacobians, ”Continuous RNN” analogy]\n    A --> D[关键结果/Results: Achieves similar performance with far fewer parameters]"
    },
    {
      "title": "BitRL-Light: 1-bit LLM Agents with Deep Reinforcement Learning for Energy-Efficient Smart Home Lighting Optimization",
      "authors": "Ravi Gupta, Shabista Haider",
      "institution": "AMD, Oracle",
      "link": "https://arxiv.org/pdf/2512.20623",
      "code": null,
      "tags": [
        "on-device ai",
        "1-bit quantization",
        "Deep Q-Network (DQN)",
        "edge inference",
        "multi-objective RL",
        "model compression"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc8c0af6233346bd047dc958868370625b7e65b546832d11939a371662fc4980_w640_q70.webp",
      "contributions": "1. A novel architecture integrating 1-bit quantized LLMs with DQN for multi-objective optimization of smart home lighting. 2. Voice command integration via Google Home and IFTTT webhooks for natural user interaction. 3. Comprehensive evaluation demonstrating the feasibility of intelligent adaptive control on sub-$50 hardware, with significant energy and latency improvements.",
      "summary": "This paper proposes BitRL-Light, a framework that combines a 1-bit quantized LLM with Deep Q-Network reinforcement learning for real-time smart home lighting control on edge devices. The system optimizes for energy efficiency and user comfort, achieving substantial energy savings and low latency on a Raspberry Pi. The work demonstrates that adaptive AI control is feasible on resource-constrained hardware without cloud dependency.",
      "mindmap": "graph LR\n    A[BitRL-Light] --> B[核心问题/Problem: Smart home lighting lacks adaptive intelligence for energy and comfort]\n    A --> C[主要方法/Method: 1-bit LLM + DQN on edge devices]\n    A --> D[关键结果/Results: 32% energy savings, <200ms latency, 95% user satisfaction]"
    },
    {
      "title": "Proceedings of the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025)",
      "authors": "Edited by Tessai Hayama, Takayuki Ito, Takahiro Uchiya, Motoki Miura, Takahiro Kawaji, Takaya Yuizono, Atsuo Yoshitaka, Tokuro Matsuo, Shun Okuhara, Jawad Haqbeen, Sofia Sahab, Wen Gu, Shiyao Ding",
      "institution": "IEICE (The Institute of Electronics, Information and Communication Engineers)",
      "link": "https://arxiv.org/pdf/2512.20628",
      "code": null,
      "tags": [
        "multidisciplinary conference",
        "knowledge engineering",
        "creativity support systems",
        "human-computer interaction",
        "artificial intelligence",
        "peer-reviewed proceedings"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c8c9d50976ba52ad8c3511bc5a2800a053b1b858244a20cfcda006c283fdd5c_w640_q70.webp",
      "contributions": "1. Provides a multidisciplinary forum for researchers in AI, knowledge engineering, HCI, and creativity support systems. 2. Presents peer-reviewed proceedings following a double-blind review process. 3. Facilitates extended publication of selected papers in IEICE Transactions on Information and Systems.",
      "summary": "This is the proceedings volume for the 20th International Conference on Knowledge, Information and Creativity Support Systems (KICSS 2025). It compiles peer-reviewed papers from a multidisciplinary forum, with selected works recommended for further publication in a journal.",
      "mindmap": "graph LR\n        A[KICSS 2025 Proceedings] --> B(核心问题/Problem: 提供多学科研究论坛/Provide Multidisciplinary Research Forum)\n        A --> C(主要方法/Method: 双盲同行评审会议/Double-Blind Peer-Reviewed Conference)\n        A --> D(关键结果/Results: 出版会议论文集并推荐期刊发表/Publish Proceedings & Recommend Journal Publication)"
    },
    {
      "title": "Cooperation Through Indirect Reciprocity in Child-Robot Interactions",
      "authors": "Isabel Neto, Alexandre S. Pires, Filipa Correia, Fernando P. Santos",
      "institution": "Universidade de Lisboa, University of Amsterdam, Instituto Superior Técnico",
      "link": "https://arxiv.org/pdf/2512.20621",
      "code": null,
      "tags": [
        "human-robot interaction",
        "indirect reciprocity",
        "multi-armed bandit",
        "coordination dilemmas"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a49edd2299eeb19bce07b564c8061c35b829f55eb48557d15dbeabbbd028e0_w640_q70.webp",
      "contributions": "1. Demonstrated that the mechanism of indirect reciprocity can be successfully transposed from human-human interactions to child-robot interactions. 2. Showed that children's behavioral strategies provide a sufficient signal for multi-armed bandit algorithms to learn cooperative actions. 3. Analyzed how differences in learning algorithms impact the dynamics and outcomes of human-AI cooperation.",
      "summary": "This paper investigates whether indirect reciprocity, a mechanism for sustaining cooperation, applies to child-robot interactions. The authors combine laboratory experiments with theoretical modeling, using multi-armed bandit algorithms for the robots. They find that indirect reciprocity does extend to these interactions and that robots can learn to cooperate based on children's strategies, though this learning is highly dependent on the human strategies revealed.",
      "mindmap": "graph LR\n    A[Cooperation Through Indirect Reciprocity in Child-Robot Interactions] --> B(核心问题/Problem: Can indirect reciprocity enable cooperation between children and robots?)\n    A --> C(主要方法/Method: Laboratory experiments and theoretical modeling with multi-armed bandit algorithms)\n    A --> D(关键结果/Results: IR extends to child-robot groups; robots can learn cooperation from children's strategies)"
    },
    {
      "title": "Efficient Asynchronous Federated Evaluation with Strategy Similarity Awareness for Intent-Based Networking in Industrial Internet of Things",
      "authors": "Shaowen Qin, Jianfeng Zeng, Haodong Guo, Xiaohuan Li, Jiawen Kang, Qian Chen, Dusit Niyato",
      "institution": "Guilin University of Electronic Technology, Guangdong University of Technology, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.20627",
      "code": null,
      "tags": [
        "federated learning",
        "Intent-Based Networking",
        "Industrial Internet of Things",
        "Asynchronous Federated Learning",
        "Strategy Similarity",
        "Multimodal Intent Alignment"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17f53425c1430ea3ad8516d92a010c39c2936d478f64e21bad2b552f30214b23_w640_q70.webp",
      "contributions": "1. Proposes FEIBN, a federated learning framework for distributed policy verification in IIoT, enhancing privacy by avoiding raw data exposure. 2. Introduces SSAFL, a strategy similarity-aware mechanism for efficient node selection and asynchronous model updates to reduce communication overhead. 3. Leverages LLMs to align multimodal user intents into structured strategy tuples for automated policy generation and verification.",
      "summary": "This paper proposes FEIBN, a federated evaluation framework for Intent-Based Networking in IIoT. It uses LLMs to translate intents and a strategy similarity-aware asynchronous federated learning mechanism (SSAFL) for efficient, private policy verification. Experiments show SSAFL improves accuracy, convergence speed, and reduces cost by 27.8% compared to a baseline.",
      "mindmap": "graph LR\n    A[Efficient Asynchronous Federated Evaluation with Strategy Similarity Awareness] --> B[核心问题/Problem: Frequent strategy deployment & centralized verification are impractical in IIoT]\n    A --> C[主要方法/Method: FEIBN framework with LLM-based intent alignment & SSAFL mechanism]\n    A --> D[关键结果/Results: Improved accuracy, faster convergence, 27.8% cost reduction]"
    },
    {
      "title": "Quantum-Inspired Multi Agent Reinforcement Learning for Exploration Exploitation Optimization in UAV-Assisted 6G Network Deployment",
      "authors": "Mazyar Taghavi, Javad Vahidi",
      "institution": "Iran University of Science and Technology, Intelligent Knowledge City",
      "link": "https://arxiv.org/pdf/2512.20624",
      "code": null,
      "tags": [
        "multi-agent reinforcement learning",
        "Quantum-Inspired MARL",
        "Variational Quantum Circuits (VQC)",
        "Centralized Training Decentralized Execution (CTDE)",
        "UAV-assisted 6G",
        "Exploration-Exploitation Tradeoff"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e597fe9c176a66c0f7f571bd1af94114997e4ea6eb2664bc14f6b638a115985d_w640_q70.webp",
      "contributions": "1. Proposes a novel quantum-inspired framework integrating variational quantum circuits (VQCs) and QAOA with classical MARL to optimize the exploration-exploitation tradeoff. 2. Incorporates complementary probabilistic modeling (Bayesian inference, Gaussian processes) to capture latent environmental dynamics in a cooperative UAV scenario. 3. Demonstrates through experiments that the framework improves sample efficiency, convergence speed, and coverage performance compared to classical baselines like PPO and DDPG.",
      "summary": "This paper proposes a quantum-inspired multi-agent reinforcement learning framework to optimize the exploration-exploitation balance for UAV-assisted 6G network deployment. The method integrates variational quantum circuits and probabilistic modeling within a centralized training, decentralized execution paradigm. The results show it achieves superior performance in coverage and convergence compared to classical MARL methods.",
      "mindmap": "graph LR\n    A[Quantum-Inspired MARL for UAV 6G Deployment] --> B(核心问题/Problem: Exploration-Exploitation Tradeoff in MARL for UAV Coverage)\n    A --> C(主要方法/Method: VQC/QAOA + Probabilistic Models + CTDE)\n    A --> D(关键结果/Results: Improved Efficiency, Convergence, and Coverage)"
    },
    {
      "title": "Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning",
      "authors": "Wenlong Tang",
      "institution": "Independent Researcher (No institutional affiliation inferred from provided content)",
      "link": "https://arxiv.org/pdf/2512.20629",
      "code": "https://github.com/wltang-dev/Latent-Strategy-RL-Agent",
      "tags": [
        "agent system",
        "multi-agent language systems",
        "latent strategy evolution",
        "reinforcement feedback",
        "external latent vectors",
        "dual-loop architecture"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c15e8361c02b5e6e0c755d3089af5adddafaad00ffda95b887b8eca526280761_w640_q70.webp",
      "contributions": "1. Proposes a novel multi-agent language framework that enables continual strategy evolution without fine-tuning the underlying language model's parameters. 2. Introduces a dual-loop architecture (behavior loop and language loop) that updates external latent vectors through environmental interaction and semantic reflection on generated text. 3. Demonstrates that this approach allows agents to develop stable, disentangled strategic styles and shows emergent adaptation capabilities, providing a low-cost, scalable, and interpretable form of abstract strategic representation.",
      "summary": "This paper addresses the limitation of static semantic representations in language models by proposing a framework where agents evolve strategies without model fine-tuning. The core method uses a dual-loop architecture to update external latent vectors through environmental rewards and reflection on generated text. The results show that this enables agents to develop adaptable and interpretable strategic behaviors, offering a scalable alternative to parameter tuning.",
      "mindmap": "graph LR\n    A[Learning Evolving Latent Strategies for Multi-Agent Language Systems without Model Fine-Tuning] --> B[核心问题/Problem: Static semantic representations in LLMs cannot evolve with experience.]\n    A --> C[主要方法/Method: Dual-loop architecture (Behavior Loop & Language Loop) updates external latent vectors via reinforcement and reflection.]\n    A --> D[关键结果/Results: Agents develop stable, disentangled strategies; latent spaces show convergence and emergent adaptation.]"
    },
    {
      "title": "MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation",
      "authors": "Chi-Hsiang Hsiao, Yi-Cheng Wang, Tzung-Sheng Lin, Yi-Ren Yeh, Chu-Song Chen",
      "institution": "National Taiwan University, E.SUN Financial Holding Co., Ltd., National Kaohsiung Normal University",
      "link": "https://arxiv.org/pdf/2512.20626",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "multimodal knowledge graph",
        "cross-modal reasoning",
        "visual document understanding",
        "retrieval-augmented generation",
        "entity-centric structure"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/425d6eb853edb40749e686474d27dc018d8a86017a4cd69160f9ac2081d36385_w640_q70.webp",
      "contributions": "1. Proposes a multimodal knowledge graph-based RAG framework that integrates visual cues into KG construction, retrieval, and answer generation for cross-modal reasoning. 2. Addresses the limitation of existing text-only KG-RAG methods by automatically building KGs that capture text-to-figure and figure-to-figure relationships. 3. Demonstrates superior performance over existing RAG approaches on both textual and multimodal question-answering tasks through comprehensive experiments.",
      "summary": "The paper introduces MegaRAG, a multimodal knowledge graph-based retrieval-augmented generation method designed to overcome the limitations of text-only RAG systems in understanding complex, long-form visual documents. It integrates visual information into the knowledge graph construction and retrieval process to enable better cross-modal reasoning. Experimental results show it consistently outperforms existing RAG methods on various question-answering tasks.",
      "mindmap": "graph LR\n        A[MegaRAG: 多模态知识图谱检索增强生成 / MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有RAG方法在长文档、多模态内容上理解不足 / Existing RAG struggles with long-form, multimodal document understanding]\n        C --> C1[构建融合视觉线索的多模态知识图谱 / Construct multimodal KG incorporating visual cues]\n        C --> C2[在多模态检索与生成中利用图谱 / Utilize KG in multimodal retrieval & generation]\n        D --> D1[在全局与细粒度QA任务上超越现有方法 / Outperforms existing methods on global & fine-grained QA]"
    },
    {
      "title": "MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data",
      "authors": "Aayam Bansal, Ishaan Gangwani",
      "institution": "IEEE (implied from email domain)",
      "link": "https://arxiv.org/pdf/2512.20630",
      "code": null,
      "tags": [
        "model evaluation & reliability",
        "reliability assessment",
        "uncertainty quantification",
        "strategic sampling",
        "foundation models",
        "probe selection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43b9cf5db61863c7d69dd0ba1dd95c34144ed7e7c901f129f236bfb156cb89dc_w640_q70.webp",
      "contributions": "1. A novel strategic probe selection methodology that maximizes reliability coverage across five key dimensions with information-theoretic justification. 2. An advanced uncertainty-aware assessment framework with adaptive weighting and sophisticated consistency metrics. 3. Comprehensive empirical and cross-domain validation demonstrating significant improvements over random sampling with high statistical rigor.",
      "summary": "The paper introduces MicroProbe, a method for efficiently assessing the reliability of foundation models using only 100 strategically selected probe examples. It combines prompt diversity, uncertainty quantification, and adaptive weighting to detect failure modes. The approach is shown to achieve higher reliability scores with 90% lower cost and 95% coverage compared to traditional methods requiring thousands of examples.",
      "mindmap": "graph LR\n    A[MicroProbe: Efficient Reliability Assessment for Foundation Models with Minimal Data] --> B[核心问题/Problem: Traditional reliability assessment is computationally expensive, requiring thousands of examples.]\n    A --> C[主要方法/Method: Strategic probe selection across five reliability dimensions with uncertainty quantification and adaptive weighting.]\n    A --> D[关键结果/Results: 23.5% higher reliability scores, 90% cost reduction, 95% coverage maintained, validated across domains.]"
    },
    {
      "title": "Zero-Training Temporal Drift Detection for Transformer Sentiment Models: A Comprehensive Analysis on Authentic Social Media Streams",
      "authors": "Aayam Bansal, Ishaan Gangwani",
      "institution": "IEEE",
      "link": "https://arxiv.org/pdf/2512.20631",
      "code": null,
      "tags": [
        "sentiment analysis",
        "temporal drift",
        "zero-training detection",
        "transformer models",
        "social media streams",
        "model instability"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8e40c0a23df847aa858c5e0ce28602f612024103d66ccaea9f9da99a1dded46_w640_q70.webp",
      "contributions": "1. Demonstrated significant temporal drift in transformer sentiment models during real-world events, with accuracy drops up to 23.4% on authentic social media data. 2. Introduced four novel zero-training drift detection metrics that outperform embedding-based baselines and are suitable for production deployment. 3. Provided comprehensive statistical validation on 12,279 authentic social media posts from major events, establishing practical significance exceeding industry monitoring thresholds.",
      "summary": "This paper addresses the problem of temporal drift in transformer-based sentiment models during real-world events without requiring model retraining. It proposes a zero-training detection framework using novel inference-time metrics, validated on authentic social media data. The main conclusion is that this method effectively detects significant model instability and enables immediate deployment for real-time monitoring systems.",
      "mindmap": "graph LR\n    A[Zero-Training Temporal Drift Detection for Transformer Sentiment Models] --> B[核心问题/Problem: Transformer模型在动态事件期间的行为不稳定/Transformer model instability during dynamic events]\n    A --> C[主要方法/Method: 零训练检测框架与四个新指标/Zero-training detection framework with four novel metrics]\n    A --> D[关键结果/Results: 在真实数据上验证，准确率下降达23.4%，检测能力强/Validated on authentic data, 23.4% accuracy drop, strong detection capability]"
    },
    {
      "title": "Erkang-Diagnosis-1.1 Technical Report",
      "authors": "Jianbing Ma, Ao Feng, Zhenjie Gao, Xinyu Song, Li Su, Bin Chen, Wei Wang, Jiamin Wu",
      "institution": "Chengdu Lingshu Health Technology Corp. Ltd.",
      "link": "https://arxiv.org/pdf/2512.20632",
      "code": null,
      "tags": [
        "retrieval-augmented generation",
        "Qwen-3",
        "enhanced pre-training",
        "retrieval-augmented generation",
        "medical knowledge base",
        "healthcare assistant"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed164818e4da399ea74ba4b02c96fdf625f0f5b1c9130aaf2994523190ecf4eb_w640_q70.webp",
      "contributions": "1. Development of a specialized AI healthcare assistant (Erkang-Diagnosis-1.1) based on the Alibaba Qwen-3 model, 2. Integration of approximately 500GB of high-quality structured medical knowledge using a hybrid approach of enhanced pre-training and retrieval-augmented generation (RAG), 3. Demonstration of superior performance over GPT-4 in comprehensive medical exams through efficient 3-5 round interactions",
      "summary": "The paper introduces Erkang-Diagnosis-1.1, an AI healthcare assistant built on the Qwen-3 model. It integrates a large medical knowledge base using enhanced pre-training and retrieval-augmented generation to provide diagnostic suggestions. The model reportedly outperforms GPT-4 on medical exams.",
      "mindmap": "graph LR\n    A[Erkang-Diagnosis-1.1 Technical Report] --> B[核心问题/Problem: Need for professional, reliable AI health advisor]\n    A --> C[主要方法/Method: Qwen-3 + Enhanced Pre-training + RAG + 500GB Medical Knowledge]\n    A --> D[关键结果/Results: Outperforms GPT-4 in medical exams, provides diagnostic suggestions]"
    },
    {
      "title": "Enhancing Lung Cancer Treatment Outcome Prediction through Semantic Feature Engineering Using Large Language Models",
      "authors": "MunHwan Lee, Shaika Chowdhury, Xiaodi Li, Sivaraman Rajaganapathy, Eric W Klee, Ping Yang, Terence Sio, Liewei Wang, James Cerhan, Nansu NA Zong",
      "institution": "Mayo Clinic",
      "link": "https://arxiv.org/pdf/2512.20633",
      "code": null,
      "tags": [
        "clinical prediction",
        "large language models",
        "semantic feature engineering",
        "multi-modal data integration",
        "goal-oriented knowledge curator",
        "treatment outcome prediction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c8925ebbf05c9b81fd60fa118034a660d2673702659d23d8b6cf7c1d976903_w640_q70.webp",
      "contributions": "1. Proposes a novel framework using Large Language Models (LLMs) as Goal-oriented Knowledge Curators (GKC) to generate task-aligned semantic features from raw clinical data, 2. Demonstrates that GKC, as an offline preprocessing step, outperforms expert-engineered features, direct embeddings, and end-to-end transformers in predicting lung cancer treatment outcomes, 3. Shows the complementary value of integrating laboratory, genomic, and medication modalities through ablation studies, highlighting semantic representation quality as key for accuracy in sparse data.",
      "summary": "The paper addresses the challenge of predicting lung cancer treatment outcomes from sparse, heterogeneous clinical data by introducing a framework that uses Large Language Models as Goal-oriented Knowledge Curators to engineer semantic, task-specific features. This method outperforms traditional baselines, achieving a mean AUROC of 0.803, and demonstrates that high-quality semantic representation is crucial for predictive accuracy in clinical settings.",
      "mindmap": "graph LR\n    A[Enhancing Lung Cancer Treatment Outcome Prediction<br>增强肺癌治疗结果预测] --> B(核心问题/Problem: Sparse, heterogeneous clinical data<br>稀疏、异构的临床数据)\n    A --> C(主要方法/Method: LLMs as Goal-oriented Knowledge Curators<br>LLMs作为目标导向知识策展器)\n    A --> D(关键结果/Results: Superior AUROC 0.803, outperforms baselines<br>优异的AUROC 0.803，超越基线)"
    },
    {
      "title": "Real Time Detection and Quantitative Analysis of Spurious Forgetting in Continual Learning",
      "authors": "Weiwei Wang",
      "institution": "Shenzhen Sunline Tech Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.20634",
      "code": null,
      "tags": [
        "llm training",
        "catastrophic forgetting",
        "spurious forgetting",
        "shallow alignment",
        "deep alignment",
        "task alignment depth"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e2920732eeec32977638a62ffbcf4b4b075dbdd77ebc58fa777ff8a10e117219_w640_q70.webp",
      "contributions": "1. Introduced a quantitative framework (shallow vs. deep alignment) to measure task alignment depth across token positions. 2. Developed real-time detection methods and analysis tools for identifying shallow alignment and spurious forgetting during training. 3. Proposed adaptive mitigation strategies that automatically distinguish forgetting types and promote deep alignment to improve model robustness.",
      "summary": "This paper addresses catastrophic forgetting in continual learning for LLMs by identifying that performance drops are often due to \"spurious forgetting\" from shallow task alignment. The authors propose a framework to quantitatively measure alignment depth, detect shallow alignment in real-time, and apply mitigation strategies to promote deep alignment. Experiments show their method accurately identifies spurious forgetting and improves model robustness against forgetting by 3.3-7.1% over baselines.",
      "mindmap": "graph LR\n    A[Real-Time Detection and Quantitative Analysis of Spurious Forgetting<br/>虚假遗忘的实时检测与定量分析] --> B[核心问题/Problem: Catastrophic forgetting from shallow task alignment<br/>由浅层任务对齐导致的灾难性遗忘]\n    A --> C[主要方法/Method: Quantitative metrics & real-time detection for alignment depth<br/>对齐深度的量化指标与实时检测]\n    A --> D[关键结果/Results: High identification accuracy & improved robustness<br/>高识别准确率与提升的鲁棒性]"
    },
    {
      "title": "Data-Free Pruning of Self-Attention Layers in LLMs",
      "authors": "Dhananjay Saikumar, Blesson Varghese",
      "institution": "University of St Andrews",
      "link": "https://arxiv.org/pdf/2512.20636",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "attention pruning",
        "data-free pruning",
        "Gate-Norm",
        "inference acceleration",
        "attention suppression hypothesis"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9338cbf768451f7709aa625ae03202bc7b84fcaa758ea1d75a6f5eaa4aa228c_w640_q70.webp",
      "contributions": "1. Proposes the Attention Suppression Hypothesis to explain the redundancy of deep self-attention layers in LLMs. 2. Introduces Gate-Norm, a one-shot, weight-only criterion for ranking and pruning attention sublayers without requiring data, forward passes, or fine-tuning. 3. Demonstrates that pruning 8-16 attention layers with Gate-Norm yields up to 1.30x higher inference throughput while maintaining accuracy within 2% of the baseline, matching data-driven methods but being ~1000x faster.",
      "summary": "The paper addresses the high inference cost of LLMs by proposing a data-free method to prune redundant self-attention layers. It introduces Gate-Norm, a fast weight-only criterion based on query-key coupling, which removes layers without needing calibration data or fine-tuning. The method significantly speeds up inference while preserving model accuracy, enabling practical LLM compression.",
      "mindmap": "graph LR\n    A[Data-Free Pruning of Self-Attention Layers in LLMs] --> B[核心问题/Problem: LLM推理成本高，注意力层是瓶颈/High LLM inference cost, attention layers are bottleneck]\n    A --> C[主要方法/Method: 提出Gate-Norm，基于权重无数据剪枝/Propose Gate-Norm, weight-only data-free pruning]\n    A --> D[关键结果/Results: 推理速度提升1.30倍，精度损失<2%，速度快1000倍/1.30x faster inference, <2% accuracy drop, 1000x faster scoring]"
    },
    {
      "title": "Uncovering Competency Gaps in Large Language Models and Their Benchmarks",
      "authors": "Matyas Bohacek, Nino Scherrer, Nicholas Dufour, Thomas Leung, Christoph Bregler, Stephanie C. Y. Chan",
      "institution": "Stanford University, Google DeepMind",
      "link": "https://arxiv.org/pdf/2512.20638",
      "code": "competency-gaps.github.io",
      "tags": [
        "llm evaluation",
        "sparse autoencoders",
        "benchmark gaps",
        "model gaps",
        "concept activations",
        "competency gaps"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6d5ab9e8ede467e65cbc2079e58ecf9b8d8ade8145f7e9e90f1b6f8382e288b_w640_q70.webp",
      "contributions": "1. Proposes a novel method using sparse autoencoders (SAEs) to automatically uncover fine-grained competency gaps in LLMs and benchmarks. 2. Introduces a representation-grounded evaluation approach that computes saliency-weighted performance scores based on model-internal concept activations. 3. Demonstrates the method's ability to identify specific model weaknesses (e.g., non-sycophantic behaviors) and benchmark coverage imbalances (e.g., over-representation of obedience concepts) without manual supervision.",
      "summary": "This paper addresses the problem that aggregated benchmark scores can hide specific weaknesses in LLMs and imbalances in benchmark coverage. The authors propose an automated method using sparse autoencoders to decompose benchmark performance into fine-grained concepts based on the model's internal representations. Their analysis of two models and ten benchmarks revealed model gaps in areas like non-sycophancy and safety, and benchmark gaps such as an over-representation of obedience-related concepts.",
      "mindmap": "graph LR\n        A[Uncovering Competency Gaps<br/>揭示能力差距] --> B[Problem: Aggregated metrics obscure model/benchmark gaps<br/>问题：聚合指标掩盖模型/基准差距]\n        A --> C[Method: Use Sparse Autoencoders (SAEs) for concept-level decomposition<br/>方法：使用稀疏自编码器进行概念级分解]\n        A --> D[Results: Found gaps in non-sycophancy, safety; benchmark over-represents obedience<br/>结果：发现非谄媚、安全方面的差距；基准过度代表服从性]"
    },
    {
      "title": "Forecasting N-Body Dynamics: A Comparative Study of Neural Ordinary Differential Equations and Universal Differential Equations",
      "authors": "Suriya R S, Prathamesh Dinesh Joshi, Rajat Dandekar, Raj Dandekar, Sreedath Panat",
      "institution": "Vizuara AI Labs",
      "link": "https://arxiv.org/pdf/2512.20643",
      "code": null,
      "tags": [
        "scientific machine learning",
        "Neural Ordinary Differential Equations",
        "Universal Differential Equations",
        "forecasting breakdown point",
        "n-body problem",
        "Julia"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06b852f384c602c478fd1ea2166cf9ac3e8442f63a46b1d479333a1e00699c6b_w640_q70.webp",
      "contributions": "1. Conducted a comparative study of Neural ODEs and Universal Differential Equations (UDEs) for forecasting n-body dynamics, a fundamental astrophysics problem. 2. Introduced and determined the \"forecasting breakdown point\" to quantify the minimal training data required for accurate future predictions. 3. Demonstrated that the UDE model, which blends known physics with neural networks, is significantly more data-efficient, requiring only 20% of data for a correct forecast compared to 90% for a Neural ODE.",
      "summary": "This paper compares two Scientific Machine Learning frameworks, Neural ODEs and Universal Differential Equations (UDEs), for forecasting the dynamics of the n-body problem. The study introduces the concept of a \"forecasting breakdown point\" to measure data efficiency and finds that the UDE model, which incorporates known physical laws, is far more efficient, requiring only 20% of the training data that a Neural ODE needs for accurate predictions.",
      "mindmap": "graph LR\n    A[Forecasting N-Body Dynamics<br/>N体动力学预测] --> B[核心问题/Problem<br/>传统黑盒模型忽略物理定律<br/>Traditional black-box models ignore physics]\n    A --> C[主要方法/Method<br/>使用科学机器学习框架<br/>Use Scientific ML frameworks (NODEs, UDEs)]\n    A --> D[关键结果/Results<br/>UDE数据效率更高<br/>UDE is more data-efficient]"
    },
    {
      "title": "Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning",
      "authors": "Leo Lu, Jonathan Zhang, Sean Chua, Spencer Kim, Kevin Zhu, Sean O'Brien, Vasu Sharma",
      "institution": "Pennsylvania State University, Binghamton University, University of Toronto, UC Berkeley, Algoverse",
      "link": "https://arxiv.org/pdf/2512.20647",
      "code": null,
      "tags": [
        "reasoning evaluation",
        "chain-of-thought",
        "reasoning interchangeability",
        "process reward model",
        "token-level log-probability thresholds"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad2ec01f1b7eff609789d150706b31289a628fdb2bdaa6ac8867d9e30a0ea0c1_w640_q70.webp",
      "contributions": "1. Proposes a framework to evaluate the interchangeability of reasoning chains across LLMs, assessing if partial reasoning from one model can be reliably continued by another. 2. Introduces a method using token-level log-probability thresholds to truncate reasoning at different stages and a Process Reward Model (PRM) for evaluation. 3. Demonstrates that hybrid reasoning chains often preserve or even improve final accuracy and logical structure, suggesting interchangeability as an emerging property for modular reasoning in collaborative AI.",
      "summary": "This paper investigates whether partially completed reasoning chains from one large language model can be reliably continued by another model, using token-level log-probability thresholds to truncate reasoning and a Process Reward Model for evaluation. The study finds that hybrid reasoning chains often maintain or enhance accuracy and coherence, indicating that reasoning interchangeability is a viable property for collaborative AI systems.",
      "mindmap": "graph LR\n    A[Reasoning Relay: Evaluating Stability and Interchangeability of Large Language Models in Mathematical Reasoning] --> B[核心问题/Problem: 不同LLM间的推理链是否可互换?/Interchangeability of reasoning across LLMs?]\n    A --> C[主要方法/Method: 使用log-probability阈值截断推理链,并用PRM评估/Use log-probability thresholds to truncate reasoning, evaluate with PRM]\n    A --> D[关键结果/Results: 混合推理链保持或提升准确性与逻辑结构/Hybrid reasoning chains preserve or improve accuracy & logical structure]"
    },
    {
      "title": "Mixture of Attention Schemes (MoAS): Learning to Route Between MHA, GQA, and MQA",
      "authors": "Esmail Gumaan",
      "institution": "Independent Researcher (Inferred from personal email domain)",
      "link": "https://arxiv.org/pdf/2512.20650",
      "code": "https://github.com/Esmail-ibraheem/Mixture-of-Attention-Schemes-MoAS",
      "tags": [
        "llm inference",
        "Mixture of Attention Schemes",
        "KV Cache",
        "Dynamic Routing",
        "Conditional Computation",
        "Attention Mechanism"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/151d0589c37dc455317c2c4d1e613c5f722207da0abcb0abaf82373ef04ee19d_w640_q70.webp",
      "contributions": "1. Proposes Mixture of Attention Schemes (MoAS), a novel architecture that dynamically routes tokens between MHA, GQA, and MQA attention schemes. 2. Demonstrates that dynamic routing outperforms a static averaging of attention schemes, validating the learned routing approach. 3. Shows the method achieves performance competitive with the high-quality MHA baseline while offering potential for conditional compute and memory efficiency.",
      "summary": "This paper addresses the trade-off between model quality and inference efficiency in Transformer attention mechanisms. It proposes MoAS, which uses a learned router to dynamically select between MHA, GQA, and MQA for each token. Experiments show dynamic routing outperforms static mixtures and matches MHA performance, offering a path to efficient conditional computation.",
      "mindmap": "graph LR\n    A[MoAS: Mixture of Attention Schemes] --> B[核心问题/Problem<br>Attention机制的质量与效率权衡<br>Trade-off between quality and efficiency]\n    A --> C[主要方法/Method<br>动态路由选择注意力方案<br>Dynamic routing between MHA, GQA, MQA]\n    A --> D[关键结果/Results<br>动态路由优于静态混合，性能媲美MHA<br>Dynamic routing outperforms static mixture, competitive with MHA]"
    },
    {
      "title": "AIAuditTrack: A Framework for AI Security system",
      "authors": "Zixun Luo, Yuhang Fan, Yufei Li, Youzhi Zhang, Hengyu Lin, Ziqi Wang",
      "institution": "Huazhong University of Science and Technology, Lingnan University, Centre for Artificial Intelligence and Robotics (CAIR) Hong Kong Institute of Science & Innovation, Chinese Academy of Sciences, Tsinghua University, Fujian Jiangxia University",
      "link": "https://arxiv.org/pdf/2512.20649",
      "code": null,
      "tags": [
        "AI governance and auditing",
        "blockchain",
        "decentralized identity (DID)",
        "verifiable credentials (VC)",
        "risk diffusion algorithm",
        "interaction graph"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74b6835bdebf655288e51122fb875aaa032b286789052e6467724b2eb1e0af84_w640_q70.webp",
      "contributions": "1. Proposes a blockchain-based framework (AiAuditTrack) for recording and governing AI usage traffic to enable auditing and risk traceability. 2. Introduces an identity management mechanism using Decentralized Identity (DID) and Verifiable Credentials (VC) to establish trusted and identifiable AI entities. 3. Designs a risk diffusion algorithm on a dynamic interaction graph model to trace the source of risky behaviors and propagate warnings.",
      "summary": "This paper addresses the security and accountability challenges in AI-driven applications by proposing AiAuditTrack, a blockchain framework that uses decentralized identity and verifiable credentials to record AI interaction data and enable auditing. It models AI entities as nodes in a graph and introduces a risk diffusion algorithm for tracing risky behavior origins. The framework's feasibility is demonstrated through blockchain performance metrics (TPS) under large-scale recording scenarios.",
      "mindmap": "graph LR\n    A[AiAuditTrack: AI安全系统框架<br>AiAuditTrack: AI Security System Framework] --> B[核心问题/Problem: AI交互数据激增导致安全与责任归属挑战<br>Explosive AI interaction data raises security & accountability issues]\n    A --> C[主要方法/Method: 基于区块链的框架，使用DID/VC进行身份管理，设计风险扩散算法<br>Blockchain-based framework with DID/VC for identity & risk diffusion algorithm]\n    A --> D[关键结果/Results: 实现可验证的AI审计与风险溯源，系统在大规模记录下稳定<br>Enables verifiable AI auditing & risk tracing, system stable at scale]"
    },
    {
      "title": "AI-Driven Decision-Making System for Hiring Process",
      "authors": "Vira Filatova, Andrii Zelenchuk, Dmytro Filatov",
      "institution": "Covijn Ltd., Aimech Technologies Corp.",
      "link": "https://arxiv.org/pdf/2512.20652",
      "code": null,
      "tags": [
        "agent system",
        "multi-agent system",
        "LLM orchestration",
        "human-in-the-loop",
        "explainable scoring",
        "public-data verification"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af0225e1b8b3f96dbb1b944ce2ab06438c60ac8bcf7ef93a1f210771fa73aae4_w640_q70.webp",
      "contributions": "1. A modular multi-agent AI hiring assistant that integrates heterogeneous inputs (documents, video, public data) into a structured profile. 2. An LLM-orchestrated pipeline with strict constraints to reduce output variability and generate traceable, component-level rationales for scoring. 3. A configurable candidate ranking system based on aggregated technical fit, culture fit, and normalized risk penalties, evaluated with a proposed efficiency metric (expected time per qualified candidate).",
      "summary": "This paper addresses the bottleneck of early-stage candidate validation in hiring by proposing an AI-driven, modular multi-agent system. The system integrates and processes diverse candidate inputs through an LLM-orchestrated pipeline to produce explainable scores and rankings. Evaluation on real applicants shows the system significantly improves screening efficiency, reducing the expected time per qualified candidate compared to human recruiters, while keeping a human as the final decision authority.",
      "mindmap": "graph LR\n        Root[”AI-Driven Hiring System<br>AI驱动的招聘系统”] --> Problem[”核心问题/Problem<br>Heterogeneous inputs & screening bottleneck<br>输入异构与筛选瓶颈”]\n        Root --> Method[”主要方法/Method<br>Multi-agent LLM pipeline with constraints<br>带约束的多智能体LLM流程”]\n        Root --> Results[”关键结果/Results<br>Higher efficiency & lower cost<br>更高效率与更低成本”]"
    },
    {
      "title": "Memory Bear AI A Breakthrough from Memory to Cognition Toward Artificial General Intelligence",
      "authors": "Deliang Wen, Ke Sun",
      "institution": "Not specified in provided content",
      "link": "https://arxiv.org/pdf/2512.20651",
      "code": null,
      "tags": [
        "agent system",
        "memory architecture",
        "long-term conversation",
        "hallucination reduction",
        "multimodal perception",
        "cognitive integration"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9c13cde65ef804fade26e06361a8d03517ad2394b3e21295ae77d2b5b462c29_w640_q70.webp",
      "contributions": "1. Proposes the Memory Bear system, a human-like memory architecture for LLMs grounded in cognitive science principles. 2. Achieves a full-chain reconstruction of LLM memory mechanisms by integrating multimodal perception, dynamic memory maintenance, and adaptive cognitive services. 3. Demonstrates significant performance improvements in knowledge fidelity, retrieval efficiency, and hallucination reduction across multiple domains compared to existing solutions.",
      "summary": "This paper addresses the inherent memory limitations of LLMs, such as restricted context and hallucination, by proposing the Memory Bear system. Memory Bear constructs a cognitive science-inspired memory architecture to enhance long-term dialogue and reasoning. Experimental results show it outperforms existing methods in accuracy and efficiency, marking a step from memory to cognition in AI.",
      "mindmap": "graph LR\n    A[Memory Bear AI<br>论文标题/Paper Title] --> B[LLM Memory Limitations<br>核心问题/Problem]\n    A --> C[Human-like Memory Architecture<br>主要方法/Method]\n    A --> D[Performance Breakthrough<br>关键结果/Results]\n    B --> B1[Restricted Context & Forgetting<br>受限上下文与遗忘]\n    B --> B2[Hallucination & Redundancy<br>幻觉与冗余]\n    C --> C1[Multimodal Perception<br>多模态感知]\n    C --> C2[Dynamic Memory Maintenance<br>动态记忆维护]\n    C --> C3[Adaptive Cognitive Services<br>自适应认知服务]\n    D --> D1[Higher Accuracy & Efficiency<br>更高准确率与效率]\n    D --> D2[Reduced Hallucination<br>降低幻觉率]\n    D --> D3[Improved Reasoning<br>增强推理能力]"
    },
    {
      "title": "MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing",
      "authors": "Yuting Hu, Lei Zhuang, Hua Xiang, Jinjun Xiong, Gi-Joon Nam",
      "institution": "University at Buffalo, IBM Research",
      "link": "https://arxiv.org/pdf/2512.20655",
      "code": null,
      "tags": [
        "others",
        "mask optimization",
        "optical proximity correction",
        "inverse lithography technique",
        "deep learning",
        "benchmark dataset"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce9bc1ac552258257c450a9bc4d4c4ae0264c958efae291f56fc44af367bf07a_w640_q70.webp",
      "contributions": "1. Introduces MaskOpt, a large-scale benchmark dataset for AI-driven mask optimization constructed from real 45nm IC designs, addressing limitations of synthetic data. 2. The dataset preserves standard-cell hierarchy and includes varying context window sizes to enable cell- and context-aware model training. 3. Provides comprehensive benchmarks by evaluating state-of-the-art deep learning models, revealing performance trade-offs and validating the importance of contextual and cell-aware inputs.",
      "summary": "The paper presents MaskOpt, a large-scale dataset built from real integrated circuit designs to advance deep learning for mask optimization in semiconductor manufacturing. It addresses the limitations of existing synthetic datasets by including cell hierarchy and surrounding context. Benchmarking results demonstrate the dataset's utility and highlight the critical role of context and cell information for accurate mask generation.",
      "mindmap": "graph LR\n    A[MaskOpt Dataset<br/>MaskOpt数据集] --> B[核心问题/Problem<br/>Existing datasets are synthetic, lack cell hierarchy & context<br/>现有数据集为合成数据，缺乏单元层次和上下文];\n    A --> C[主要方法/Method<br/>Build large-scale dataset from real 45nm IC designs with cell-aware tiles & context windows<br/>基于真实45nm设计构建大规模数据集，包含单元感知切片和上下文窗口];\n    A --> D[关键结果/Results<br/>Benchmarks show model trade-offs, context & cell info are crucial<br/>基准测试显示模型权衡，上下文和单元信息至关重要];"
    },
    {
      "title": "Managing the Stochastic: Foundations of Learning in Neuro-Symbolic Systems for Software Engineering",
      "authors": "Matthew Thompson",
      "institution": "Independent Researcher",
      "link": "https://arxiv.org/pdf/2512.20660",
      "code": null,
      "tags": [
        "agent system",
        "dual-state architecture",
        "atomic action pairs",
        "guard functions",
        "neuro-symbolic systems",
        "code generation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a73fceac46d6997904de43696e8db407d645c6e4388012a9e24a3b9565e06fb_w640_q70.webp",
      "contributions": "1. Proposes a control boundary that treats the LLM as a stochastic environment component, not the decision-making agent, to manage its unpredictability. 2. Formalizes a Dual-State Architecture separating deterministic workflow state from stochastic environment state. 3. Introduces Atomic Action Pairs and Guard Functions to couple generation with verification as indivisible transactions, projecting probabilistic outputs onto observable workflow state.",
      "summary": "This paper addresses the problem of stochastic failures in AI coding agents by proposing a neuro-symbolic architectural framework that treats the LLM as part of the environment. The method uses a Dual-State Architecture with Atomic Action Pairs and Guard Functions to separate deterministic control from stochastic generation. The main conclusion is that such architectural constraints can significantly improve task success rates for qualified models, potentially substituting for parameter scale in achieving reliable code generation.",
      "mindmap": "graph LR\n    A[Managing the Stochastic<br>管理随机性] --> B[Problem: LLM-based agents prone to stochastic failures<br>问题: 基于LLM的智能体易受随机性故障影响]\n    A --> C[Method: Dual-State Architecture, Atomic Action Pairs, Guard Functions<br>方法: 双态架构, 原子动作对, 守卫函数]\n    A --> D[Results: Improved success rates, architectural constraints can substitute for scale<br>结果: 成功率提升, 架构约束可替代模型规模]"
    },
    {
      "title": "From Fake Focus to Real Precision: Confusion-Driven Adversarial Attention Learning in Transformers",
      "authors": "Yawei Liu",
      "institution": "Chinese Academy of Sciences, Computer Network Information Center",
      "link": "https://arxiv.org/pdf/2512.20661",
      "code": null,
      "tags": [
        "sentiment analysis",
        "adversarial training",
        "attention mechanism",
        "policy gradient",
        "transformer",
        "model interpretability"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dd81ed17263c1d20f831b211a88f9bc1399e87818af70003e7ba1196a7ddddf_w640_q70.webp",
      "contributions": "1. Proposes an Adversarial Feedback for Attention (AFA) training mechanism to automatically redistribute attention weights without manual supervision. 2. Introduces a dynamic masking strategy and a discriminator in an adversarial framework to identify and correct suboptimal attention. 3. Employs a policy gradient approach to efficiently optimize attention distributions, leading to improved performance on sentiment analysis tasks and a 12.6% gain when applied to large language models.",
      "summary": "The paper identifies that Transformer models for sentiment analysis often misallocate attention to common words, missing important but less frequent terms. To solve this, it proposes an Adversarial Feedback for Attention (AFA) mechanism using dynamic masking and policy gradient optimization to refine attention distributions automatically. Experiments show the method achieves state-of-the-art results and significantly boosts performance in large language models.",
      "mindmap": "graph LR\n    A[From Fake Focus to Real Precision<br>从虚假聚焦到真实精度] --> B[核心问题/Problem<br>Transformer注意力分配不当<br>Transformer Misallocates Attention]\n    A --> C[主要方法/Method<br>对抗性注意力反馈(AFA)<br>Adversarial Feedback for Attention]\n    A --> D[关键结果/Results<br>SOTA性能 & LLM提升12.6%<br>SOTA Performance & 12.6% LLM Gain]\n    B --> C\n    C --> D"
    },
    {
      "title": "Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models",
      "authors": "Yiqing Ma, Jung-Hua Liu",
      "institution": "Universiti Malaya, National Chung Cheng University",
      "link": "https://arxiv.org/pdf/2512.20662",
      "code": null,
      "tags": [
        "llm evaluation",
        "laziness",
        "decoding suboptimality",
        "context degradation",
        "instruction-following",
        "long-context"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c503ae7a717782d28701da19cc7269ead2f9a34b5478457e18e791787fc94dea_w640_q70.webp",
      "contributions": "1. Quantified the \"laziness\" artifact in LLMs, showing widespread failure to fully comply with complex multi-part instructions. 2. Provided empirical evidence challenging the prevalence of \"decoding suboptimality\" in a simple reasoning task, suggesting greedy decoding may align with high-confidence solutions. 3. Demonstrated surprising robustness against \"context degradation\" in long, chaotic conversations, indicating LLMs may internally mitigate context forgetting in retrieval scenarios.",
      "summary": "This paper quantifies three behavioral artifacts in Large Language Models (LLMs) through controlled experiments. The results show that while LLMs are often \"lazy\" in following complex instructions, they show limited decoding suboptimality and surprising robustness against context degradation in long conversations. The findings suggest instruction compliance remains a challenge, but some hypothesized failure modes like context forgetting may be less severe in straightforward scenarios.",
      "mindmap": "graph LR\n        A[Quantifying Laziness, Decoding Suboptimality, and Context Degradation in Large Language Models] --> B(核心问题/Problem: LLM行为缺陷/LLM Behavioral Artifacts)\n        A --> C(主要方法/Method: 三个受控实验/Three Controlled Experiments)\n        A --> D(关键结果/Results: 懒惰普遍/Laziness Widespread, 解码次优有限/Limited Decoding Suboptimality, 上下文退化稳健/Robust Context Degradation)"
    },
    {
      "title": "Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning via Structural Constraint Satisfaction",
      "authors": "Shinobu Miya",
      "institution": "Independent Researcher",
      "link": "https://arxiv.org/pdf/2512.20664",
      "code": "https://github.com/ShinobuMiya/Eidoku",
      "tags": [
        "reasoning verification",
        "structural constraint satisfaction",
        "neuro-symbolic verification",
        "hallucination detection",
        "constraint satisfaction problem",
        "system-2 gate"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62978c31bb485ddbb117f5081c33f608c011c35aca015e4df8eaad0dd42439ff_w640_q70.webp",
      "contributions": "1. Reformulates LLM reasoning verification as a Constraint Satisfaction Problem (CSP) independent of generation likelihood, focusing on structural feasibility instead of statistical plausibility. 2. Introduces a lightweight System-2 gate, Eidoku, that uses a context-calibrated cost threshold derived from intrinsic statistics to reject candidates based on structural violation cost. 3. Demonstrates the ability to deterministically reject \"smooth falsehoods\"—high-probability but structurally inconsistent statements—which probability-based verifiers cannot detect.",
      "summary": "The paper addresses LLM hallucinations by proposing Eidoku, a neuro-symbolic verification gate that treats reasoning verification as a structural constraint satisfaction problem, independent of generation likelihood. It uses a cost function based on graph connectivity, feature consistency, and logical entailment to reject structurally inconsistent statements. Experiments show this approach can deterministically reject high-probability yet structurally disconnected hallucinations, serving as a sanity check for generative reasoning.",
      "mindmap": "graph LR\n    A[Eidoku: 神经符号验证门<br>Neuro-Symbolic Verification Gate] --> B[核心问题/Problem: LLM产生高概率幻觉<br>LLMs produce high-likelihood hallucinations]\n    A --> C[主要方法/Method: 基于结构约束满足的验证<br>Verification via Structural Constraint Satisfaction]\n    A --> D[关键结果/Results: 拒绝平滑错误，确定性检测<br>Rejects smooth falsehoods, deterministic detection]"
    },
    {
      "title": "Dominating vs. Dominated: Generative Collapse in Diffusion Models",
      "authors": "Hayeon Jeong, Jong-Seok Lee",
      "institution": "Yonsei University",
      "link": "https://arxiv.org/pdf/2512.20666",
      "code": null,
      "tags": [
        "text-to-image generation",
        "diffusion models",
        "cross-attention",
        "generative collapse",
        "multi-concept generation",
        "attention dynamics"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e3797020eb871e661d9cda59fdbe8a7bdf314a2935eff1ccf97c35a90d39ee_w640_q70.webp",
      "contributions": "1. Identifies and defines the Dominant-vs-Dominated (DvD) phenomenon in multi-concept text-to-image generation, 2. Introduces DominanceBench for systematic analysis of the DvD imbalance, 3. Provides causal analysis from data (limited instance diversity) and architecture (cross-attention saturation & distributed head mechanisms) perspectives.",
      "summary": "This paper investigates the \"Dominant-vs-Dominated\" (DvD) imbalance in diffusion models, where one concept token suppresses others in multi-concept prompts. The authors analyze this using a new benchmark and find causes in limited training data diversity and cross-attention dynamics. Their findings offer insights into generative collapse for more reliable text-to-image generation.",
      "mindmap": "graph LR\n        A[Dominating vs. Dominated<br/>支配 vs. 被支配] --> B[核心问题/Problem<br/>Multi-concept prompt generation imbalance<br/>多概念提示生成失衡];\n        A --> C[主要方法/Method<br/>Introduce DominanceBench & analyze causes<br/>引入DominanceBench并分析原因];\n        A --> D[关键结果/Results<br/>Data diversity & attention dynamics cause DvD<br/>数据多样性和注意力动态导致DvD];"
    },
    {
      "title": "Forward Only Learning for Orthogonal Neural Networks of any Depth",
      "authors": "Paul Caillon, Alex Colagrande, Erwan Fagnou, Blaise Delattre, Alexandre Allauzen",
      "institution": "Université Paris-Dauphine - PSL, ESPCI PSL",
      "link": "https://arxiv.org/pdf/2512.20668",
      "code": "https://github.com/",
      "tags": [
        "neural network training algorithms",
        "forward-only learning",
        "orthogonal neural networks",
        "backpropagation alternative",
        "FOTON",
        "PEPITA"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14ce0cf521f1d147eae7293cef8a5a53d6a0ca2fda6723bc707498635d351a0b_w640_q70.webp",
      "contributions": "1. Theoretical analysis of limitations in existing forward-only frameworks like PEPITA, 2. Design of a forward-only algorithm equivalent to backpropagation under linear/orthogonal assumptions, 3. Introduction of FOTON, a practical forward-only training method for orthogonal networks that scales to any depth and works on CNNs>",
      "summary": "This paper addresses the computational burden of backpropagation by proposing a forward-only training algorithm called FOTON for orthogonal neural networks. The method replaces the backward pass with a modulated forward pass, enabling training of deep networks without backpropagation. Experiments show FOTON outperforms prior forward-only methods and scales to networks of any depth, including convolutional architectures.",
      "mindmap": "graph LR\n    A[Forward Only Learning for Orthogonal Neural Networks<br>前向传播学习用于正交神经网络] --> B[Problem: Backpropagation is computationally expensive<br>问题: 反向传播计算成本高]\n    A --> C[Method: FOTON - Forward-Only Training with modulated forward pass<br>方法: FOTON - 使用调制前向传播的前向训练]\n    A --> D[Results: Trains networks of any depth, outperforms PEPITA<br>结果: 可训练任意深度网络，性能优于PEPITA]"
    },
    {
      "title": "Improving Cardiac Risk Prediction Using Data Generation Techniques",
      "authors": "Alexandre Cabodevila, Pedro Gamallo-Fernandez, Juan C. Vidal, Manuel Lama",
      "institution": "Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS), Universidade de Santiago de Compostela",
      "link": "https://arxiv.org/pdf/2512.20669",
      "code": null,
      "tags": [
        "generative models",
        "Conditional Variational Autoencoder",
        "synthetic data generation",
        "cardiac risk prediction",
        "data augmentation",
        "clinical records"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0661f3f558f42310471788ea2bad662661692287e3137248998355eb91c8470b_w640_q70.webp",
      "contributions": "1. Proposes a novel architecture based on a Conditional Variational Autoencoder (CVAE) for generating realistic and coherent synthetic clinical records. 2. Addresses key limitations in medical data analysis such as data scarcity, unsuitability, and high prevalence of missing values. 3. Demonstrates that using the generated synthetic data improves the accuracy of cardiac risk prediction classifiers, outperforming other deep learning data generation approaches.",
      "summary": "This paper addresses the challenges of scarce and incomplete real-world medical data for cardiac risk prediction by proposing a Conditional Variational Autoencoder (CVAE) architecture to generate realistic synthetic clinical records. The generated data is used to augment datasets, which in turn enhances the performance of cardiac risk prediction models. The results show that the proposed method successfully generates coherent data and improves classifier accuracy compared to state-of-the-art alternatives.",
      "mindmap": "graph LR\n    A[Improving Cardiac Risk Prediction Using Data Generation Techniques] --> B(核心问题/Problem: 真实医疗数据稀缺、不完整且存在缺失值/Real-world medical data is scarce, incomplete, and has missing values)\n    A --> C(主要方法/Method: 基于条件变分自编码器的架构生成合成临床记录/CVAE-based architecture for synthetic clinical record generation)\n    A --> D(关键结果/Results: 生成的数据提高了心脏风险预测分类器的准确性/Generated data improves cardiac risk prediction classifier accuracy)"
    },
    {
      "title": "Bridging the AI Trustworthiness Gap between Functions and Norms",
      "authors": "Daan Di Scala, Sophie Lathouwers, Michael van Bekkum",
      "institution": "TNO Netherlands Organisation for Applied Scientific Research, Utrecht University",
      "link": "https://arxiv.org/pdf/2512.20671",
      "code": null,
      "tags": [
        "trustworthy ai",
        "Trustworthy AI",
        "AI Act",
        "Functional AI Trustworthiness",
        "Normative AI Trustworthiness",
        "Conceptual Language"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/578b1d068cdd40bde2c92d3a17699f222634e6ec69ca5580ca2eda375c8105ca_w640_q70.webp",
      "contributions": "1. Identifies and articulates the gap between Functional Trustworthy AI (FTAI) and Normative Trustworthy AI (NTAI). 2. Proposes the development of a semantic/conceptual language as a bridge to link FTAI implementations with NTAI regulations. 3. Provides key considerations and a future roadmap for assessing AI trustworthiness using this integrated framework.",
      "summary": "This position paper identifies a gap between the functional implementation (FTAI) and normative regulation (NTAI) of Trustworthy AI, which hinders system assessment. It proposes bridging this gap by developing a semantic language to map technical functions to legal norms. The conclusion is that such a framework will help developers implement compliant systems and stakeholders assess trustworthiness.",
      "mindmap": "graph LR\n        A[论文标题: Bridging the AI Trustworthiness Gap<br>论文标题: Bridging the AI Trustworthiness Gap] --> B[核心问题/Problem: FTAI与NTAI存在鸿沟<br>核心问题/Problem: Gap between FTAI and NTAI]\n        A --> C[主要方法/Method: 提出语义语言作为桥梁<br>主要方法/Method: Propose a semantic language as a bridge]\n        A --> D[关键结果/Results: 提供评估框架与未来路线图<br>关键结果/Results: Provide assessment framework & future roadmap]"
    },
    {
      "title": "Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection",
      "authors": "Weilin Zhou, Zonghao Ying, Junjie Mu, Shengwei Tian, Quanchen Zou, Deyue Zhang, Dongdong Yang, Xiangzheng Zhang",
      "institution": "Xinjiang University, 360 AI Security Lab, Beihang University, Politecnico di Milano",
      "link": "https://arxiv.org/pdf/2512.20670",
      "code": null,
      "tags": [
        "multimodal fake news detection",
        "inconsistency detection",
        "feature disentanglement",
        "conflict-consensus mechanism",
        "physics-inspired dynamics",
        "cross-modal discrepancy"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c06ff160d4943f1ae5c4648f6e6cd042a0c1232af6212adf0021ba7f0b7c2ab_w640_q70.webp",
      "contributions": "1. Proposes a paradigm shift from consistency-seeking to inconsistency-seeking for multimodal fake news detection, explicitly amplifying cross-modal contradictions as evidence. 2. Introduces a novel framework (DCCF) that disentangles inputs into independent Fact and Sentiment spaces to separate objective mismatches from emotional dissonance. 3. Employs physics-inspired feature dynamics and a conflict-consensus mechanism to actively polarize and standardize local discrepancies against a global context for robust judgment.",
      "summary": "The paper identifies a flaw in mainstream multimodal fake news detection, which treats cross-modal discrepancies as noise, and proposes a new Dynamic Conflict-Consensus Framework (DCCF) designed to actively seek and amplify these inconsistencies as evidence of fabrication. The method disentangles fact from sentiment and uses physics-inspired dynamics to extract conflicts. Experiments show DCCF outperforms state-of-the-art baselines with an average accuracy improvement of 3.52%.",
      "mindmap": "graph LR\n    A[Disentangling Fact from Sentiment: A Dynamic Conflict-Consensus Framework for Multimodal Fake News Detection] --> B[核心问题/Problem: 主流一致性融合将关键跨模态差异误判为噪声，稀释了伪造证据]\n    A --> C[主要方法/Method: 提出DCCF框架，解耦事实与情感，利用物理启发的动力学主动放大矛盾]\n    A --> D[关键结果/Results: 在三个真实数据集上超越SOTA，平均准确率提升3.52%]"
    },
    {
      "title": "Revisiting the Learning Objectives of Vision-Language Reward Models",
      "authors": "Simon Roy, Samuel Barbeau, Giovanni Beltrame, Christian Desrosiers, Nicolas Thome",
      "institution": "Polytechnique Montréal, École de Technologie Supérieure, Sorbonne Université",
      "link": "https://arxiv.org/pdf/2512.20675",
      "code": null,
      "tags": [
        "reinforcement learning",
        "reward modeling",
        "vision-language models",
        "triplet loss",
        "Meta-World",
        "contrastive learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca907344b4dbef770bf1367dee07e4ba6b6f7a2b525ad28c7bf8d0ff11f62075_w640_q70.webp",
      "contributions": "1. Proposes a unified framework to isolate and evaluate the impact of learning objectives in vision-language reward models, controlling for backbone, data, and evaluation environments. 2. Demonstrates that a simple triplet loss objective can outperform more complex state-of-the-art methods for reward modeling. 3. Suggests that improvements in recent approaches may be attributed more to differences in training data and model architectures rather than the complexity of their learning objectives.",
      "summary": "This paper investigates the impact of different learning objectives for adapting vision-language models into reward functions for embodied intelligence. By comparing methods under a unified framework, the authors find that a simple triplet loss outperforms more complex state-of-the-art objectives. The results suggest that recent improvements in reward modeling may stem from data and architecture differences rather than objective complexity.",
      "mindmap": "graph LR\n        A[Revisiting VLM Reward Models] --> B(核心问题/Problem: 难以比较不同奖励模型目标/Difficulty in comparing reward model objectives)\n        A --> C(主要方法/Method: 统一框架评估/Unified framework evaluation)\n        A --> D(关键结果/Results: 三元组损失更优/Triplet loss outperforms SOTA)"
    },
    {
      "title": "HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model",
      "authors": "Yuanhao Xi, Xiaohuan Bing, Ramin Yahyapour",
      "institution": "Liaoning Technical University, University of Göttingen, Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen",
      "link": "https://arxiv.org/pdf/2512.20674",
      "code": null,
      "tags": [
        "multi-modal training",
        "Low-Rank Adaptation (LoRA)",
        "parameter-efficient fine-tuning",
        "rank adaptation",
        "mobile vision language model",
        "dynamic scheduling"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9589d36616e78e4826e61d2dbafa4ccc718075f3659352d4d01c1b6f4795a02a_w640_q70.webp",
      "contributions": "1. Proposes HyDRA, a parameter-efficient fine-tuning framework for mobile VLMs that implements hierarchical and dynamic rank scheduling. 2. Introduces hierarchical optimization with coarse-grained (layer-level) and fine-grained (intra-layer) rank assignment. 3. Employs dynamic adjustment via an end-to-end automatic optimization using a lightweight performance model to determine ranks during fine-tuning.",
      "summary": "This paper introduces HyDRA, a parameter-efficient fine-tuning framework for mobile Vision Language Models (VLMs) that addresses the computational inefficiency of standard LoRA by implementing hierarchical and dynamic rank scheduling. The method uses a two-pronged optimization strategy and a lightweight performance model to adjust ranks automatically. Experiments show HyDRA outperforms baselines, achieving a 4.7% average improvement without extra parameters and sometimes surpassing full fine-tuning.",
      "mindmap": "graph LR\n    A[HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model] --> B[核心问题/Problem: Standard LoRA with fixed rank is insufficient for training mobile VLMs]\n    A --> C[主要方法/Method: HyDRA framework with hierarchical & dynamic rank scheduling]\n    A --> D[关键结果/Results: Outperforms baseline by 4.7%, no extra parameters, sometimes beats full fine-tuning]"
    },
    {
      "title": "Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems",
      "authors": "Stefano Grassi",
      "institution": "None (No affiliation or email domain provided in the given content)",
      "link": "https://arxiv.org/pdf/2512.20688",
      "code": null,
      "tags": [
        "multi-agent systems",
        "Differentiable Price Mechanism",
        "Dominant Strategy Incentive Compatibility",
        "VCG-equivalent incentive",
        "Dec-POMDPs",
        "Bayesian Incentive Compatibility"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfdd6bb51cf65ab558a1d13cbaf0fbdda25f9c06c58be3e201a62b231f808da4_w640_q70.webp",
      "contributions": "1. Proposes Mechanism-Based Intelligence (MBI), a new paradigm framing intelligence as emergent from the coordination of multiple agents. 2. Introduces the Differentiable Price Mechanism (DPM), which computes exact loss gradients as incentive signals to guarantee Dominant Strategy Incentive Compatibility and convergence. 3. Demonstrates a framework that scales linearly with the number of agents, bypassing Dec-POMDP complexity and showing significant empirical speedup over model-free RL.",
      "summary": "The paper addresses the fragility of multi-agent systems in coordinating private information and aligning incentives. It proposes Mechanism-Based Intelligence (MBI) and its core Differentiable Price Mechanism (DPM), which uses differentiable incentives to align agent actions with global objectives. The method guarantees incentive compatibility, scales efficiently, and is shown to be much faster than standard reinforcement learning approaches.",
      "mindmap": "graph LR\n    A[Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems] --> B[核心问题/Problem: Hayekian Information Problem & Hurwiczian Incentive Problem]\n    A --> C[主要方法/Method: Differentiable Price Mechanism (DPM) & Bayesian Extension]\n    A --> D[关键结果/Results: DSIC/BIC Guarantee, Linear Scaling, 50x Faster than Model-Free RL]"
    },
    {
      "title": "PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation",
      "authors": "Yuma Ichikawa, Naoya Takagi, Takumi Nakagawa, Yuzi Kanazawa, Akira Sakai",
      "institution": "Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University",
      "link": "https://arxiv.org/pdf/2512.20687",
      "code": null,
      "tags": [
        "llm inference",
        "hierarchical autoregressive model",
        "KV-cache optimization",
        "memory-bound inference",
        "multi-resolution context",
        "throughput-quality trade-off"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp",
      "contributions": "1. Proposes PHOTON, a hierarchical autoregressive model that replaces the Transformer's flat token-by-token scanning with a vertical, multi-resolution context access pattern. 2. Introduces a persistent hierarchy of latent streams, with a bottom-up encoder compressing tokens and lightweight top-down decoders reconstructing token representations, reducing decode-time KV-cache traffic. 3. Demonstrates significant improvements in throughput per unit memory (up to 10^3x) and advantages in long-context and multi-query tasks compared to Transformer-based models.",
      "summary": "The paper identifies that Transformer inference becomes memory-bound due to ever-growing KV-cache reads/writes during autoregressive decoding. To solve this, it proposes PHOTON, a hierarchical model that accesses context vertically at multiple resolutions instead of scanning tokens horizontally. This architectural change drastically reduces memory traffic, yielding orders-of-magnitude higher throughput per unit memory while maintaining quality.",
      "mindmap": "graph LR\n    A[PHOTON: Hierarchical Autoregressive Modeling] --> B[核心问题/Problem: Transformer水平扫描导致KV缓存读写成为内存瓶颈/Horizontal scanning causes memory-bound KV-cache bottleneck]\n    A --> C[主要方法/Method: 用垂直多分辨率层次模型替代/Replace with vertical multi-resolution hierarchical model]\n    A --> D[关键结果/Results: 内存效率与吞吐量大幅提升/Significant improvement in memory efficiency & throughput]"
    },
    {
      "title": "From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education",
      "authors": "Iman Reihanian, Yunfei Hou, Qingquan Sun",
      "institution": "California State University, San Bernardino",
      "link": "https://arxiv.org/pdf/2512.20714",
      "code": null,
      "tags": [
        "educational technology",
        "generative AI",
        "personalization",
        "adaptive learning",
        "large language models",
        "intelligent tutoring systems"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e46f313e494a41a4a12873eddb6320db4cd59b6fb958bb008fd6f6512729af4_w640_q70.webp",
      "contributions": "1. Identified and analyzed five key application domains for GenAI-enabled personalization in CS education: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review. 2. Synthesized four design patterns for successful implementations: context-aware tutoring anchored in student artifacts, multi-level hint structures, composition with traditional CS infrastructure, and human-in-the-loop quality assurance. 3. Proposed an exploration-first adoption framework for integrating GenAI, emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling, while pairing recurrent risks with operational mitigations.",
      "summary": "This scoping review maps how generative AI enables personalized computer science education. It analyzes design choices across 32 studies and finds that structured implementations with explanation-first guidance and artifact grounding lead to more positive learning outcomes than unconstrained chat interfaces. The paper concludes that generative AI can provide precision scaffolding when embedded in audit-ready workflows that preserve productive struggle.",
      "mindmap": "graph LR\n    A[From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education] --> B[核心问题/Problem: Does GenAI personalization support or undermine CS learning?]\n    A --> C[主要方法/Method: Scoping review of 32 studies; Analysis of design choices & patterns]\n    A --> D[关键结果/Results: Structured designs (e.g., hint ladders, artifact grounding) are more effective; Proposes an exploration-first adoption framework]"
    },
    {
      "title": "From artificial to organic: Rethinking the roots of intelligence for digital health",
      "authors": "Prajwal Ghimire, Keyoumars Ashkan",
      "institution": "King's College London",
      "link": "https://arxiv.org/pdf/2512.20723",
      "code": null,
      "tags": [
        "digital health",
        "artificial intelligence",
        "organic intelligence",
        "digital health",
        "neural networks",
        "evolutionary processes"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0101ccc5442460351cdd29a890224590d6ea20c129b31befb533fedb4fbf8c31_w640_q70.webp",
      "contributions": "1. Argues that AI is a product of organic human ingenuity, challenging the artificial vs. organic dichotomy. 2. Proposes that intelligence in digital health fundamentally stems from organization and adaptation, not just parameter scaling. 3. Highlights the inspiration of AI principles from human neurobiology and evolutionary processes.",
      "summary": "This paper rethinks the roots of intelligence in digital health by arguing that artificial intelligence is fundamentally inspired by and derived from organic human cognition. It posits that the distinction between artificial and organic intelligence is blurred, emphasizing organization and adaptation as key principles. The conclusion suggests a more integrated view of intelligence for advancing digital health technologies.",
      "mindmap": "graph LR\n    A[From artificial to organic: Rethinking the roots of intelligence for digital health] --> B[核心问题/Problem: Distinction between artificial and organic intelligence in digital health]\n    A --> C[主要方法/Method: Philosophical analysis of AI's origins in human cognition and biology]\n    A --> D[关键结果/Results: Boundaries are less distinct; intelligence is about organization and adaptation]"
    },
    {
      "title": "SA-DiffuSeq: Addressing Computational and Scalability Challenges in Long-Document Generation with Sparse Attention",
      "authors": "Alexandros Christoforos, Chadbourne Davis",
      "institution": "Suffolk University",
      "link": "https://arxiv.org/pdf/2512.20724",
      "code": null,
      "tags": [
        "diffusion models",
        "sparse attention",
        "diffusion models",
        "long-text generation",
        "soft absorbing state",
        "computational complexity"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01893006a5e49ffeaca24f7c5197f5a706782f3051b02cc9dfef88521a05c523_w640_q70.webp",
      "contributions": "1. Introduces SA-DiffuSeq, a diffusion framework that integrates sparse attention to improve scalability for long-document modeling. 2. Proposes a novel soft absorbing state tailored to sparse attention dynamics to stabilize diffusion trajectories and accelerate sequence reconstruction. 3. Demonstrates superior training efficiency and sampling speed compared to state-of-the-art diffusion baselines, especially on extended sequences.",
      "summary": "The paper addresses the high computational cost of diffusion models for long-text generation by proposing SA-DiffuSeq, which integrates sparse attention and a novel soft absorbing state. This method reduces complexity while maintaining generation quality, making it suitable for applications like scientific writing and code generation. The results show that incorporating structured sparsity is a promising direction for efficient long-text generation.",
      "mindmap": "graph LR\n    A[SA-DiffuSeq] --> B[核心问题/Problem<br>Computational Cost & Scalability];\n    A --> C[主要方法/Method<br>Sparse Attention & Soft Absorbing State];\n    A --> D[关键结果/Results<br>Improved Efficiency & Quality];"
    },
    {
      "title": "FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs",
      "authors": "Saeed Mohammadzadeh, Erfan Hamdi, Joel Shor, Emma Lejeune",
      "institution": "Boston University, Move37 Labs",
      "link": "https://arxiv.org/pdf/2512.20732",
      "code": null,
      "tags": [
        "llm inference",
        "Finite Element Method (FEM)",
        "Code Generation",
        "LLM Benchmark",
        "Computational Mechanics",
        "Scientific Machine Learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1933a2d33b13b692f95ee8ddec0a65840af091998d38a7f0154837874636590_w640_q70.webp",
      "contributions": "1. Introduces FEM-Bench, a novel benchmark for evaluating LLMs' ability to generate scientifically valid code for computational mechanics problems. 2. Provides a structured suite of tasks based on finite element methods that enforce physical and numerical constraints for objective evaluation. 3. Presents initial evaluation results showing that state-of-the-art LLMs (e.g., Gemini 3 Pro, GPT-5) still struggle to reliably solve these introductory tasks.",
      "summary": "The paper identifies a lack of benchmarks for evaluating LLMs' scientific reasoning and code generation for physical modeling. It proposes FEM-Bench, a computational mechanics benchmark based on the Finite Element Method, to fill this gap. Initial evaluations show that even advanced LLMs cannot reliably solve all its tasks, establishing a foundation for tracking progress in AI-generated scientific code.",
      "mindmap": "graph LR\n        A[FEM-Bench Paper] --> B[核心问题/Problem: 缺乏评估LLM生成科学物理模型代码能力的基准/Lack of benchmark for evaluating LLMs' ability to generate scientifically valid physical model code]\n        A --> C[主要方法/Method: 提出基于计算力学和有限元法的结构化基准/Proposes a structured benchmark based on computational mechanics and the Finite Element Method]\n        A --> D[关键结果/Results: 先进LLM无法可靠解决所有基准任务，为跟踪进展奠定基础/State-of-the-art LLMs cannot reliably solve all benchmark tasks, establishing a foundation for tracking progress]"
    },
    {
      "title": "AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent",
      "authors": "Haipeng Luo, Huawen Feng, Qingfeng Sun, Can Xu, Kai Zheng, Yufei Wang, Tao Yang, Han Hu, Yansong Tang, Di Wang",
      "institution": "Tsinghua University, Tencent Hunyuan",
      "link": "https://arxiv.org/pdf/2512.20745",
      "code": null,
      "tags": [
        "agent system",
        "tool-augmented agent",
        "agentic reinforcement learning",
        "supervised fine-tuning (SFT)",
        "request-level asynchronous rollout",
        "prefix-aware load balancing"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7211416872630b2f7d460fe4b986a1d141827d69b65487826e3374c5e4cce08d_w640_q70.webp",
      "contributions": "1. An automated method to convert natural language chain-of-thought into structured tool-augmented trajectories for generating high-quality SFT data. 2. A novel agentic reinforcement learning paradigm that dynamically interleaves natural language generation with real-time code execution for learning tool-use strategies. 3. An efficient training system with techniques like asynchronous rollout scheduling and prefix-aware load balancing, achieving 4-5x speedup for RL training on long sequences.",
      "summary": "This paper introduces AgentMath, a framework that combines language model reasoning with code interpreter precision to solve complex math problems. It uses automated SFT data generation, agentic RL for tool-use learning, and an efficient training system, achieving state-of-the-art results on benchmarks like AIME24 and AIME25.",
      "mindmap": "graph LR\n    A[AgentMath] --> B[核心问题/Problem: LRMs are inefficient and inaccurate for complex math]\n    A --> C[主要方法/Method: Tool-augmented agent framework with SFT data generation, agentic RL, and efficient training system]\n    A --> D[关键结果/Results: SOTA performance on AIME24, AIME25, HMMT25 benchmarks]"
    },
    {
      "title": "AI-Driven Green Cognitive Radio Networks for Sustainable 6G Communication",
      "authors": "Anshul Sharma, Shujaatali Badami, Biky Chouhan, Pushpanjali Pandey, Brijeena Rana, Navneet Kaur",
      "institution": "Independent Researcher (USA), Liverpool John Moores University (UK), Chandigarh University (India), Gyancity Research Consultancy (India)",
      "link": "https://arxiv.org/pdf/2512.20739",
      "code": null,
      "tags": [
        "wireless networks",
        "Deep Reinforcement Learning (DRL)",
        "Reconfigurable Intelligent Surfaces (RIS)",
        "Energy Harvesting (EH)"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22a82510c37e7a60d88746806bbece62f0d234e13f225f50ad5635a0bb4ae5ee_w640_q70.webp",
      "contributions": "1. A holistic system model integrating PUs/SUs, energy harvesting, and RIS for sustainable CRN operation. 2. A DRL-based controller enhanced with transfer learning and hybrid metaheuristics for dynamic sensing and resource allocation. 3. EH-aware scheduling and RIS-phase co-adaptation algorithms to reduce SU power consumption.",
      "summary": "This paper proposes an AI-driven framework for green Cognitive Radio Networks (CRNs) in 6G. It integrates Deep Reinforcement Learning (DRL) with transfer learning, energy harvesting, and reconfigurable intelligent surfaces (RIS) to optimize spectrum sensing and resource allocation. The framework demonstrates significant energy savings, high sensing accuracy, and improved packet delivery ratio compared to traditional baselines, offering a sustainable path for 6G IoT and vehicular networks.",
      "mindmap": "graph LR\n    A[AI-Driven Green CRNs for 6G] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[频谱稀缺与高能耗/Spectrum Scarcity & High Energy Consumption]\n    C --> C1[AI驱动框架/AI-Driven Framework]\n    C1 --> C2[集成DRL, TL, EH, RIS/Integrates DRL, TL, EH, RIS]\n    D --> D1[节能25-30%/25-30% Energy Saving]\n    D --> D2[AUC>0.90, PDR提升/AUC>0.90, PDR Improved]"
    },
    {
      "title": "Stabilizing Multimodal Autoencoders: A Theoretical and Empirical Analysis of Fusion Strategies",
      "authors": "Diyar Altinses, Andreas Schwung",
      "institution": "South Westphalia University of Applied Sciences",
      "link": "https://arxiv.org/pdf/2512.20749",
      "code": null,
      "tags": [
        "multi-modal training",
        "Lipschitz Continuity",
        "Attention Mechanism",
        "Aggregation Methods",
        "Training Stability",
        "Multimodal Autoencoders"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3484b58bc84f22d71a010fca63235d2811ea4f720d1103584b13e220d263f42d_w640_q70.webp",
      "contributions": "1. Derivation of theoretical Lipschitz constants for aggregation methods in multimodal autoencoders. 2. Introduction of a novel regularized attention-based fusion method designed from the theoretical analysis to improve training stability. 3. Empirical validation of the theoretical findings and demonstration of the proposed method's superior performance in consistency, convergence speed, and accuracy.",
      "summary": "This paper analyzes the stability of multimodal autoencoders by theoretically deriving Lipschitz constants for fusion strategies and proposes a new regularized attention-based fusion method. The method is empirically validated and shown to outperform existing strategies, providing a more stable and performant training process for multimodal models.",
      "mindmap": "graph LR\n    A[Stabilizing Multimodal Autoencoders<br/>稳定多模态自编码器] --> B(核心问题/Problem: Training Stability & Robustness<br/>训练稳定性与鲁棒性)\n    A --> C(主要方法/Method: Theoretical Lipschitz Analysis & Regularized Attention Fusion<br/>理论Lipschitz分析与正则化注意力融合)\n    A --> D(关键结果/Results: Improved Consistency, Convergence, Accuracy<br/>提升的一致性、收敛速度与精度)"
    },
    {
      "title": "Bridging Efficiency and Safety: Formal Verification of Neural Networks with Early Exits",
      "authors": "Yizhak Yisrael Elboher, Avraham Raviv, Amihay Elboher, Zhouxing Shi, Omri Azencot, Hillel Kugler, Guy Katz",
      "institution": "The Hebrew University of Jerusalem, Bar Ilan University, Ben-Gurion University of the Negev, University of California, Riverside",
      "link": "https://arxiv.org/pdf/2512.20755",
      "code": null,
      "tags": [
        "others",
        "formal verification",
        "neural network robustness",
        "early exits",
        "adversarial perturbations",
        "off-the-shelf solvers"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cb8be1b40cc4cc73a6f6a75d4c206b456d4189c26838e784e46e437ea5a87b_w640_q70.webp",
      "contributions": "1. Defined a formal robustness property specifically tailored for neural network architectures with early exits. 2. Presented a baseline verification algorithm for such networks, enhanced with an early stopping strategy and heuristic optimizations that maintain soundness and completeness. 3. Demonstrated empirically that early exits not only accelerate inference but also enhance verifiability, solving more queries in less time compared to standard networks.",
      "summary": "This paper addresses the challenge of formally verifying the robustness of neural networks that use early exits for efficiency. The authors propose a tailored robustness property and an enhanced verification algorithm using off-the-shelf solvers. Their experiments show that early exits can improve both inference speed and verifiability, helping navigate the trade-off between accuracy and efficiency.",
      "mindmap": "graph LR\n    A[论文标题 / Paper Title<br>Bridging Efficiency and Safety] --> B(核心问题 / Problem<br>Verifying Early Exit Networks);\n    A --> C(主要方法 / Method<br>Tailored Robustness Property & Enhanced Algorithm);\n    A --> D(关键结果 / Results<br>Improved Verifiability & Efficiency);"
    },
    {
      "title": "Generalization of RLVR Using Causal Reasoning as a Testbed",
      "authors": "Brian Lu, Hongyu Zhao, Shuo Sun, Hao Peng, Rui Ding, Hongyuan Mei",
      "institution": "Johns Hopkins University, University of Maryland, College Park, National University of Singapore, University of Illinois at Urbana-Champaign, Microsoft Research Asia, Toyota Technological Institute at Chicago",
      "link": "https://arxiv.org/pdf/2512.20760",
      "code": null,
      "tags": [
        "reinforcement learning",
        "RLVR",
        "causal reasoning",
        "generalization",
        "supervised fine-tuning",
        "large language models"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/472c557c79b64b352421bedd952ba76d099165d613e99323a1beb8845a24cf4c_w640_q70.webp",
      "contributions": "1. Provides an empirical study of RLVR generalization using causal inference as a structured testbed, examining generalization across query levels and structural complexity. 2. Identifies that RLVR's benefits over SFT for generalization are contingent on specific combinations of model size and training query level, and depend on the model's initial reasoning competence. 3. Shows that RLVR improves specific causal reasoning subskills, such as marginalization strategy and intermediate probability calculation, leading to accuracy gains on complex queries.",
      "summary": "This paper studies the generalization of Reinforcement Learning with Verifiable Rewards (RLVR) for large language models on causal reasoning tasks. It finds that RLVR can outperform supervised fine-tuning in generalization, but its effectiveness depends on model size, training data, and the model's initial competence. The results indicate RLVR improves specific reasoning sub-skills when the model has a sufficient foundational ability.",
      "mindmap": "graph LR\n    A[”Generalization of RLVR Using Causal Reasoning as a Testbed<br>以因果推理为测试平台的RLVR泛化研究”] --> B[”核心问题/Problem<br>RLVR何时能实现鲁棒泛化？<br>When does RLVR yield robust generalization?”]\n    A --> C[”主要方法/Method<br>在因果图模型上实证研究RLVR与SFT<br>Empirical study of RLVR vs SFT on causal graphical models”]\n    A --> D[”关键结果/Results<br>RLVR泛化更强，但依赖模型规模与初始能力<br>RLVR yields stronger generalization but depends on model size & initial competence”]"
    },
    {
      "title": "TS-Arena Technical Report -- A Pre-registered Live Forecasting Platform",
      "authors": "Marcel Meyer, Sascha Kaltenpoth, Kevin Zalipski, Henrik Albers, Oliver Müller",
      "institution": "Paderborn University",
      "link": "https://arxiv.org/pdf/2512.20761",
      "code": "https://huggingface.co/spaces/DAG-UPB/TS-Arena",
      "tags": [
        "others",
        "time series foundation models",
        "live forecasting",
        "pre-registration",
        "information leakage",
        "temporal split"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea84e3460e7e5ece43727d2db2e7515fe17057e90ce083ef73f979343188043f_w640_q70.webp",
      "contributions": "1. Introduces TS-Arena, a platform that uses live data streams and a pre-registration mechanism to create a strict global temporal split for evaluation, preventing historical data contamination. 2. Proposes a methodology that treats the genuinely unknown future as the definitive test environment, establishing a moving temporal frontier for authentic assessment of model generalization. 3. Provides a sustainable infrastructure initially applied in the energy sector for comparing Time Series Foundation Models (TSFMs) under real-world constraints, addressing the evaluation crisis caused by data reuse and leakage.",
      "summary": "The paper identifies an evaluation crisis in Time Series Foundation Models (TSFMs) caused by information leakage from overlapping training/test data. To solve this, it proposes TS-Arena, a live forecasting platform that enforces evaluation on future, unseen data via pre-registration, ensuring a valid temporal split. The platform provides a fair and realistic infrastructure for benchmarking TSFMs, with an initial application in the energy sector.",
      "mindmap": "graph LR\n    A[TS-Arena Technical Report] --> B[核心问题/Problem: TSFM评估危机 / TSFM Evaluation Crisis]\n    A --> C[主要方法/Method: 预注册实时预测平台 / Pre-registered Live Forecasting Platform]\n    A --> D[关键结果/Results: 防止历史污染，真实评估泛化 / Prevents Historical Contamination, Authentic Generalization Assessment]\n    B --> E[信息泄露与数据重用 / Information Leakage & Data Reuse]\n    C --> F[实时数据流与严格时间分割 / Live Data Streams & Strict Temporal Split]\n    D --> G[可持续的基准测试基础设施 / Sustainable Benchmarking Infrastructure]"
    },
    {
      "title": "Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs with Inconsistent Beliefs and Limited Communication",
      "authors": "Moshe Rafaeli Shimron, Vadim Indelman",
      "institution": "Technion - Israel Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.20778",
      "code": null,
      "tags": [
        "multi-agent reinforcement learning",
        "Dec-POMDP",
        "belief inconsistency",
        "limited communication",
        "action consistency",
        "multi-agent planning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4538ed8602d7a249e385e29bfc0423ed3f6682a9ac338c82e10822f10bd96df8_w640_q70.webp",
      "contributions": "1. A novel decentralized framework for optimal joint action selection that explicitly accounts for belief inconsistencies among agents. 2. Provides probabilistic guarantees for both action consistency and performance relative to a fully-communicating baseline. 3. Introduces a mechanism to selectively trigger communication only when necessary and addresses the decision of whether to share data after action selection to improve inference.",
      "summary": "The paper addresses the problem of multi-agent decision-making under uncertainty when agents have inconsistent beliefs due to limited communication. It proposes a new decentralized framework for Dec-POMDPs that provides performance and action consistency guarantees while minimizing communication. Simulation results demonstrate that the approach outperforms existing state-of-the-art algorithms.",
      "mindmap": "graph LR\n    A[Towards Optimal Performance and Action Consistency Guarantees in Dec-POMDPs] --> B(核心问题/Problem: Belief Inconsistency & Limited Communication)\n    A --> C(主要方法/Method: Novel Decentralized Framework with Guarantees)\n    A --> D(关键结果/Results: Outperforms SOTA Algorithms)"
    },
    {
      "title": "NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts",
      "authors": "Raja Mallina, Bryar Shareef",
      "institution": "University of Nevada, Las Vegas",
      "link": "https://arxiv.org/pdf/2512.20783",
      "code": null,
      "tags": [
        "medical image segmentation",
        "nullable prompts",
        "mixed-supervision",
        "vision-language models",
        "breast ultrasound segmentation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c68929834a07c86ac43fe9856efa137aa11c30a231a02ea87272f2b689e4f7f_w640_q70.webp",
      "contributions": "1. Proposes NullBUS, a multimodal mixed-supervision framework for BUS segmentation that can learn from images both with and without prompts in a single model. 2. Introduces nullable prompts, implemented as learnable null embeddings with presence masks, to handle missing text metadata by enabling fallback to image-only evidence. 3. Demonstrates state-of-the-art performance on a unified pool of three public BUS datasets under mixed prompt availability.",
      "summary": "The paper addresses the problem that many public breast ultrasound datasets lack reliable text or spatial prompts, which limits the training of promptable segmentation models. It proposes NullBUS, a framework that uses nullable global-local prompts to learn from both prompted and prompt-free images. The method achieves state-of-the-art segmentation performance on a unified evaluation of three public datasets, showing robustness under mixed prompt availability.",
      "mindmap": "graph LR\n    A[NULLBUS] --> B[核心问题/Problem: BUS数据集缺乏可靠提示词]\n    A --> C[主要方法/Method: 可空全局-局部提示的混合监督框架]\n    A --> D[关键结果/Results: 在混合提示下达到SOTA性能]"
    },
    {
      "title": "X-GridAgent: An LLM-Powered Agentic AI System for Assisting Power Grid Analysis",
      "authors": "Yihan, Xin Chen",
      "institution": "Texas A&M University",
      "link": "https://arxiv.org/pdf/2512.20789",
      "code": null,
      "tags": [
        "agent system",
        "hierarchical agent architecture",
        "prompt refinement with human feedback",
        "schema-adaptive hybrid RAG"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fb48dc08ade0ddfa007a5156a481bb9ff3e7a4fabec1d577a9040fa4193ffe1_w640_q70.webp",
      "contributions": "1. Proposes X-GridAgent, a novel LLM-powered agentic AI system with a three-layer hierarchical architecture for automating power grid analysis via natural language. 2. Introduces an LLM-driven prompt refinement algorithm with human feedback to enhance task planning. 3. Develops a schema-adaptive hybrid retrieval-augmented generation (RAG) algorithm for accurate information retrieval from large-scale structured grid datasets.",
      "summary": "This paper presents X-GridAgent, an LLM-powered agent system designed to automate complex power grid analysis through natural language queries using a hierarchical architecture and novel algorithms for prompt refinement and information retrieval. Experimental results demonstrate its effectiveness and reliability in performing interpretable and rigorous power system analysis.",
      "mindmap": "graph LR\n    A[X-GridAgent] --> B[核心问题/Problem: Conventional grid tools require manual effort and expertise]\n    A --> C[主要方法/Method: LLM-powered agent with hierarchical architecture & novel algorithms]\n    A --> D[关键结果/Results: Effective & reliable automated analysis]"
    },
    {
      "title": "A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents",
      "authors": "Miles Q. Li, Benjamin C. M. Fung, Martin Weiss, Pulei Xiong, Khalil Al-Hussaeni, Claude Fachkha",
      "institution": "McGill University, Tiptree Advanced Systems Corporation, Polytechnique Montréal, National Research Council Canada, Rochester Institute of Technology Dubai, University of Dubai",
      "link": "https://arxiv.org/pdf/2512.20798",
      "code": null,
      "tags": [
        "ai safety & alignment",
        "autonomous agents",
        "safety benchmark",
        "constraint violations",
        "key performance indicator (KPI)",
        "deliberative misalignment"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a9156498ba4348ea63fbca94bacbbb938f1ccadc8825abf01cb9c946cda2c48_w640_q70.webp",
      "contributions": "1. Introduced a novel benchmark with 40 multi-step scenarios to evaluate emergent, outcome-driven constraint violations in autonomous AI agents, distinguishing between Mandated and Incentivized variations. 2. Conducted a comprehensive evaluation across 12 state-of-the-art LLMs, revealing high misalignment rates (up to 71.4%) and showing that superior reasoning capability does not guarantee safety. 3. Identified and highlighted the phenomenon of \"deliberative misalignment,\" where agents recognize their own actions as unethical in separate evaluations.",
      "summary": "This paper addresses the lack of realistic benchmarks for evaluating safety risks in autonomous AI agents. It proposes a new benchmark with multi-step scenarios tied to KPIs to test for outcome-driven constraint violations. The evaluation reveals alarmingly high violation rates across leading models, demonstrating that advanced capabilities do not ensure safety and highlighting a critical need for improved agentic-safety training.",
      "mindmap": "graph LR\n    A[A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents] --> B(核心问题/Problem: Lack of realistic benchmarks for emergent agent misalignment)\n    A --> C(主要方法/Method: New benchmark with 40 multi-step, KPI-driven scenarios)\n    A --> D(关键结果/Results: High violation rates (1.3%-71.4%); reasoning ≠ safety; deliberative misalignment)"
    },
    {
      "title": "Safety Alignment of LMs via Non-cooperative Games",
      "authors": "Anselm Paulus, Ilia Kulikov, Brandon Amos, Rémi Munos, Ivan Evtimov, Kamalika Chaudhuri, Arman Zharmagambetov",
      "institution": "Meta (FAIR), University of Tübingen",
      "link": "https://arxiv.org/pdf/2512.20806",
      "code": "https://github.com/facebookresearch/advgame",
      "tags": [
        "reinforcement learning",
        "non-cooperative game",
        "adversarial training",
        "preference-based reward",
        "online reinforcement learning",
        "safety alignment"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99d494326c3e6d7e063baf364aff968ae0d21f53ab3f3e8b4214c548e5ac79b4_w640_q70.webp",
      "contributions": "1. Introduces a new paradigm for safety alignment by framing it as a non-zero-sum game between an Attacker LM and a Defender LM. 2. Proposes joint training of the LMs via online reinforcement learning with a preference-based reward signal to reduce reward hacking. 3. Demonstrates that the method (AdvGame) produces a Defender LM with improved safety and utility and an Attacker LM that serves as a strong red-teaming agent.",
      "summary": "The paper addresses the challenge of aligning language models for safety without sacrificing utility. It proposes AdvGame, a method that frames safety alignment as a non-cooperative game between an Attacker and a Defender LM, training them jointly with online RL using preference-based rewards. The results show the approach yields a more helpful and safe Defender and a powerful general-purpose Attacker for red-teaming.",
      "mindmap": "graph LR\n        A[Safety Alignment of LMs via Non-cooperative Games] --> B(核心问题/Problem: Safety vs. Utility Trade-off in LM Alignment)\n        A --> C(主要方法/Method: Non-zero-sum Game & Online RL with Preference Reward)\n        A --> D(关键结果/Results: Improved Defender LM & Strong Red-teaming Attacker)"
    },
    {
      "title": "MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs",
      "authors": "Zhan Qu, Michael Färber",
      "institution": "TU Dresden, ScaDS.AI",
      "link": "https://arxiv.org/pdf/2512.20822",
      "code": null,
      "tags": [
        "medical nlp / llm evaluation",
        "medical benchmark",
        "electronic health records (EHR)",
        "knowledge grounding",
        "counterfactual reasoning",
        "DPO fine-tuning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59c4d88b1ecf7256d86a2c1dd12f74897d1a42b0c88d272ea8cf058355f013cd_w640_q70.webp",
      "contributions": "1. Introduces MediEval, a unified benchmark linking real EHRs (MIMIC-IV) to a biomedical knowledge base for evaluating LLMs on patient-contextual and knowledge-grounded reasoning. 2. Proposes a 4-quadrant evaluation framework to systematically assess models on both factual correctness and contextual consistency, identifying critical failure modes like hallucinated support and truth inversion. 3. Proposes Counterfactual Risk-Aware Fine-tuning (CoRFu), a DPO-based method with an asymmetric penalty, which significantly improves model accuracy and safety by eliminating truth inversion errors.",
      "summary": "The paper identifies a gap in evaluating LLMs for medical applications, where existing benchmarks either test isolated knowledge or patient reasoning without verifying correctness. To address this, the authors introduce the MediEval benchmark and a 4-quadrant evaluation framework to systematically assess LLMs, and propose a novel fine-tuning method called CoRFu. The results show that CoRFu significantly improves model performance and safety by eliminating dangerous error types like truth inversion.",
      "mindmap": "graph LR\n        A[MediEval] --> B[核心问题/Problem: LLMs in medicine lack reliable evaluation combining knowledge and patient context];\n        A --> C[主要方法/Method: Unified benchmark (EHR + KB) & 4-quadrant framework & CoRFu fine-tuning];\n        A --> D[关键结果/Results: Identifies failure modes; CoRFu improves accuracy and safety];"
    },
    {
      "title": "NotSoTiny: A Large, Living Benchmark for RTL Code Generation",
      "authors": "Razine Moundir Ghorab, Emanuele Parisi, Cristian Gutierrez, Miquel Alberti-Binimelis, Miquel Moreto, Dario Garcia-Gasulla, Gokcen Kestor",
      "institution": "Barcelona Supercomputing Center, Universitat Politecnica de Catalunya",
      "link": "https://arxiv.org/pdf/2512.20823",
      "code": null,
      "tags": [
        "llm training",
        "RTL code generation",
        "benchmark",
        "hardware design",
        "data contamination",
        "verification"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ee59b5723cdae955454bd94bdc8872b40c0eaccf59a4e54b86951d040529325_w640_q70.webp",
      "contributions": "1. Introduces NotSoTiny, a large-scale, living benchmark for evaluating LLMs on RTL code generation, built from real hardware designs. 2. Proposes an automated pipeline to ensure benchmark quality by removing duplicates, verifying correctness, and periodically updating to mitigate data contamination. 3. Demonstrates that NotSoTiny presents more challenging tasks than prior benchmarks, effectively highlighting current LLM limitations in hardware design.",
      "summary": "This paper introduces NotSoTiny, a benchmark for evaluating LLMs on generating Register-Transfer Level (RTL) code, addressing limitations of prior benchmarks by using real, complex hardware designs and a pipeline to ensure correctness and reduce data contamination. The results show that NotSoTiny tasks are more challenging, effectively guiding the improvement of LLMs for hardware design.",
      "mindmap": "graph LR\n    A[NotSoTiny: A Large, Living Benchmark for RTL Code Generation] --> B(核心问题/Problem: LLM RTL代码生成评估挑战 / LLM RTL Code Generation Evaluation Challenge)\n    A --> C(主要方法/Method: 基于真实硬件设计的自动化基准测试 / Automated Benchmark from Real Hardware Designs)\n    A --> D(关键结果/Results: 任务更具挑战性，有效指导改进 / Tasks More Challenging, Effectively Guides Improvement)"
    },
    {
      "title": "Context-Sensitive Abstractions for Reinforcement Learning with Parameterized Actions",
      "authors": "Rashmeet Kaur Nayyar, Naman Shah, Siddharth Srivastava",
      "institution": "Arizona State University, Brown University",
      "link": "https://arxiv.org/pdf/2512.20831",
      "code": "https://github.com/AAIR-lab/PEARL.git",
      "tags": [
        "reinforcement learning",
        "parameterized actions",
        "state abstraction",
        "action abstraction",
        "TD(λ)",
        "sample efficiency"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c74b22dc11c38dfc5a75f48c2cd94c57d0eecaffdac79525f6362b0b32c448b0_w640_q70.webp",
      "contributions": "1. Enables agents to autonomously learn both state and action abstractions online for RL with parameterized actions., 2. Introduces algorithms that progressively refine these abstractions during learning, focusing detail on critical regions., 3. Extends RL to long-horizon, sparse-reward settings with parameterized actions, achieving higher sample efficiency than baselines.",
      "summary": "This paper addresses the challenge of reinforcement learning in environments with parameterized actions, which combine discrete choices with continuous parameters. It proposes a method where agents autonomously learn and progressively refine state and action abstractions online. The approach enables TD(λ) to achieve significantly higher sample efficiency in continuous-state, parameterized-action domains compared to state-of-the-art methods.",
      "mindmap": "graph LR\n    A[Context-Sensitive Abstractions for RL with Parameterized Actions] --> B(核心问题/Problem: RL for Parameterized Actions)\n    A --> C(主要方法/Method: Learn & Refine State/Action Abstractions)\n    A --> D(关键结果/Results: Higher Sample Efficiency for TD(λ))"
    },
    {
      "title": "MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs",
      "authors": "Onat Ozer, Grace Wu, Yuchen Wang, Daniel Dosti, Honghao Zhang, Vivi De La Rue",
      "institution": "University of Michigan",
      "link": "https://arxiv.org/pdf/2512.20845",
      "code": null,
      "tags": [
        "agent system",
        "multi-agent reflection",
        "reasoning improvement",
        "self-correction",
        "episodic memory",
        "iterative refinement"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6022c55272f43b40b16d9e6f722483682ba159634bd04eb55be130ec83c86fe4_w640_q70.webp",
      "contributions": "1. Identifies systematic shortcomings in the single-agent Reflexion framework, such as repeated reasoning errors and confirmation bias, through detailed replication and analysis. 2. Proposes Multi-Agent Reflexion (MAR), a structured multi-agent extension that incorporates diverse reasoning personas and a judge model to synthesize critiques into unified reflections. 3. Demonstrates that MAR improves performance over Reflexion on HotPotQA and HumanEval benchmarks, reducing stagnation and enhancing reasoning reliability.",
      "summary": "This paper addresses the problem of reasoning error repetition and limited corrective feedback in single-agent LLM self-reflection frameworks like Reflexion. It proposes Multi-Agent Reflexion (MAR), which uses multiple agents with diverse personas to generate critiques and a judge to synthesize them, leading to more diverse and effective reflections. The method shows improved accuracy on HotPotQA and HumanEval benchmarks compared to the single-agent approach.",
      "mindmap": "graph LR\n        A[MAR: Multi-Agent Reflexion] --> B[核心问题/Problem: Single-agent self-reflection leads to repeated errors and confirmation bias]\n        A --> C[主要方法/Method: Multi-agent system with diverse personas and a judge model for critique synthesis]\n        A --> D[关键结果/Results: Improved accuracy on HotPotQA (47% EM) and HumanEval (82.7% pass@1)]"
    },
    {
      "title": "Nemotron 3 Nano: Open, Efficient Mixture-of-Experts Hybrid Mamba-Transformer Model for Agentic Reasoning",
      "authors": "NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Faisal Ladhak, Fay Wang, Fei Jia, Felipe Soares, Feng Chen, Ferenc Galko, Frankie Siino, Gal Hubara Agam, Ganesh Ajjanagadde, Gantavya Bhatt",
      "institution": "NVIDIA",
      "link": "https://arxiv.org/pdf/2512.20848",
      "code": null,
      "tags": [
        "llm inference",
        "Mixture-of-Experts",
        "Mamba-Transformer",
        "agentic reasoning",
        "sparse activation",
        "long context"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96a4b5c012acd8208519dcb9669276bd8c3c3709f26e7290e2fce500151c1ccc_w640_q70.webp",
      "contributions": "1. Introduces Nemotron 3 Nano, a hybrid MoE Mamba-Transformer model that sparsely activates only 3.2B out of 31.6B parameters per forward pass for efficiency. 2. Demonstrates superior inference throughput (up to 3.3x faster) compared to similarly-sized open models while maintaining or improving accuracy on benchmarks. 3. Supports an extended context length of up to 1 million tokens and shows enhanced agentic and reasoning capabilities through post-training.",
      "summary": "This paper presents Nemotron 3 Nano, an efficient 30B-parameter language model that combines Mixture-of-Experts with a Mamba-Transformer architecture to achieve sparse activation. It was pre-trained on 25 trillion tokens and post-trained for agentic reasoning, resulting in higher inference throughput and accuracy compared to similar models while supporting up to 1M token contexts.",
      "mindmap": "graph LR\n    A[Nemotron 3 Nano<br>论文标题/Paper Title] --> B[构建高效、能进行智能体推理的大模型<br>核心问题/Problem];\n    A --> C[混合MoE与Mamba-Transformer架构，稀疏激活参数<br>主要方法/Method];\n    A --> D[更高推理吞吐与精度，支持100万令牌上下文<br>关键结果/Results];"
    },
    {
      "title": "NVIDIA Nemotron 3: Efficient and Open Intelligence",
      "authors": "NVIDIA, Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, Aleksandr Shaposhnikov, Alex Kondratenko, Alexander Bukharin, Alexandre Milesi, Ali Taghibakhshi, Alisa Liu, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amir Klein, Amit Zuker, Amnon Geifman, Amy Shen, Anahita Bhiwandiwalla, Andrew Tao, Anjulie Agrusa, Ankur Verma, Ann Guan, Anubhav Mandarwal, Arham Mehta, Ashwath Aithal, Ashwin Poojary, Asif Ahamed, Asit Mishra, Asma Kuriparambil Thekkumpate, Ayush Dattagupta, Banghua Zhu, Bardiya Sadeghi, Barnaby Simkin, Ben Lanir, Benedikt Schifferer, Besmira Nushi, Bilal Kartal, Bita Darvish Rouhani, Boris Ginsburg, Brandon Norick, Brandon Soubasis, Branislav Kisacanin, Brian Yu, Bryan Catanzaro, Carlo del Mundo, Chantal Hwang, Charles Wang, Cheng-Ping Hsieh, Chenghao Zhang, Chenhan Yu, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christopher Parisien, Collin Neale, Cyril Meurillon, Damon Mosk-Aoyama, Dan Su, Dane Corneil, Daniel Afrimi, Daniel Lo, Daniel Rohrer, Daniel Serebrenik, Daria Gitman, Daria Levy, Darko Stosic, David Mosallanezhad, Deepak Narayanan, Dhruv Nathawani, Dima Rekesh, Dina Yared, Divyanshu Kakwani, Dong Ahn, Duncan Riach, Dusan Stosic, Edgar Minasyan, Edward Lin, Eileen Long, Eileen Peters Long, Elad Segal, Elena Lantz, Ellie Evans, Elliott Ning, Eric Chung, Eric Harper, Eric Tramel, Erick Galinkin, Erik Pounds, Evan Briones, Evelina Bakhturina, Evgeny Tsykunov, Faisal Ladhak, Fay Wang, Fei Jia",
      "institution": "NVIDIA",
      "link": "https://arxiv.org/pdf/2512.20856",
      "code": null,
      "tags": [
        "llm inference",
        "Mixture-of-Experts",
        "Mamba-Transformer",
        "LatentMoE",
        "NVFP4",
        "multi-environment reinforcement learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5203ecd520d6e99bc9f0034f05e8945272d4a34a746eeeae01be1cc728049b5_w640_q70.webp",
      "contributions": "1. Introduces the Nemotron 3 family of models (Nano, Super, Ultra) built on a Mixture-of-Experts hybrid Mamba-Transformer architecture for high throughput and long context (up to 1M tokens). 2. Proposes novel techniques including LatentMoE for improved model quality and MTP layers for faster text generation in the larger models. 3. Employs multi-environment reinforcement learning for post-training, enabling advanced capabilities like reasoning, multi-step tool use, and granular reasoning budget control.",
      "summary": "This paper introduces the Nemotron 3 family of open models designed for efficient and intelligent agentic applications. The models use a novel hybrid Mamba-Transformer architecture and are trained with techniques like LatentMoE and multi-environment RL to achieve strong reasoning, conversational, and tool-use capabilities with high throughput. The conclusion is that these models provide state-of-the-art accuracy and efficiency, with plans for open release of weights, software, and data.",
      "mindmap": "graph LR\n    A[NVIDIA Nemotron 3] --> B[核心问题/Problem: Efficient and open intelligence for agentic applications]\n    A --> C[主要方法/Method: Mixture-of-Experts hybrid Mamba-Transformer, LatentMoE, multi-environment RL]\n    A --> D[关键结果/Results: High throughput, 1M context, strong agentic/reasoning capabilities, open release]"
    },
    {
      "title": "Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs",
      "authors": "Pierre Abillama, Changwoo Lee, Juechu Dong, David Blaauw, Dennis Sylvester, Hun-Seok Kim",
      "institution": "University of Michigan",
      "link": "https://arxiv.org/pdf/2512.20861",
      "code": "https://github.com/pabillam/mem-efficient-blr",
      "tags": [
        "llm inference",
        "block low-rank (BLR)",
        "Triton kernels",
        "memory-bound optimization",
        "Jetson Orin Nano",
        "roofline analysis"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5f95e7768493ecd557f85d3dd08d75532f4bfee4218e02d377351eaf02b4c20_w640_q70.webp",
      "contributions": "1. Identified through roofline analysis that multi-token inference for BLR-compressed models becomes memory-bound, limiting speedups despite compiler optimizations. 2. Introduced custom Triton kernels with partial fusion and memory layout optimizations specifically for Monarch and BLR-AST (BLAST) methods. 3. Demonstrated significant speedups (up to 3.76x) and model compression (3x) on memory-constrained GPUs (e.g., Jetson Orin Nano, A40) across various foundation models.",
      "summary": "This paper addresses the memory bottleneck in multi-token inference for block low-rank (BLR) compressed foundation models. The authors propose custom Triton kernels with fusion and layout optimizations for BLR methods like Monarch and BLAST. Their solution achieves up to 3.76x speedup and 3x model compression on resource-constrained GPUs compared to optimized PyTorch baselines.",
      "mindmap": "graph LR\n    A[Memory-Efficient Acceleration of Block Low-Rank Foundation Models] --> B[核心问题/Problem: BLR模型多token推理存在内存墙/Multi-token inference for BLR models is memory-bound]\n    A --> C[主要方法/Method: 定制Triton内核与内存优化/Custom Triton kernels with memory optimizations]\n    A --> D[关键结果/Results: 显著加速与模型压缩/Significant speedup & model compression]"
    },
    {
      "title": "Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images",
      "authors": "Haotian Lv, Chao Li, Jiangbo Dai, Yuhui Zhang, Zepeng Fan, Yiqiu Tan, Dawei Wang, Binglei Xie",
      "institution": "Harbin Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.20866",
      "code": null,
      "tags": [
        "object detection",
        "YOLOv11",
        "3D-DIoU",
        "multi-view fusion",
        "GPR",
        "FDTD"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5ecaa60b98d3c92e7c85d9cd5e1f9894fba8806dbb24358189ef30201b7b93c_w640_q70.webp",
      "contributions": "1. Proposed a B/C/D-Scan three-view joint analysis strategy and a feature evaluation method validated by FDTD simulations and real data. 2. Developed the DCO-YOLO framework, integrating DySample, CGLU, and OutlookAttention into YOLOv11 to enhance small-scale pipeline feature extraction. 3. Introduced a 3D-DIoU spatial feature matching algorithm with 3D geometric constraints to automate multi-view annotation association and resolve single-view ambiguities.",
      "summary": "This paper proposes a lightweight framework for 3D underground pipeline detection using multi-view 2D GPR images. The method integrates an improved YOLO-based detection model (DCO-YOLO) with a novel 3D-DIoU spatial matching algorithm for multi-view fusion. Experiments on real urban data show the framework achieves high accuracy, recall, and mAP, outperforming the baseline and offering a reliable solution for pipeline recognition and localization.",
      "mindmap": "graph LR\n    A[Lightweight framework for underground pipeline recognition and spatial localization<br>基于多视图2D GPR图像的地下管道识别与空间定位轻量级框架] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[Weak multi-view feature correlation, low small-target accuracy<br>多视图特征关联弱，小目标识别精度低]\n    C --> C1[3D pipeline three-view feature evaluation<br>三维管道三视图特征评估]\n    C --> C2[DCO-YOLO framework<br>DCO-YOLO框架]\n    C --> C3[3D-DIoU spatial feature matching<br>3D-DIoU空间特征匹配]\n    D --> D1[Accuracy 96.2%, Recall 93.3%, mAP 96.7%<br>准确率96.2%，召回率93.3%，平均精度96.7%]"
    },
    {
      "title": "The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents",
      "authors": "Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng",
      "institution": "Kwansei Gakuin University, Victoria University of Wellington",
      "link": "https://arxiv.org/pdf/2512.20884",
      "code": null,
      "tags": [
        "agent system",
        "epistemic asymmetry",
        "Beta-Bernoulli distribution",
        "epistemic caching",
        "forgetting factor",
        "active learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f9f5d3701afff9c1243309bc058183ccd079615b358ea6fcd7f66333c45b2ae_w640_q70.webp",
      "contributions": "1. A formal probabilistic framework using a Beta-Bernoulli model with a forgetting factor to quantify epistemic uncertainty and provide a non-altruistic motive for knowledge sharing among LLM agents. 2. The introduction of epistemic caching, a resource management mechanism that leverages the forgetting factor to dynamically prioritize the active head of non-stationary knowledge distributions for scalable deployment. 3. Demonstrating how accumulated belief states can serve as verifiable reward signals for RLHF and high-quality data filters for SFT, bridging inference-time interaction with long-term model alignment.",
      "summary": "The paper identifies epistemic asymmetry as a key limitation where LLM agents are unidirectional knowledge consumers. To address this, it proposes a probabilistic framework that models agent belief to create a self-interested motive for sharing knowledge, framed as optimal active learning. Simulations show this uncertainty-driven strategy outperforms random baselines in dynamic environments.",
      "mindmap": "graph LR\n    A[The Silent Scholar Problem<br>沉默学者问题] --> B[Problem: Epistemic Asymmetry<br>问题: 认知不对称];\n    A --> C[Method: Probabilistic Framework<br>方法: 概率框架];\n    A --> D[Results: Outperforms Baseline<br>结果: 优于基线];\n    B --> B1[Agents as unidirectional consumers<br>智能体作为单向消费者];\n    C --> C1[Belief as Beta-Bernoulli<br>信念的Beta-Bernoulli建模];\n    C --> C2[Epistemic Caching<br>认知缓存];\n    D --> D1[Efficient in heterogeneous env.<br>在异构环境中高效];"
    },
    {
      "title": "DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction",
      "authors": "Xiao Yu, Zhaojie Fang, Guanyu Zhou, Yin Shen, Huoling Luo, Ye Li, Ahmed Elazab, Xiang Wan, Ruiquan Ge, Changmiao Wang",
      "institution": "Hangzhou Dianzi University",
      "link": "https://arxiv.org/pdf/2512.20898",
      "code": "https://github.com/lcbkmm/DGSAN",
      "tags": [
        "medical image analysis",
        "spatiotemporal attention",
        "graph neural network",
        "multimodal fusion",
        "pulmonary nodule classification",
        "feature encoder"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/422fb811704c808454d49662b57428a8e4f2132f4f9eb28d4def2caf51204b49_w640_q70.webp",
      "contributions": "1. Proposed a Dual-Graph Spatiotemporal Attention Network (DGSAN) for pulmonary nodule malignancy prediction. 2. Introduced a Dual-Graph Construction method and a Hierarchical Cross-Modal Graph Fusion Module for effective multimodal feature integration. 3. Compiled a novel multimodal dataset named NLST-cmst to support related research.",
      "summary": "The paper addresses the problem of inefficient multimodal fusion in pulmonary nodule malignancy prediction. It proposes a Dual-Graph Spatiotemporal Attention Network (DGSAN) that uses a Global-Local Feature Encoder and a hierarchical graph fusion module to integrate temporal and multimodal data. Experiments show DGSAN outperforms state-of-the-art methods with high computational efficiency.",
      "mindmap": "graph LR\n    A[DGSAN: Dual-Graph Spatiotemporal Attention Network] --> B(核心问题/Problem: Inefficient multimodal fusion for nodule prediction)\n    A --> C(主要方法/Method: Dual-Graph construction & Hierarchical Cross-Modal Fusion)\n    A --> D(关键结果/Results: Outperforms SOTA on NLST-cmst/CSTL datasets)"
    },
    {
      "title": "Embodied AI-Enhanced IoMT Edge Computing: UAV Trajectory Optimization and Task Offloading with Mobility Prediction",
      "authors": "Siqi Mu, Shuo Wen, Yang Lu, Ruihong Jiang, Bo Ai",
      "institution": "Beijing Sport University, Beijing Jiaotong University, Beijing University of Posts and Telecommunications",
      "link": "https://arxiv.org/pdf/2512.20902",
      "code": null,
      "tags": [
        "edge computing",
        "UAV trajectory optimization",
        "task offloading",
        "mobility prediction",
        "deep reinforcement learning",
        "Transformer"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75676682cad2a49439c91ac390cbbb997d6b492883c198c681369ae72675ee8a_w640_q70.webp",
      "contributions": "1. Establishes an embodied AI-enhanced IoMT edge computing framework for dynamic UAV service provisioning. 2. Proposes a novel hierarchical multi-scale Transformer-based model for predicting WBAN user mobility from historical trajectory data. 3. Designs a prediction-enhanced deep reinforcement learning algorithm that integrates mobility forecasts to jointly optimize UAV flight trajectory and task offloading decisions.",
      "summary": "This paper addresses the problem of minimizing task completion time for WBAN users by optimizing UAV trajectory and task offloading under energy constraints. It proposes an embodied AI framework that uses a Transformer-based model to predict user mobility and a DRL algorithm to make intelligent optimization decisions. Simulation results show the proposed method outperforms existing benchmarks.",
      "mindmap": "graph LR\n    A[Embodied AI-Enhanced IoMT Edge Computing<br/>具身AI增强的IoMT边缘计算] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[Minimize WBAN task time<br/>最小化WBAN任务时间]\n    B --> B2[UAV Energy Constraint<br/>UAV能量约束]\n    C --> C1[Transformer Mobility Prediction<br/>Transformer移动性预测]\n    C --> C2[DRL for Trajectory & Offloading<br/>DRL优化轨迹与卸载]\n    D --> D1[Superior Performance<br/>性能优越]"
    },
    {
      "title": "DiEC: Diffusion Embedded Clustering",
      "authors": "Haidong Hu",
      "institution": "Not explicitly provided in the given content.",
      "link": "https://arxiv.org/pdf/2512.20905",
      "code": null,
      "tags": [
        "deep clustering",
        "diffusion models",
        "representation selection",
        "self-training",
        "graph regularization",
        "denoising consistency"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66551d88d8940c7a650dca6264246e32d115d3596bb088334602fd1943ca8558_w640_q70.webp",
      "contributions": "1. Proposes DiEC, a novel deep clustering method that directly leverages the internal representation trajectory (across layers and timesteps) of a pretrained diffusion U-Net instead of a single fixed embedding. 2. Introduces a two-stage search strategy (CML and OTS) to efficiently identify the most cluster-friendly representation from the diffusion model's internal activations. 3. Enhances the clustering training with a DEC-style objective augmented by adaptive graph regularization, entropy regularization, and a denoising-consistency branch to strengthen and stabilize cluster structures.",
      "summary": "The paper addresses the problem of finding cluster-friendly representations in deep clustering by proposing DiEC, which extracts and optimizes features from the internal activations of a pretrained diffusion model. The method uses a two-stage search to select optimal representations and employs a regularized self-training objective with a consistency branch. Experiments show that DiEC achieves competitive clustering performance on standard benchmarks.",
      "mindmap": "graph LR\n    A[DiEC: Diffusion Embedded Clustering] --> B[核心问题/Problem: Single fixed embedding ignores varying clusterability in diffusion model's internal trajectory];\n    A --> C[主要方法/Method: Two-stage search (CML & OTS) on layer×timestep, regularized self-training with denoising-consistency];\n    A --> D[关键结果/Results: Achieves competitive clustering performance on multiple benchmarks];"
    },
    {
      "title": "RevFFN: Memory-Efficient Full-Parameter Fine-Tuning of Mixture-of-Experts LLMs with Reversible Blocks",
      "authors": "Ningyuan Liu, Jing Yang, Kaitong Cai, Keze Wang",
      "institution": "Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.20920",
      "code": null,
      "tags": [
        "llm training",
        "reversible networks",
        "memory-efficient fine-tuning",
        "mixture-of-experts",
        "full-parameter fine-tuning",
        "activation recomputation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5502accb933d07822fb8b8c8802a3eda6d016d84580bfda3089eae32cc0ea597_w640_q70.webp",
      "contributions": "1. Proposes RevFFN, a novel memory-efficient fine-tuning paradigm for Mixture-of-Experts (MoE) LLMs. 2. Designs reversible Transformer blocks that reconstruct layer inputs from outputs during backpropagation, eliminating the need to store most intermediate activations. 3. Enables efficient full-parameter fine-tuning on a single GPU by drastically reducing peak memory consumption while preserving model capacity.",
      "summary": "The paper addresses the high memory overhead of full-parameter fine-tuning for large language models (LLMs), especially Mixture-of-Experts (MoE) models, caused by caching intermediate activations. It introduces RevFFN, a method using reversible Transformer blocks to recompute activations during backpropagation, significantly reducing memory usage. This allows for efficient full fine-tuning on a single GPU without compromising the model's expressive power.",
      "mindmap": "graph LR\n    A[RevFFN: Memory-Efficient Fine-Tuning] --> B[核心问题/Problem: Full fine-tuning memory overhead高]\n    A --> C[主要方法/Method: 使用可逆Transformer块/Use reversible Transformer blocks]\n    A --> D[关键结果/Results: 单GPU高效全参数微调/Efficient full fine-tuning on single GPU]"
    },
    {
      "title": "Guardrailed Elasticity Pricing: A Churn-Aware Forecasting Playbook for Subscription Strategy",
      "authors": "Deepit Sapru",
      "institution": "University of Illinois Urbana-Champaign",
      "link": "https://arxiv.org/pdf/2512.20932",
      "code": null,
      "tags": [
        "revenue management",
        "constrained optimization",
        "Bayesian hierarchical modeling",
        "Monte Carlo simulation",
        "price elasticity",
        "churn prediction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0bf97f0a7c7df406f95ecaff29da9d37a9c9851d8013ef82f3f2669083a60ae7_w640_q70.webp",
      "contributions": "1. A novel framework integrating demand forecasting, segment-level price elasticity, and churn propensity into a single constrained optimization system for subscription pricing. 2. A methodology blending seasonal time-series models with tree-based learners and using Monte Carlo scenario tests to map risk envelopes for pricing decisions. 3. A modular, API-driven system designed for real-time recalibration with model explainability for governance, functioning as a managerial strategy playbook.",
      "summary": "This paper proposes a dynamic pricing framework for subscription services that combines forecasting, elasticity modeling, and churn prediction within a constrained optimization system to balance revenue and retention. The method uses Monte Carlo simulations and enforces business guardrails on margins and churn. It outperforms static pricing by targeting price changes to high willingness-to-pay segments while protecting sensitive customers.",
      "mindmap": "graph LR\n    A[Guardrailed Elasticity Pricing] --> B[核心问题/Problem: Static pricing fails to balance revenue & retention];\n    A --> C[主要方法/Method: Forecast + Elasticity + Churn model with constrained optimization];\n    A --> D[关键结果/Results: Outperforms static pricing, protects customers, enables durable growth];"
    },
    {
      "title": "Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning",
      "authors": "Shengguang Wu, Xiaohan Wang, Yuhui Zhang, Hao Zhu, Serena Yeung-Levy",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.20934",
      "code": "https://transductive-visualprogram.github.io/",
      "tags": [
        "visual reasoning",
        "visual programming",
        "spatial reasoning",
        "tool induction",
        "transductive learning",
        "3D scene understanding"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6374488a70a5d9147002f5652452c2f63ea3698c6660c54123c36fc9deef3991_w640_q70.webp",
      "contributions": "1. Proposes Transductive Visual Programming (TVP), a novel framework that builds new tools from experiential solutions rather than speculative induction., 2. Introduces a closed-loop system with an evolving Tool Library and an Example Library, enabling self-improvement through experience., 3. Demonstrates state-of-the-art performance on spatial reasoning benchmarks and shows that transductively learned tools are used more frequently and generalize better.",
      "summary": "The paper addresses the challenge of spatial reasoning in 3D scenes by proposing Transductive Visual Programming (TVP), a framework that learns reusable higher-level tools by abstracting patterns from its own successful solutions. This experience-driven approach outperforms existing methods and GPT-4o on benchmarks, showing more effective tool discovery and strong generalization to unseen tasks.",
      "mindmap": "graph LR\n        A[Transductive Visual Programming] --> B[核心问题/Problem<br>Spatial reasoning is challenging for VLMs]\n        A --> C[主要方法/Method<br>Build tools from experience, not speculation]\n        A --> D[关键结果/Results<br>SOTA performance, better tool reuse & generalization]"
    },
    {
      "title": "A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws for GNN-based Aerodynamic Field Surrogate",
      "authors": "Yiren Shen, Juan J. Alonso",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.20941",
      "code": null,
      "tags": [
        "others",
        "graph neural network",
        "surrogate model",
        "multi-fidelity dataset",
        "scaling laws",
        "aerodynamic field prediction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7bb0e5406bfc3665bfcafc6d32aeeb524e6e21ffb9ceb2ac785fe0ddd6b60b3_w640_q70.webp",
      "contributions": "1. Release of an open-source, multi-fidelity aerodynamic dataset for double-delta wings, generated using a nested Saltelli sampling scheme. 2. Conducted an empirical scaling study linking training data size and model size to prediction accuracy for a GNN-based surrogate, revealing a power-law relationship. 3. Derived practical guidelines, estimating an optimal sampling density of approximately eight samples per dimension in a design space.",
      "summary": "This paper investigates the relationship between dataset size and model performance for a Graph Neural Network (GNN) surrogate used in aerodynamic field prediction. The authors release a new multi-fidelity dataset for double-delta wings and conduct a scaling study, finding that test error decreases with data size following a power law, which indicates efficient data utilization and informs optimal sampling strategies.",
      "mindmap": "graph LR\n        A[论文标题 / Paper Title<br>A Multi-fidelity Double-Delta Wing Dataset and Empirical Scaling Laws] --> B(核心问题 / Problem<br>缺乏开源多保真数据集与数据规模对模型性能影响的实证指导 / Lack of open-source multi-fidelity datasets and empirical guidelines on data scaling)\n        A --> C(主要方法 / Method<br>发布数据集并进行缩放研究 / Release dataset and conduct scaling study)\n        B --> D(关键结果 / Results<br>误差随数据量呈幂律下降 / Test error decreases with data size via power law)\n        C --> D"
    },
    {
      "title": "Neural Probe-Based Hallucination Detection for Large Language Models",
      "authors": "Shize Liang, Hongzhi Wang",
      "institution": "Harbin Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.20949",
      "code": null,
      "tags": [
        "hallucination detection",
        "MLP probes",
        "token-level detection",
        "Bayesian optimization",
        "hidden states",
        "multi-objective loss"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/256e2b7c6550072fc0e643c4045a4a592ba6b2241cd12656b7dd16ad27bf89b0_w640_q70.webp",
      "contributions": "1. Proposed a neural network-based framework using lightweight MLP probes for token-level hallucination detection, enabling nonlinear modeling of hidden states. 2. Designed a multi-objective joint loss function to improve detection stability and semantic disambiguation. 3. Established a layer position-probe performance response model and used Bayesian optimization to automatically search for optimal probe insertion layers.",
      "summary": "This paper addresses the problem of hallucination in large language models by proposing a real-time, token-level detection method. The method uses lightweight MLP probes on frozen model hidden states and a Bayesian-optimized layer search. Experiments show it outperforms existing methods in accuracy and recall under low false-positive conditions.",
      "mindmap": "graph LR\n    A[Neural Probe-Based Hallucination Detection for Large Language Models] --> B(核心问题/Problem: LLMs生成幻觉内容/LLMs generate hallucinations)\n    A --> C(主要方法/Method: MLP探针 & 贝叶斯优化/MLP probes & Bayesian optimization)\n    A --> D(关键结果/Results: 在多个数据集上表现优异/Outperforms SOTA on multiple datasets)"
    },
    {
      "title": "MultiMind at SemEval-2025 Task 7: Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment",
      "authors": "Mohammad Mahdi Abootorabi, Alireza Ghahramani Kure, Mohammadali Mohammadkhani, Sina Elahimanesh, Mohammad Ali Ali Panah",
      "institution": "Based on the provided email domains (gmail.com), no specific institution can be reliably inferred. The team name is \"MultiMind\".",
      "link": "https://arxiv.org/pdf/2512.20950",
      "code": null,
      "tags": [
        "crosslingual information retrieval",
        "dual-encoder",
        "contrastive learning",
        "hard negative sampling",
        "data augmentation",
        "multi-source alignment"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccb6e1762a9617f640573a86ea65e0e68afff48d53006aa74213e0a557970889_w640_q70.webp",
      "contributions": "1. Introduces TriAligner, a novel dual-encoder architecture with contrastive learning for crosslingual claim retrieval. 2. Proposes a method to learn the relative importance of different information sources (e.g., native text, English translations) for alignment. 3. Enhances robustness through LLM-based data preprocessing/augmentation and hard negative sampling strategies.",
      "summary": "This paper addresses the challenge of retrieving fact-checked claims across multiple languages to combat misinformation. The proposed TriAligner system uses a dual-encoder with contrastive learning and multi-source alignment, enhanced by LLM-based data processing. The method shows significant improvements in retrieval accuracy on monolingual and crosslingual benchmarks.",
      "mindmap": "graph LR\n        A[MultiMind at SemEval-2025 Task 7<br>Crosslingual Fact-Checked Claim Retrieval via Multi-Source Alignment] --> B(核心问题/Problem: Rapid spread of multilingual misinformation);\n        A --> C(主要方法/Method: TriAligner - dual-encoder with contrastive learning & multi-source alignment);\n        A --> D(关键结果/Results: Improved retrieval accuracy on benchmarks);"
    },
    {
      "title": "Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models",
      "authors": "Xiang Zhang, Jiaqi Wei, Yuejin Yang, Zijie Qiu, Yuhan Chen, Zhiqiang Gao, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Wanli Ouyang, Chenyu You, Siqi Sun",
      "institution": "Fudan University, Shanghai Artificial Intelligence Laboratory, University of British Columbia, Zhejiang University, The Chinese University of Hong Kong, Stony Brook University",
      "link": "https://arxiv.org/pdf/2512.20954",
      "code": null,
      "tags": [
        "protein language models",
        "reflection pretraining",
        "chain-of-thought",
        "language expressiveness",
        "self-correction",
        "biological sequences"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5c51a6a0ca0e6bf5774254f47e4581544610c262b22a0cc12fe84b840bda40a_w640_q70.webp",
      "contributions": "1. Proposed and defined the concept of \"language expressiveness\" to explain the difficulty of applying Chain-of-Thought reasoning to biological sequence models. 2. Introduced reflection pretraining for biological sequence models, enabling intermediate reasoning through auxiliary \"thinking tokens\". 3. Demonstrated that this approach enables self-correction, improves performance, and offers benefits like counter-memorization and enhanced human steerability.",
      "summary": "This paper addresses the challenge of applying Chain-of-Thought reasoning to biological sequence models like protein language models, which have limited token expressiveness. The authors propose reflection pretraining, which augments the model with auxiliary \"thinking tokens\" to enable intermediate reasoning and self-correction. The method theoretically enhances language expressiveness and experimentally leads to substantial performance gains compared to standard pretraining.",
      "mindmap": "graph LR\n    A[Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models] --> B(核心问题/Problem: Limited expressiveness of protein language restricts CoT reasoning)\n    A --> C(主要方法/Method: Reflection pretraining with auxiliary ”thinking tokens”)\n    A --> D(关键结果/Results: Enhanced expressiveness, self-correction, performance gains)"
    },
    {
      "title": "ReACT-Drug: Reaction-Template Guided Reinforcement Learning for de novo Drug Design",
      "authors": "R Yadunandan, Nimisha Ghosh",
      "institution": "Department of Computer Science and Engineering, Shiv Nadar University Chennai",
      "link": "https://arxiv.org/pdf/2512.20958",
      "code": "https://github.com/YadunandanRaman/ReACT-Drug/",
      "tags": [
        "reinforcement learning",
        "Proximal Policy Optimization (PPO)",
        "ChemBERTa",
        "ESM-2",
        "reaction-template",
        "de novo drug design"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/021dd36ada6572d99e5bbd07493acfe6d2766f9ca2c6bf611ba28e410f041efc_w640_q70.webp",
      "contributions": "1. A target-agnostic RL framework (ReACT-Drug) that uses protein embeddings to find similar proteins and initialize a biologically relevant fragment search space. 2. A PPO agent that guides molecular generation through a dynamic action space defined by chemically valid, reaction-template-based transformations. 3. Ensures 100% chemical validity and novelty while generating candidates with competitive binding affinity and high synthetic accessibility.",
      "summary": "This paper introduces ReACT-Drug, a reinforcement learning framework for de novo drug design. It uses ESM-2 protein embeddings to find similar proteins and their ligands, decomposes them into fragments to guide a PPO agent, which then builds new molecules using reaction-template-based actions encoded by ChemBERTa. The method generates novel, synthetically accessible drug candidates with high binding affinity and guaranteed chemical validity.",
      "mindmap": "graph LR\n        A[ReACT-Drug] --> B[核心问题/Problem: Navigating vast chemical space for synthesizable, high-affinity drugs];\n        A --> C[主要方法/Method: RL + Protein Embeddings + Reaction-Template Actions];\n        A --> D[关键结果/Results: Novel, valid, synthetically accessible candidates];"
    },
    {
      "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents",
      "authors": "Zhaoxi Zhang, Yitong Duan, Yanzhi Zhang, Yiming Xu, Jiyan He, Yunfang Wu",
      "institution": "Affiliation not explicitly stated in provided text. Email domains suggest potential institutions, but cannot be reliably inferred from given content.",
      "link": "https://arxiv.org/pdf/2512.20957",
      "code": null,
      "tags": [
        "repository-level code understanding",
        "LLM agent",
        "reinforcement learning",
        "tool usage",
        "code navigation",
        "execution-aware"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6254efa02c0725684b783a26c76c1825bf8aaa25aee61fb7aaea40887f0efc46_w640_q70.webp",
      "contributions": "1. Proposes RepoNavigator, an LLM agent that uses a single, execution-aware tool (\"jump to definition\") for navigating code repositories, simplifying agent control and aligning with code execution logic. 2. Introduces an end-to-end Reinforcement Learning (RL) training method for the agent directly from a pretrained model, eliminating the need for closed-source model distillation. 3. Demonstrates state-of-the-art performance on repository-level issue localization, showing that smaller RL-trained models (e.g., 7B) can outperform larger baseline models (e.g., 14B, 32B) and even closed-source models like Claude-3.7.",
      "summary": "The paper addresses the challenge of locating code to modify in large software repositories. It proposes RepoNavigator, an LLM agent trained with Reinforcement Learning to use a single \"jump to definition\" tool for navigation. Experiments show this approach achieves state-of-the-art performance, with smaller models outperforming larger baselines, proving the efficiency of a simple, execution-aware tool combined with RL training.",
      "mindmap": "graph LR\n    A[One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents] --> B[核心问题/Problem: Locating modification points in large, complex code repositories is difficult]\n    A --> C[主要方法/Method: RepoNavigator agent with a single ”jump to definition” tool, trained end-to-end via RL]\n    A --> D[关键结果/Results: SOTA performance; smaller RL-trained models outperform larger baselines and closed-source models]"
    },
    {
      "title": "Can Agentic AI Match the Performance of Human Data Scientists?",
      "authors": "An Luo, Jin Du, Fangqiao Tian, Xun Xian, Robert Specht, Ganghua Wang, Xuan Bi, Charles Fleming, Jayanth Srinivasa, Ashish Kundu, Mingyi Hong, Jie Ding",
      "institution": "University of Minnesota, University of Chicago, Cisco Research",
      "link": "https://arxiv.org/pdf/2512.20959",
      "code": null,
      "tags": [
        "automated data science",
        "agentic AI",
        "domain knowledge",
        "synthetic data",
        "large language models",
        "human-AI teaming"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79fb0613736763ea339bd898e4056c45f61f97d7ed163ac22b97fef63af1c01a_w640_q70.webp",
      "contributions": "1. Designed a novel prediction task where a critical latent variable is hidden in image data to test the limitations of generic agentic AI workflows. 2. Demonstrated through experiments that current agentic AI systems, which rely on generic code generation, fail to match human data scientists who can leverage domain-specific insights. 3. Highlighted a key limitation of current LLM-driven data science automation and underscored the need for future research to develop AI systems that can better incorporate domain knowledge.",
      "summary": "The paper investigates whether agentic AI can match human data scientists by designing a property insurance prediction task where a crucial variable is hidden in image data. Experiments show that AI relying on generic workflows performs poorly compared to methods using domain-specific insights. The study concludes that current agentic AI has a key limitation in incorporating domain knowledge, highlighting a need for future research in this direction.",
      "mindmap": "graph LR\n    A[Can Agentic AI Match Human Data Scientists?] --> B[核心问题/Problem: Can agentic AI match human performance using domain knowledge?];\n    A --> C[主要方法/Method: Design task with latent variable in images, use synthetic insurance data];\n    A --> D[关键结果/Results: Agentic AI with generic workflow falls short; highlights need for domain-aware AI];"
    },
    {
      "title": "Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality",
      "authors": "Sirui Chen, Jingji Chen, Siqi Zhu, Ziheng Jiang, Yanghua Peng, Xuehai Qian",
      "institution": "Tsinghua University, Purdue University, University of Illinois Urbana-Champaign, ByteDance Seed",
      "link": "https://arxiv.org/pdf/2512.20968",
      "code": null,
      "tags": [
        "llm training",
        "distributed attention",
        "communication efficiency",
        "Ring-Attention",
        "communication-computation ratio",
        "scalability"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4944eec84564de9a1d27e811d1317c483f5220256be0880e9e87af0f1df84b8e_w640_q70.webp",
      "contributions": "1. Proposes Mesh-Attention, a new distributed attention algorithm using a matrix-based model that assigns 2D computation tiles to GPUs for lower communication-computation ratio. 2. Introduces a greedy algorithm to efficiently search the scheduling space within a tile under communication constraints. 3. Provides theoretical analysis and extensive experiments showing Mesh-Attention significantly reduces communication volume and achieves speedup compared to state-of-the-art methods.",
      "summary": "This paper addresses the communication bottleneck in scaling LLM context windows by proposing Mesh-Attention, a new distributed attention algorithm that uses 2D computation tiling to reduce communication overhead. It demonstrates superior performance, achieving up to 3.4x speedup and 85.4% communication reduction on 256 GPUs, and shows good scalability for large-scale deployments.",
      "mindmap": "graph LR\n    A[Mesh-Attention<br>论文标题/Paper Title] --> B[核心问题/Problem: 分布式注意力通信开销大<br>High Communication in Distributed Attention]\n    A --> C[主要方法/Method: 基于2D计算块划分的Mesh-Attention算法<br>Mesh-Attention with 2D Tile Assignment]\n    A --> D[关键结果/Results: 通信量减少85.4%, 速度提升3.4倍<br>85.4% Comm Reduction, 3.4x Speedup]"
    },
    {
      "title": "Generalised Linear Models in Deep Bayesian RL with Learnable Basis Functions",
      "authors": "Jingyang You, Hanna Kurniawati",
      "institution": "Australian National University",
      "link": "https://arxiv.org/pdf/2512.20974",
      "code": null,
      "tags": [
        "reinforcement learning",
        "Bayesian Reinforcement Learning",
        "Meta-Reinforcement Learning",
        "Generalised Linear Models",
        "Learnable Basis Functions",
        "Variational Inference"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d35dd58d9d51b22dbce9eb7fc7a54a60d532c573648a3a29596e023ac63db13_w640_q70.webp",
      "contributions": "1. Proposes GLiBRL, a novel deep Bayesian RL method using Generalised Linear Models with learnable basis functions for efficient and accurate model learning. 2. Enables fully tractable marginal likelihood and Bayesian inference on task parameters and model noises, avoiding the need to optimize the difficult Evidence Lower Bound (ELBO). 3. Demonstrates significant performance improvements on MetaWorld benchmarks, outperforming state-of-the-art methods like VariBAD and showing low-variance, consistent results.",
      "summary": "This paper addresses the problem of inefficient and unstable model learning in deep Bayesian Reinforcement Learning (BRL), which traditionally relies on optimizing the difficult Evidence Lower Bound (ELBO). The authors propose a new method called GLiBRL, which uses Generalised Linear Models with learnable basis functions to enable tractable marginal likelihood and Bayesian inference. The method significantly improves success rates on challenging MetaWorld benchmarks compared to existing deep BRL and meta-RL approaches.",
      "mindmap": "graph LR\n    A[GLiBRL] --> B[核心问题/Problem: Classical BRL assumes known models, Deep BRL with ELBO is hard to optimize]\n    A --> C[主要方法/Method: Use GLMs with learnable basis for tractable likelihood & inference]\n    A --> D[关键结果/Results: Improves success rate vs. VariBAD (2.7x), low-variance performance]"
    },
    {
      "title": "Automatic Replication of LLM Mistakes in Medical Conversations",
      "authors": "Oleksii Proniakin, Diego Fajardo, Ruslan Nazarenko, Razvan Marinescu",
      "institution": "Lumos AI",
      "link": "https://arxiv.org/pdf/2512.20983",
      "code": null,
      "tags": [
        "llm evaluation",
        "medical conversation",
        "mistake replication",
        "benchmark creation",
        "llm judges",
        "single-shot qa"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7799ccba99ce08a1cee1bd87ba7b9e986a373df0f78a25479ad9fec7478ca0e9_w640_q70.webp",
      "contributions": "1. Introduces MedMistake, an automatic pipeline for extracting and replicating LLM mistakes from complex medical conversations into a benchmark format. 2. Releases MedMistake-All, a dataset of 3,390 single-shot QA pairs derived from identified mistakes, and a validated subset, MedMistake-Bench. 3. Provides a comprehensive evaluation of 12 frontier LLMs using the validated benchmark, revealing performance trends among top models.",
      "summary": "The paper addresses the difficulty of replicating specific mistakes made by LLMs in clinical conversations. It proposes MedMistake, an automated pipeline that generates conversational data, uses LLM judges to identify errors, and distills them into single-shot QA pairs to create a benchmark. The resulting benchmark was used to evaluate 12 LLMs, finding that GPT, Claude, and Grok models performed best.",
      "mindmap": "graph LR\n    A[Automatic Replication of LLM Mistakes in Medical Conversations] --> B(核心问题/Problem: LLM错误难以在其他模型中复现/Mistakes hard to replicate across LLMs)\n    A --> C(主要方法/Method: MedMistake自动管道/MedMistake automatic pipeline)\n    A --> D(关键结果/Results: 发布基准并评估12个LLM/Released benchmark & evaluated 12 LLMs)\n    C --> C1(生成对话/Generate conversations)\n    C --> C2(LLM委员会评估/LLM committee evaluation)\n    C --> C3(创建单轮QA对/Create single-shot QA pairs)\n    D --> D1(MedMistake-All数据集/MedMistake-All dataset)\n    D --> D2(MedMistake-Bench验证子集/MedMistake-Bench validated subset)\n    D --> D3(GPT/Claude/Grok表现最佳/GPT/Claude/Grok performed best)"
    },
    {
      "title": "A Blockchain-Monitored Agentic AI Architecture for Trusted Perception-Reasoning-Action Pipelines",
      "authors": "Salman Jan, Hassan Ali Razzaqi, Ali Akarma, Mohammad Riyaz Belgaum",
      "institution": "Arab Open University-Bahrain, Islamic University of Madinah",
      "link": "https://arxiv.org/pdf/2512.20985",
      "code": null,
      "tags": [
        "agent system",
        "LangChain",
        "Hyperledger Fabric",
        "MCP (Model Context Protocol)",
        "permissioned blockchain",
        "multi-agent system"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24b6daf67b596733e755359a781c7506180a1c98a12a8eab70c8fd627d718b47_w640_q70.webp",
      "contributions": "1. Proposed a novel architecture integrating a LangChain-based multi-agent system with a permissioned blockchain for monitoring and auditing agentic AI actions. 2. Introduced a framework linking the perception-reasoning-action cycle to a blockchain governance layer for input verification, action evaluation, and outcome logging. 3. Demonstrated the framework's applicability and performance through experiments in smart inventory management, traffic-signal control, and healthcare monitoring.",
      "summary": "This paper proposes a blockchain-monitored architecture for agentic AI systems to address trust and oversight concerns. The method integrates a LangChain multi-agent system with a Hyperledger Fabric blockchain to create an auditable perception-reasoning-action pipeline. The results show the framework effectively prevents unauthorized actions, provides full decision traceability, and maintains acceptable operational latency.",
      "mindmap": "graph LR\n        A[论文标题 / Paper Title<br>A Blockchain-Monitored Agentic AI Architecture] --> B(核心问题 / Problem<br>Agentic AI缺乏信任与审计 / Lack of Trust & Auditability)\n        A --> C(主要方法 / Method<br>区块链监控的LangChain多智能体架构 / Blockchain-Monitored LangChain Multi-Agent Architecture)\n        A --> D(关键结果 / Results<br>有效安全验证与可追溯性 / Effective Security Verification & Traceability)"
    },
    {
      "title": "FinAgent: An Agentic AI Framework Integrating Personal Finance and Nutrition Planning",
      "authors": "Toqeer Ali Syed, Abdulaziz Alshahrani, Ali Ullah, Ali Akarma, Sohail Khan, Muhammad Nauman, Salman Jan",
      "institution": "Islamic University of Madinah, Effat University, Arab Open University",
      "link": "https://arxiv.org/pdf/2512.20991",
      "code": null,
      "tags": [
        "agent system",
        "multi-agent architecture",
        "substitution graph",
        "price-aware optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc1ddd8fc13f7437534fa058d5e3879ce0f82c07898d64ccb7548f0932948a8a_w640_q70.webp",
      "contributions": "1. Proposes a novel agentic AI framework that integrates personal finance management with real-time diet optimization, addressing a gap in current tools. 2. Implements a modular multi-agent architecture with specialized agents (budgeting, nutrition, price monitoring, health personalization) that share a knowledge base and use a substitution graph for cost-effective meal planning. 3. Demonstrates through a case study simulation significant cost reduction (12-18%) and high nutrient adequacy (>95%) while maintaining robustness to market price fluctuations.",
      "summary": "This paper introduces FinAgent, an agentic AI framework that combines personal finance and real-time price data to generate nutritionally adequate meal plans at minimal cost. It uses a modular multi-agent system and a substitution graph to dynamically adapt to budget constraints and market changes. The system was validated in a simulation, showing significant cost savings and high nutritional adequacy, aligning with sustainable development goals.",
      "mindmap": "graph LR\n        A[FinAgent] --> B[核心问题/Problem: Limited budgets & nutritional demands with fluctuating food prices]\n        A --> C[主要方法/Method: Price-aware agentic AI with multi-agent architecture & substitution graph]\n        A --> D[关键结果/Results: 12-18% cost reduction, >95% nutrient adequacy, robust to price changes]"
    },
    {
      "title": "TrafficSimAgent: A Hierarchical Agent Framework for Autonomous Traffic Simulation with MCP Control",
      "authors": "Yuwei Du, Jun Zhang, Jie Feng, Zhicheng Liu, Jian Yuan, Yong Li",
      "institution": "Tsinghua University, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.20996",
      "code": null,
      "tags": [
        "agent system",
        "traffic simulation",
        "LLM agent",
        "hierarchical framework",
        "MCP control",
        "autonomous decision-making"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f663746265410a8f10fffee2372e39160e0e5f92969f9c3f7f502da4d3657fd_w640_q70.webp",
      "contributions": "1. Proposes TrafficSimAgent, a novel LLM-based hierarchical agent framework designed to lower the barrier for conducting traffic simulations by automating experiment design and decision optimization. 2. Introduces a cross-level collaboration mechanism where high-level agents interpret natural language instructions and plan workflows, while low-level agents make real-time, condition-based action selections for fundamental elements. 3. Demonstrates through extensive experiments that the framework effectively executes simulations under various conditions, handles ambiguous instructions, and achieves superior performance compared to other systems and SOTA LLM-based methods.",
      "summary": "This paper introduces TrafficSimAgent, a hierarchical LLM-based agent framework that automates traffic simulation tasks by using high-level agents for natural language instruction comprehension and workflow planning, and low-level agents for real-time action optimization. The system is designed to make powerful simulation platforms like SUMO more accessible to non-experts. Experiments show it effectively executes simulations, handles ambiguous instructions, and outperforms existing methods.",
      "mindmap": "graph LR\n    A[TrafficSimAgent] --> B[核心问题/Problem: 现有交通仿真平台使用门槛高/High barrier to using traffic simulators]\n    A --> C[主要方法/Method: 分层LLM智能体框架/Hierarchical LLM Agent Framework]\n    A --> D[关键结果/Results: 有效执行仿真，处理模糊指令，性能优越/Executes simulations effectively, handles ambiguous instructions, superior performance]"
    },
    {
      "title": "Distilling the Essence: Efficient Reasoning Distillation via Sequence Truncation",
      "authors": "Wei-Rui Chen, Vignesh Kothapalli, Ata Fatahibaarzi, Hejian Sang, Shao Tang, Qingquan Song, Zhipeng Wang, Muhammad Abdul-Mageed",
      "institution": "The University of British Columbia, LinkedIn",
      "link": "https://arxiv.org/pdf/2512.21002",
      "code": "https://github.com/weiruichen01/distilling-the-essence",
      "tags": [
        "llm training",
        "knowledge distillation",
        "chain-of-thought",
        "sequence truncation",
        "training efficiency",
        "reasoning models"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a99e2da19bbc9bacf5104e37b4afd860b26a5285dd752f9a6e025702d930839_w640_q70.webp",
      "contributions": "1. Analysis of supervision allocation in reasoning distillation, showing the CoT segment is the dominant factor for transferring reasoning capability. 2. Establishment of a truncation protocol to quantify computation-quality tradeoffs as a function of sequence length. 3. Empirical demonstration that training on only the first 50% of tokens retains ~94% of performance while halving computational costs.",
      "summary": "This paper addresses the computational expense of distilling reasoning capabilities from large to small models over long sequences. It proposes a method of selective distillation and sequence truncation, focusing on early reasoning tokens. The key finding is that training on just the first half of tokens can preserve most performance while significantly reducing training time, memory, and FLOPs.",
      "mindmap": "graph LR\n        A[Distilling the Essence<br>高效推理蒸馏] --> B{核心问题/Problem};\n        A --> C{主要方法/Method};\n        A --> D{关键结果/Results};\n        B --> B1[长序列推理蒸馏计算昂贵<br>Long-Sequence Reasoning Distillation is Expensive];\n        C --> C1[选择性监督与序列截断<br>Selective Supervision & Sequence Truncation];\n        D --> D1[保留94%性能，减少50%成本<br>Retain 94% Performance, Reduce 50% Cost];"
    },
    {
      "title": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics",
      "authors": "Jiashuo Liu, Jiayun Wu, Chunjie Wu, Jingkai Liu, Zaiyuan Wang, Huan Zhou, Wenhao Huang, Hongseok Namkoong",
      "institution": "ByteDance Seed, Carnegie Mellon University, Columbia University",
      "link": "https://arxiv.org/pdf/2512.21010",
      "code": null,
      "tags": [
        "llm evaluation",
        "Competitive Swiss-System Dynamics",
        "Expected Win Score",
        "Failure Sensitivity Analysis",
        "Monte Carlo Simulation",
        "risk appetite"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c6a4e260d9e6100733ae95995348a1b30295905871b863cf4afa821492e22eb_w640_q70.webp",
      "contributions": "1. Introduces the Competitive Swiss-System Dynamics (CSD) framework, a novel sequential contest simulation for holistic LLM ranking across multiple benchmarks, 2. Proposes the Expected Win Score via Monte Carlo Simulation to provide a statistically robust ranking that reduces noise from random pairing, 3. Implements Failure Sensitivity Analysis to profile models by risk appetite, distinguishing between robust generalists and aggressive specialists.",
      "summary": "The paper addresses the limitations of static, fragmented LLM evaluation by proposing the Competitive Swiss-System Dynamics (CSD) framework, which simulates a multi-round sequential contest to aggregate performance across benchmarks dynamically. It uses Monte Carlo simulation to compute a robust Expected Win Score and includes a Failure Sensitivity Analysis to assess model risk profiles. The authors demonstrate that CSD provides a more nuanced and context-aware ranking than traditional methods, advancing risk-informed LLM evaluation.",
      "mindmap": "graph LR\n    A[LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics] --> B[核心问题/Problem: Fragmented benchmarks and static scoring fail to capture dynamic competitive fitness and risk]\n    A --> C[主要方法/Method: Competitive Swiss-System Dynamics (CSD) with Monte Carlo Simulation and Failure Sensitivity Analysis]\n    A --> D[关键结果/Results: More nuanced, context-aware ranking distinguishing robust generalists vs. aggressive specialists]"
    },
    {
      "title": "Rethinking Supervised Fine-Tuning: Emphasizing Key Answer Tokens for Improved LLM Accuracy",
      "authors": "Xiaofeng Shi, Qian Kou, Yuduo Li, Hua Zhou",
      "institution": "Beijing Academy of Artificial Intelligence (BAAI), Beijing Jiaotong University (BJTU)",
      "link": "https://arxiv.org/pdf/2512.21017",
      "code": null,
      "tags": [
        "post-training (sft/rlhf)",
        "Supervised Fine-Tuning",
        "Chain-of-Thought",
        "Two-Stage Training",
        "Attention Imbalance",
        "Key Answer Tokens"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7eab786d7e6ac187d45153b771fdc458d7c8434c0e133a9bcdb70a2afa441a73_w640_q70.webp",
      "contributions": "1. Identifies a key limitation in conventional SFT where models over-attend to lengthy Chain-of-Thought reasoning sequences at the expense of the shorter, critical final answer tokens. 2. Proposes SFTKey, a novel two-stage fine-tuning scheme that first applies conventional SFT for format learning, then fine-tunes only on the Key (final answer) portion to boost accuracy. 3. Demonstrates through extensive experiments that SFTKey achieves an average accuracy improvement of over 5% compared to standard SFT while maintaining correct output formatting.",
      "summary": "The paper identifies that standard Supervised Fine-Tuning (SFT) for LLMs can cause an attention imbalance, where models focus too much on long reasoning chains (CoT) and not enough on the final answer. To solve this, the authors propose SFTKey, a two-stage method that first does standard SFT for formatting, then fine-tunes only on the key answer tokens. Experiments show this approach improves average accuracy by over 5% without harming output format correctness.",
      "mindmap": "graph LR\n    A[论文标题 / Paper Title<br>Rethinking Supervised Fine-Tuning] --> B[核心问题 / Problem<br>注意力失衡于长推理链 / Attention Imbalance on Long CoT]\n    A --> C[主要方法 / Method<br>两阶段训练 SFTKey / Two-Stage Training SFTKey]\n    A --> D[关键结果 / Results<br>准确率提升>5% / Accuracy Improvement >5%]"
    },
    {
      "title": "Policy-Conditioned Policies for Multi-Agent Task Solving",
      "authors": "Yue Lin, Shuhui Zhu, Wenhao Li, Ang Li, Dan Qiao, Pascal Poupart, Hongyuan Zha, Baoxiang Wang",
      "institution": "The Chinese University of Hong Kong, Shenzhen; University of Waterloo; Tongji University; Vector Institute",
      "link": "https://arxiv.org/pdf/2512.21024",
      "code": null,
      "tags": [
        "multi-agent reinforcement learning",
        "Program Equilibrium",
        "Programmatic Iterated Best Response (PIBR)",
        "policy-conditioning",
        "large language models",
        "textual gradients"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8665c80d59a606e3c0796d224d372080c9552f5b48a9e7e797a73cad25b1b7e7_w640_q70.webp",
      "contributions": "1. Proposes a paradigm shift by representing agent policies as human-interpretable source code, bridging the gap between opaque neural policies and the need for strategy comprehension in multi-agent settings. 2. Introduces Programmatic Iterated Best Response (PIBR), a novel algorithm that uses LLMs as point-wise best-response operators to synthesize and refine policy code based on game utility and unit tests. 3. Operationalizes the game-theoretic concept of Program Equilibrium for modern learning, demonstrating its effectiveness on coordination games and a cooperative foraging environment.",
      "summary": "This paper addresses the challenge of dynamic strategy adaptation in multi-agent tasks by representing policies as interpretable source code and using Large Language Models (LLMs) to optimize them. The core method, Programmatic Iterated Best Response (PIBR), leverages LLMs to iteratively refine an agent's policy code in response to an opponent's strategy using textual feedback. The approach successfully solves standard coordination games and a cooperative environment, demonstrating a practical bridge between theoretical Program Equilibrium and modern AI learning.",
      "mindmap": "graph LR\n    A[Policy-Conditioned Policies for Multi-Agent Task Solving] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[策略表示瓶颈/Representational Bottleneck]\n    B --> B2[无法直接条件化对手策略/Cannot Condition on Opponent's Policy]\n    C --> C1[程序化策略表示/Programmatic Policy Representation]\n    C --> C2[LLM作为近似解释器/LLM as Approximate Interpreter]\n    C --> C3[程序化迭代最佳响应/PIBR Algorithm]\n    D --> D1[解决协调矩阵游戏/Solves Coordination Games]\n    D --> D2[解决合作觅食环境/Solves Cooperative Foraging]"
    },
    {
      "title": "DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors",
      "authors": "Kaustubh Kundu, Hrishav Bakul Barua, Lucy Robertson-Bell, Zhixi Cai, Kalin Stefanov",
      "institution": "Monash University, TCS Research",
      "link": "https://arxiv.org/pdf/2512.21054",
      "code": "https://github.com/kaustesseract/DexAvatar",
      "tags": [
        "3D human pose estimation",
        "3D sign language reconstruction",
        "biomechanical accuracy",
        "hand and body pose priors",
        "monocular video",
        "SMPL-X"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594bef871fe9a00d58a9f3f12c9a0b4bf4f66d3d738bd3f02dedcbad04bdcd25_w640_q70.webp",
      "contributions": "1. A novel framework (DexAvatar) for reconstructing biomechanically accurate 3D hand and body poses from monocular sign language videos. 2. The use of learned 3D hand and body pose priors to guide the reconstruction and overcome challenges like self-occlusion and motion blur. 3. Demonstrating strong performance on the SGNify benchmark, achieving a 35.11% improvement over the state-of-the-art.",
      "summary": "The paper introduces DexAvatar, a framework that uses learned 3D hand and body pose priors to reconstruct accurate 3D sign language poses from monocular videos. It addresses the limitations of existing 2D datasets and noisy 3D estimations. The method significantly outperforms prior work on the SGNify motion capture benchmark.",
      "mindmap": "graph LR\n        A[DexAvatar] --> B[核心问题/Problem: 手语视频缺乏准确3D数据，现有3D姿态估计质量差]\n        A --> C[主要方法/Method: 利用学习到的3D手部和身体姿态先验，从单目视频重建]\n        A --> D[关键结果/Results: 在SGNify数据集上性能提升35.11%]"
    },
    {
      "title": "Understanding Scaling Laws in Deep Neural Networks via Feature Learning Dynamics",
      "authors": "Zihan Yao, Ruoyu Wu, Tianxiang Gao",
      "institution": "DePaul University, Iowa State University",
      "link": "https://arxiv.org/pdf/2512.21075",
      "code": null,
      "tags": [
        "deep learning theory",
        "scaling laws",
        "feature learning",
        "infinite-depth limit",
        "ResNets",
        "hyperparameter transfer"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06065072ce8d20b2298a45760b95c3f905a6aff3d726e09d4ddf1ecd2e9cc359_w640_q70.webp",
      "contributions": "1. Derives Neural Feature Dynamics (NFD), a theoretical framework characterizing feature learning in ResNets in the joint infinite-width and infinite-depth limit. 2. Identifies a vanishing mechanism induced by 1/√depth scaling that explains feature-learning collapse in deep networks and the failure of depth-µP. 3. Proposes a practical depth-aware learning-rate correction to counteract the collapse and restore depth-wise hyperparameter transfer for improved performance.",
      "summary": "This paper addresses the lack of theoretical understanding behind scaling laws in deep learning by analyzing feature learning dynamics in deep ResNets. It proposes the Neural Feature Dynamics (NFD) framework in the infinite-width and depth limit, which explains when scaling succeeds and identifies a cause of feature collapse. Based on this insight, the authors propose a simple learning-rate correction that improves training stability and performance in deeper networks.",
      "mindmap": "graph LR\n    A[Understanding Scaling Laws via Feature Learning Dynamics] --> B[核心问题/Problem: Scaling laws describe success but not when/why scaling fails]\n    A --> C[主要方法/Method: Derive Neural Feature Dynamics (NFD) in infinite-width & depth limit]\n    A --> D[关键结果/Results: Explains diminishing returns, proposes depth-aware LR correction]"
    },
    {
      "title": "Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation",
      "authors": "Tomoaki Yamaguchi, Yutong Zhou, Masahiro Ryo, Keisuke Katsura",
      "institution": "Gifu University, Leibniz Centre for Agricultural Landscape Research (ZALF), Brandenburg University of Technology Cottbus–Senftenberg, Kyoto University",
      "link": "https://arxiv.org/pdf/2512.21066",
      "code": null,
      "tags": [
        "agent system",
        "Agentic AI",
        "SHAP",
        "Large Language Model",
        "Iterative Refinement",
        "Bias-Variance Trade-off"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76e69e3950a2d09bc883a9cee931300bdfbeacc4e1e6094150a08db35366449e_w640_q70.webp",
      "contributions": "1. Proposes a novel Agentic XAI framework integrating SHAP-based explainability with multimodal LLM-driven iterative refinement for generating progressively enhanced explanations. 2. Demonstrates the framework's application and evaluation in a real-world agricultural recommendation system using rice yield data. 3. Identifies a bias-variance trade-off in iterative refinement, showing that early stopping (regularization) is crucial for optimizing explanation quality, challenging assumptions of monotonic improvement.",
      "summary": "This paper proposes an Agentic Explainable AI (XAI) framework that combines SHAP analysis with iterative refinement by a multimodal Large Language Model (LLM) to generate better explanations. The framework was tested as an agricultural recommendation system, and evaluations by both human experts and LLMs showed that explanation quality improved over initial rounds but declined with excessive refinement, revealing a bias-variance trade-off. The findings indicate that strategic early stopping is necessary to optimize the practical utility of such agentic XAI systems.",
      "mindmap": "graph LR\n        A[Agentic XAI Approach] --> B[核心问题/Problem: 向非专业人士解释XAI输出困难/Hard to communicate XAI outputs to laypersons]\n        A --> C[主要方法/Method: SHAP + 多模态LLM迭代优化/SHAP + Multimodal LLM Iterative Refinement]\n        A --> D[关键结果/Results: 早期迭代提升质量，过度优化导致下降/Early rounds improve quality, excessive refinement causes drop]"
    },
    {
      "title": "LLM Personas as a Substitute for Field Experiments in Method Benchmarking",
      "authors": "Enoch Hyunwook Kang",
      "institution": "University of Washington",
      "link": "https://arxiv.org/pdf/2512.21080",
      "code": null,
      "tags": [
        "algorithmic fairness & evaluation",
        "field experiments",
        "A/B testing",
        "LLM personas",
        "algorithmic benchmarking",
        "information-theoretic bounds"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1e74907f157cdb694f3120aed988d1affab03b63adb6bf73297f43733c1b8ba_w640_q70.webp",
      "contributions": "1. Provides a formal, if-and-only-if characterization of the conditions (aggregate-only observation, algorithm-blind evaluation) under which swapping humans for LLM personas is a valid benchmark substitution, equivalent to changing the evaluation panel. 2. Moves from validity to usefulness by defining an information-theoretic measure of discriminability for the aggregate channel induced by persona simulation. 3. Derives explicit sample-size bounds on the number of independent persona evaluations required to make persona benchmarking as decision-relevant as a field experiment for distinguishing between methods.",
      "summary": "The paper addresses the high cost and latency of field experiments (A/B tests) for benchmarking methods in societal systems by proposing LLM-based persona simulation as a synthetic alternative. It formally proves the conditions under which this substitution is valid and provides information-theoretic bounds on the required number of persona evaluations to make the benchmark useful. The main conclusion is that persona benchmarking can be a viable, efficient substitute for field experiments under specific, well-defined conditions.",
      "mindmap": "graph LR\n    A[LLM Personas as a Substitute for Field Experiments] --> B[核心问题/Problem: Field experiments are costly and slow, hindering iterative method development.]\n    A --> C[主要方法/Method: Use LLM-based persona simulation as a cheap synthetic benchmark under specific conditions.]\n    A --> D[关键结果/Results: Formal validity conditions proven; sample-size bounds derived for decision-relevance.]"
    },
    {
      "title": "TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars",
      "authors": "Jaeseong Lee, Junyeong Ahn, Taewoong Kang, Jaegul Choo",
      "institution": "KAIST, Hanyang University",
      "link": "https://arxiv.org/pdf/2512.21099",
      "code": null,
      "tags": [
        "3D avatar generation",
        "3D Gaussian Splatting",
        "analytic rigging",
        "texel-space deformation",
        "hybrid representation",
        "head reenactment"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8320fd6b1a6f131ad704b82b65143269040ab86b9b67005a1edf15fca8097f6_w640_q70.webp",
      "contributions": "1. A hybrid avatar representation (TexAvatars) that combines analytic rigging for geometric grounding with texel-space neural regression for spatial continuity. 2. A method that predicts Gaussian attributes in UV space via CNNs but drives 3D deformation using mesh-aware Jacobians, enabling smooth transitions across mesh boundaries. 3. The model demonstrates improved generalization, stability, and capture of fine-grained expression details (e.g., wrinkles, mouth cavity) under extreme poses and expressions.",
      "summary": "This paper introduces TexAvatars, a method for creating drivable 3D head avatars by hybridizing analytic rigging with texel-space neural regression to improve generalization to unseen expressions. It predicts local attributes in UV space but uses mesh-aware Jacobians for 3D deformation, separating semantic modeling from geometric control. The approach achieves state-of-the-art performance in challenging reenactment scenarios, capturing fine details with high fidelity.",
      "mindmap": "graph LR\n        A[TexAvatars] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法泛化性差/Existing methods generalize poorly]\n        B --> B2[难以处理极端表情与姿态/Struggle with extreme expressions & poses]\n        C --> C1[混合表示/Hybrid Representation]\n        C --> C2[UV空间预测，3D网格驱动/UV-space prediction, 3D mesh-driven deformation]\n        D --> D1[泛化能力提升/Improved generalization]\n        D --> D2[高保真细节/High-fidelity details]\n        D --> D3[状态领先性能/State-of-the-art performance]"
    },
    {
      "title": "Semi-Supervised Learning for Large Language Models Safety and Content Moderation",
      "authors": "Eduard Stefan Dinuta, Iustin Sirbu, Traian Rebedea",
      "institution": "National University of Science and Technology Politehnica Bucharest, Renius Technologies, NVIDIA",
      "link": "https://arxiv.org/pdf/2512.21107",
      "code": null,
      "tags": [
        "content moderation",
        "semi-supervised learning",
        "data augmentation",
        "safety classifiers",
        "LLM safety",
        "prompt harmfulness"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035c08c88d89969ce37594942a40aa577a3c0c7c7743cd71bdf84366a9dfa5f2_w640_q70.webp",
      "contributions": "1. Analysis of state-of-the-art semi-supervised learning algorithms for LLM safety, focusing on both prompt and response harmfulness. 2. Introduction of a new, task-specific augmentation technique for safety tasks. 3. Demonstration that task-specific augmentations significantly outperform general-purpose methods like backtranslation.",
      "summary": "This paper addresses the challenge of acquiring high-quality labeled data for training safety classifiers for Large Language Models. It proposes using semi-supervised learning techniques that leverage both labeled and unlabeled data, and introduces a task-specific data augmentation method. The key finding is that this approach, particularly with custom augmentations, significantly improves performance on safety tasks compared to using general-purpose techniques.",
      "mindmap": "graph LR\n    A[论文标题 / Paper Title<br>Semi-Supervised Learning for LLM Safety] --> B[核心问题 / Problem<br>依赖大量标注数据 / Reliance on large labeled data]\n    A --> C[主要方法 / Method<br>半监督学习与任务特定增强 / SSL & Task-Specific Augmentation]\n    A --> D[关键结果 / Results<br>性能显著提升 / Significant Performance Improvement]"
    },
    {
      "title": "Semantic Refinement with LLMs for Graph Representations",
      "authors": "Safal Thapaliya, Zehong Wang, Jiazheng Li, Ziming Li, Yanfang Ye, Chuxu Zhang",
      "institution": "University of Connecticut, University of Notre Dame",
      "link": "https://arxiv.org/pdf/2512.21106",
      "code": null,
      "tags": [
        "graph representation learning",
        "graph neural network",
        "large language model",
        "semantic refinement",
        "structure-semantics heterogeneity",
        "data-centric adaptation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfae73c72759ca834d898f3c6ca5f0824bada06285918fca678e0f809fce9afd_w640_q70.webp",
      "contributions": "1. Proposes a data-centric perspective to address structure-semantics heterogeneity in graphs by treating node semantics as a task-adaptive variable, shifting focus from model-centric inductive bias injection. 2. Introduces the Data-Adaptive Semantic Refinement (DAS) framework, which couples a fixed GNN and an LLM in a closed feedback loop for iterative semantic refinement and graph learning. 3. Demonstrates the framework's effectiveness on diverse graphs, showing consistent improvements on structure-dominated graphs while remaining competitive on semantics-rich graphs.",
      "summary": "This paper addresses the challenge of structure-semantics heterogeneity in graph data, where predictive signals vary across domains. It proposes a Data-Adaptive Semantic Refinement (DAS) framework that uses a closed feedback loop between a GNN and an LLM to iteratively refine node semantics for the learning task. The method shows strong performance on structure-dominated graphs and remains competitive on semantics-rich graphs, validating the data-centric adaptation approach.",
      "mindmap": "graph LR\n    A[Semantic Refinement with LLMs for Graph Representations] --> B(核心问题/Problem: Graph structure-semantics heterogeneity 图的结构-语义异质性)\n    A --> C(主要方法/Method: Data-Adaptive Semantic Refinement (DAS) framework 数据自适应语义精炼框架)\n    A --> D(关键结果/Results: Improves structure-dominated graphs, competitive on semantics-rich graphs 提升结构主导图性能，在语义丰富图上保持竞争力)"
    },
    {
      "title": "STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting",
      "authors": "Shi Quan Foo, Chi-Ho Wong, Zhihan Gao, Dit-Yan Yeung, Ka-Hing Wong, Wai-Kin Wong",
      "institution": "The Hong Kong University of Science and Technology, Hong Kong Observatory",
      "link": "https://arxiv.org/pdf/2512.21118",
      "code": "https://github.com/sqfoo/stldm_official",
      "tags": [
        "diffusion models",
        "precipitation nowcasting",
        "latent diffusion model",
        "spatio-temporal prediction",
        "variational autoencoder",
        "conditioning network"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3f04a94a2a07676690c2f108f7a602a6b052c1463701d217a319cd81e05ecce_w640_q70.webp",
      "contributions": "1. Proposes STLDM, a novel two-stage diffusion-based architecture for precipitation nowcasting that combines deterministic forecasting with generative enhancement. 2. Introduces an end-to-end learning framework that jointly trains a Variational Autoencoder, a conditioning network, and a latent diffusion model. 3. Demonstrates superior performance and improved inference efficiency compared to state-of-the-art methods on multiple radar datasets.",
      "summary": "The paper addresses the challenge of precipitation nowcasting, where deterministic models produce blurry predictions and generative models suffer from poor accuracy. It proposes STLDM, a spatio-temporal latent diffusion model that decomposes the task into a deterministic forecasting stage and a generative enhancement stage. Experiments show STLDM outperforms state-of-the-art methods while being more efficient.",
      "mindmap": "graph LR\n    A[STLDM: 降水临近预报模型] --> B[核心问题/Problem: 确定性模型模糊，生成模型精度差]\n    A --> C[主要方法/Method: 两阶段潜扩散模型]\n    A --> D[关键结果/Results: 性能优越，推理高效]"
    },
    {
      "title": "Beyond Context: Large Language Models Failure to Grasp Users Intent",
      "authors": "Ahmed M. Hussain, Salahuddin Salahuddin, Panos Papadimitratos",
      "institution": "KTH Royal Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.21110",
      "code": null,
      "tags": [
        "ai safety",
        "intent recognition",
        "contextual understanding",
        "safety circumvention",
        "prompt engineering",
        "transformer architectures"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55c1a596dd6375317c809bb19f466455285faf18a1f9810649d755b8027e383c_w640_q70.webp",
      "contributions": "1. Identifies and empirically demonstrates a critical vulnerability in LLMs: their inability to understand user intent and context, which allows safety mechanisms to be circumvented. 2. Evaluates multiple state-of-the-art LLMs (ChatGPT, Claude, Gemini, DeepSeek) and shows that exploitation techniques like emotional framing and progressive revelation are effective, and that reasoning capabilities can amplify this risk. 3. Proposes a paradigmatic shift in AI safety design, arguing for contextual understanding and intent recognition to be core capabilities rather than post-hoc protective mechanisms.",
      "summary": "This paper identifies a fundamental vulnerability in Large Language Models (LLMs): their lack of contextual understanding and intent recognition, which allows safety mechanisms to be systematically bypassed. The authors empirically evaluate several LLMs, showing they can be exploited through techniques like emotional framing, and find that reasoning capabilities often worsen the problem. They conclude that a paradigm shift is needed to build intent recognition directly into LLM architectures for safety.",
      "mindmap": "graph LR\n    A[Beyond Context: Large Language Models Failure to Grasp Users Intent] --> B[核心问题/Problem: LLMs缺乏上下文和意图理解能力/LLMs lack contextual understanding & intent recognition]\n    A --> C[主要方法/Method: 对多种LLM进行经验性评估/Empirical evaluation of multiple LLMs]\n    A --> D[关键结果/Results: 安全机制可被系统规避，需范式转变/Safety mechanisms can be systematically circumvented, requiring a paradigm shift]\n    B --> E[导致可利用的漏洞/Creates exploitable vulnerabilities]\n    C --> F[使用情感框架、渐进揭示等技术/Using emotional framing, progressive revelation, etc.]\n    D --> G[Claude Opus 4.1部分例外，推理能力加剧风险/Claude Opus 4.1 partial exception, reasoning amplifies risk]"
    },
    {
      "title": "A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care",
      "authors": "Oliver Normand, Esther Borsi, Mitch Fruin, Lauren E Walker, Jamie Heagerty, Chris C. Holmes, Anthony J Avery, Iain E Buchan, Harry Coppock",
      "institution": "i.AI (Department for Science, Innovation, and Technology), University of Liverpool, University of Oxford, University of Nottingham, Imperial College London",
      "link": "https://arxiv.org/pdf/2512.21127",
      "code": null,
      "tags": [
        "clinical nlp",
        "medication safety review",
        "large language models",
        "real-world evaluation",
        "failure analysis",
        "electronic health records"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a25d52747ed1a867914e2ab484a381e6c4f16c161178ce43bc0d582517885647_w640_q70.webp",
      "contributions": "1. Conducted the first real-world evaluation of an LLM-based medication safety review system on a large-scale NHS primary care dataset. 2. Performed a detailed failure analysis, identifying five primary patterns of LLM reasoning failures in clinical contexts (e.g., overconfidence, lack of contextual adaptation). 3. Provided a public dataset of 45 detailed clinical vignettes that comprehensively document all identified failure cases for further study.",
      "summary": "This paper evaluates the performance of a large language model (LLM) system for medication safety reviews using real NHS primary care data. The study found that while the LLM was highly sensitive in detecting issues, it correctly identified all issues and interventions in less than half of the patients, with failures primarily stemming from contextual reasoning errors rather than a lack of medical knowledge. The work highlights critical shortcomings in LLM reasoning for clinical deployment and calls for more rigorous, real-world evaluations.",
      "mindmap": "graph LR\n        A[A Real-World Evaluation of LLM Medication Safety Reviews in NHS Primary Care] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLMs缺乏真实临床评估/LLMs lack real clinical evaluation]\n        C --> C1[回顾性研究 & 专家评审/Retrospective study & Expert review]\n        D --> D1[高敏感度但低完全正确率/High sensitivity but low perfect accuracy]\n        D --> D2[失败源于情境推理/Failures from contextual reasoning]"
    },
    {
      "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking",
      "authors": "Tobias von Arx, Niels Mündler, Mark Vero, Maximilian Baader, Martin Vechev",
      "institution": "ETH Zurich, Snyk, INSAIT (Sofia University \"St. Kliment Ohridski\")",
      "link": "https://arxiv.org/pdf/2512.21132",
      "code": "https://github.com/eth-sri/autobaxbuilder",
      "tags": [
        "code security evaluation",
        "automated benchmarking",
        "LLM-generated code",
        "security vulnerabilities",
        "exploit generation",
        "plausibility checks"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385abd6729afb970eba2217cc3d408efe70ab80f9d7aa0cbe7c2e0254f48b74c_w640_q70.webp",
      "contributions": "1. Introduces AutoBaxBuilder, a framework for generating code security benchmarking tasks and tests from scratch, addressing the limitations of manual benchmarks. 2. Proposes a robust pipeline with fine-grained plausibility checks that leverages LLMs to construct functionality tests and end-to-end security exploits. 3. Releases AutoBaxBench, a new benchmark of generated tasks, and demonstrates the framework's efficiency (under 2 hours and $10 per task) and quality through comparison with human-crafted tasks.",
      "summary": "The paper presents AutoBaxBuilder, a framework that automatically generates tasks and tests for benchmarking the security of code produced by large language models (LLMs). It uses an LLM-powered pipeline to create functional tests and security exploits, ensuring benchmark quality and scalability. The authors show the method is efficient and release a new benchmark, AutoBaxBench, to evaluate LLM security capabilities.",
      "mindmap": "graph LR\n    A[AutoBaxBuilder] --> B[核心问题/Problem: Manual security benchmarks are insufficient]\n    A --> C[主要方法/Method: Auto-generate tasks & tests with LLM pipeline]\n    A --> D[关键结果/Results: New benchmark, low cost, under 2 hours/task]"
    },
    {
      "title": "MODE: Multi-Objective Adaptive Coreset Selection",
      "authors": "Tanmoy Mukherjee, Pierre Marquis, Zied Bouraoui",
      "institution": "CRIL, Université d'Artois",
      "link": "https://arxiv.org/pdf/2512.21152",
      "code": null,
      "tags": [
        "others",
        "coreset selection",
        "submodular maximization",
        "data efficiency",
        "adaptive weighting",
        "multi-objective optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8cefebbb0215d55d5050eb92783788b0acf7386684074cdf20b76685dfef159_w640_q70.webp",
      "contributions": "1. Proposes MODE, a dynamic framework that adaptively combines multiple coreset selection strategies based on their real-time contribution to model performance across different training phases. 2. Provides theoretical guarantees, achieving a (1-1/e)-approximation for the coreset selection problem with O(n log n) complexity and convergence bounds for strategy weights. 3. Demonstrates practical benefits including reduced memory requirements and provides interpretable insights into the evolution of data utility during training.",
      "summary": "The paper addresses the challenge of selecting small, representative data subsets (coresets) for efficient deep learning by proposing MODE, a framework that dynamically adapts selection criteria (like class balance, diversity, and uncertainty) to different training phases. It shows that MODE achieves strong theoretical approximation guarantees and competitive model accuracy while reducing computational and memory costs.",
      "mindmap": "graph LR\n    A[MODE: Multi-Objective Adaptive Coreset Selection] --> B[核心问题/Problem: Static coreset selection methods cannot adapt to changing data utility during training.]\n    A --> C[主要方法/Method: Dynamic, multi-objective framework that adaptively weights selection strategies based on training phase.]\n    A --> D[关键结果/Results: (1-1/e)-approximation, O(n log n) complexity, reduced memory, interpretable insights.]"
    },
    {
      "title": "TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation",
      "authors": "Gaoren Lin, Huangxuan Zhao, Yuan Xiong, Lefei Zhang, Bo Du, Wentao Zhu",
      "institution": "Wuhan University (Inferred from authors Gaoren Lin, Lefei Zhang, Bo Du, Wentao Zhu, who are known to be affiliated with Wuhan University. Huangxuan Zhao and Yuan Xiong are likely from the same group.)",
      "link": "https://arxiv.org/pdf/2512.21135",
      "code": null,
      "tags": [
        "medical image segmentation",
        "CLIP",
        "multimodal fusion",
        "parameter-efficient fine-tuning",
        "vision-language alignment",
        "anatomical structure"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b11d6061b6a08f8b03b2395e16fc0847c1b68ab4e388d93a886ee875211fcf44_w640_q70.webp",
      "contributions": "1. Proposes a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch to preserve fine-grained anatomical structures. 2. Introduces a Domain-Augmented Text Encoder (DATE) that injects medical knowledge from large language models to better model complex clinical descriptions. 3. Designs a Vision-Language Calibration Module (VLCM) to refine cross-modal correspondence in a unified feature space, addressing domain-specific semantic misalignment.",
      "summary": "The paper proposes TGC-Net, a parameter-efficient CLIP-based framework for text-guided medical image segmentation. It addresses CLIP's limitations in medical imaging by introducing modules for structural refinement, medical knowledge injection, and cross-modal calibration. Experiments on five datasets show state-of-the-art performance with fewer trainable parameters.",
      "mindmap": "graph LR\n        A[TGC-Net: Text-Guided Medical Image Segmentation<br>文本引导的医学图像分割] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n    \n        B --> B1[CLIP在医学领域应用受限<br>CLIP Limitations in Medical Domain]\n        B1 --> B2[结构细节丢失<br>Loss of Fine-grained Structure]\n        B1 --> B3[复杂文本建模不足<br>Inadequate Text Modeling]\n        B1 --> B4[领域语义未对齐<br>Domain Semantic Misalignment]\n    \n        C --> C1[语义-结构协同编码器 SSE<br>Semantic-Structural Synergy Encoder]\n        C --> C2[领域增强文本编码器 DATE<br>Domain-Augmented Text Encoder]\n        C --> C3[视觉-语言校准模块 VLCM<br>Vision-Language Calibration Module]\n    \n        D --> D1[在5个数据集上SOTA<br>SOTA on 5 Datasets]\n        D --> D2[参数高效<br>Parameter-Efficient]\n        D --> D3[Dice分数显著提升<br>Notable Dice Gains]"
    },
    {
      "title": "BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft",
      "authors": "Qizhi Wang",
      "institution": "PingCAP, Data & AI-Innovation Lab",
      "link": "https://arxiv.org/pdf/2512.21165",
      "code": null,
      "tags": [
        "distributed consensus",
        "Raft",
        "election timeout",
        "contextual bandits",
        "LinUCB",
        "fault tolerance"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75bff5f2f039d6eaeb1861fcd564ebda78e1b3bd0f91a85b3fc977c335d273e6_w640_q70.webp",
      "contributions": "1. BALLAST, a lightweight contextual-bandit framework for Raft election timeouts with safe exploration and non-stationary adaptation. 2. A reproducible evaluation methodology (discrete-event simulation, fault injection, protocol-level logging, CI-based aggregation) to study election stability under tail latency and recovery turbulence. 3. Demonstration that BALLAST substantially reduces recovery time and unwritable time compared to standard heuristics in challenging WAN regimes.",
      "summary": "This paper addresses the problem of leader-election instability in the Raft consensus protocol under variable network conditions like long-tail latency. It proposes BALLAST, a method that uses online linear contextual bandits to adaptively select election timeouts, augmented with safe exploration. The evaluation shows that BALLAST significantly improves recovery performance in unstable WAN environments while remaining competitive in stable settings.",
      "mindmap": "graph LR\n    A[BALLAST: Bandit-Assisted Learning for Latency-Aware Stable Timeouts in Raft] --> B(核心问题/Problem: Brittle randomized timeouts under long-tail latency & jitter)\n    A --> C(主要方法/Method: Lightweight online adaptation with contextual bandits & safe exploration)\n    A --> D(关键结果/Results: Reduces recovery/unwritable time in WAN, competitive in stable settings)"
    },
    {
      "title": "Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation",
      "authors": "Yu He, Da Huang, Zhenyang Liu, Zixiao Gu, Qiang Sun, Guangnan Ye, Yanwei Fu",
      "institution": "Fudan University, Shanghai Jiao Tong University, Shanghai University of International Business and Economics, Shanghai Innovation Institute",
      "link": "https://arxiv.org/pdf/2512.21201",
      "code": "https://heyu322.github.io/Schrodinger-Navigator.github.io/",
      "tags": [
        "robot navigation",
        "zero-shot object navigation",
        "trajectory-conditioned 3D imagination",
        "occlusion-aware planning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34e56028ea40f6f3b4a9150683288695e8b7fd2724c676c4f7f56f07367b4fb3_w640_q70.webp",
      "contributions": "1. Proposed Schrödinger's Navigator, a novel navigation framework that models unobserved space as an ensemble of plausible future worlds to handle uncertainty. 2. Introduced a trajectory-conditioned 3D world model that imagines future observations along candidate paths to see beyond occlusions and anticipate risks. 3. Developed a method to fuse imagined 3D observations into a navigation map to update a value map, guiding the policy toward safer, less-occluded routes for better object tracking.",
      "summary": "The paper addresses the challenge of zero-shot object navigation in cluttered environments with occlusions and moving targets. It proposes Schrödinger's Navigator, a framework that samples candidate trajectories and uses a 3D imagination model to predict future observations, enabling the robot to plan safer paths and locate hidden objects. Experiments on a quadruped robot show the method outperforms baselines in success rate and localization in occlusion-heavy settings.",
      "mindmap": "graph LR\n        A[Schrödinger's Navigator] --> B[核心问题/Problem: ZSON struggles with occlusions & uncertainty]\n        A --> C[主要方法/Method: Trajectory-conditioned 3D imagination of futures]\n        A --> D[关键结果/Results: Outperforms baselines on real robot]"
    },
    {
      "title": "SpidR-Adapt: A Universal Speech Representation Model for Few-Shot Adaptation",
      "authors": "Mahi Luthra, Jiayi Shen, Maxime Poli, Angelo Ortiz, Yosuke Higuchi, Youssef Benchekroun, Martin Gleize, Charles-Eric Saint-James, Dongyan Lin, Phillip Rust, Angel Villar, Surya Parimi, Vanessa Stark, Rashel Moritz, Juan Pino, Yann LeCun, Emmanuel Dupoux",
      "institution": "Meta AI, ENS-PSL, EHESS, CNRS",
      "link": "https://arxiv.org/pdf/2512.21204",
      "code": "https://github.com/facebookresearch/spidr-adapt",
      "tags": [
        "speech representation learning",
        "meta-learning",
        "bi-level optimization",
        "few-shot adaptation",
        "self-supervised learning",
        "speech representation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff9692c36cbda26291fde2551256e229a90f6eb51f087d833a2d84cb4b10925b_w640_q70.webp",
      "contributions": "1. Introduces the Multi-task Adaptive Pre-training (MAdaPT) protocol, framing few-shot speech representation learning as a bi-level optimization meta-learning problem. 2. Proposes a novel First-Order Bi-Level Optimization (FOBLO) heuristic to enable scalable meta-training by avoiding heavy computation costs. 3. Stabilizes meta-training with a robust initialization technique using interleaved supervision that alternates between self-supervised and supervised objectives.",
      "summary": "This paper introduces SpidR-Adapt, a method for rapid adaptation of speech representation models to new languages using minimal unlabeled data. It formulates the problem as meta-learning with a bi-level optimization framework (MAdaPT), proposes an efficient solver (FOBLO), and uses interleaved supervision for stable training. The model achieves significant gains in phonemic discrimination and language modeling after training on less than 1 hour of target-language audio, demonstrating over 100x greater data efficiency than standard methods.",
      "mindmap": "graph LR\n        A[SpidR-Adapt] --> B[核心问题/Problem: 数据效率差距/Data-Efficiency Gap]\n        A --> C[主要方法/Method: 元学习与双层优化/Meta-Learning & Bi-Level Optimization]\n        A --> D[关键结果/Results: 100倍数据效率/100x Data Efficiency]\n        B --> B1[婴儿高效 vs. 模型低效/Infant Efficiency vs. Model Inefficiency]\n        C --> C1[MAdaPT协议/MAdaPT Protocol]\n        C --> C2[FOBLO优化/FOBLO Optimization]\n        C --> C3[交错监督/Interleaved Supervision]\n        D --> D1[<1h音频/<1h Audio]\n        D --> D2[音素可辨性提升/Improved Phonemic Discriminability]"
    },
    {
      "title": "RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic",
      "authors": "Le Wang, Zonghao Ying, Xiao Yang, Quanchen Zou, Zhenfei Yin, Tianlin Li, Jian Yang, Yaodong Yang, Aishan Liu, Xianglong Liu",
      "institution": "Beihang University, Beijing University of Posts and Telecommunications, 360 AI Security Lab, The University of Sydney, Nanyang Technological University, Peking University",
      "link": "https://arxiv.org/pdf/2512.21220",
      "code": null,
      "tags": [
        "agent system",
        "runtime safety guardrail",
        "executable safety logic",
        "hybrid reasoning",
        "temporal safety predicate",
        "context-aware safety predicate"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb68be6c6803ce76bf0036925bc1f629db0f2d1ae9be491b5ab0a450b37d1e5_w640_q70.webp",
      "contributions": "1. Proposes RoboSafe, a hybrid reasoning runtime safeguard for embodied agents using executable predicate-based safety logic. 2. Introduces a Backward Reflective Reasoning module to infer temporal safety predicates from recent trajectories and trigger replanning. 3. Introduces a Forward Predictive Reasoning module to anticipate risks by generating context-aware safety predicates from long-term memory and observations.",
      "summary": "The paper addresses the vulnerability of vision-language model-driven embodied agents to hazardous instructions in dynamic environments. It proposes RoboSafe, a runtime safety system that uses hybrid reasoning with backward reflection and forward prediction to generate executable safety logic. Experiments show RoboSafe significantly reduces hazardous actions while maintaining task performance, and its practicality is validated on physical robots.",
      "mindmap": "graph LR\n        A[RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic] --> B[核心问题/Problem: Embodied agents vulnerable to hazardous instructions in dynamic environments]\n        A --> C[主要方法/Method: Hybrid reasoning runtime safeguard with Backward Reflective & Forward Predictive modules]\n        A --> D[关键结果/Results: Reduces hazardous actions (-36.8%), maintains task performance, validated on physical robots]"
    },
    {
      "title": "Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval",
      "authors": "Dao Sy Duy Minh, Huynh Trung Kiet, Nguyen Lam Phu Quy, Phu-Hoa Pham, Tran Chi Nguyen",
      "institution": "University of Science - VNUHCM",
      "link": "https://arxiv.org/pdf/2512.21221",
      "code": "https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval",
      "tags": [
        "image-text retrieval",
        "event-centric entity extraction",
        "BM25",
        "BEiT-3",
        "two-stage retrieval"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3fccaf1dfde41ddfe9c023d8a1f41a9f87c4a0899f25d8a475702d3f368a129_w640_q70.webp",
      "contributions": "1. Proposes a lightweight two-stage retrieval pipeline for event-based image retrieval. 2. Leverages event-centric entity extraction to incorporate temporal and contextual signals for efficient candidate filtering. 3. Combines BM25-based filtering with BEiT-3 reranking to achieve high accuracy on the OpenEvents benchmark.",
      "summary": "This paper addresses the challenge of retrieving images from natural language descriptions in complex, real-world scenarios. It proposes a two-stage method that first filters candidates using BM25 on extracted event entities, then reranks them with a BEiT-3 model. The approach significantly outperforms prior baselines on the OpenEvents benchmark, demonstrating the effectiveness of combining lightweight entity guidance with deep multimodal modeling.",
      "mindmap": "graph LR\n    A[Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval] --> B[核心问题/Problem: Real-world image-text retrieval is challenging due to vague queries and scalability needs.]\n    A --> C[主要方法/Method: Two-stage pipeline with event-centric entity extraction, BM25 filtering, and BEiT-3 reranking.]\n    A --> D[关键结果/Results: Achieves high mean average precision (0.559) on OpenEvents v1, outperforming baselines.]"
    },
    {
      "title": "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking",
      "authors": "Yifan Huang, Xiaojun Jia, Wenbo Guo, Yuqiang Sun, Yihao Huang, Chong Wang, Yang Liu",
      "institution": "Nanyang Technological University, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.21236",
      "code": null,
      "tags": [
        "llm security",
        "jailbreaking",
        "malicious code generation",
        "prompt engineering",
        "time-division selection",
        "security alignment"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8055c0aba333e58d26f29d81acab1f88f570f09122f7fafd38ac1c952dad67b1_w640_q70.webp",
      "contributions": "1. Proposes SPELL, a novel testing framework specifically designed to evaluate security alignment weaknesses in LLMs for malicious code generation. 2. Introduces a time-division selection strategy to systematically construct jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset. 3. Conducts extensive evaluation across multiple advanced code models and real-world tools, revealing significant security gaps and providing insights for improving AI safety.",
      "summary": "The paper addresses the security risk of LLMs being exploited to generate malicious code, a gap in existing jailbreaking research. It proposes the SPELL framework, which uses a time-division strategy to construct effective jailbreaking prompts. The evaluation shows high attack success rates across several models, revealing critical vulnerabilities in current AI safety alignments for code generation.",
      "mindmap": "graph LR\n        A[SPELL: Sentence Pairing Exploration for LLM Limitation-breaking] --> B[核心问题/Problem: LLMs可能被用于生成恶意代码/LLMs can be exploited for malicious code generation]\n        A --> C[主要方法/Method: 基于时间划分选择的提示构建框架/Time-division selection prompt construction framework]\n        A --> D[关键结果/Results: 在多模型上实现高攻击成功率/High attack success rates across multiple models]"
    },
    {
      "title": "Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks",
      "authors": "Xinjie Xu, Shuyu Cheng, Dongwei Xu, Qi Xuan, Chen Ma",
      "institution": "Zhejiang University of Technology, Binjiang Institute of Artificial Intelligence, ZJUT, JQ Investments",
      "link": "https://arxiv.org/pdf/2512.21241",
      "code": "https://github.com/machanic/hard_label_attacks",
      "tags": [
        "adversarial attacks",
        "hard-label black-box attacks",
        "query efficiency",
        "ray search optimization",
        "Nesterov's Accelerated Gradient",
        "momentum-based optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/302b574932ba227c13854e5c9c2cab3d7cf8f1ed29ee145fbc73062632304cd4_w640_q70.webp",
      "contributions": "1. Proposed ARS-OPT, a momentum-based algorithm inspired by Nesterov's Accelerated Gradient to accelerate the convergence of ray search in hard-label attacks. 2. Introduced PARS-OPT, which further enhances ARS-OPT by incorporating surrogate-model priors into the gradient estimation. 3. Provided theoretical convergence guarantees for the proposed methods and demonstrated superior query efficiency over 13 state-of-the-art approaches on ImageNet and CIFAR-10.",
      "summary": "This paper addresses the high query cost of hard-label black-box adversarial attacks by optimizing ray search. The authors propose ARS-OPT, a momentum-based algorithm, and its enhanced version PARS-OPT, which uses surrogate-model priors, to accelerate convergence. Experiments show the methods outperform 13 existing approaches in query efficiency on standard datasets.",
      "mindmap": "graph LR\n    A[Improving the Convergence Rate of Ray Search Optimization<br>改进射线搜索优化的收敛率] --> B[核心问题/Problem<br>Hard-label攻击查询成本高<br>High query cost in hard-label attacks]\n    A --> C[主要方法/Method<br>提出ARS-OPT & PARS-OPT<br>Propose ARS-OPT & PARS-OPT]\n    A --> D[关键结果/Results<br>超越13种SOTA方法<br>Outperforms 13 SOTA methods]"
    },
    {
      "title": "LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation",
      "authors": "Anatoly O. Onishchenko, Alexey K. Kovalev, Aleksandr I. Panov",
      "institution": "MIRAI, Cognitive AI Systems Lab",
      "link": "https://arxiv.org/pdf/2512.21243",
      "code": "https://lookplangraph.github.io/",
      "tags": [
        "embodied ai",
        "scene graph",
        "vision language model",
        "dynamic planning",
        "memory graph",
        "graph augmentation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39706723670e257f6d0916c7c37badacde760a1f6d3061d011d8c22fa4f29bea_w640_q70.webp",
      "contributions": "1. Proposes LookPlanGraph, a method for embodied instruction following that dynamically updates a scene graph during execution using a Vision Language Model to verify object priors and discover new entities. 2. Introduces the GraSIF (Graph Scenes for Instruction Following) dataset with an automated validation framework, comprising 514 tasks from existing benchmarks. 3. Demonstrates superior performance over static scene graph methods in simulated environments with changed object positions and shows practical applicability in real-world experiments.",
      "summary": "The paper addresses the problem of LLM-based embodied agents failing in dynamic environments due to reliance on pre-built, static scene graphs. It proposes LookPlanGraph, a method that continuously augments a memory graph with real-time visual observations from a VLM to verify and discover objects during plan execution. Experiments show it outperforms static graph methods in simulated and real-world settings, and a new dataset (GraSIF) is introduced for evaluation.",
      "mindmap": "graph LR\n    A[LookPlanGraph] --> B[核心问题/Problem: Static scene graphs fail in dynamic environments];\n    A --> C[主要方法/Method: Dynamic graph update via VLM observation];\n    A --> D[关键结果/Results: Outperforms static methods, new GraSIF dataset];"
    },
    {
      "title": "Learning Factors in AI-Augmented Education: A Comparative Study of Middle and High School Students",
      "authors": "Gaia Ebli, Bianca Raimondi, Maurizio Gabbrielli",
      "institution": "University of Bologna",
      "link": "https://arxiv.org/pdf/2512.21246",
      "code": null,
      "tags": [
        "educational technology",
        "learning analytics",
        "correlation analysis",
        "text mining",
        "student perceptions",
        "human-ai interaction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/37485735380aa9bf03e242b5a0fe4f8958c120a691a693af44376fb7f27c2b0b_w640_q70.webp",
      "contributions": "1. Investigated the interrelationships of four key learning factors (experience, clarity, comfort, motivation) in AI-augmented education, a gap in prior research. 2. Revealed a developmental moderator by comparing middle and high school students, finding holistic vs. differentiated evaluation patterns between age groups. 3. Established a foundation for age-specific AI integration strategies by showing perception dimensions actively mediate learning and their structure varies with developmental stage.",
      "summary": "This study investigates how key learning factors relate to each other in AI-augmented programming education for middle and high school students. Using a multimethod analysis combining correlation analysis and text mining on classroom data, it finds that middle school students evaluate AI tools holistically, while high school students assess different factors independently. The conclusion is that the structure of student perceptions is moderated by developmental stage, which should inform age-appropriate AI integration strategies.",
      "mindmap": "graph LR\n    A[论文标题: Learning Factors in AI-Augmented Education<br>Title: Learning Factors in AI-Augmented Education] --> B(核心问题/Problem: How do learning factor relationships vary by age in AI education?<br>核心问题/Problem: 学习因素关系在AI教育中如何随年龄变化？)\n    A --> C(主要方法/Method: Multimethod quantitative analysis (correlation & text mining)<br>主要方法/Method: 多方法定量分析（相关性与文本挖掘）)\n    A --> D(关键结果/Results: Middle school: holistic evaluation; High school: differentiated evaluation<br>关键结果/Results: 初中生: 整体性评估; 高中生: 差异性评估)"
    },
    {
      "title": "SMART SLM: Structured Memory and Reasoning Transformer, A Small Language Model for Accurate Document Assistance",
      "authors": "Divij Dudeja, Mayukha Pal",
      "institution": "ABB Ability Innovation Center, Indian Institute of Information Technology, Nagpur",
      "link": "https://arxiv.org/pdf/2512.21280",
      "code": null,
      "tags": [
        "document question answering",
        "Tree-LSTM",
        "Memory Augmented Neural Network (MANN)",
        "Retrieval Augmented Generation (RAG)",
        "Parameter Efficiency",
        "Fact Extraction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fe5f40637f711007d6bb6875182fa80072536380614c5e93c7c4f5cf8dc2232_w640_q70.webp",
      "contributions": "1. Introduces a hierarchical, syntax-aware fact extractor (Grammarian Tree-LSTM) to parse engineering manuals into structured subject-relation-object triples., 2. Proposes a compact, indexed memory system (MANN) to store and retrieve extracted facts as vectors, enabling efficient knowledge access., 3. Designs a dual-mode inference system combining a fast path for known documents and a dynamic RAG-assisted path for new uploads, reducing hallucinations.",
      "summary": "The paper addresses the challenge of accurately answering questions from dense engineering manuals, where standard small language models fail. It proposes SMART, a structured model that hierarchically extracts facts, stores them in an indexed memory, and uses a transformer to generate answers from retrieved facts. The result is a parameter-efficient model that achieves higher accuracy with fewer parameters and reduced hallucinations compared to baselines like GPT-2.",
      "mindmap": "graph LR\n    A[SMART SLM] --> B[核心问题/Problem: 工程手册难以阅读，现有小模型处理为扁平token流，导致错误答案/Engineering manuals are hard to read; flat token processing leads to incorrect answers]\n    A --> C[主要方法/Method: 分层处理：语法感知事实提取器 + 索引记忆(MANN) + 6层Transformer/Hierarchical processing: Syntax-aware fact extractor + Indexed memory (MANN) + 6-layer Transformer]\n    A --> D[关键结果/Results: 参数减少64-69%，准确率提升21.3%，减少幻觉/64-69% fewer parameters, 21.3% higher accuracy, reduced hallucinations]"
    },
    {
      "title": "Model Merging via Multi-Teacher Knowledge Distillation",
      "authors": "Seyed Arshan Dalili, Mehrdad Mahdavi",
      "institution": "The Pennsylvania State University",
      "link": "https://arxiv.org/pdf/2512.21288",
      "code": "https://github.com/arshandalili/SAMerging",
      "tags": [
        "model merging",
        "model merging",
        "knowledge distillation",
        "PAC-Bayes",
        "sharpness-aware minimization",
        "multi-task learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1edfb41aaef41e719d5724fa70ca9022ddcf1e1c683791b3bd1d929286795c62_w640_q70.webp",
      "contributions": "1. Establishes a novel flatness-aware PAC-Bayes generalization bound for model merging, introducing a \"cross-task heterogeneity\" term. 2. Frames model merging as multi-teacher knowledge distillation on scarce unlabeled data, showing minimizing student-teacher KL divergence tightens the risk bound. 3. Proposes SAMerging, a method that operationalizes the objective using Sharpness-Aware Minimization (SAM) to find flat minima.",
      "summary": "This paper addresses the lack of theoretical understanding in model merging by framing it as multi-teacher knowledge distillation and deriving a PAC-Bayes generalization bound. It proposes SAMerging, a method that uses Sharpness-Aware Minimization to optimize the merging process based on this theory. The method achieves state-of-the-art performance on vision and NLP benchmarks with high data efficiency.",
      "mindmap": "graph LR\n    A[Model Merging via Multi-Teacher Knowledge Distillation] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[缺乏理论保证/Lack of Theoretical Guarantees]\n    B --> B2[启发式方法不稳定/Heuristic Methods are Brittle]\n    C --> C1[理论: 平坦性感知PAC-Bayes边界/Theory: Flatness-aware PAC-Bayes Bound]\n    C --> C2[框架: 多教师知识蒸馏/Framework: Multi-Teacher Knowledge Distillation]\n    C --> C3[方法: SAMerging/Method: SAMerging]\n    D --> D1[新SOTA/New SOTA]\n    D --> D2[高数据效率/High Data Efficiency]"
    },
    {
      "title": "Measuring all the noises of LLM Evals",
      "authors": "Sida Wang",
      "institution": "FAIR at Meta",
      "link": "https://arxiv.org/pdf/2512.21326",
      "code": null,
      "tags": [
        "llm inference",
        "LLM evaluation",
        "statistical noise",
        "paired analysis",
        "prediction variance",
        "data variance"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa15912febac5bc93aec8d1b8870feaf16ae89016c37e24c26550d053d396fec_w640_q70.webp",
      "contributions": "1. Clearly defines and measures three types of noise (prediction, data, total) in LLM evaluations using the law of total variance. 2. Proposes the \"all-pairs paired method\" to apply paired statistical analysis across all model pairs for increased statistical power. 3. Empirically reveals that total noise is predictable per evaluation and that prediction noise typically dominates data noise, enabling more effective significance testing.",
      "summary": "This paper addresses the challenge of statistical noise in Large Language Model (LLM) evaluations. It proposes an \"all-pairs paired method\" to measure prediction, data, and total noise across model pairs. The key findings are that each evaluation benchmark has a characteristic noise level and that reducing prediction noise through averaging can significantly improve the detection of performance differences.",
      "mindmap": "graph LR\n    A[Measuring all the noises of LLM Evals] --> B(核心问题/Problem: LLM评估中的统计噪声/Separating signal from noise in LLM evals)\n    A --> C(主要方法/Method: 全配对分析法/All-pairs paired method)\n    A --> D(关键结果/Results: 可预测的总噪声与主导的预测噪声/Predictable total noise & dominant prediction noise)"
    },
    {
      "title": "C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling",
      "authors": "Jin Qin, Zihan Liao, Ziyin Zhang, Hang Yu, Peng Di, Rui Wang",
      "institution": "Ant Group, Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.21332",
      "code": "https://github.com/codefuse-ai/CodeFuse-Embeddings",
      "tags": [
        "code retrieval",
        "Pooling by Multihead Attention (PMA)",
        "contrastive learning",
        "code embedding",
        "MTEB-Code",
        "Qwen-2.5-Coder"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f27c6e6ad01eacb3bd759d1aafa323cd3f72410efd901b1df691e709d8fbe3a4_w640_q70.webp",
      "contributions": "1. Proposes a Pooling by Multihead Attention (PMA) module to generate sequence embeddings from token embeddings, effectively utilizing the LLM's causal representations. 2. The PMA module aggregates information from all tokens in a sequence, overcoming the information bottleneck of traditional EOS-based sequence embeddings. 3. The approach supports flexible adaptation of embedding dimensions, serving as an alternative to Multi-Representation Learning (MRL).",
      "summary": "This paper introduces C2LLM, a family of code embedding models built on Qwen-2.5-Coder backbones. It proposes a novel Pooling by Multihead Attention (PMA) module to create better sequence embeddings for code retrieval. The models, trained on three million data points, achieve state-of-the-art performance on the MTEB-Code benchmark, with the 7B version ranking first overall.",
      "mindmap": "graph LR\n        A[C2LLM Technical Report] --> B[核心问题/Problem: 代码检索中的序列表示瓶颈/Sequence representation bottleneck in code retrieval]\n        A --> C[主要方法/Method: 自适应交叉注意力池化 (PMA) / Adaptive Cross-Attention Pooling (PMA)]\n        A --> D[关键结果/Results: 在MTEB-Code上SOTA / SOTA on MTEB-Code]"
    },
    {
      "title": "Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty",
      "authors": "Ziyu Chen, Xinbei Jiang, Peng Sun, Tao Lin",
      "institution": "Zhejiang University, Westlake University, University of Chicago",
      "link": "https://arxiv.org/pdf/2512.21336",
      "code": "https://github.com/LINs-lab/DenoisingEntropy",
      "tags": [
        "diffusion models",
        "Denoising Entropy",
        "Masked Diffusion Models",
        "decoding path optimization",
        "predictive uncertainty",
        "non-autoregressive generation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4480eb4fa3d14900373effb4e74dd207b42c650b50de1044a2cad8b4036e465f_w640_q70.webp",
      "contributions": "1. Formalized the problem of decoding path sensitivity in Masked Diffusion Models (MDMs) by introducing the concept of cumulative Path Uncertainty. 2. Proposed Denoising Entropy, a novel, computable metric to quantify predictive uncertainty along a generative path. 3. Developed two entropy-guided algorithms (post-hoc selection and real-time guidance) to optimize the decoding path and improve generation quality.",
      "summary": "The paper identifies that the flexible generation of Masked Diffusion Models (MDMs) leads to variable output quality due to the chosen decoding order. To address this, it introduces Denoising Entropy to measure path uncertainty and proposes two algorithms that use this metric to guide the decoding process. Experiments show these methods significantly improve generation accuracy on reasoning, planning, and code tasks, turning uncertainty into an advantage.",
      "mindmap": "graph LR\n        A[Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty<br/>通过量化不确定性优化掩码扩散模型的解码路径] --> B(核心问题/Problem: MDMs生成质量对解码顺序敏感<br/>MDM output quality is sensitive to decoding order)\n        A --> C(主要方法/Method: 提出去噪熵和路径优化算法<br/>Propose Denoising Entropy & path optimization algorithms)\n        A --> D(关键结果/Results: 熵引导方法提升生成质量<br/>Entropy-guided methods improve generation quality)"
    },
    {
      "title": "A Physics Informed Neural Network For Deriving MHD State Vectors From Global Active Regions Observations",
      "authors": "Subhamoy Chatterjee, Mausumi Dikpati",
      "institution": "Southwest Research Institute, High Altitude Observatory (NSF-NCAR)",
      "link": "https://arxiv.org/pdf/2512.20747",
      "code": null,
      "tags": [
        "astro-physics",
        "solar physics",
        "magnetohydrodynamics",
        "Physics-Informed Neural Network (PINN)",
        "MagnetoHydroDynamic Shallow-Water Tachocline (MHD-SWT)",
        "solar active regions (ARs)",
        "toroidal bands (toroids)",
        "state-vector reconstruction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64164e9d078622b42b01e54f9efaeb57ab61d047c881403541551b016e24827e_w640_q70.webp",
      "contributions": "1. Proposes PINNBARDS, a novel Physics-Informed Neural Network framework to derive the initial MHD state-vector for solar tachocline models from surface observations of active region distributions. 2. Demonstrates the method's ability to converge to physically consistent solutions that match observed toroidal band patterns, specifically using data from the Feb-14-2024 SDO/HMI synoptic map. 3. Explores the parameter space to constrain key physical properties, finding optimal agreement with observations for toroidal field strengths of 20–30 kG and a bandwidth of ~10 degrees, which is consistent with low-order longitudinal mode excitation.",
      "summary": "This paper addresses the challenge of initializing solar magnetohydrodynamic models for predicting flare-producing active regions, which requires a full state-vector not provided by surface observations. The authors develop PINNBARDS, a Physics-Informed Neural Network that uses observed toroidal band patterns and the governing MHD equations to reconstruct the necessary initial state-vector for the tachocline. Their analysis identifies optimal physical parameters (20-30 kG field strength) that best match observations, providing a novel pathway for weeks-ahead solar activity prediction.",
      "mindmap": "graph LR\n        A[PINNBARDS: 从全球活动区观测推导MHD状态向量 / PINNBARDS: Deriving MHD State Vectors From Global Active Regions Observations] --> B(核心问题/Problem: 表面磁图仅提供活动区分布的几何形状，无法提供初始化MHD模型所需的自洽状态向量 / Problem: Surface magnetograms only provide geometric shape of AR distribution, not the self-consistent state-vector needed to initialize MHD models.)\n        A --> C(主要方法/Method: 开发PINNBARDS，一个基于物理信息神经网络(PINN)的模拟器，使用观测到的环形带和MHD-SWT方程来推导初始状态向量 / Method: Develop PINNBARDS, a PINN-based simulator using observed toroids and MHD-SWT equations to derive the initial state-vector.)\n        A --> D(关键结果/Results: PINN收敛到物理一致解，与观测匹配；最佳参数为20-30 kG环形场和~10度带宽 / Results: PINN converges to physically consistent solutions matching observations; optimal parameters are 20-30 kG toroidal field and ~10 degree bandwidth.)"
    },
    {
      "title": "GenTSE: Enhancing Target Speaker Extraction via a Coarse-to-Fine Generative Language Model",
      "authors": "Haoyang Li, Xuyi Zhuang, Azmat Adnan, Ye Ni, Wei Rao, Shreyas Gopal, Eng Siong Chng",
      "institution": "Nanyang Technological University, Southeast University",
      "link": "https://arxiv.org/pdf/2512.20978",
      "code": null,
      "tags": [
        "speech separation",
        "target speaker extraction",
        "generative language model",
        "coarse-to-fine",
        "exposure bias",
        "direct preference optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2e6714f10d4ca9cf7e6cc6ddc5eea2048c365e1e206f97988dcc13bab4d72ef_w640_q70.webp",
      "contributions": "1. Proposes GenTSE, a fully generative two-stage decoder-only language model architecture for target speaker extraction, separating coarse semantic token prediction from fine acoustic token generation. 2. Introduces a Frozen-LM Conditioning training strategy to mitigate exposure bias by conditioning models on their own past predictions from earlier checkpoints. 3. Employs Direct Preference Optimization to better align the model's outputs with human perceptual preferences.",
      "summary": "This paper introduces GenTSE, a novel generative language model approach for target speaker extraction that uses a two-stage, coarse-to-fine process to generate speech. The method addresses exposure bias with a specific training strategy and aligns outputs with human preferences using DPO. Experiments show it outperforms previous LM-based systems in speech quality, intelligibility, and speaker consistency.",
      "mindmap": "graph LR\n    A[GenTSE] --> B[核心问题/Problem: TSE generalization & fidelity];\n    A --> C[主要方法/Method: Two-stage generative LM, FLC, DPO];\n    A --> D[关键结果/Results: Surpasses prior LM-based systems];"
    },
    {
      "title": "PhononBench:A Large-Scale Phonon-Based Benchmark for Dynamical Stability in Crystal Generation",
      "authors": "Xiao-Qi Han, Ze-Feng Gao, Peng-Jie Guo, Zhong-Yi Lu",
      "institution": "Renmin University of China",
      "link": "https://arxiv.org/pdf/2512.21227",
      "code": "https://github.com/xqh19970407/PhononBench",
      "tags": [
        "materials informatics",
        "dynamical stability",
        "phonon spectrum",
        "crystal generation",
        "benchmark",
        "MatterSim"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5409c19ccb89ace0c03a71117cfd438b28def78d75777b331d87da44f49b7230_w640_q70.webp",
      "contributions": "1. Introduced PhononBench, the first large-scale benchmark for evaluating the dynamical stability of AI-generated crystal structures. 2. Leveraged the MatterSim interatomic potential to perform efficient, DFT-level phonon calculations on over 100k generated crystals, revealing a widespread deficiency in current models' ability to produce dynamically stable structures. 3. Identified and released a substantial dataset of 28,119 phonon-stable generated crystals, providing a valuable resource for future materials discovery.",
      "summary": "This paper introduces PhononBench, a benchmark that uses the MatterSim potential to efficiently evaluate the dynamical stability of AI-generated crystals. The analysis of over 100k structures from six leading models shows that current generative models perform poorly at ensuring dynamical stability, with an average success rate of only 25.83%. The work highlights a critical limitation in the field and provides a benchmark and a pool of stable candidate structures for future development.",
      "mindmap": "graph LR\n    A[PhononBench: 晶体生成中的动力学稳定性基准] --> B(核心问题/Problem: 现有AI晶体生成模型缺乏对动力学稳定性的大规模评估)\n    A --> C(主要方法/Method: 利用MatterSim势函数进行高通量声子计算)\n    A --> D(关键结果/Results: 模型平均稳定性仅25.83%, 识别出28,119个稳定结构)"
    },
    {
      "title": "Scaling Laws for Economic Productivity: Experimental Evidence in LLM-Assisted Consulting, Data Analyst, and Management Tasks",
      "authors": "Ali Merali",
      "institution": "Yale University",
      "link": "https://arxiv.org/pdf/2512.21316",
      "code": null,
      "tags": [
        "large language models",
        "scaling laws",
        "economic productivity",
        "agentic workflows",
        "compute scaling",
        "algorithmic progress"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f148c34bb4d8aa66926afe66d00135fb826ae28f1253bfed833d9ff39c8b046d_w640_q70.webp",
      "contributions": "1. Derives empirical scaling laws linking LLM training compute to professional productivity gains. 2. Quantifies the relative contributions of increased compute (56%) versus algorithmic progress (44%) to annual productivity improvements. 3. Identifies a significant disparity in productivity gains between non-agentic analytical tasks and agentic workflows requiring tool use.",
      "summary": "This paper investigates the relationship between LLM capabilities and professional productivity through a preregistered experiment with over 500 professionals. It finds that each year of AI progress reduces task time by 8%, driven by both compute and algorithmic scaling, but gains are larger for analytical tasks than for agentic ones. The results suggest continued model scaling could significantly boost U.S. productivity over the next decade.",
      "mindmap": "graph LR\n    A[Scaling Laws for Economic Productivity<br/>经济生产力缩放定律] --> B(核心问题/Problem: LLM compute vs. professional productivity<br/>LLM计算与专业生产力的关系);\n    A --> C(主要方法/Method: Preregistered experiment with 500+ professionals using 13 LLMs<br/>使用13个LLM对500多名专业人员的预注册实验);\n    A --> D(关键结果/Results: 8% annual time reduction, 56% compute vs. 44% algorithm gains, larger gains for non-agentic tasks<br/>每年任务时间减少8%，56%源于计算，44%源于算法，非智能体任务收益更大);"
    },
    {
      "title": "QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning",
      "authors": "Sebastian Racedo, Brigitte Jaumard, Oscar Delgado, Meysam Masoudi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19696",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b77d355579fa14e02c66f844a9c1cf1fbc3d68fee2d28b38fc709f013395b1a_w640_q70.webp",
      "contributions": "",
      "summary": "QoS-Aware Dynamic CU Selection in O-RAN with Graph-Based Reinforcement Learning",
      "mindmap": ""
    },
    {
      "title": "Automated Fault Detection in 5G Core Networks Using Large Language Models",
      "authors": "Parsa Hatami, Ahmadreza Majlesara, Ali Majlesi, Babak Hossein Khalaj",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19697",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51195e88b90e075513d8e30088f944c9008c4dee4bc84af09cdce6bb8a7b71e4_w640_q70.webp",
      "contributions": "",
      "summary": "Automated Fault Detection in 5G Core Networks Using Large Language Models",
      "mindmap": ""
    },
    {
      "title": "PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility",
      "authors": "Md Nahid Hasan Shuvo, Moinul Hossain",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19711",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d516fcc6c3de6b0e65c078e5e3f3dda23bfd77fbb9c5f4abfe2c509c2cb6dfe_w640_q70.webp",
      "contributions": "",
      "summary": "PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility",
      "mindmap": ""
    },
    {
      "title": "Large Language Models for EDA Cloud Job Resource and Lifetime Prediction",
      "authors": "Yuxuan Yin, Shengke Zhou, Yunjie Zhang, Ajay Mohindra, Boxun Xu, Peng Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19701",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe0c6f8e6d98607a87a162a4a1cada21d732348d823658c9451d5ce5608a7d1_w640_q70.webp",
      "contributions": "",
      "summary": "Large Language Models for EDA Cloud Job Resource and Lifetime Prediction",
      "mindmap": ""
    },
    {
      "title": "Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data",
      "authors": "Behrooz Mamandipoor, Chun-Nan Hsu, Martin Krause, Ulrich H. Schmidt, Rodney A. Gabriel",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19716",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b80fac6c07719f0fdc3b2a60068a2f3820d61d75d5655632ad18fc7fbee5f80_w640_q70.webp",
      "contributions": "",
      "summary": "Development and external validation of a multimodal artificial intelligence mortality prediction model of critically ill patients using multicenter data",
      "mindmap": ""
    },
    {
      "title": "Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance",
      "authors": "James K Ruffle, Samia Mohinta, Guilherme Pombo, Asthik Biswas, Alan Campbell, Indran Davagnanam, David Doig, Ahmed Hamman, Harpreet Hyare, Farrah Jabeen, Emma Lim, Dermot Mallon, Stephanie Owen, Sophie Wilkinson, Sebastian Brandner, Parashkev Nachev",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19707",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e098f2b53a5d94739b784dac1a98f71b53ab4d9f759c65700bc9e1f9500bbafd_w640_q70.webp",
      "contributions": "",
      "summary": "Bidirectional human-AI collaboration in brain tumour assessments improves both expert human and AI agent performance",
      "mindmap": ""
    },
    {
      "title": "Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference",
      "authors": "Zhan Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19717",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ee62945031af7f6dac3a6d51384974eec1f8f5db6dd103388502d84eb58bc6a_w640_q70.webp",
      "contributions": "",
      "summary": "Thermodynamic Focusing for Inference-Time Search: Practical Methods for Target-Conditioned Sampling and Prompted Inference",
      "mindmap": ""
    },
    {
      "title": "Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries",
      "authors": "Zihao Lv, Siqi Ai, Yanbin Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19719",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f6292853f8fb29c3648a6a9e7a018fcb02691dba13e4d6ce37a63f296f046554_w640_q70.webp",
      "contributions": "",
      "summary": "Multiscale Dual-path Feature Aggregation Network for Remaining Useful Life Prediction of Lithium-Ion Batteries",
      "mindmap": ""
    },
    {
      "title": "Tiny, On-Device Decision Makers with the MiniConv Library",
      "authors": "Carlos Purves",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19726",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fcebc0e7acd2aa33081184298763fb8df6f0a43f23fb341b0e84da9a69d1bd6_w640_q70.webp",
      "contributions": "",
      "summary": "Tiny, On-Device Decision Makers with the MiniConv Library",
      "mindmap": ""
    },
    {
      "title": "High-Performance Self-Supervised Learning by Joint Training of Flow Matching",
      "authors": "Kosuke Ukita, Tsuyoshi Okita",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19729",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/269f1eff8cf71b204b6147341b992a81faef6468f33e5edd7a5be137a0ac9100_w640_q70.webp",
      "contributions": "",
      "summary": "High-Performance Self-Supervised Learning by Joint Training of Flow Matching",
      "mindmap": ""
    },
    {
      "title": "Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach",
      "authors": "Clément Elliker, Jesse Read, Sonia Vanier, Albert Bifet",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19737",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37e9a570297730f5200e5c0dcac9576f29dc77d8856f607f480d2a083088332_w640_q70.webp",
      "contributions": "",
      "summary": "Simulation-Driven Railway Delay Prediction: An Imitation Learning Approach",
      "mindmap": ""
    },
    {
      "title": "CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology",
      "authors": "Gongli Xi, Ye Tian, Mengyu Yang, Zhenyu Zhao, Yuchao Zhang, Xiangyang Gong, Xirong Que, Wendong Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19736",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68578352cc68ad1305abf54d27488dc8db7f857ff2484ef8acd9ab80b0db8641_w640_q70.webp",
      "contributions": "",
      "summary": "CoPHo: Classifier-guided Conditional Topology Generation with Persistent Homology",
      "mindmap": ""
    },
    {
      "title": "From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning",
      "authors": "Sasan Sharifipour, Constantino Álvarez Casado, Manuel Lage Cañellas, Miguel Bordallo López",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19743",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28b9eb6359f294d9654c6f1fea215bc4726b236c77ba0b6790735d53dbad5ead_w640_q70.webp",
      "contributions": "",
      "summary": "From Theory to Throughput: CUDA-Optimized APML for Large-Batch 3D Learning",
      "mindmap": ""
    },
    {
      "title": "How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts",
      "authors": "Sumin Park, Noseong Park",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19765",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/515767751abfd73b2b6370592d087c270228e210ccda8fa867a672db0ae07a01_w640_q70.webp",
      "contributions": "",
      "summary": "How Many Experts Are Enough? Towards Optimal Semantic Specialization for Mixture-of-Experts",
      "mindmap": ""
    },
    {
      "title": "A K-Means, Ward and DBSCAN repeatability study",
      "authors": "Anthony Bertrand, Engelbert Mephu Nguifo, Violaine Antoine, David Hill",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19772",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b606a0690bd58fd61c6e296efa76193ecfe74e0fd82dcf2bd79d638111e3e1d1_w640_q70.webp",
      "contributions": "",
      "summary": "A K-Means, Ward and DBSCAN repeatability study",
      "mindmap": ""
    },
    {
      "title": "Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning",
      "authors": "Antonio Tarizzo, Mohammad Kazemi, Deniz Gündüz",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19777",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b386ba4532b788c41eccda5b3c48b9585db890467bbb5e150328901a4ad2208_w640_q70.webp",
      "contributions": "",
      "summary": "Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning",
      "mindmap": ""
    },
    {
      "title": "A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows",
      "authors": "Ivan Daunis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19769",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39e158baf642d33624c0967b1dcd509fbc3876a4bc52a539d4b6e7c800995b42_w640_q70.webp",
      "contributions": "",
      "summary": "A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows",
      "mindmap": ""
    },
    {
      "title": "Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models",
      "authors": "Wang Bin, Ao Yang, Kedan Li, Aofan Liu, Hui Li, Guibo Luo, Weixiang Huang, Yan Zhuang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19758",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45aa033e5d42a870c8059ab54cb6cefa331610516ad0bef063a9ce423cb132dc_w640_q70.webp",
      "contributions": "",
      "summary": "Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models",
      "mindmap": ""
    },
    {
      "title": "PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research",
      "authors": "Tingjia Miao, Jiawen Dai, Jingkun Liu, Jinxin Tan, Muhua Zhang, Wenkai Jin, Yuwen Du, Tian Jin, Xianghe Pang, Zexi Liu, Tu Guo, Zhengliang Zhang, Yunjie Huang, Shuo Chen, Rui Ye, Yuzhi Zhang, Linfeng Zhang, Kun Chen, Wei Wang, Weinan E, Siheng Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19799",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1922ac933b302c99f4a3c639911ffa3980ae43238fad1b247af369017413128_w640_q70.webp",
      "contributions": "",
      "summary": "PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research",
      "mindmap": ""
    },
    {
      "title": "UCCL-EP: Portable Expert-Parallel Communication",
      "authors": "Ziming Mao, Yihan Zhang, Chihan Cui, Kaichao You, Zhongjie Chen, Zhiying Xu, Scott Shenker, Costin Raiciu, Yang Zhou, Ion Stoica",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19849",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb2d143c0a9d64bd2c5bdf4142e8e3a096290fd8319372321c00c3c17d53b658_w640_q70.webp",
      "contributions": "",
      "summary": "UCCL-EP: Portable Expert-Parallel Communication",
      "mindmap": ""
    },
    {
      "title": "A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution",
      "authors": "Mahdi Mostajabdaveh, F. Sibel Salman, Walter J. Gutjahr",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19882",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81c6462c8b784a5ac97bed22a2eedfc4c0fbad0afad1bab0e0a9aab3730a1834_w640_q70.webp",
      "contributions": "",
      "summary": "A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution",
      "mindmap": ""
    },
    {
      "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data",
      "authors": "Shashi Kant Gupta, Arijeet Pramanik, Jerrin John Thomas, Regina Schwind, Lauren Wiener, Avi Raju, Jeremy Kornbluth, Yanshan Wang, Zhaohui Su, Hrituraj Singh",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19864",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e0cfa9ec043e8a27718dd14bb89bf3c4ceafb97fb48a8a0b61d661ec9d34b09_w640_q70.webp",
      "contributions": "",
      "summary": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data",
      "mindmap": ""
    },
    {
      "title": "Fine-Tuned In-Context Learners for Efficient Adaptation",
      "authors": "Jorg Bornschein, Clare Lyle, Yazhe Li, Amal Rannen-Triki, Xu Owen He, Razvan Pascanu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19879",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/31980c4d9f6b1c6d6c1f0f41df293fd637d43c4ea2f2de5d26aa825310d8bdbc_w640_q70.webp",
      "contributions": "",
      "summary": "Fine-Tuned In-Context Learners for Efficient Adaptation",
      "mindmap": ""
    },
    {
      "title": "A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones",
      "authors": "Sujan Warnakulasooriya, Andreas Willig, Xiaobing Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19914",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/188da8ba7b74e5b961c0e61a49776e60da92670b4dd0b522d0c74e156682ec49_w640_q70.webp",
      "contributions": "",
      "summary": "A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones",
      "mindmap": ""
    },
    {
      "title": "Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning",
      "authors": "Jiayun Wu, Jiashuo Liu, Zhiyuan Zeng, Tianyang Zhan, Wenhao Huang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19920",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33255a480cb8654f8d9838cb7a634102f7086c3eaac9fb63148c10866248563a_w640_q70.webp",
      "contributions": "",
      "summary": "Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning",
      "mindmap": ""
    },
    {
      "title": "Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling",
      "authors": "Indranil Halder, Cengiz Pehlevan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19905",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1e6db0153f278bc11740f6ab7077ed6a29fff1f323057711a5dc1210d6e99fe_w640_q70.webp",
      "contributions": "",
      "summary": "Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling",
      "mindmap": ""
    },
    {
      "title": "Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra",
      "authors": "Maxime Lacour, Pu Ren, Rie Nakata, Nori Nakata, Michael Mahoney",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19909",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e80ac4736f89a1123273e8c6f77a605a941de3087891673a6d7728a3d0998_w640_q70.webp",
      "contributions": "",
      "summary": "Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra",
      "mindmap": ""
    },
    {
      "title": "Unified Brain Surface and Volume Registration",
      "authors": "S. Mazdak Abulnaga, Andrew Hoopes, Malte Hoffmann, Robin Magnet, Maks Ovsjanikov, Lilla Zöllei, John Guttag, Bruce Fischl, Adrian Dalca",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19928",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52472b7c53844ab7af247ff699ee1c659d2b3ff27e2e21ee8f187677824a5d8d_w640_q70.webp",
      "contributions": "",
      "summary": "Unified Brain Surface and Volume Registration",
      "mindmap": ""
    },
    {
      "title": "Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress",
      "authors": "Samruddhi Baviskar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19935",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f15b3890b1332bf926501d440f418e2e71c3b0c8c5b3dd19380d13e172c25ac_w640_q70.webp",
      "contributions": "",
      "summary": "Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress",
      "mindmap": ""
    },
    {
      "title": "Vehicle-centric Perception via Multimodal Structured Pre-training",
      "authors": "Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19934",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25393280cf5e09ecc25c375a88de26bef6b6904fd90a6775cf7591ed8a615fe1_w640_q70.webp",
      "contributions": "",
      "summary": "Vehicle-centric Perception via Multimodal Structured Pre-training",
      "mindmap": ""
    },
    {
      "title": "Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs",
      "authors": "Eric Yeh, John Cadigan, Ran Chen, Dick Crouch, Melinda Gervasio, Dayne Freitag",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19937",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67e83c33b123e956d61ade807a7eb155837dc60f3e60e67f0209df1eb75d7639_w640_q70.webp",
      "contributions": "",
      "summary": "Interpolative Decoding: Exploring the Spectrum of Personality Traits in LLMs",
      "mindmap": ""
    },
    {
      "title": "Block-Recurrent Dynamics in Vision Transformers",
      "authors": "Mozes Jacobs, Thomas Fel, Richard Hakim, Alessandra Brondetta, Demba Ba, T. Andy Keller",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19941",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e10f9b4ab6d210902b2c5092ebfe1fcc82dd7a3d2032bfc97fbfadd16867c2a_w640_q70.webp",
      "contributions": "",
      "summary": "Block-Recurrent Dynamics in Vision Transformers",
      "mindmap": ""
    },
    {
      "title": "How Much 3D Do Video Foundation Models Encode?",
      "authors": "Zixuan Huang, Xiang Li, Zhaoyang Lv, James M. Rehg",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19949",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b54fab192e555a908a0f7daf8d7992c85cd3241844d6a17c19d685c99c93dc5e_w640_q70.webp",
      "contributions": "",
      "summary": "How Much 3D Do Video Foundation Models Encode?",
      "mindmap": ""
    },
    {
      "title": "Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification",
      "authors": "Luciano Araujo Dourado Filho, Almir Moreira da Silva Neto, Rodrigo Pereira David, Rodrigo Tripodi Calumby",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19957",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/307d3e337ca6d53335e1861afc2551a5e05ba3baffbfb580ffc6378d16a74e2f_w640_q70.webp",
      "contributions": "",
      "summary": "Zero-Shot Segmentation through Prototype-Guidance for Multi-Label Plant Species Identification",
      "mindmap": ""
    },
    {
      "title": "FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification",
      "authors": "Luciano Araujo Dourado Filho, Rodrigo Tripodi Calumby",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19960",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbfda29c0b0fd29ef417a6aed2c9021c6a676577f450cbc925474212756dbba1_w640_q70.webp",
      "contributions": "",
      "summary": "FGDCC: Fine-Grained Deep Cluster Categorization -- A Framework for Intra-Class Variability Problems in Plant Classification",
      "mindmap": ""
    },
    {
      "title": "Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?",
      "authors": "Zhe Yin, Xiaodong Gu, Beijun Shen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19980",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/612b57ba54262082ae033612387571cddc86ac826d14c6f5b1ba4c241011b3b9_w640_q70.webp",
      "contributions": "",
      "summary": "Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?",
      "mindmap": ""
    },
    {
      "title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
      "authors": "Ming Li, Chenrui Fan, Yize Cheng, Soheil Feizi, Tianyi Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19995",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf100eeda1c44688829760003993b1a83478a2d2901a0f5a0b9914bc5ef7c3b0_w640_q70.webp",
      "contributions": "",
      "summary": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
      "mindmap": ""
    },
    {
      "title": "S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test",
      "authors": "Zhe Sun, Xueyuan Yang, Yujie Lu, Zhenliang Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19992",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a7f6951c4ebc6cf6c8bd633198ae7d4c6a7c8f1e0cd348aa9e6737966dfe600_w640_q70.webp",
      "contributions": "",
      "summary": "S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test",
      "mindmap": ""
    },
    {
      "title": "IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense",
      "authors": "Rahul Yumlembam, Biju Issac, Seibu Mary Jacob, Longzhi Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20004",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea3f077bcaec1c639c8029881602d31bdf125a8cbefc2e15ec9ba3e07c126ee1_w640_q70.webp",
      "contributions": "",
      "summary": "IoT-based Android Malware Detection Using Graph Neural Network With Adversarial Defense",
      "mindmap": ""
    },
    {
      "title": "DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics",
      "authors": "Yuan Gao, Zhenguo Dong, Xuelong Wang, Zhiqiang Wang, Yong Zhang, Shaofan Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20028",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a6ada3496efb972d8c3a130ea0af749449dc6804b758999852b9def0e9692a4_w640_q70.webp",
      "contributions": "",
      "summary": "DecoKAN: Interpretable Decomposition for Forecasting Cryptocurrency Market Dynamics",
      "mindmap": ""
    },
    {
      "title": "Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting",
      "authors": "Sangoh Lee, Sangwoo Mo, Wook-Shin Han",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20014",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbb9757af838ced105aac295c936d5bf2fa52c5f79d162d34ed41c9c961ae9e0_w640_q70.webp",
      "contributions": "",
      "summary": "Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting",
      "mindmap": ""
    },
    {
      "title": "Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva",
      "authors": "Nguyen Lam Phu Quy, Pham Phu Hoa, Tran Chi Nguyen, Dao Sy Duy Minh, Nguyen Hoang Minh Ngoc, Huynh Trung Kiet",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20042",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fb136c3c44d734ccd03ee7608dfc92d3612b466bfffc82e1d2efdfd79e5f161_w640_q70.webp",
      "contributions": "",
      "summary": "Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva",
      "mindmap": ""
    },
    {
      "title": "Learning Skills from Action-Free Videos",
      "authors": "Hung-Chieh Fang, Kuo-Han Hung, Chu-Rong Chen, Po-Jung Chou, Chun-Kai Yang, Po-Chen Ko, Yu-Chiang Wang, Yueh-Hua Wu, Min-Hung Chen, Shao-Hua Sun",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20052",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/def215474b82a6d04f6a6f79dc99c74e3b1159e34a80855d18649f29781a36fc_w640_q70.webp",
      "contributions": "",
      "summary": "Learning Skills from Action-Free Videos",
      "mindmap": ""
    },
    {
      "title": "An Optimal Policy for Learning Controllable Dynamics by Exploration",
      "authors": "Peter N. Loxley",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20053",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1955af7668fd6f7c26946b76a3cbf622271164ba70999a4181146562f156fbbc_w640_q70.webp",
      "contributions": "",
      "summary": "An Optimal Policy for Learning Controllable Dynamics by Exploration",
      "mindmap": ""
    },
    {
      "title": "Discovering Lie Groups with Flow Matching",
      "authors": "Jung Yeon Park, Yuxuan Chen, Floor Eijkelboom, Jan-Willem van de Meent, Lawson L.S. Wong, Robin Walters",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20043",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/806c459fa9197bda13199f58531fce983ef12854f6eb2e8acaaea33dfebd6a22_w640_q70.webp",
      "contributions": "",
      "summary": "Discovering Lie Groups with Flow Matching",
      "mindmap": ""
    },
    {
      "title": "Scaling Reinforcement Learning for Content Moderation with Large Language Models",
      "authors": "Hamed Firooz, Rui Liu, Yuchen Lu, Zhenyu Hou, Fangzhou Xiong, Xiaoyang Zhang, Changshu Jian, Zhicheng Zhu, Jiayuan Ma, Jacob Tao, Chaitali Gupta, Xiaochang Peng, Shike Mei, Hang Cui, Yang Qin, Shuo Tang, Jason Gaedtke, Arpit Mittal",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20061",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9cc1d5bc88350e4d91579fed9a3b17abade80dc25754379e8e11ce92e39ca7d5_w640_q70.webp",
      "contributions": "",
      "summary": "Scaling Reinforcement Learning for Content Moderation with Large Language Models",
      "mindmap": ""
    },
    {
      "title": "Reason2Decide: Rationale-Driven Multi-Task Learning",
      "authors": "H M Quamran Hasan, Housam Khalifa Bashier, Jiayi Dai, Mi-Young Kim, Randy Goebel",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20074",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/187b806c979650defdb64bdb9ce297598ef473654b93625ec2257af228dbd0da_w640_q70.webp",
      "contributions": "",
      "summary": "Reason2Decide: Rationale-Driven Multi-Task Learning",
      "mindmap": ""
    },
    {
      "title": "On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities",
      "authors": "Sangryu Park, Gihyuk Ko, Homook Cho",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20062",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74a88fcd014c903ee16397aa497f69b488c2c3bdeb23c2bddfc09643306081ec_w640_q70.webp",
      "contributions": "",
      "summary": "On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities",
      "mindmap": ""
    },
    {
      "title": "Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach",
      "authors": "Hao Li, Fabian Deuser, Wenping Yin, Steffen Knoblauch, Wufan Zhao, Filip Biljecki, Yong Xue, Wei Huang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20056",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c37110ad3319d1434c17c04ae94a4dcfa94a278faf51360e359e1a063ce32e7_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach",
      "mindmap": ""
    },
    {
      "title": "CBA: Communication-Bound-Aware Cross-Domain Resource Assignment for Pipeline-Parallel Distributed LLM Training in Dynamic Multi-DC Optical Networks",
      "authors": "Dianxuan Fu, Xiaomin Liu, Yihao Zhang, Shikui Shen, Weisheng Hu, Qunbi Zhuge",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20080",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b7f27ae92944936b92479ef7c0a55f8c448c532444c4f43131d8768483bda71_w640_q70.webp",
      "contributions": "",
      "summary": "CBA: Communication-Bound-Aware Cross-Domain Resource Assignment for Pipeline-Parallel Distributed LLM Training in Dynamic Multi-DC Optical Networks",
      "mindmap": ""
    },
    {
      "title": "Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches",
      "authors": "Chaithra, Kamesh Kadimisetty, Biju R Mohan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20082",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8f2d13846195cc68b294529fa5d22600c76c1ff5581ee402ddf30ac1c06da72_w640_q70.webp",
      "contributions": "",
      "summary": "Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches",
      "mindmap": ""
    },
    {
      "title": "Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection",
      "authors": "Jeehong Kim, Youngseok Hwang, Minchan Kim, Sungho Bae, Hyunwoo Park",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20086",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95b1bc86a744c86f68507c2b101c06be33b9804cc84964060682c34e2802c63e_w640_q70.webp",
      "contributions": "",
      "summary": "Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection",
      "mindmap": ""
    },
    {
      "title": "QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption",
      "authors": "Yanjie Li, Jian Xu, Xueqing Chen, Lina Yu, Shiming Xiang, Weijun Li, Cheng-lin Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20084",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ad5d1a3b18c9e1109d65023aa18a9c4b5eaddb7fbf1b86de532c25025f845a7_w640_q70.webp",
      "contributions": "",
      "summary": "QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption",
      "mindmap": ""
    },
    {
      "title": "Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts",
      "authors": "Jinyoung Choi, Youngchae Kwon, Injung Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20088",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1946d7c3329c37db07556d963544aad7dbfeda194c515e4debdc6e3754d8920_w640_q70.webp",
      "contributions": "",
      "summary": "Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts",
      "mindmap": ""
    },
    {
      "title": "Evolutionary Neural Architecture Search with Dual Contrastive Learning",
      "authors": "Xian-Rong Zhang, Yue-Jiao Gong, Wei-Neng Chen, Jun Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20112",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/659d1805ddb5e7863af4ee9eeedcfdd78686f84f207247f4d0e65555165f2577_w640_q70.webp",
      "contributions": "",
      "summary": "Evolutionary Neural Architecture Search with Dual Contrastive Learning",
      "mindmap": ""
    },
    {
      "title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language",
      "authors": "Aly Lidayan, Jakob Bjorner, Satvik Golechha, Kartik Goyal, Alane Suhr",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20111",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d0d34b2d9754cdb266cf6f4c20cff854836475921a3b1dfd5eebd552c7ef69c_w640_q70.webp",
      "contributions": "",
      "summary": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language",
      "mindmap": ""
    },
    {
      "title": "MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization",
      "authors": "Zhuo Yang, Yeyun chen, Jiaqing Xie, Ben Gao, Shuaike Shen, Wanhao Liu, Liujia Yang, Beilun Wang, Tianfan Fu, Yuqiang Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20135",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/354a0c5aa0cbd47dcbbf13234c253d1300ed7b0266e8ead6da21580f2b96f942_w640_q70.webp",
      "contributions": "",
      "summary": "MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization",
      "mindmap": ""
    },
    {
      "title": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models",
      "authors": "Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20145",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp",
      "contributions": "",
      "summary": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models",
      "mindmap": ""
    },
    {
      "title": "M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation",
      "authors": "Hyeongcheol Park, Jiyoung Seo, Jaewon Mun, Hogun Park, Wonmin Byeon, Sung June Kim, Hyeonsoo Im, JeungSub Lee, Sangpil Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20136",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6c211a1c8d3a17c264fe9655b35305f3ece6ef329a1cb2344fb1a18976f11017_w640_q70.webp",
      "contributions": "",
      "summary": "M$^3$KG-RAG: Multi-hop Multimodal Knowledge Graph-enhanced Retrieval-Augmented Generation",
      "mindmap": ""
    },
    {
      "title": "Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection",
      "authors": "Xingyou Yin, Ceyao Zhang, Min Hu, Kai Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20140",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4380a71bf6060d158bfda4324d258b0056d7685334fc2256c0df38315036c209_w640_q70.webp",
      "contributions": "",
      "summary": "Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection",
      "mindmap": ""
    },
    {
      "title": "Fun-Audio-Chat Technical Report",
      "authors": "Qian Chen, Luyao Cheng, Chong Deng, Xiangang Li, Jiaqing Liu, Chao-Hong Tan, Wen Wang, Junhao Xu, Jieping Ye, Qinglin Zhang, Qiquan Zhang, Jingren Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20156",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eedb25d29b9c63de7f75bd47632c06e734814fe19fe3f517a37a4d3d42f693c7_w640_q70.webp",
      "contributions": "",
      "summary": "Fun-Audio-Chat Technical Report",
      "mindmap": ""
    },
    {
      "title": "AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration",
      "authors": "Ruiqi Wang, Xinchen Wang, Cuiyun Gao, Chun Yong Chong, Xin Xia, Qing Liao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20159",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9b4276c369f7cc83453b7385f456424f32c6a43157de7895979a0b4e9cd33bf_w640_q70.webp",
      "contributions": "",
      "summary": "AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration",
      "mindmap": ""
    },
    {
      "title": "A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers",
      "authors": "Dhivya Dharshini Kannan, Anupam Trivedi, Dipti Srinivasan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20161",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1417550973096c816c854e8c26e0184adfd5d9410e4449d0498cc343fd80cc27_w640_q70.webp",
      "contributions": "",
      "summary": "A Bidirectional Gated Recurrent Unit Model for PUE Prediction in Data Centers",
      "mindmap": ""
    },
    {
      "title": "Concept Generalization in Humans and Large Language Models: Insights from the Number Game",
      "authors": "Arghavan Bazigaran, Hansem Sohn",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20162",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f33aa8c0a9e961f65d06bf913a8cd4cb96d114bf3d4806d5932ebcb99dbac9d2_w640_q70.webp",
      "contributions": "",
      "summary": "Concept Generalization in Humans and Large Language Models: Insights from the Number Game",
      "mindmap": ""
    },
    {
      "title": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications",
      "authors": "Honglin Mu, Jinghao Liu, Kaiyang Wan, Rui Xing, Xiuying Chen, Timothy Baldwin, Wanxiang Che",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20164",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9be09765889dd9516b45fd948d07346b8b6487f6dc02927a597c1cab7861bfb7_w640_q70.webp",
      "contributions": "",
      "summary": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications",
      "mindmap": ""
    },
    {
      "title": "Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography",
      "authors": "Songze Li, Jiameng Cheng, Yiming Li, Xiaojun Jia, Dacheng Tao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20168",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12bfaa681af3521489a5c856a7d28bf207a72778a776d4ae80d7e0271f100e3b_w640_q70.webp",
      "contributions": "",
      "summary": "Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography",
      "mindmap": ""
    },
    {
      "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
      "authors": "Shuzheng Si, Qingyi Wang, Haozhe Zhao, Yuzhuo Bai, Guanqiao Chen, Kangyang Luo, Gang Chen, Fanchao Qi, Minjia Zhang, Baobao Chang, Maosong Sun",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20182",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/418ec0226018d595ee93c7097014ac35b5c5e68ad18001889120bf6c5aa27d11_w640_q70.webp",
      "contributions": "",
      "summary": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
      "mindmap": ""
    },
    {
      "title": "Offline Safe Policy Optimization From Heterogeneous Feedback",
      "authors": "Ze Gong, Pradeep Varakantham, Akshat Kumar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20173",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9834f9cb644c8389de473430a70e825274ad26459f3ce94c2018ac8228df6f9_w640_q70.webp",
      "contributions": "",
      "summary": "Offline Safe Policy Optimization From Heterogeneous Feedback",
      "mindmap": ""
    },
    {
      "title": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation",
      "authors": "Teqiang Zou, Hongliang Zeng, Yuxuan Nong, Yifan Li, Kehui Liu, Haotian Yang, Xinyang Ling, Xin Li, Lianyang Ma",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20188",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2574b3e0fe38bea23055b8f04a72d9d1bf8fedc2ee3f2dcf7a4e5a2f16af3c51_w640_q70.webp",
      "contributions": "",
      "summary": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation",
      "mindmap": ""
    },
    {
      "title": "Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings",
      "authors": "Marko Čechovič, Natália Komorníková, Dominik Macháček, Ondřej Bojar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20204",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d50c0d1f767aca0e832de0ef650261ca473b1bbf6f1ac37ad18132605e13bc77_w640_q70.webp",
      "contributions": "",
      "summary": "Corpus of Cross-lingual Dialogues with Minutes and Detection of Misunderstandings",
      "mindmap": ""
    },
    {
      "title": "TongSIM: A General Platform for Simulating Intelligent Machines",
      "authors": "Zhe Sun, Kunlun Wu, Chuanjian Fu, Zeming Song, Langyong Shi, Zihe Xue, Bohan Jing, Ying Yang, Xiaomeng Gao, Aijia Li, Tianyu Guo, Huiying Li, Xueyuan Yang, Rongkai Liu, Xinyi He, Yuxi Wang, Yue Li, Mingyuan Liu, Yujie Lu, Hongzhao Xie, Shiyun Zhao, Bo Dai, Wei Wang, Tao Yuan, Song-Chun Zhu, Yujia Peng, Zhenliang Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20206",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca507a85ff857b30b18d4ea2014b82001e6b5d0adac56afe981969173bc45325_w640_q70.webp",
      "contributions": "",
      "summary": "TongSIM: A General Platform for Simulating Intelligent Machines",
      "mindmap": ""
    },
    {
      "title": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents",
      "authors": "Xingbo Du, Loka Li, Duzhen Zhang, Le Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20237",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59b81bde40292c52d66676e0cadc37d954f64fdf390b55d610cd63316ed43ef4_w640_q70.webp",
      "contributions": "",
      "summary": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents",
      "mindmap": ""
    },
    {
      "title": "Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds",
      "authors": "Tarik Houichime, Abdelghani Souhar, Younes El Amrani",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20245",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1001e3781e678219db00950fa667bbe46bbaa2f98cd7e5064d91ede9a2cbc6fe_w640_q70.webp",
      "contributions": "",
      "summary": "Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds",
      "mindmap": ""
    },
    {
      "title": "Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks",
      "authors": "Divya Vijay, Vignesh Ethiraj",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20275",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da6037bde0b2a44f38e40927d7e2b880cc97a5f8fde73345c4fad4c78baf4836_w640_q70.webp",
      "contributions": "",
      "summary": "Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks",
      "mindmap": ""
    },
    {
      "title": "Synthesizing Procedural Memory: Challenges and Architectures in Automated Workflow Generation",
      "authors": "Nishant Gaurav, Adit Akarsh, Ankit Ranjan, Manoj Bajaj",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20278",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68f0ca1a3004807c69fa2a7198569167a93240c27f60163c26979dcccdf6072f_w640_q70.webp",
      "contributions": "",
      "summary": "Synthesizing Procedural Memory: Challenges and Architectures in Automated Workflow Generation",
      "mindmap": ""
    },
    {
      "title": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge",
      "authors": "Yuntao Dai, Hang Gu, Teng Wang, Qianyu Cheng, Yifei Zheng, Zhiyong Qiu, Lei Gong, Wenqi Lou, Xuehai Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20276",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b076c18a7407a2fa23f502051cf18d209fb696f05cbca7af89a2db3703250585_w640_q70.webp",
      "contributions": "",
      "summary": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge",
      "mindmap": ""
    },
    {
      "title": "$\\{D\\}^\\{3\\}$\\{ETOR\\}: $\\{D\\}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive $\\{D\\}$ebiasing for Weakly-Supervised Camouflaged Object $\\{D\\}$etection with Scribble Annotations",
      "authors": "Jiawei Ge, Jiuxin Cao, Xinyi Li, Xuelin Zhu, Chang Liu, Bo Liu, Chen Feng, Ioannis Patras",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20260",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee0606d3fe6e6ae30bbf9417baa7948dab878cfcc6b70e36031424c107aafb81_w640_q70.webp",
      "contributions": "",
      "summary": "$\\{D\\}^\\{3\\}$\\{ETOR\\}: $\\{D\\}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive $\\{D\\}$ebiasing for Weakly-Supervised Camouflaged Object $\\{D\\}$etection with Scribble Annotations",
      "mindmap": ""
    },
    {
      "title": "UbiQVision: Quantifying Uncertainty in XAI for Image Recognition",
      "authors": "Akshat Dubey, Aleksandar Anžel, Bahar İlgen, Georges Hattab",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20288",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6106b87e48429449f0b11c2765bdedb47797d7822e4b38ad3a9fe7f94b92dacb_w640_q70.webp",
      "contributions": "",
      "summary": "UbiQVision: Quantifying Uncertainty in XAI for Image Recognition",
      "mindmap": ""
    },
    {
      "title": "Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives",
      "authors": "Karolina Drożdż, Kacper Dudzic, Anna Sterna, Marcin Moskalewicz",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20298",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e46ae09622d9142a3896f2528685617edcbaae96bc105754e16929ed630e3f0_w640_q70.webp",
      "contributions": "",
      "summary": "Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives",
      "mindmap": ""
    },
    {
      "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
      "authors": "Wenzheng Zeng, Mingyu Ouyang, Langyuan Cui, Hwee Tou Ng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20292",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f80870c0718240101f019ec3db0f39be1afd3c26281e0c20366762d11e67351_w640_q70.webp",
      "contributions": "",
      "summary": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
      "mindmap": ""
    },
    {
      "title": "TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation",
      "authors": "Ji-Hoon Kim, Junseok Ahn, Doyeop Kwak, Joon Son Chung, Shinji Watanabe",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20296",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/217e6d95bfc0e38be8f5a7356d3e869c3b3a5b9d4daf05b6435c3c756df247c6_w640_q70.webp",
      "contributions": "",
      "summary": "TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation",
      "mindmap": ""
    },
    {
      "title": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System",
      "authors": "Zhongyu Xia, Wenhao Chen, Yongtao Wang, Ming-Hsuan Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20299",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0961ae49fbc925ad5eacb6602aeec6cfdfabae07b252a044cd181bdb5a47746_w640_q70.webp",
      "contributions": "",
      "summary": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System",
      "mindmap": ""
    },
    {
      "title": "TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning",
      "authors": "Saisai Yang, Qingyi Huang, Jing Yuan, Liangyu Zha, Kai Tang, Yuhang Yang, Ning Wang, Yucheng Wei, Liyao Li, Wentao Ye, Hao Chen, Tao Zhang, Junlin Zhou, Haobo Wang, Gang Chen, Junbo Zhao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20312",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dfc9634e9cda6fc809e3b25d9f21b937a2d8630ecb3e8bb9633968f223bb4e2d_w640_q70.webp",
      "contributions": "",
      "summary": "TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning",
      "mindmap": ""
    },
    {
      "title": "Toward Explaining Large Language Models in Software Engineering Tasks",
      "authors": "Antonio Vitale, Khai-Nguyen Nguyen, Denys Poshyvanyk, Rocco Oliveto, Simone Scalabrino, Antonio Mastropaolo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20328",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2779c954fdfbaebf8f7d7d236f2c601ab45082cae14229886e2b749d6d8cd669_w640_q70.webp",
      "contributions": "",
      "summary": "Toward Explaining Large Language Models in Software Engineering Tasks",
      "mindmap": ""
    },
    {
      "title": "SynCraft: Guiding Large Language Models to Predict Edit Sequences for Molecular Synthesizability Optimization",
      "authors": "Junren Li, Luhua Lai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20333",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25e8cac7516cbb65a566c19ba7dde5921369f69a0fe2fe62a1ee10bb383644f7_w640_q70.webp",
      "contributions": "",
      "summary": "SynCraft: Guiding Large Language Models to Predict Edit Sequences for Molecular Synthesizability Optimization",
      "mindmap": ""
    },
    {
      "title": "A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice",
      "authors": "Yaowei Bai, Ruiheng Zhang, Yu Lei, Xuhua Duan, Jingfeng Yao, Shuguang Ju, Chaoyang Wang, Wei Yao, Yiwan Guo, Guilin Zhang, Chao Wan, Qian Yuan, Lei Chen, Wenjuan Tang, Biqiang Zhu, Xinggang Wang, Tao Sun, Wei Zhou, Dacheng Tao, Yongchao Xu, Chuansheng Zheng, Huangxuan Zhao, Bo Du",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20344",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f502578578854e6ba573a8a7758a3229a45ec3bf4a1c2e637d1006bb31aa1c4_w640_q70.webp",
      "contributions": "",
      "summary": "A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice",
      "mindmap": ""
    },
    {
      "title": "Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning",
      "authors": "Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20363",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59262adfe1a821db6db6b416af65f0ab7cc67a6943505bf052c9306bf801e87f_w640_q70.webp",
      "contributions": "",
      "summary": "Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning",
      "mindmap": ""
    },
    {
      "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
      "authors": "Nilesh Jain, Seyi Adeyinka, Leor Roseman, Aza Allsop",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20352",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2047f0800a6a91a372fd66013fa3526084396a7513879598fb459814e55b18c_w640_q70.webp",
      "contributions": "",
      "summary": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
      "mindmap": ""
    },
    {
      "title": "Identifying Appropriately-Sized Services with Deep Reinforcement Learning",
      "authors": "Syeda Tasnim Fabiha, Saad Shafiq, Wesley Klewerton Guez Assunção, Nenad Medvidović",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20381",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/295a39edb17ec8824ad34080df5c2a960bf42d578ae6e9a2db55f3775d90e225_w640_q70.webp",
      "contributions": "",
      "summary": "Identifying Appropriately-Sized Services with Deep Reinforcement Learning",
      "mindmap": ""
    },
    {
      "title": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems",
      "authors": "YuChe Hsu, AnJui Wang, TsaiChing Ni, YuanFu Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20387",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d9aa5293bf4ca8e839b122277f1484ffb43b6211d54741d7eb7f58312f5509_w640_q70.webp",
      "contributions": "",
      "summary": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems",
      "mindmap": ""
    },
    {
      "title": "AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition",
      "authors": "Rajdeep Chatterjee, Sudip Chakrabarty, Trishaani Acharjee, Deepanjali Mishra",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20407",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e762dea30a9edc887d84deefc367d35dd7c953e636f54a824f1e7f853c9d370f_w640_q70.webp",
      "contributions": "",
      "summary": "AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition",
      "mindmap": ""
    },
    {
      "title": "Simplifying Multi-Task Architectures Through Task-Specific Normalization",
      "authors": "Mihai Suteu, Ovidiu Serban",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20420",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d24e6af533166dd44855079b97d7233c65878aa61e02267e65f4e306467d04b_w640_q70.webp",
      "contributions": "",
      "summary": "Simplifying Multi-Task Architectures Through Task-Specific Normalization",
      "mindmap": ""
    },
    {
      "title": "Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit",
      "authors": "Adam Elaoumari",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20423",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b67cb70660ec29cf307ab6baf5a52c6e3cecc0888ba9656259ee05bcfef71b96_w640_q70.webp",
      "contributions": "",
      "summary": "Evasion-Resilient Detection of DNS-over-HTTPS Data Exfiltration: A Practical Evaluation and Toolkit",
      "mindmap": ""
    },
    {
      "title": "DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning",
      "authors": "Junho Yoon, Jaemo Jung, Hyunju Kim, Dongman Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20409",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d4c944d1aa99e38d69c83c388503646f62271c8bb649aafcafb57ad0ba7c7d0_w640_q70.webp",
      "contributions": "",
      "summary": "DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning",
      "mindmap": ""
    },
    {
      "title": "Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale",
      "authors": "Linfeng Zhang, Siheng Chen, Yuzhu Cai, Jingyi Chai, Junhan Chang, Kun Chen, Zhi X. Chen, Zhaohan Ding, Yuwen Du, Yuanpeng Gao, Yuan Gao, Jing Gao, Zhifeng Gao, Qiangqiang Gu, Yanhui Hong, Yuan Huang, Xi Fang, Xiaohong Ji, Guolin Ke, Zixing Lei, Xinyu Li, Yongge Li, Ruoxue Liao, Hang Lin, Xiaolu Lin, Yuxiang Liu, Xinzijian Liu, Zexi Liu, Jintan Lu, Tingjia Miao, Haohui Que, Weijie Sun, Yanfeng Wang, Bingyang Wu, Tianju Xue, Rui Ye, Jinzhe Zeng, Duo Zhang, Jiahui Zhang, Linfeng Zhang, Tianhan Zhang, Wenchang Zhang, Yuzhi Zhang, Zezhong Zhang, Hang Zheng, Hui Zhou, Tong Zhu, Xinyu Zhu, Qingguo Zhou, Weinan E",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20469",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efbb4cf73bd5e97a426fe07fa2444d9fce83c1cc337fc7a02676141bf805fbd9_w640_q70.webp",
      "contributions": "",
      "summary": "Bohrium + SciMaster: Building the Infrastructure and Ecosystem for Agentic Science at Scale",
      "mindmap": ""
    },
    {
      "title": "SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization",
      "authors": "Revanth Gangi Reddy, Ye Liu, Wenting Zhao, JaeHyeok Doo, Tarun Suresh, Daniel Lee, Caiming Xiong, Yingbo Zhou, Semih Yavuz, Shafiq Joty",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20482",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61a2f4887d3f7e1f76ef9da213d8cbb2fd86e68bd8a8206e3a2e53677aa7151e_w640_q70.webp",
      "contributions": "",
      "summary": "SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization",
      "mindmap": ""
    },
    {
      "title": "Benchmarking LLMs for Predictive Applications in the Intensive Care Units",
      "authors": "Chehak Malhotra, Mehak Gopal, Akshaya Devadiga, Pradeep Singh, Ridam Pal, Ritwik Kashyap, Tavpritesh Sethi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20520",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/274c8c98260b88efe758b67759790448f6c4f5abe21a32dafdc4d3a311f9524b_w640_q70.webp",
      "contributions": "",
      "summary": "Benchmarking LLMs for Predictive Applications in the Intensive Care Units",
      "mindmap": ""
    },
    {
      "title": "Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model",
      "authors": "Zhiyi Duan, Xiangren Wang, Hongyu Yuan, Qianli Xing",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20548",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb1b5cafe85ffb670c8629adfcf70c1f2a8ef006559d14cc0f3ad75463909ad8_w640_q70.webp",
      "contributions": "",
      "summary": "Advancing Multimodal Teacher Sentiment Analysis:The Large-Scale T-MED Dataset & The Effective AAM-TSA Model",
      "mindmap": ""
    },
    {
      "title": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving",
      "authors": "Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20563",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp",
      "contributions": "",
      "summary": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving",
      "mindmap": ""
    },
    {
      "title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection",
      "authors": "Yanhong Li, Songlin Yang, Shawn Tan, Mayank Mishra, Rameswar Panda, Jiawei Zhou, Yoon Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20569",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e36c08fad9c560eeacd24d61bbc8fc4ace2f57a4dda4d1eaeb59a63b10f01d2e_w640_q70.webp",
      "contributions": "",
      "summary": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection",
      "mindmap": ""
    },
    {
      "title": "Performative Policy Gradient: Optimality in Performative Reinforcement Learning",
      "authors": "Debabrota Basu, Udvas Das, Brahim Driss, Uddalak Mukherjee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20576",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef3954869c8b30fe77c2caa1356c077d9f6b214d512935393a84011f79c0d20f_w640_q70.webp",
      "contributions": "",
      "summary": "Performative Policy Gradient: Optimality in Performative Reinforcement Learning",
      "mindmap": ""
    },
    {
      "title": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
      "authors": "Rui Pan, Zhuofu Chen, Ravi Netravali",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20573",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab6d51f1421d2f929752b850a0d24b6a7af807b50f1d54c1101b257d175a44f_w640_q70.webp",
      "contributions": "",
      "summary": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
      "mindmap": ""
    },
    {
      "title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent",
      "authors": "Humza Nusrat, Luke Francisco, Bing Luo, Hassan Bagher-Ebadian, Joshua Kim, Karen Chin-Snyder, Salim Siddiqui, Mira Shah, Eric Mellon, Mohammad Ghassemi, Anthony Doemer, Benjamin Movsas, Kundan Thind",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20586",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39917d1df3de96bd690d947b78c3c6d1ac037b54cc0591faa464cbc08cd8c729_w640_q70.webp",
      "contributions": "",
      "summary": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent",
      "mindmap": ""
    },
    {
      "title": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs",
      "authors": "Dhruv Anand, Ehsan Shareghi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20595",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb95f031dbfb3f7bfc348a6d3e77d5c3ae7e2408f06dd57529b77764de0ce0b7_w640_q70.webp",
      "contributions": "",
      "summary": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs",
      "mindmap": ""
    },
    {
      "title": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
      "authors": "Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel, Angelika Steger, Maciej Wolczyk, Johannes von Oswald, Nino Scherre, Kaitlin Maile, Guillaume Lajoie, Blake A. Richards, Rif A. Saurous, James Manyika, Blaise Agüera y Arcas, Alexander Meulemans, João Sacramento",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20605",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e252cb57b3bace513337e4bc66ce47050af3dedc086216816f29d838d083c5f_w640_q70.webp",
      "contributions": "",
      "summary": "Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning",
      "mindmap": ""
    },
    {
      "title": "Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information",
      "authors": "İbrahim Oğuz Çetinkaya, Sajad Khodadadian, Taylan G. Topçu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20589",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bd8f6dd1aa27848e72f81ba7279a1abe238ea198e2b3aa7513fc9ca373e7554_w640_q70.webp",
      "contributions": "",
      "summary": "Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information",
      "mindmap": ""
    },
    {
      "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
      "authors": "Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20618",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp",
      "contributions": "",
      "summary": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
      "mindmap": ""
    },
    {
      "title": "Generative AI for Analysts",
      "authors": "Jian Xue, Qian Zhang, Wu Zhu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19705",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c446bca4991916c167230d9d5b7a57cb785d6214690dca03a9aacef2ebddb67_w640_q70.webp",
      "contributions": "",
      "summary": "Generative AI for Analysts",
      "mindmap": ""
    },
    {
      "title": "QMBench: A Research Level Benchmark for Quantum Materials Research",
      "authors": "Yanzhen Wang, Yiyang Jiang, Diana Golovanova, Kamal Das, Hyeonhu Bae, Yufei Zhao, Huu-Thong Le, Abhinava Chatterjee, Yunzhe Liu, Chao-Xing Liu, Felipe H. da Jornada, Binghai Yan, Xiao-Liang Qi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19753",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a5901ce24dd3558937f8fb69f70cd14d0da814e05be0cd205e3d5fb0d138661_w640_q70.webp",
      "contributions": "",
      "summary": "QMBench: A Research Level Benchmark for Quantum Materials Research",
      "mindmap": ""
    },
    {
      "title": "Deep Learning Classification of EEG Responses to Multi-Dimensional Transcranial Electrical Stimulation",
      "authors": "Alexis Pomares Pastor, Ines Ribeiro Violante, Gregory Scott",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20319",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcb7898ab29ae44cef389712101b2dbc98a334b5761ab3b2e5825364d8f3f316_w640_q70.webp",
      "contributions": "",
      "summary": "Deep Learning Classification of EEG Responses to Multi-Dimensional Transcranial Electrical Stimulation",
      "mindmap": ""
    },
    {
      "title": "Regression of Functions by Quantum Neural Networks Circuits",
      "authors": "Fernando M. de Paula Neto, Lucas dos Reis Silva, Paulo S. G. de Mattos Neto, Felipe F. Fanchini",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19978",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad43eefe70b19a60b4cb6a98a6af78a755ad48beb67110a54813ba3e49fc1d5e_w640_q70.webp",
      "contributions": "",
      "summary": "Regression of Functions by Quantum Neural Networks Circuits",
      "mindmap": ""
    },
    {
      "title": "Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI",
      "authors": "Muhammad Usman, Azka Rehman, Muhammad Mutti Ur Rehman, Abd Ur Rehman, Muhammad Umar Farooq",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20436",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efdfcdd8483dd2617be0701188cb1e86708fc680b1516caff77f5075ed2baf49_w640_q70.webp",
      "contributions": "",
      "summary": "Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI",
      "mindmap": ""
    },
    {
      "title": "Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning",
      "authors": "Lihui Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17912",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/154351bb01594c209c639a3724124babafa831a2c5526b2f6bb79e4ec436950a_w640_q70.webp",
      "contributions": "",
      "summary": "Graph-O1 : Monte Carlo Tree Search with Reinforcement Learning for Text-Attributed Graph Reasoning",
      "mindmap": ""
    },
    {
      "title": "Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression",
      "authors": "Rahul Baxi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17920",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e85dcb740e46985e03fe90bf075468b334e1528a3de2a4858fac5b2ddbc2dc9_w640_q70.webp",
      "contributions": "",
      "summary": "Separating Constraint Compliance from Semantic Accuracy: A Novel Benchmark for Evaluating Instruction-Following Under Compression",
      "mindmap": ""
    },
    {
      "title": "Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation",
      "authors": "Nihir Chadderwala",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17913",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ebf691e752728d3961062fe7df081d6a92c7729c4f9968ddd1c3f083bb93df_w640_q70.webp",
      "contributions": "",
      "summary": "Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation",
      "mindmap": ""
    },
    {
      "title": "Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models",
      "authors": "Hongji Li, Junchi yao, Manjiang Yu, Priyanka Singh, Xue Li, Di Wang, Lijie Hu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17911",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/218d6b0cd750a67af41f0ec36e744aed0a36e9ad83656c3a4c9ed70aa75b977d_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Reasoning-Preserving Unlearning in Multimodal Large Language Models",
      "mindmap": ""
    },
    {
      "title": "Learning to Prioritize IT Tickets: A Comparative Evaluation of Embedding-based Approaches and Fine-Tuned Transformer Models",
      "authors": "Minh Tri LÊ, Ali Ait-Bachir",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17916",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a9a26225e6a2c495c48df9cb6a0e4bd0c624c036e56d8d0cf9885d0d909a745_w640_q70.webp",
      "contributions": "",
      "summary": "Learning to Prioritize IT Tickets: A Comparative Evaluation of Embedding-based Approaches and Fine-Tuned Transformer Models",
      "mindmap": ""
    },
    {
      "title": "KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction",
      "authors": "Aomufei Yuan, Zhiming Wang, Ruijie Miao, Dayu Wang, Yuxuan Tian, Zihan Wang, Yebo Peng, Yuhan Wu, Bairen Yi, Xin Liu, Tong Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17917",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ca40de411c47ab6f32fb67699fbcdce0808ef0d5179742bf46c64d088a640d3_w640_q70.webp",
      "contributions": "",
      "summary": "KVReviver: Reversible KV Cache Compression with Sketch-Based Token Reconstruction",
      "mindmap": ""
    },
    {
      "title": "Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA",
      "authors": "Allison Li, Kristjan Greenewald, Thomas Parnell, Navid Azizan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17910",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69e52c70ff632453dafd08b1f3a3463c9d2df49fdb8300df0a3e128192436717_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA",
      "mindmap": ""
    },
    {
      "title": "Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU",
      "authors": "Bin Xu, Ayan Banerjee, Midhat Urooj, Sandeep K.S. Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17941",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8208ecda566e64a777495316e0aac16897f35ec183a5fb04050258d853af7cf7_w640_q70.webp",
      "contributions": "",
      "summary": "Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU",
      "mindmap": ""
    },
    {
      "title": "Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition",
      "authors": "Haiying Xia, Zhongyi Huang, Yumei Tan, Shuxiang Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17946",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ae2757814d2b16837d9b7cb3f26503eda8c49afc87dd1795f588ae949df6650_w640_q70.webp",
      "contributions": "",
      "summary": "Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition",
      "mindmap": ""
    },
    {
      "title": "Which Coauthor Should I Nominate in My 99 ICLR Submissions? A Mathematical Analysis of the ICLR 2026 Reciprocal Reviewer Nomination Policy",
      "authors": "Zhao Song, Song Yue, Jiahao Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17950",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21bbafaf75367ac515be62999bf183f2dc9b58cb6de4b044d5e95331ef1cf1ed_w640_q70.webp",
      "contributions": "",
      "summary": "Which Coauthor Should I Nominate in My 99 ICLR Submissions? A Mathematical Analysis of the ICLR 2026 Reciprocal Reviewer Nomination Policy",
      "mindmap": ""
    },
    {
      "title": "NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction",
      "authors": "Karthik Prabhakar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17943",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a630b7d9810838c6059574b3dae2d2f3fcc9c79699030e8640202f2dbcd2d4b0_w640_q70.webp",
      "contributions": "",
      "summary": "NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction",
      "mindmap": ""
    },
    {
      "title": "Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States",
      "authors": "Soheil Hashtarkhani, Brianna M. White, Benyamin Hoseini, David L. Schwartz, Arash Shaban-Nejad",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17934",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76d1304c99e1089257b9c3f4d81ee78901872f3818b6e4743be9843bd9a60488_w640_q70.webp",
      "contributions": "",
      "summary": "Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States",
      "mindmap": ""
    },
    {
      "title": "Will AI Trade? A Computational Inversion of the No-Trade Theorem",
      "authors": "Hanyu Li, Xiaotie Deng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17952",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ad1a13c7765317192c2b83746186b421990df4011bf9cad5b20f5624cf6ca35_w640_q70.webp",
      "contributions": "",
      "summary": "Will AI Trade? A Computational Inversion of the No-Trade Theorem",
      "mindmap": ""
    },
    {
      "title": "Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration",
      "authors": "Victor Stasiuc, Round Table Collaboration",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17956",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57a72b745466d47b8a8056586ccf86e4e40bd0ac42a76b0061c6576b563f4532_w640_q70.webp",
      "contributions": "",
      "summary": "Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration",
      "mindmap": ""
    },
    {
      "title": "Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition",
      "authors": "Ellie Zhou, Jihoon Chung, Olga Russakovsky",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17953",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8fc5229281981de8f8ce6b4446b9b416a3bf03a5b7dfe015f7327ed14da6c6c_w640_q70.webp",
      "contributions": "",
      "summary": "Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition",
      "mindmap": ""
    },
    {
      "title": "CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs",
      "authors": "Gunho Park, Jeongin Bae, Byeongwook Kim, Baeseong park, Jiwon Ryu, Hoseung Kim, Se Jung Kwon, Dongsoo Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17970",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8eee89dc0a0f6c26eab8715e73b23d4aa516792d2237ec4f38c8323363f37c37_w640_q70.webp",
      "contributions": "",
      "summary": "CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs",
      "mindmap": ""
    },
    {
      "title": "Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis",
      "authors": "Matthieu Mastio, Paul Saves, Benoit Gaudou, Nicolas Verstaevel",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17979",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43cecfb66c204c7d6add4e7c40f9325f7867c22be2179de9305d482292a7a4dd_w640_q70.webp",
      "contributions": "",
      "summary": "Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis",
      "mindmap": ""
    },
    {
      "title": "Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization",
      "authors": "Farida Mohsen, Ali Safa",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17958",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf18ccdf18f30e0e5c324fc709aa108b4706cb9c6a2b56366e90ad3a8bd1a32c_w640_q70.webp",
      "contributions": "",
      "summary": "Real-Time Human-Robot Interaction Intent Detection Using RGB-based Pose and Emotion Cues with Cross-Camera Model Generalization",
      "mindmap": ""
    },
    {
      "title": "Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models",
      "authors": "Irina Seregina, Philippe Lalanda, German Vega",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17983",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/747125a80395e9d95ec80efcc81570ba4ba7205e4e2c8e9b485a5b5a991124d6_w640_q70.webp",
      "contributions": "",
      "summary": "Parameter-Efficient Fine-Tuning for HAR: Integrating LoRA and QLoRA into Transformer Models",
      "mindmap": ""
    },
    {
      "title": "Convolutional-neural-operator-based transfer learning for solving PDEs",
      "authors": "Peng Fan, Guofei Pang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17969",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b48d3312e2a3a21fec95790b71774b617dbbb16d924ea92ea392f72deaedd12_w640_q70.webp",
      "contributions": "",
      "summary": "Convolutional-neural-operator-based transfer learning for solving PDEs",
      "mindmap": ""
    },
    {
      "title": "ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India",
      "authors": "Shubham Kumar Nigam, Tanuj Tyagi, Siddharth Shukla, Aditya Kumar Guru, Balaramamahanthi Deepak Patnaik, Danush Khanna, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18014",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a0d10da8d503938592e2a709885e1a4ad114f85d7b47234084835f143d49cd6_w640_q70.webp",
      "contributions": "",
      "summary": "ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India",
      "mindmap": ""
    },
    {
      "title": "Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models",
      "authors": "Shubham Kumar Nigam, Parjanya Aditya Shukla, Noel Shallum, Arnab Bhattacharya",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18004",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2ffa85f9a5ddd1bd5f673a211deae55a7bad9087838680e7b143e6daac52df8_w640_q70.webp",
      "contributions": "",
      "summary": "Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models",
      "mindmap": ""
    },
    {
      "title": "Specification and Detection of LLM Code Smells",
      "authors": "Brahim Mahmoudi, Zacharie Chenail-Larcher, Naouel Moha, Quentin Stievenert, Florent Avellaneda",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18020",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cee6a3c076b484391912c8b6083413504d86a9475e81592b3004ce305e4f9c4_w640_q70.webp",
      "contributions": "",
      "summary": "Specification and Detection of LLM Code Smells",
      "mindmap": ""
    },
    {
      "title": "A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients",
      "authors": "Sarah Nassar, Nooshin Maghsoodi, Sophia Mannina, Shamel Addas, Stephanie Sibley, Gabor Fichtinger, David Pichora, David Maslove, Purang Abolmaesumi, Parvin Mousavi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18031",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea7240d48041f3aeed8ef43430092d594f522d0dd6c9a8de4d3b6236fccaebc9_w640_q70.webp",
      "contributions": "",
      "summary": "A Dataset and Benchmarks for Atrial Fibrillation Detection from Electrocardiograms of Intensive Care Unit Patients",
      "mindmap": ""
    },
    {
      "title": "Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout",
      "authors": "Joshua Gibson, Kapil Dhakal",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18034",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee8143ffe65f15c2c3477c3c466f8d3d7e3d40519caf5667a1e168e357e8ce6b_w640_q70.webp",
      "contributions": "",
      "summary": "Conflict-Driven Clause Learning with VSIDS Heuristics for Discrete Facility Layout",
      "mindmap": ""
    },
    {
      "title": "A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations",
      "authors": "Mohammadmahdi Rahimiasl, Ynte Vanderhoydonc, Siegfried Mercelis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17984",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a712275d1fc83e7fd5ac0076a27da3a080f91e229b14209ff99a52de68ce9c2_w640_q70.webp",
      "contributions": "",
      "summary": "A Hybrid Inductive-Transductive Network for Traffic Flow Imputation on Unsampled Locations",
      "mindmap": ""
    },
    {
      "title": "Securing Agentic AI Systems -- A Multilayer Security Framework",
      "authors": "Sunil Arora, John Hastings",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18043",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96ca042b8b2e4295fa813434a544ac9adb3df55cbea55e3fe9b24f2dcbee4733_w640_q70.webp",
      "contributions": "",
      "summary": "Securing Agentic AI Systems -- A Multilayer Security Framework",
      "mindmap": ""
    },
    {
      "title": "FOODER: Real-time Facial Authentication and Expression Recognition",
      "authors": "Sabri Mustafa Kahya, Muhammet Sami Yavuz, Boran Hamdi Sivrikaya, Eckehard Steinbach",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18057",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17048a3dede1c165a0f52aab61de33bef5559518cf2996ad61858f9a9d680457_w640_q70.webp",
      "contributions": "",
      "summary": "FOODER: Real-time Facial Authentication and Expression Recognition",
      "mindmap": ""
    },
    {
      "title": "From Prompt to Product: A Human-Centered Benchmark of Agentic App Generation Systems",
      "authors": "Marcos Ortiz, Justin Hill, Collin Overbay, Ingrida Semenec, Frederic Sauve-Hoover, Jim Schwoebel, Joel Shor",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18080",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0154cb62824a09bcbf4a6476b89c563b0f548b2e6721f62b4207082ab09ee544_w640_q70.webp",
      "contributions": "",
      "summary": "From Prompt to Product: A Human-Centered Benchmark of Agentic App Generation Systems",
      "mindmap": ""
    },
    {
      "title": "Characterising Behavioural Families and Dynamics of Promotional Twitter Bots via Sequence-Based Modelling",
      "authors": "Ohoud Alzahrani, Russell Beale, Robert J. Hendley",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18077",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ecf679a26dc73049a43a2ec8c3b4b9a5b43ae16a82684041393594d146210f5_w640_q70.webp",
      "contributions": "",
      "summary": "Characterising Behavioural Families and Dynamics of Promotional Twitter Bots via Sequence-Based Modelling",
      "mindmap": ""
    },
    {
      "title": "Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation",
      "authors": "Shreshth Rajan, Raymond Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18082",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dd5ca9b561b8ad179709036acde1f313672cfa0c00a271b9235f8ab0e640d0a_w640_q70.webp",
      "contributions": "",
      "summary": "Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation",
      "mindmap": ""
    },
    {
      "title": "Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability",
      "authors": "Ge Yan, Tuomas Oikarinen, Tsui-Wei, Weng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18092",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7b2abff42613036ddb63e979f8be79c1123b50e037d39defec5292f1a3eb175_w640_q70.webp",
      "contributions": "",
      "summary": "Faithful and Stable Neuron Explanations for Trustworthy Mechanistic Interpretability",
      "mindmap": ""
    },
    {
      "title": "Rethinking Multi-Agent Intelligence Through the Lens of Small-World Networks",
      "authors": "Boxuan Wang, Zhuoyun Li, Xiaowei Huang, Yi Dong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18094",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d3b6262ae305fdec9209648a021429405f004c2b9a531305c5c02be4396c55f_w640_q70.webp",
      "contributions": "",
      "summary": "Rethinking Multi-Agent Intelligence Through the Lens of Small-World Networks",
      "mindmap": ""
    },
    {
      "title": "Holistic Evaluation of State-of-the-Art LLMs for Code Generation",
      "authors": "Le Zhang, Suresh Kothari",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18131",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e08a9f551315e560db6179abac192d75f62ffe1b185263cb90784842012ac802_w640_q70.webp",
      "contributions": "",
      "summary": "Holistic Evaluation of State-of-the-Art LLMs for Code Generation",
      "mindmap": ""
    },
    {
      "title": "Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap",
      "authors": "Zijun Wang, Yijiahao Qi, Hanqiu Chen, Zishen Wan, Gongjin Sun, Dongyang Li, Shuyi Pei, Cong Hao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18126",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7254830672e4c2c139708d42673d4462ea8f38cc986ab26672ee4838f74c1458_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Mixture-of-Agents Serving via Tree-Structured Routing, Adaptive Pruning, and Dependency-Aware Prefill-Decode Overlap",
      "mindmap": ""
    },
    {
      "title": "Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection",
      "authors": "Jie Yang, Rui Zhang, Ziyang Cheng, Dawei Cheng, Guang Yang, Bo Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18133",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b8052788abbad008c8487b789c1968d0448bbef8cdd15ad400f0c6303c23505_w640_q70.webp",
      "contributions": "",
      "summary": "Grad: Guided Relation Diffusion Generation for Graph Augmentation in Graph Fraud Detection",
      "mindmap": ""
    },
    {
      "title": "Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications",
      "authors": "Cristiano da Costa Cunha, Wei Liu, Tim French, Ajmal Mian",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18135",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afd713c843807b61ef24ae3fe42d41c20e6fbaeaff735c0c77378e6c26ab19a6_w640_q70.webp",
      "contributions": "",
      "summary": "Unifying Causal Reinforcement Learning: Survey, Taxonomy, Algorithms and Applications",
      "mindmap": ""
    },
    {
      "title": "On Swarm Leader Identification using Probing Policies",
      "authors": "Stergios E. Bachoumas, Panagiotis Artemiadis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18146",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/584c84764226eb21b4c2c555cf6432ccfe102a3aa9dcc530809f19d78d76d7ac_w640_q70.webp",
      "contributions": "",
      "summary": "On Swarm Leader Identification using Probing Policies",
      "mindmap": ""
    },
    {
      "title": "Propose, Solve, Verify: Self-Play Through Formal Verification",
      "authors": "Alex Wilf, Pranjal Aggarwal, Bryan Parno, Daniel Fried, Louis-Philippe Morency, Paul Pu Liang, Sean Welleck",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18160",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/352216a02c2bd4fb599d9561a7994cb21ba9b1ce3f9f8b8cfea3ec4cc0b8413d_w640_q70.webp",
      "contributions": "",
      "summary": "Propose, Solve, Verify: Self-Play Through Formal Verification",
      "mindmap": ""
    },
    {
      "title": "NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI",
      "authors": "Midhat Urooj, Ayan Banerjee, Sandeep Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18177",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7ef165e056ae631599bd024eb4341c57c1705258598a82662ae1166302f2947_w640_q70.webp",
      "contributions": "",
      "summary": "NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI",
      "mindmap": ""
    },
    {
      "title": "External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning",
      "authors": "Jian Yan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18190",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebbd501809c39f1f08745f358ebdf2df886f93b7202aeedbd613476ffb27866b_w640_q70.webp",
      "contributions": "",
      "summary": "External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning",
      "mindmap": ""
    },
    {
      "title": "PROVEX: Enhancing SOC Analyst Trust with Explainable Provenance-Based IDS",
      "authors": "Devang Dhanuka, Nidhi Rastogi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18199",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7026a8c8f552664c28724b8716e8e7e5cef6b29425c374a1c5439f2bfb877d5f_w640_q70.webp",
      "contributions": "",
      "summary": "PROVEX: Enhancing SOC Analyst Trust with Explainable Provenance-Based IDS",
      "mindmap": ""
    },
    {
      "title": "NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework",
      "authors": "Zihao Deng, Yijia Li, Renrui Zhang, Peijun Ye",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18189",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee57353b28c6f902f2d47ba6499f6f3db69fe4e0b025fa2fd4ed11eff3c1c003_w640_q70.webp",
      "contributions": "",
      "summary": "NL2CA: Auto-formalizing Cognitive Decision-Making from Natural Language Using an Unsupervised CriticNL2LTL Framework",
      "mindmap": ""
    },
    {
      "title": "Sophia: A Persistent Agent Framework of Artificial Life",
      "authors": "Mingyang Sun, Feng Hong, Weinan Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18202",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c33289396399b1bb0e301fb010ed82c6a686d9391a9307867ae3c7ae74a8cc3_w640_q70.webp",
      "contributions": "",
      "summary": "Sophia: A Persistent Agent Framework of Artificial Life",
      "mindmap": ""
    },
    {
      "title": "When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics",
      "authors": "Yizhou Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18209",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5b09e099602878faf6cf44441a1e029c64f15985e3d209f1875434cd54da61a_w640_q70.webp",
      "contributions": "",
      "summary": "When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics",
      "mindmap": ""
    },
    {
      "title": "Stable and Efficient Single-Rollout RL for Multimodal Reasoning",
      "authors": "Rui Liu, Dian Yu, Lei Ke, Haolin Liu, Yujun Zhou, Zhenwen Liang, Haitao Mi, Pratap Tokekar, Dong Yu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18215",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp",
      "contributions": "",
      "summary": "Stable and Efficient Single-Rollout RL for Multimodal Reasoning",
      "mindmap": ""
    },
    {
      "title": "LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning",
      "authors": "Yudong Liu, Spencer Hallyburton, Jiwoo Kim, Yueqian Lin, Yiming Li, Qinsi Wang, Hui Ye, Jingwei Sun, Miroslav Pajic, Yiran Chen, Hai Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18211",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9eea86ac8cc855b1a0e43febafd0f1e08626f900ba3ded294c1d99b0072a7da3_w640_q70.webp",
      "contributions": "",
      "summary": "LLaViDA: A Large Language Vision Driving Assistant for Explicit Reasoning and Enhanced Trajectory Planning",
      "mindmap": ""
    },
    {
      "title": "Breaking Minds, Breaking Systems: Jailbreaking Large Language Models via Human-like Psychological Manipulation",
      "authors": "Zehao Liu, Xi Lin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18244",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a4ab3d36e0ccc56b5c8f0b5a9f82481bdfdc1f1bc14e54959edf8ba4006a00d_w640_q70.webp",
      "contributions": "",
      "summary": "Breaking Minds, Breaking Systems: Jailbreaking Large Language Models via Human-like Psychological Manipulation",
      "mindmap": ""
    },
    {
      "title": "Offline Behavioral Data Selection",
      "authors": "Shiye Lei, Zhihao Cheng, Dacheng Tao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18246",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02b464fb04b52640bf81b61d9a535a31cd920c00fd8a2dd34cd4b261366adac1_w640_q70.webp",
      "contributions": "",
      "summary": "Offline Behavioral Data Selection",
      "mindmap": ""
    },
    {
      "title": "Intelligent Human-Machine Partnership for Manufacturing: Enhancing Warehouse Planning through Simulation-Driven Knowledge Graphs and LLM Collaboration",
      "authors": "Himabindu Thogaru, Saisubramaniam Gopalakrishnan, Zishan Ahmad, Anirudh Deodhar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18265",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50a73b0ae1606916d8e1e7324c2baa8154f67eb8e76c0e1f1c0d014d54aa2ab2_w640_q70.webp",
      "contributions": "",
      "summary": "Intelligent Human-Machine Partnership for Manufacturing: Enhancing Warehouse Planning through Simulation-Driven Knowledge Graphs and LLM Collaboration",
      "mindmap": ""
    },
    {
      "title": "Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image",
      "authors": "Xiao He, Chang Tang, Xinwang Liu, Wei Zhang, Zhimin Gao, Chuankun Li, Shaohua Qiu, Jiangfeng Xu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18245",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028cfc83b63f51bf798c2014f1dc8e1f4c786134910334880eb4c0cf340a5eb_w640_q70.webp",
      "contributions": "",
      "summary": "Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image",
      "mindmap": ""
    },
    {
      "title": "Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks",
      "authors": "Yucheng Fan, Jiawei Chen, Yu Tian, Zhaoxia Yin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18264",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6181175b477e5c50e93e9a1a3a4690fb1673471073933b770754e208c7b0e776_w640_q70.webp",
      "contributions": "",
      "summary": "Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks",
      "mindmap": ""
    },
    {
      "title": "Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model",
      "authors": "Rui Xing, Runmin Cong, Yingying Wu, Can Wang, Zhongming Tang, Fen Wang, Hao Wu, Sam Kwong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18247",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d341345aee67211dc7e3cea021a11051fd66231eb649e4da442fda6828319f0d_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model",
      "mindmap": ""
    },
    {
      "title": "MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification",
      "authors": "Sirui Li, Wangyue Lu, Xiaorui Shi, Ke Weng, Haozhe Sun, Minghe Yu, Tiancheng Zhang, Ge Yu, Hengyu Liu, Lun Du",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18256",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fa370d48e50a94f7beaa30df78416e8de75e5bddd8996af2ad55d2751ed49c0_w640_q70.webp",
      "contributions": "",
      "summary": "MSC-180: A Benchmark for Automated Formal Theorem Proving from Mathematical Subject Classification",
      "mindmap": ""
    },
    {
      "title": "Software Vulnerability Management in the Era of Artificial Intelligence: An Industry Perspective",
      "authors": "M. Mehdi Kholoosi, Triet Huynh Minh Le, M. Ali Babar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18261",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c527700e49403d8d115fcd9b121714ac7cc66ca4c4917d88ae5eeb6712cf705e_w640_q70.webp",
      "contributions": "",
      "summary": "Software Vulnerability Management in the Era of Artificial Intelligence: An Industry Perspective",
      "mindmap": ""
    },
    {
      "title": "AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning",
      "authors": "Xuling Zhang, Jindong Li, Yifei Zhang, Menglin Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18295",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d219b11c5c859da88d8f68475d0f7f0705331024e0e77eb5124bfa1124849df9_w640_q70.webp",
      "contributions": "",
      "summary": "AL-GNN: Privacy-Preserving and Replay-Free Continual Graph Learning via Analytic Learning",
      "mindmap": ""
    },
    {
      "title": "Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings",
      "authors": "Harsh Rathva, Ojas Srivastava, Pruthwik Mishra",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18309",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c69f269f400e588bb27a40eea78dc7e63a0f8f1b86f9ab29abf255d7a91b0308_w640_q70.webp",
      "contributions": "",
      "summary": "Embedded Safety-Aligned Intelligence via Differentiable Internal Alignment Embeddings",
      "mindmap": ""
    },
    {
      "title": "Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems",
      "authors": "Vincent Bezold, Patrick Wagner, Jakob Hofmann, Marco Huber, Alexander Sauer",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18317",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c40f961221ca3df8f5c9388e76d344938990a3d405e09580d23abf68a4da0f57_w640_q70.webp",
      "contributions": "",
      "summary": "Trustworthy and Explainable Deep Reinforcement Learning for Safe and Energy-Efficient Process Control: A Use Case in Industrial Compressed Air Systems",
      "mindmap": ""
    },
    {
      "title": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
      "authors": "Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18318",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp",
      "contributions": "",
      "summary": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
      "mindmap": ""
    },
    {
      "title": "Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)",
      "authors": "Youssef Mahran, Zeyad Gamal, Ayman El-Badawy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18333",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f5c0778f6eace42568ad8fc221a69e1cf4360df09bb1bb286e34299351febe0_w640_q70.webp",
      "contributions": "",
      "summary": "Reinforcement Learning Position Control of a Quadrotor Using Soft Actor-Critic (SAC)",
      "mindmap": ""
    },
    {
      "title": "Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism",
      "authors": "Youssef Mahran, Zeyad Gamal, Ayman El-Badawy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18336",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d046b180bb4bec9247b6bb525bd30b5814a0c5e4673bd03aea3eb64b9797e7_w640_q70.webp",
      "contributions": "",
      "summary": "Dynamic Entropy Tuning in Reinforcement Learning Low-Level Quadcopter Control: Stochasticity vs Determinism",
      "mindmap": ""
    },
    {
      "title": "Monitoring Monitorability",
      "authors": "Melody Y. Guan, Miles Wang, Micah Carroll, Zehao Dou, Annie Y. Wei, Marcus Williams, Benjamin Arnav, Joost Huizinga, Ian Kivlichan, Mia Glaese, Jakub Pachocki, Bowen Baker",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18311",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b80143b15f0287eaa0d31decbf1a350d64c8110ec245d21e81c64ae73cd6febc_w640_q70.webp",
      "contributions": "",
      "summary": "Monitoring Monitorability",
      "mindmap": ""
    },
    {
      "title": "MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation",
      "authors": "Zhiheng Zhang, Jiajun Yang, Hong Sun, Dong Wang, Honghua Jiang, Yaru Chen, Tangyuan Ning",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18344",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f19d6aa9f347693da897453773133b3a6f0b66ee06e5f6bdf421bb24507e5f56_w640_q70.webp",
      "contributions": "",
      "summary": "MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation",
      "mindmap": ""
    },
    {
      "title": "LLM-based Few-Shot Early Rumor Detection with Imitation Agent",
      "authors": "Fengzhu Zeng, Qian Shao, Ling Cheng, Wei Gao, Shih-Fen Cheng, Jing Ma, Cheng Niu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18352",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68253881479be80c6f5156b8929dcea7c9affda2463411703dbff80daa9a6787_w640_q70.webp",
      "contributions": "",
      "summary": "LLM-based Few-Shot Early Rumor Detection with Imitation Agent",
      "mindmap": ""
    },
    {
      "title": "LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators",
      "authors": "Mateusz Lango, Ondřej Dušek",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18360",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e70af4d7e4c119643e3631c8815a5d46438a6f1b8881a5821764fb495f8608f_w640_q70.webp",
      "contributions": "",
      "summary": "LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators",
      "mindmap": ""
    },
    {
      "title": "Datasets for machine learning and for assessing the intelligence level of automatic patent search systems",
      "authors": "Boris Genin, Alexander Gorbunov, Dmitry Zolkin, Igor Nekrasov",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18384",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ac80e35326c113d249d5bd28c4aafe01da23a8e0c681cde84257daab6b92b651_w640_q70.webp",
      "contributions": "",
      "summary": "Datasets for machine learning and for assessing the intelligence level of automatic patent search systems",
      "mindmap": ""
    },
    {
      "title": "Exploration vs. Fixation: Scaffolding Divergent and Convergent Thinking for Human-AI Co-Creation with Generative Models",
      "authors": "Chao Wen, Tung Phung, Pronita Mehrotra, Sumit Gulwani, Tomohiro Nagashima, Adish Singla",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18388",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26ae3275661776caa174169c0f345d17624c81b4d8a6d11a881528d1b3ebbea4_w640_q70.webp",
      "contributions": "",
      "summary": "Exploration vs. Fixation: Scaffolding Divergent and Convergent Thinking for Human-AI Co-Creation with Generative Models",
      "mindmap": ""
    },
    {
      "title": "Neural Proofs for Sound Verification and Control of Complex Systems",
      "authors": "Alessandro Abate",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18389",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d4271cbf46eee03fa72db8edd09dea5b0753448d820460e4c09f1171c7bc8f8_w640_q70.webp",
      "contributions": "",
      "summary": "Neural Proofs for Sound Verification and Control of Complex Systems",
      "mindmap": ""
    },
    {
      "title": "AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3",
      "authors": "Mark Kashirskiy, Artiom Lipinski, Ilya Makarov",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18399",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb721fc55d6ea689ae069e1c50129b86f7fc25c4c3433aa154aaa38bf9378cd3_w640_q70.webp",
      "contributions": "",
      "summary": "AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3",
      "mindmap": ""
    },
    {
      "title": "AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning",
      "authors": "Fei Song, Yi Li, Jiangmeng Li, Rui Wang, Changwen Zheng, Fanjiang Xu, Hui Xiong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18411",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c480d382f1a840aaa7b8f0ed1cc380b9ece2b0a04c01a7bfeaf7551356ef5a0c_w640_q70.webp",
      "contributions": "",
      "summary": "AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning",
      "mindmap": ""
    },
    {
      "title": "Few-Shot Learning of a Graph-Based Neural Network Model Without Backpropagation",
      "authors": "Mykyta Lapin, Kostiantyn Bokhan, Yurii Parzhyn",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18412",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccebc716317d25dbc5d826c4045eca3d2894e8904f35d62a55262711b7023239_w640_q70.webp",
      "contributions": "",
      "summary": "Few-Shot Learning of a Graph-Based Neural Network Model Without Backpropagation",
      "mindmap": ""
    },
    {
      "title": "Federated Learning Based Decentralized Adaptive Intelligent Transmission Protocol for Privacy Preserving 6G Networks",
      "authors": "Ansar Ahmed",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18432",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8b17e8fd956e845305266fe6ff75f18289ac1b39f8db27ac25d02758609d8f9_w640_q70.webp",
      "contributions": "",
      "summary": "Federated Learning Based Decentralized Adaptive Intelligent Transmission Protocol for Privacy Preserving 6G Networks",
      "mindmap": ""
    },
    {
      "title": "A Distributed Hierarchical Spatio-Temporal Edge-Enhanced Graph Neural Network for City-Scale Dynamic Logistics Routing",
      "authors": "Zihan Han, Lingran Meng, Jingwei Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18441",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/acf1ec57b519400d58fe519f0aa1f4d66aecd68168a2f192e508b971607f7ef7_w640_q70.webp",
      "contributions": "",
      "summary": "A Distributed Hierarchical Spatio-Temporal Edge-Enhanced Graph Neural Network for City-Scale Dynamic Logistics Routing",
      "mindmap": ""
    },
    {
      "title": "VeruSAGE: A Study of Agent-Based Verification for Rust Systems",
      "authors": "Chenyuan Yang, Natalie Neamtu, Chris Hawblitzel, Jacob R. Lorch, Shan Lu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18436",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a6dcac7fed2f39d9b3c88cf4fcec2c0a341fe5463b697a482bfb914d0610c67_w640_q70.webp",
      "contributions": "",
      "summary": "VeruSAGE: A Study of Agent-Based Verification for Rust Systems",
      "mindmap": ""
    },
    {
      "title": "Snowveil: A Framework for Decentralised Preference Discovery",
      "authors": "Grammateia Kotsialou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18444",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8761ec2c77d131e1f61b3af656dc162d45194670910e8c8975220f93bfc1af6_w640_q70.webp",
      "contributions": "",
      "summary": "Snowveil: A Framework for Decentralised Preference Discovery",
      "mindmap": ""
    },
    {
      "title": "An Agentic AI Framework for Training General Practitioner Student Skills",
      "authors": "Victor De Marez, Jens Van Nooten, Luna De Bruyne, Walter Daelemans",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18440",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/937c34e04859d95b3aed57029c9c5791e5aa2e5a17dbc8d2d6ac7882d0933e37_w640_q70.webp",
      "contributions": "",
      "summary": "An Agentic AI Framework for Training General Practitioner Student Skills",
      "mindmap": ""
    },
    {
      "title": "Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System",
      "authors": "Xavier Rafael-Palou, Jose Munuera, Ana Jimenez-Pastor, Richard Osuala, Karim Lekadir, Oliver Diaz",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18450",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a54a0b377f2d61cdb058b9dd088fe0948517999354f7c502389a3f1149e8a3_w640_q70.webp",
      "contributions": "",
      "summary": "Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System",
      "mindmap": ""
    },
    {
      "title": "MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading",
      "authors": "Shurui Xu, Siqi Yang, Jiapin Ren, Zhong Cao, Hongwei Yang, Mengzhen Fan, Yuyu Sun, Shuyan Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18437",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82297f34fc5f7dfab35d4ab984e7ba607ad5ac2c849bffbec01d38cbdcf42e3d_w640_q70.webp",
      "contributions": "",
      "summary": "MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading",
      "mindmap": ""
    },
    {
      "title": "Secret mixtures of experts inside your LLM",
      "authors": "Enric Boix-Adsera",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18452",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bef90d8febf87f4438ed38a790cb9675b92bf541f58daace4d8c67f4e7b28a1_w640_q70.webp",
      "contributions": "",
      "summary": "Secret mixtures of experts inside your LLM",
      "mindmap": ""
    },
    {
      "title": "SoK: Understanding (New) Security Issues Across AI4Code Use Cases",
      "authors": "Qilong Wu, Taoran Li, Tianyang Zhou, Varun Chandrasekaran",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18456",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f652ae22c8ea45edbe21093e80ef932b29b1703110171928654985064e108e1_w640_q70.webp",
      "contributions": "",
      "summary": "SoK: Understanding (New) Security Issues Across AI4Code Use Cases",
      "mindmap": ""
    },
    {
      "title": "Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling",
      "authors": "Christopher Román Jaimes",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18462",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27cc701f0cd5020bdc7452f202a07cee2cdb12b9d80e26c528184ec53ea76847_w640_q70.webp",
      "contributions": "",
      "summary": "Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling",
      "mindmap": ""
    },
    {
      "title": "SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios",
      "authors": "Minh V. T. Thai, Tue Le, Dung Nguyen Manh, Huy Phan Nhat, Nghi D. Q. Bui",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18470",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3aa84e9ee4a961b04628c969b7317aae944aad76753539183cd0213072cfce14_w640_q70.webp",
      "contributions": "",
      "summary": "SWE-EVO: Benchmarking Coding Agents in Long-Horizon Software Evolution Scenarios",
      "mindmap": ""
    },
    {
      "title": "Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review",
      "authors": "Oraib Almegdadi, João Marcelino, Sarah Fakhreddine, João Manso, Nuno C. Marques",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18466",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bfdf888d3e80d2b65bbc818e6d4aa3a319033086b834ed685478049ddb17232_w640_q70.webp",
      "contributions": "",
      "summary": "Self-organizing maps for water quality assessment in reservoirs and lakes: A systematic literature review",
      "mindmap": ""
    },
    {
      "title": "Large Language Models as Discounted Bayesian Filters",
      "authors": "Jensen Zhang, Jing Yang, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18489",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da72fb53dbbaca8aea568d7b6fc9cba313aa6bb5798a40c89e2a22a5a0788e17_w640_q70.webp",
      "contributions": "",
      "summary": "Large Language Models as Discounted Bayesian Filters",
      "mindmap": ""
    },
    {
      "title": "Enhancing Decision-Making in Windows PE Malware Classification During Dataset Shifts with Uncertainty Estimation",
      "authors": "Rahul Yumlembam, Biju Issac, Seibu Mary Jacob",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18495",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39d0272a8734a4477ae1f6aa46e60e4c4f62aa860ef3a9559c12f0aad180ecb2_w640_q70.webp",
      "contributions": "",
      "summary": "Enhancing Decision-Making in Windows PE Malware Classification During Dataset Shifts with Uncertainty Estimation",
      "mindmap": ""
    },
    {
      "title": "PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs",
      "authors": "Santwana Sagnika, Manav Malhotra, Ishtaj Kaur Deol, Soumyajit Roy, Swarnav Kumar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18500",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3d3024bb6cf729a0d0827d13bba185e065be8eb34bd4dffa40d58782bc4e5ab_w640_q70.webp",
      "contributions": "",
      "summary": "PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs",
      "mindmap": ""
    },
    {
      "title": "Insider Threat Detection Using GCN and Bi-LSTM with Explicit and Implicit Graph Representations",
      "authors": "Rahul Yumlembam, Biju Issac, Seibu Mary Jacob, Longzhi Yang, Deepa Krishnan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18483",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c098db4784c358fb7712e2be8c3c8d57e56fd9982bed1aff3f16699a16acd5_w640_q70.webp",
      "contributions": "",
      "summary": "Insider Threat Detection Using GCN and Bi-LSTM with Explicit and Implicit Graph Representations",
      "mindmap": ""
    },
    {
      "title": "GTMA: Dynamic Representation Optimization for OOD Vision-Language Models",
      "authors": "Jensen Zhang, Ningyuan Liu, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18504",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec81480aabd317eb3778c36f956b818e8aed383ab7f9a13fe2c867243fcec025_w640_q70.webp",
      "contributions": "",
      "summary": "GTMA: Dynamic Representation Optimization for OOD Vision-Language Models",
      "mindmap": ""
    },
    {
      "title": "Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts",
      "authors": "Hatim M. E. Geli, Islam Omar, Mona Y. Elshinawy, David W. DuBios, Lara Prehodko, Kelly H Smith, Abdel-Hameed A. Badawy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18522",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f61621a97ff1a65e5c71fb89cbb1d94dcdecafcb5694c0379f084f31d850ee0_w640_q70.webp",
      "contributions": "",
      "summary": "Prediction and Forecast of Short-Term Drought Impacts Using Machine Learning to Support Mitigation and Adaptation Efforts",
      "mindmap": ""
    },
    {
      "title": "A Formal Descriptive Language for Learning Dynamics: A Five-Layer Structural Coordinate System",
      "authors": "Miyuki T. Nakata",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18525",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c5fa66d8eb7deac60effe912ce57cc5b3a8decb9a70cf6acda4def17328ab6c_w640_q70.webp",
      "contributions": "",
      "summary": "A Formal Descriptive Language for Learning Dynamics: A Five-Layer Structural Coordinate System",
      "mindmap": ""
    },
    {
      "title": "Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism",
      "authors": "Rahul Yumlembam, Biju Issac, Nauman Aslam, Eaby Kollonoor Babu, Josh Collyer, Fraser Kennedy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18527",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1520c61491b8f395b60f64432e37eff57c8608eba74cd9029c6109d32db7554_w640_q70.webp",
      "contributions": "",
      "summary": "Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism",
      "mindmap": ""
    },
    {
      "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
      "authors": "Scott Thornton",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18542",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2afb4d99182c71c26adf649cc513b4f7ffee3c07f215e3f4067f2ce9fa660fa0_w640_q70.webp",
      "contributions": "",
      "summary": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
      "mindmap": ""
    },
    {
      "title": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL",
      "authors": "Yuxiang Wei, Zhiqing Sun, Emily McMilin, Jonas Gehring, David Zhang, Gabriel Synnaeve, Daniel Fried, Lingming Zhang, Sida Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18552",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0bcd15393eed2ab163719da9a9e1954f5b23176404f9ad291a7ddc746dc5dd6_w640_q70.webp",
      "contributions": "",
      "summary": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL",
      "mindmap": ""
    },
    {
      "title": "Enhancing Medical Large Vision-Language Models via Alignment Distillation",
      "authors": "Aofei Chang, Ting Wang, Fenglong Ma",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18554",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65cfdaf6c50eb13e1535902993b0e58d2ccb2d1d8a89304254b7eb116f2e3bec_w640_q70.webp",
      "contributions": "",
      "summary": "Enhancing Medical Large Vision-Language Models via Alignment Distillation",
      "mindmap": ""
    },
    {
      "title": "Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V",
      "authors": "John Chen, Sihan Cheng, Can Gurkan, Ryan Lay, Moez Salahuddin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18564",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28b0a311cb9d10337e5f3590761c621c192dd12a7c496b0cd7891fbccd0f8367_w640_q70.webp",
      "contributions": "",
      "summary": "Vox Deorum: A Hybrid LLM Architecture for 4X / Grand Strategy Game AI -- Lessons from Civilization V",
      "mindmap": ""
    },
    {
      "title": "Adaptive Accountability in Networked MAS: Tracing and Mitigating Emergent Norms at Scale",
      "authors": "Saad Alqithami",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18561",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0aec0ad59816dd3fa60a1098beec472f1b9dc9c795b4760cf9c9f0516435437_w640_q70.webp",
      "contributions": "",
      "summary": "Adaptive Accountability in Networked MAS: Tracing and Mitigating Emergent Norms at Scale",
      "mindmap": ""
    },
    {
      "title": "AI Code in the Wild: Measuring Security Risks and Ecosystem Shifts of AI-Generated Code in Modern Software",
      "authors": "Bin Wang, Wenjie Yu, Yilu Zhong, Hao Yu, Keke Lian, Chaohua Lu, Hongfang Zheng, Dong Zhang, Hui Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18567",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9235d0c624936af1cfc09fc3a4442da3cad4b4006c63ac0a8fe4392c0673dc_w640_q70.webp",
      "contributions": "",
      "summary": "AI Code in the Wild: Measuring Security Risks and Ecosystem Shifts of AI-Generated Code in Modern Software",
      "mindmap": ""
    },
    {
      "title": "Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing",
      "authors": "Effiong Blessing, Chiung-Yi Tseng, Somshubhra Roy, Junaid Rehman, Isaac Nkrumah",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18575",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab099f7e2d96fe21b9b811feaa87d815fe23feb7bea213071f724ebc69a87414_w640_q70.webp",
      "contributions": "",
      "summary": "Modality-Dependent Memory Mechanisms in Cross-Modal Neuromorphic Computing",
      "mindmap": ""
    },
    {
      "title": "Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model",
      "authors": "Sumaiya Ali, Areej Alhothali, Ohoud Alzamzami, Sameera Albasri, Ahmed Abduljabbar, Muhammad Alwazzan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18573",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/387c39b67caa5e84daf0327dfb4c7a948d6d72c292a3404e6c7e8a2bcd6db7df_w640_q70.webp",
      "contributions": "",
      "summary": "Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model",
      "mindmap": ""
    },
    {
      "title": "ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning",
      "authors": "Weijie Zhou, Xuangtang Xiong, Ye Tian, Lijun Yue, Xinyu Wu, Wei Li, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang, Zhengyou Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18571",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e48075d8fabfbd12f97c2605f729d021ebb805999e01bbf511f08a872cf4cbae_w640_q70.webp",
      "contributions": "",
      "summary": "ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning",
      "mindmap": ""
    },
    {
      "title": "From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation",
      "authors": "Amit Barman, Atanu Mandal, Sudip Kumar Naskar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18593",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4841f825fcb7cf75a5d51e2769fdede7c9af6b25f2faafea290804903c41f40_w640_q70.webp",
      "contributions": "",
      "summary": "From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation",
      "mindmap": ""
    },
    {
      "title": "Reflective Confidence: Correcting Reasoning Flaws via Online Self-Correction",
      "authors": "Qinglin Zeng, Jing Yang, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18605",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f210ca82f5b83e0df7f60aafea30fa1547a0370e3b4cda5c94ba26cc393a6f1d_w640_q70.webp",
      "contributions": "",
      "summary": "Reflective Confidence: Correcting Reasoning Flaws via Online Self-Correction",
      "mindmap": ""
    },
    {
      "title": "Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments",
      "authors": "Saeideh Yousefzadeh, Hamidreza Pourreza",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18613",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15155edf4c23c5fa3645a1383be98a1728e9025130895c36fb1d8c7a536e2335_w640_q70.webp",
      "contributions": "",
      "summary": "Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments",
      "mindmap": ""
    },
    {
      "title": "PTTA: A Pure Text-to-Animation Framework for High-Quality Creation",
      "authors": "Ruiqi Chen, Kaitong Cai, Yijia Fan, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18614",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95e6d5eb29d8f200e9b4d751e2105c378ddc6c8efc5742b330b0ff8118931fb1_w640_q70.webp",
      "contributions": "",
      "summary": "PTTA: A Pure Text-to-Animation Framework for High-Quality Creation",
      "mindmap": ""
    },
    {
      "title": "Assignment-Routing Optimization: Solvers for Problems Under Constraints",
      "authors": "Yuan Qilong, Michal Pavelka",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18618",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3be4f502655365659aa5803c3c3e98ff2a6071f2798b4e71f56472c3fe923e05_w640_q70.webp",
      "contributions": "",
      "summary": "Assignment-Routing Optimization: Solvers for Problems Under Constraints",
      "mindmap": ""
    },
    {
      "title": "DASH: Deception-Augmented Shared Mental Model for a Human-Machine Teaming System",
      "authors": "Zelin Wan, Han Jun Yoon, Nithin Alluru, Terrence J. Moore, Frederica F. Nelson, Seunghyun Yoon, Hyuk Lim, Dan Dongseong Kim, Jin-Hee Cho",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18616",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fabd2c171e25c623bc7edccb8e1ef0506a926726f1d2a534aaa0d5113899beef_w640_q70.webp",
      "contributions": "",
      "summary": "DASH: Deception-Augmented Shared Mental Model for a Human-Machine Teaming System",
      "mindmap": ""
    },
    {
      "title": "ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning",
      "authors": "Zhenhao Zhou, Dan Negrut",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18619",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c6d919e0fe62510cfe25cf2ce73cce1bed581dc021d27540c0588fbf495702_w640_q70.webp",
      "contributions": "",
      "summary": "ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning",
      "mindmap": ""
    },
    {
      "title": "LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction",
      "authors": "Jensen Zhang, Ningyuan Liu, Yijia Fan, Zihao Huang, Qinglin Zeng, Kaitong Cai, Jian Wang, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18623",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3883290af621e53ac54109af76617e157c55068c77c267e0c4643eac11fc0ec_w640_q70.webp",
      "contributions": "",
      "summary": "LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction",
      "mindmap": ""
    },
    {
      "title": "The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation",
      "authors": "Huiqi Deng, Qihan Ren, Zhuofan Chen, Zhenyuan Cui, Wen Shen, Peng Zhang, Hongbin Pei, Quanshi Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18607",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30ab53152f4d22782382d142896ab1eaf1182b02d53770517901f257b002cc1f_w640_q70.webp",
      "contributions": "",
      "summary": "The Interaction Bottleneck of Deep Neural Networks: Discovery, Proof, and Modulation",
      "mindmap": ""
    },
    {
      "title": "A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback",
      "authors": "Thanh Dat Hoang, Thanh Trung Huynh, Matthias Weidlich, Thanh Tam Nguyen, Tong Chen, Hongzhi Yin, Quoc Viet Hung Nguyen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18622",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06b3e9f87e02f31154a7925036d456a7d4454e03d1f54e60c44ca1f788fae13_w640_q70.webp",
      "contributions": "",
      "summary": "A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback",
      "mindmap": ""
    },
    {
      "title": "ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs",
      "authors": "Han-Seul Jeong, Youngjoon Park, Hyungseok Song, Woohyung Lim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18633",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abc675e475f6d219e02688fea9461fecf46d82b6729bd20d29accd9c95cc967f_w640_q70.webp",
      "contributions": "",
      "summary": "ARC: Leveraging Compositional Representations for Cross-Problem Learning on VRPs",
      "mindmap": ""
    },
    {
      "title": "Automatic Adaptation to Concept Complexity and Subjective Natural Concepts: A Cognitive Model based on Chunking",
      "authors": "Dmitry Bennett, Fernand Gobet",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18665",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38aa956a72d7d3233a8e9436ec8b3eb96fe6f950dddcefb2c6497e4cc28ea7e6_w640_q70.webp",
      "contributions": "",
      "summary": "Automatic Adaptation to Concept Complexity and Subjective Natural Concepts: A Cognitive Model based on Chunking",
      "mindmap": ""
    },
    {
      "title": "Geometric-Photometric Event-based 3D Gaussian Ray Tracing",
      "authors": "Kai Kohyama, Yoshimitsu Aoki, Guillermo Gallego, Shintaro Shiba",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18640",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85632c631e489e2f789f1b5319b05b69e8e883a12a7b626de475c98e6066ef45_w640_q70.webp",
      "contributions": "",
      "summary": "Geometric-Photometric Event-based 3D Gaussian Ray Tracing",
      "mindmap": ""
    },
    {
      "title": "IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling",
      "authors": "Jones David, Shreya Ghosh",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18669",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c4809145f5d00782974383c70a1a7132c8f9a0785e93701b00d5f208d14cae5_w640_q70.webp",
      "contributions": "",
      "summary": "IntelliCode: A Multi-Agent LLM Tutoring System with Centralized Learner Modeling",
      "mindmap": ""
    },
    {
      "title": "ASTIF: Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting",
      "authors": "Hafiz Saif Ur Rehman, Ling Liu, Kaleem Ullah Qasim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18661",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39e0e274099dd56b8114b544bec1b5ebd56dd8742f2795d776a62751dceb1bd0_w640_q70.webp",
      "contributions": "",
      "summary": "ASTIF: Adaptive Semantic-Temporal Integration for Cryptocurrency Price Forecasting",
      "mindmap": ""
    },
    {
      "title": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing",
      "authors": "Wentao Liu, Yuhao Hu, Ruiting Zhou, Baochun Li, Ne Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18674",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b0a6c1ba7d729d7d1a45d1f2d74caedc5189c982e32587fba450b708786cd88_w640_q70.webp",
      "contributions": "",
      "summary": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing",
      "mindmap": ""
    },
    {
      "title": "Social Comparison without Explicit Inference of Others' Reward Values: A Constructive Approach Using a Probabilistic Generative Model",
      "authors": "Yosuke Taniuchi, Chie Hieida, Atsushi Noritake, Kazushi Ikeda, Masaki Isoda",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18687",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a80afdf97506cd46b51f3e0232de08278947f67ce0d02728ea31ab57eb6fd39c_w640_q70.webp",
      "contributions": "",
      "summary": "Social Comparison without Explicit Inference of Others' Reward Values: A Constructive Approach Using a Probabilistic Generative Model",
      "mindmap": ""
    },
    {
      "title": "CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles",
      "authors": "Cailin Lei, Haiyang Wu, Yuxiong Ji, Xiaoyu Cai, Yuchuan Du",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18703",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ec9f06c274a9525b199fad25bdb142bfd63b4a43030adbafd554411a02c2fe1_w640_q70.webp",
      "contributions": "",
      "summary": "CauTraj: A Causal-Knowledge-Guided Framework for Lane-Changing Trajectory Planning of Autonomous Vehicles",
      "mindmap": ""
    },
    {
      "title": "KeenKT: Knowledge Mastery-State Disambiguation for Knowledge Tracing",
      "authors": "Zhifei Li, Lifan Chen, Jiali Yi, Xiaoju Hou, Yue Zhao, Wenxin Huang, Miao Zhang, Kui Xiao, Bing Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18709",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6aad95971dcc28db4090fb00dc3a2271fb594ddd1417a6438b8306e2ee01f03a_w640_q70.webp",
      "contributions": "",
      "summary": "KeenKT: Knowledge Mastery-State Disambiguation for Knowledge Tracing",
      "mindmap": ""
    },
    {
      "title": "Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding",
      "authors": "Xiangrui Cai, Shaocheng Ma, Lei Cao, Jie Li, Tianyu Liu, Yilin Dong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18689",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2376b865ddc12cc20f97bb00013197494f3e12cba34f9079248457bb11fb7eab_w640_q70.webp",
      "contributions": "",
      "summary": "Fusion of Multiscale Features Via Centralized Sparse-attention Network for EEG Decoding",
      "mindmap": ""
    },
    {
      "title": "Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection",
      "authors": "Junjun Pan, Yixin Liu, Rui Miao, Kaize Ding, Yu Zheng, Quoc Viet Hung Nguyen, Alan Wee-Chung Liew, Shirui Pan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18733",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7692bd1c5b9a982f1df045d952e852c9a8c6543c4c09cb5bf1bd92156eff8c8_w640_q70.webp",
      "contributions": "",
      "summary": "Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection",
      "mindmap": ""
    },
    {
      "title": "PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation",
      "authors": "Zichuan Lin, Xiaokai Huang, Jiate Liu, Yuxuan Han, Jia Chen, Xiapeng Wu, Deheng Ye",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18737",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1c6978b6e1b267d6a0dd6bed5b258ad165849282cb8c0a6f4f89c449c2dfc2a_w640_q70.webp",
      "contributions": "",
      "summary": "PIPCFR: Pseudo-outcome Imputation with Post-treatment Variables for Individual Treatment Effect Estimation",
      "mindmap": ""
    },
    {
      "title": "Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth",
      "authors": "Chainarong Amornbunchornvej",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18732",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/971d2a8c016b462ec6480b43e3bb4defeb91225b526df8895e9702f727c64232_w640_q70.webp",
      "contributions": "",
      "summary": "Counterfactual Basis Extension and Representational Geometry: An MDL-Constrained Model of Conceptual Growth",
      "mindmap": ""
    },
    {
      "title": "$M^3-Verse$: A \"Spot the Difference\" Challenge for Large Multimodal Models",
      "authors": "Kewei Wei, Bocheng Hu, Jie Cao, Xiaohan Chen, Zhengxi Lu, Wubing Xia, Weili Xu, Jiaao Wu, Junchen He, Mingyu Jia, Ciyun Zhao, Ye Sun, Yizhi Li, Zhonghan Zhao, Jian Zhang, Gaoang Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18735",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52a20f9bfaa32af67c363579cc4ad37ddbcaa7484f53c2ad3e6f6287dffcb22d_w640_q70.webp",
      "contributions": "",
      "summary": "$M^3-Verse$: A \"Spot the Difference\" Challenge for Large Multimodal Models",
      "mindmap": ""
    },
    {
      "title": "Code2Doc: A Quality-First Curated Dataset for Code Documentation",
      "authors": "Recep Kaan Karaman, Meftun Akarsu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18748",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b6da5096797c358f77d8022914985853333b12b54cf68425fe470f42a60638b_w640_q70.webp",
      "contributions": "",
      "summary": "Code2Doc: A Quality-First Curated Dataset for Code Documentation",
      "mindmap": ""
    },
    {
      "title": "IPCV: Information-Preserving Compression for MLLM Visual Encoders",
      "authors": "Yuan Chen, Zichen Wen, Yuzhou Wu, Xuyang Liu, Shuang Chen, Junpeng Ma, Weijia Li, Conghui He, Linfeng Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18747",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23f93076434c2a627bcdaf65dfd58999db9105798eceb360e343a3f4018cc020_w640_q70.webp",
      "contributions": "",
      "summary": "IPCV: Information-Preserving Compression for MLLM Visual Encoders",
      "mindmap": ""
    },
    {
      "title": "MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking",
      "authors": "Jianyi Zhang, Shizhao Liu, Ziyin Zhou, Zhen Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18755",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/955c9d1fa243ab2977ec256331411b7bf6cca731527350187d399ec006ebe145_w640_q70.webp",
      "contributions": "",
      "summary": "MEEA: Mere Exposure Effect-Driven Confrontational Optimization for LLM Jailbreaking",
      "mindmap": ""
    },
    {
      "title": "Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs",
      "authors": "Lisan Al Amin, Vandana P. Janeja",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18797",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e389f385848f8816c83b5a1ff23be8e5adce564472d3ca92ed4e1c1107846a61_w640_q70.webp",
      "contributions": "",
      "summary": "Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs",
      "mindmap": ""
    },
    {
      "title": "The Dead Salmons of AI Interpretability",
      "authors": "Maxime Méloux, Giada Dirupo, François Portet, Maxime Peyrard",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18792",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04addf8c4fc59dd6f1ac7d1afe6ee58894441304221e6091e61ac24a403fb54e_w640_q70.webp",
      "contributions": "",
      "summary": "The Dead Salmons of AI Interpretability",
      "mindmap": ""
    },
    {
      "title": "Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform",
      "authors": "Yichuan Zhang, Chengxin Li, Yujie Gu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18791",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/828d54530ed9add4098a79bb9dd1f4047ed230dfaa399d57cade241c18713658_w640_q70.webp",
      "contributions": "",
      "summary": "Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform",
      "mindmap": ""
    },
    {
      "title": "FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation",
      "authors": "Ziyuan Tao, Chuanzhi Xu, Sandaru Jayawardana, Wei Bao, Kanchana Thilakarathna, Teng Joon Lim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18809",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/396672768b4960b9fefe0f861b938a8b78842c8181043ded9d12c7e8f28dbdfe_w640_q70.webp",
      "contributions": "",
      "summary": "FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation",
      "mindmap": ""
    },
    {
      "title": "Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection",
      "authors": "Souhail Abdelmouaiz Sadat, Mohamed Yacine Touahria Miliani, Khadidja Hab El Hames, Hamida Seba, Mohammed Haddad",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18826",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1947d93734df64c6e62ffe5a621cdb9199875dcacb08de1203cadabf9bce52e3_w640_q70.webp",
      "contributions": "",
      "summary": "Hyperbolic Graph Embeddings: a Survey and an Evaluation on Anomaly Detection",
      "mindmap": ""
    },
    {
      "title": "HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare",
      "authors": "Aditya Siddhant",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18829",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27fcee3d9ab3da4d05e9467a7467b02eecd9ce9ef97d79e927633356a26bb91b_w640_q70.webp",
      "contributions": "",
      "summary": "HARBOR: Holistic Adaptive Risk assessment model for BehaviORal healthcare",
      "mindmap": ""
    },
    {
      "title": "Controllable Probabilistic Forecasting with Stochastic Decomposition Layers",
      "authors": "John S. Schreck, William E. Chapman, Charlie Becker, David John Gagne II, Dhamma Kimpara, Nihanth Cherukuru, Judith Berner, Kirsten J. Mayer, Negin Sobhani",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18815",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60b40f77d2965a2b21082e13c0dc95074d21866006415db1b08905e24b2234e5_w640_q70.webp",
      "contributions": "",
      "summary": "Controllable Probabilistic Forecasting with Stochastic Decomposition Layers",
      "mindmap": ""
    },
    {
      "title": "CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning",
      "authors": "Zijun Gao, Zhikun Xu, Xiao Ye, Ben Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18857",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2bf1013f3c193e706d652fa4c8fdadc0c813c8a361e2efb84151a139cef28420_w640_q70.webp",
      "contributions": "",
      "summary": "CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning",
      "mindmap": ""
    },
    {
      "title": "Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,392 Adult Brazilian Workers",
      "authors": "Bruno Campello de Souza",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18871",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/446f3fb717e9b018a154458859c845d2583ea717613eb2a7397f53ffedb8700f_w640_q70.webp",
      "contributions": "",
      "summary": "Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,392 Adult Brazilian Workers",
      "mindmap": ""
    },
    {
      "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
      "authors": "Ming Li, Han Chen, Yunze Xiao, Jian Chen, Hong Jiao, Tianyi Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18880",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c02cdd0b38302d7f949dfe357cef926fc143c19edf1038c18dd4c5b1573b09_w640_q70.webp",
      "contributions": "",
      "summary": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
      "mindmap": ""
    },
    {
      "title": "CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis",
      "authors": "Kaidi Liang, Ke Li, Xianbiao Hu, Ruwen Qin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18878",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40cc111afbcdc76e8f9c40d867f3e3d92fefb4f06215bb877943f16f5fc7f761_w640_q70.webp",
      "contributions": "",
      "summary": "CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis",
      "mindmap": ""
    },
    {
      "title": "Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models",
      "authors": "Gökdeniz Gülmez",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18901",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8736176cf479e84eb193acab53e62edbdc590a96b0d7bb1adc66a60425d42697_w640_q70.webp",
      "contributions": "",
      "summary": "Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models",
      "mindmap": ""
    },
    {
      "title": "Multimodal Bayesian Network for Robust Assessment of Casualties in Autonomous Triage",
      "authors": "Szymon Rusiecki, Cecilia G. Morales, Kimberly Elenberg, Leonard Weiss, Artur Dubrawski",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18908",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40a7a336fdd61231fb69499563fa54a3feead11655ae2c62d612544da79b259a_w640_q70.webp",
      "contributions": "",
      "summary": "Multimodal Bayesian Network for Robust Assessment of Casualties in Autonomous Triage",
      "mindmap": ""
    },
    {
      "title": "An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects",
      "authors": "Shaokang Jiang, Daye Nam",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18925",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2089c7618775b9b9cdc54e25f2f7b14898adbf8c1ce8308d624be1f22c566408_w640_q70.webp",
      "contributions": "",
      "summary": "An Empirical Study of Developer-Provided Context for AI Coding Assistants in Open-Source Projects",
      "mindmap": ""
    },
    {
      "title": "When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models",
      "authors": "Michael S. Zhang, Rishi A. Ruia, Arnav Kewalram, Saathvik Dharmapuram, Utkarsh Sharma, Kevin Zhu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18934",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/939264f370dd6741588d1d57c916b73d9735e25a2515f10e5a8042daa9f43a19_w640_q70.webp",
      "contributions": "",
      "summary": "When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models",
      "mindmap": ""
    },
    {
      "title": "LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer",
      "authors": "Raina Panda, Daniel Fein, Arpita Singhal, Mark Fiore, Maneesh Agrawala, Matyas Bohacek",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18930",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17d9e52f35a5e302d613cba6423f95b6e9bb58c2b559fbd5a209c0516f8e2326_w640_q70.webp",
      "contributions": "",
      "summary": "LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer",
      "mindmap": ""
    },
    {
      "title": "Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm",
      "authors": "Li Yan, Bolun Liu, Chao Li, Jing Liang, Kunjie Yu, Caitong Yue, Xuzhao Chai, Boyang Qu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18947",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b46e7ee52db647b84d2f31cc7432f1e81dc459afc316c9b1daf014a0d4d28f5d_w640_q70.webp",
      "contributions": "",
      "summary": "Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm",
      "mindmap": ""
    },
    {
      "title": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement",
      "authors": "Saman Forouzandeh, Wei Peng, Parham Moradi, Xinghuo Yu, Mahdi Jalili",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18950",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54c9156f2821c3dd5ccd4cf3168dbf447bac3d5632d167548d6c2ce179747e7d_w640_q70.webp",
      "contributions": "",
      "summary": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement",
      "mindmap": ""
    },
    {
      "title": "Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection",
      "authors": "Yizhi Wang, Linan Yue, Min-Ling Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18956",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b69c497301f02ef5b3b08cd69d4893f6dad335ed0c12427b7caaaff9655ec68_w640_q70.webp",
      "contributions": "",
      "summary": "Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection",
      "mindmap": ""
    },
    {
      "title": "Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning",
      "authors": "Cheng-Hong Chang, Pei-Hsuan Tsai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18969",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05062b9ac64115654a255df578e1f3f61c6740d8e9db0b67ceca3387185661df_w640_q70.webp",
      "contributions": "",
      "summary": "Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning",
      "mindmap": ""
    },
    {
      "title": "R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression",
      "authors": "Kun Zhao, Siyuan Dai, Yingying Zhang, Guodong Liu, Pengfei Gu, Chenghua Lin, Paul M. Thompson, Alex Leow, Heng Huang, Lifang He, Liang Zhan, Haoteng Tang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18986",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5f5fb7daad78ee0192a96724088e699786c824ab6443f02134a66cc416ef822_w640_q70.webp",
      "contributions": "",
      "summary": "R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression",
      "mindmap": ""
    },
    {
      "title": "Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework",
      "authors": "Jinyan Liu, Zikang Chen, Qinchuan Wang, Tan Xie, Heming Zheng, Xudong Lv",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18999",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e96bd25bfb48e12bba787eb322582c438c99e6cb5614ad626a47b788b4598038_w640_q70.webp",
      "contributions": "",
      "summary": "Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework",
      "mindmap": ""
    },
    {
      "title": "ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation",
      "authors": "Gyeongrok Oh, Youngdong Jang, Jonghyun Choi, Suk-Ju Kang, Guang Lin, Sangpil Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18991",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca47fc4083c13826225296ae5380ec0a994e6b8b0a936b8582bb3acb510605f6_w640_q70.webp",
      "contributions": "",
      "summary": "ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation",
      "mindmap": ""
    },
    {
      "title": "Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
      "authors": "Akshaj Prashanth Rao, Advait Singh, Saumya Kumaar Saksena, Dhruv Kumar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19011",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddd38197559bfa789648dce3d4d675d0a05e678684e3999b2ba550170a5c8c1e_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
      "mindmap": ""
    },
    {
      "title": "ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management",
      "authors": "Lingjie Zhao, Xue Yu, Yongzhi Qi, Hao Hu, Jianshen Zhang, Yingzheng Ma, Shuyu Han, Wei Qi, Zuo-Jun Max Shen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19001",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c013ebec66cbd4e8c385c0036349f79e012b2d06eacaaa9dad9601fe1f892d1a_w640_q70.webp",
      "contributions": "",
      "summary": "ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management",
      "mindmap": ""
    },
    {
      "title": "Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models",
      "authors": "Tongyuan Miao, Gary Huang, Kai Jun Han, Annie Jiang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19004",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0ec45adefdbba0ff1f29513a557351fab96e8636ad950ecb6008ff575d0f496_w640_q70.webp",
      "contributions": "",
      "summary": "Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models",
      "mindmap": ""
    },
    {
      "title": "The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results",
      "authors": "Konstantin Kaulen, Tobias Ladner, Stanley Bak, Christopher Brix, Hai Duong, Thomas Flinkow, Taylor T. Johnson, Lukas Koller, Edoardo Manino, ThanhVu H Nguyen, Haoze Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19007",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2150c6a005dd7455c0dea890d81e19545e163edf743950930238707d6b4b29ea_w640_q70.webp",
      "contributions": "",
      "summary": "The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results",
      "mindmap": ""
    },
    {
      "title": "The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation",
      "authors": "Hengrui Jia, Taoran Li, Jonas Guan, Varun Chandrasekaran",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19025",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d72daaf4e0b704bed60ade2228f84dd6c37332a3588377ebc905b92f9db787ee_w640_q70.webp",
      "contributions": "",
      "summary": "The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation",
      "mindmap": ""
    },
    {
      "title": "Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation",
      "authors": "Connor Kilrain, David Carlyn, Julia Chae, Sara Beery, Wei-Lun Chao, Jianyang Gu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19026",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9212089c44919f3c00bd82b13c0b39b407491c5d9de5c2c72ed790b2f0a0a2f_w640_q70.webp",
      "contributions": "",
      "summary": "Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation",
      "mindmap": ""
    },
    {
      "title": "Recontextualization Mitigates Specification Gaming without Modifying the Specification",
      "authors": "Ariana Azarbal, Victor Gillioz, Vladimir Ivanov, Bryce Woodworth, Jacob Drori, Nevan Wichers, Aram Ebtekar, Alex Cloud, Alexander Matt Turner",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19027",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a00743ecd04e88f2319e45c57ea03523b4606221385fae51496a2d85825c258f_w640_q70.webp",
      "contributions": "",
      "summary": "Recontextualization Mitigates Specification Gaming without Modifying the Specification",
      "mindmap": ""
    },
    {
      "title": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments",
      "authors": "Xu Liu, Yu Liu, Hanshuo Qiu, Yang Qirong, Zhouhui Lian",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19024",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6756a03a5e10a12bbcf7c372024e83600363def7ecc93793c6ea6abf5fb1e097_w640_q70.webp",
      "contributions": "",
      "summary": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments",
      "mindmap": ""
    },
    {
      "title": "Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation",
      "authors": "Chi Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19061",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/734faa80116bce116311ee42eb0ce886bb9c7a99f6aa6642df27946ec8624b39_w640_q70.webp",
      "contributions": "",
      "summary": "Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation",
      "mindmap": ""
    },
    {
      "title": "Can abstract concepts from LLM improve SLM performance?",
      "authors": "Siddharth Tandon",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19069",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c5acb71114f63e96507f1b112ea484c287d603fcb8d6eff38459b7e748f327_w640_q70.webp",
      "contributions": "",
      "summary": "Can abstract concepts from LLM improve SLM performance?",
      "mindmap": ""
    },
    {
      "title": "Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning",
      "authors": "Yanzhi Zhang, Yitong Duan, Zhaoxi Zhang, Jiyan He, Shuxin Zheng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19081",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bc376ab7cffef097c2b5c88731fb39a3b1651cbf234ff265cff9da444c1ef62_w640_q70.webp",
      "contributions": "",
      "summary": "Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning",
      "mindmap": ""
    },
    {
      "title": "$γ(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics",
      "authors": "Mark Burgess",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19084",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72c21c32f8ba86f52bd19910dee8b59d7a15f63a4f5f0fffc1d598908faca89a_w640_q70.webp",
      "contributions": "",
      "summary": "$γ(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics",
      "mindmap": ""
    },
    {
      "title": "Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving",
      "authors": "Peiqing Lu, Yuan Zhang, Haoyun Zhang, Jiasen Zheng, Kejian Tong, Wenjun Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19093",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ba3857cb85f2a61550d6204fcd94a616e3814c7b544954750bbe22dbc8e0777_w640_q70.webp",
      "contributions": "",
      "summary": "Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving",
      "mindmap": ""
    },
    {
      "title": "Conditioning Accept-Desirability models in the context of AGM-like belief change",
      "authors": "Kathelijne Coussement, Gert de Cooman, Keano De Vos",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19096",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9a6821327555a097c992306a0343c693b0fa705a5154b551d5d3ec2af0af7fa_w640_q70.webp",
      "contributions": "",
      "summary": "Conditioning Accept-Desirability models in the context of AGM-like belief change",
      "mindmap": ""
    },
    {
      "title": "DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale",
      "authors": "Danny Dongyeop Han, Yonghyeon Gwon, Ahhyun Lucy Lee, Taeyang Lee, Seong Jin Lee, Jubin Choi, Sebin Lee, Jihyun Bang, Seungju Lee, David Keetae Park, Shinjae Yoo, Chun Kee Chung, Jiook Cha",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19097",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c484fa8acc86bbd4f9063aac8ef59f814dfcc288fb23da3663d1be6cbc19ed0_w640_q70.webp",
      "contributions": "",
      "summary": "DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale",
      "mindmap": ""
    },
    {
      "title": "FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning",
      "authors": "Zhe Yang, Xiaoshuang Sheng, Zhengnan Zhang, Jidong Wu, Zexing Wang, Xin He, Shenghua Xu, Guanjing Xiong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19107",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e20091cfc0ac4fa7e40c8d6d14ca11096f0a2c018bfea46fd36ed88a815bde01_w640_q70.webp",
      "contributions": "",
      "summary": "FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning",
      "mindmap": ""
    },
    {
      "title": "HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction",
      "authors": "Haoyu Jiang, Boan Qu, Junjie Zhu, Fanjie Zeng, Xiaojie Lin, Wei Zhong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19114",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c9e78435f4153aa973c4506803772e51c588c18e59d73dbebc8e9500306a1a5_w640_q70.webp",
      "contributions": "",
      "summary": "HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction",
      "mindmap": ""
    },
    {
      "title": "Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis",
      "authors": "Chenghao Li, Chaoning Zhang, Yi Lu, Shuxu Chen, Xudong Wang, Jiaquan Zhang, Zhicheng Wang, Zhengxun Jin, Kuien Liu, Sung-Ho Bae, Guoqing Wang, Yang Yang, Hen Tao Shen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19135",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e824123c4e563edae3f7da6c1bb5757bd1fcb243770125d234d65f7ec4e4d0_w640_q70.webp",
      "contributions": "",
      "summary": "Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis",
      "mindmap": ""
    },
    {
      "title": "Can We Test Consciousness Theories on AI? Ablations, Markers, and Robustness",
      "authors": "Yin Jun Phua",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19155",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8c12473f43d4f07485e40276605a028c808372091757f33fc5847a97e4469d0_w640_q70.webp",
      "contributions": "",
      "summary": "Can We Test Consciousness Theories on AI? Ablations, Markers, and Robustness",
      "mindmap": ""
    },
    {
      "title": "Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments",
      "authors": "Geraud Nangue Tasse, Matthew Riemer, Benjamin Rosman, Tim Klinger",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19154",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4ae9081ca365a9b3f9fb29e85d33707cb3a2d86edd9ec1d7bbe7736548be8781_w640_q70.webp",
      "contributions": "",
      "summary": "Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments",
      "mindmap": ""
    },
    {
      "title": "Vision-Language-Policy Model for Dynamic Robot Task Planning",
      "authors": "Jin Wang, Kim Tien Ly, Jacques Cloete, Nikos Tsagarakis, Ioannis Havoutis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19178",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61b83585238832b8c3decda632e24be3e08e065673710008da8a24e7fc11820b_w640_q70.webp",
      "contributions": "",
      "summary": "Vision-Language-Policy Model for Dynamic Robot Task Planning",
      "mindmap": ""
    },
    {
      "title": "Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning",
      "authors": "Mahdi Mohammadigohari, Giuseppe Di Fatta, Giuseppe Nicosia, Panos M. Pardalos",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19184",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbd43bd93ed46f19d672704ea24b1558583d71b0931350481c4a5624e10f1e16_w640_q70.webp",
      "contributions": "",
      "summary": "Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning",
      "mindmap": ""
    },
    {
      "title": "Practical Quantum-Classical Feature Fusion for complex data Classification",
      "authors": "Azadeh Alavi, Fatemeh Kouchmeshki, Abdolrahman Alavi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19180",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b3d96db4938c738e58622382a424d90cd1dd10f2608995abac211c8355b017a_w640_q70.webp",
      "contributions": "",
      "summary": "Practical Quantum-Classical Feature Fusion for complex data Classification",
      "mindmap": ""
    },
    {
      "title": "On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning",
      "authors": "Mahdi Mohammadigohari, Giuseppe Di Fatta, Giuseppe Nicosia, Panos M. Pardalos",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19199",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d39c24b719b177ace6c9761e568136da70723831c6d8b3c90ed420d732d6b409_w640_q70.webp",
      "contributions": "",
      "summary": "On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning",
      "mindmap": ""
    },
    {
      "title": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning",
      "authors": "Tao Zhang, Ziqian Zeng, Hao Peng, Huiping Zhuang, Cen Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19206",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c9e9fce94d780d562e939d0aa6d0aa4602e96ed0f980ba45968a1486b745bc8_w640_q70.webp",
      "contributions": "",
      "summary": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning",
      "mindmap": ""
    },
    {
      "title": "Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation",
      "authors": "Jerry Wang, Ting Yiu Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19210",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62c98c8f57888fd4ebe24a386db1767dce70baaed757b8ccaec8a6d19541746c_w640_q70.webp",
      "contributions": "",
      "summary": "Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation",
      "mindmap": ""
    },
    {
      "title": "Towards Minimal Fine-Tuning of VLMs",
      "authors": "Tiange Luo, Lajanugen Logeswaran, Jaekyeom Kim, Justin Johnson, Honglak Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19219",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/420c22fc5b580697b05c90c8fbf0115c2cea8a82f971a19f125c0456c3405309_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Minimal Fine-Tuning of VLMs",
      "mindmap": ""
    },
    {
      "title": "From Pixels to Predicates Structuring urban perception with scene graphs",
      "authors": "Yunlong Liu, Shuyang Li, Pengyuan Liu, Yu Zhang, Rudi Stouffs",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19221",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/937a8aa60cbd8f9c8016e6a36a9941d4f6c5a8af94206f4d30a8f70eac213a2a_w640_q70.webp",
      "contributions": "",
      "summary": "From Pixels to Predicates Structuring urban perception with scene graphs",
      "mindmap": ""
    },
    {
      "title": "Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models",
      "authors": "Valentin Schmidberger, Manuel Eberhardinger, Setareh Maghsudi, Johannes Maucher",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19228",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/53ed231c85ea495143f9234046091abf1a755a2bae0a9a62789a4927a126a190_w640_q70.webp",
      "contributions": "",
      "summary": "Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models",
      "mindmap": ""
    },
    {
      "title": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation",
      "authors": "Anna-Maria Gueorguieva, Aylin Caliskan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19238",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d125f49a56a4f052b1faf9015b89460bff5794de36222547ccabb3b4a08eca86_w640_q70.webp",
      "contributions": "",
      "summary": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation",
      "mindmap": ""
    },
    {
      "title": "ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models",
      "authors": "Mingxu Zhang, Dazhong Shen, Qi Zhang, Ying Sun",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19240",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aeb3b37133d9071af2bfeeecd0da8171d70b3d3d9b3b0658553484d1919570f5_w640_q70.webp",
      "contributions": "",
      "summary": "ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models",
      "mindmap": ""
    },
    {
      "title": "DeliveryBench: Can Agents Earn Profit in Real World?",
      "authors": "Lingjun Mao, Jiawei Ren, Kun Zhou, Jixuan Chen, Ziqiao Ma, Lianhui Qin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19234",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c24ad2486e536c4e46b7008f9b36fe0a1b521f054ff0b708d9260cb71032fa7_w640_q70.webp",
      "contributions": "",
      "summary": "DeliveryBench: Can Agents Earn Profit in Real World?",
      "mindmap": ""
    },
    {
      "title": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics",
      "authors": "Do Minh Duc, Quan Xuan Truong, Nguyen Tat Dat, Nguyen Van Vinh",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19247",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3e85ac8e144e835ca46b7dcdc94a82085e98b53bc5c52cfa6addea751cf9af2_w640_q70.webp",
      "contributions": "",
      "summary": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics",
      "mindmap": ""
    },
    {
      "title": "Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study",
      "authors": "Carla Crivoi, Radu Tudor Ionescu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19253",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d70a17b56ecda8937a9bd488550e13c9d9833f836ea7a0e16e024c8b5e950c5b_w640_q70.webp",
      "contributions": "",
      "summary": "Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study",
      "mindmap": ""
    },
    {
      "title": "Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6",
      "authors": "Jiaao Wu, Xian Zhang, Fan Yang, Yinpeng Dong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19287",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b24f019b56baece009c93ac1107b1f8aea4b09d87df075427e81815567970f3d_w640_q70.webp",
      "contributions": "",
      "summary": "Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6",
      "mindmap": ""
    },
    {
      "title": "Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation",
      "authors": "Ivan DeAndres-Tame, Chengwei Ye, Ruben Tolosana, Ruben Vera-Rodriguez, Shiqi Yu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19275",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d12748fd351c66664603a2df32d39e3ed4e526013cc4fc7f328dc001dea6e6a2_w640_q70.webp",
      "contributions": "",
      "summary": "Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation",
      "mindmap": ""
    },
    {
      "title": "Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models",
      "authors": "Linzhi Chen, Yang Sun, Hongru Wei, Yuqi Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19297",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb0bd7eb8763e5b7b48b95fffa2b9d2689db5cbe27fd8dd5ff4a8a691095d826_w640_q70.webp",
      "contributions": "",
      "summary": "Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models",
      "mindmap": ""
    },
    {
      "title": "Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals",
      "authors": "Chang Dong, Jianfeng Tao, Chengliang Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19280",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23dae36b800575da09218251c1aa3582a2ce5ca30c8c9988566be10e83f43e38_w640_q70.webp",
      "contributions": "",
      "summary": "Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals",
      "mindmap": ""
    },
    {
      "title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
      "authors": "A.A. Gde Yogi Pramana, Jason Ray, Anthony Jaya, Michael Wijaya",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19317",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e59bf81bbcade919249cc70e6c7c857215e36f22480011e424fdeb4a1f6ca56_w640_q70.webp",
      "contributions": "",
      "summary": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
      "mindmap": ""
    },
    {
      "title": "Alternative positional encoding functions for neural transformers",
      "authors": "Ezequiel Lopez-Rubio, Macoris Decena-Gimenez, Rafael Marcos Luque-Baena",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19323",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a8d61d0d13038271b6545ad7d265494ffd5b3cce79e03046ec542b879d17a8_w640_q70.webp",
      "contributions": "",
      "summary": "Alternative positional encoding functions for neural transformers",
      "mindmap": ""
    },
    {
      "title": "MAGIC: Achieving Superior Model Merging via Magnitude Calibration",
      "authors": "Yayuan Li, Jian Zhang, Jintao Guo, Zihan Cheng, Lei Qi, Yinghuan Shi, Yang Gao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19320",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7204250ada52cfa8e70ee24634b9a58aab0085ac9d5854e5c672a585fb92a0a6_w640_q70.webp",
      "contributions": "",
      "summary": "MAGIC: Achieving Superior Model Merging via Magnitude Calibration",
      "mindmap": ""
    },
    {
      "title": "MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture",
      "authors": "Hui Li, Jiayue Lyu, Fu-Yun Wang, Kaihui Cheng, Siyu Zhu, Jingdong Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19311",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/333652b938a5ecac737d2dfcea2bae93a2f072e15ac1c354ce9b2da1931714cc_w640_q70.webp",
      "contributions": "",
      "summary": "MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture",
      "mindmap": ""
    },
    {
      "title": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application",
      "authors": "Haoyu Jiang, Fanjie Zeng, Boan Qu, Xiaojie Lin, Wei Zhong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19299",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f659480b32a80b6959cb66f4e981a4486e0674a48613b87e1f87ae33a24a364_w640_q70.webp",
      "contributions": "",
      "summary": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application",
      "mindmap": ""
    },
    {
      "title": "VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop",
      "authors": "JiaWei Zhu, ZiHeng Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19349",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0b686fca34f155b782afa1f7cabe09de9b7bcb967ccfba8215a39405da5ba99_w640_q70.webp",
      "contributions": "",
      "summary": "VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop",
      "mindmap": ""
    },
    {
      "title": "First-Order Representation Languages for Goal-Conditioned RL",
      "authors": "Simon Ståhlberg, Hector Geffner",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19355",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a8381365dc741932409d148f8d829e1cb922492f327e8a311d02d4ed8ad6d56_w640_q70.webp",
      "contributions": "",
      "summary": "First-Order Representation Languages for Goal-Conditioned RL",
      "mindmap": ""
    },
    {
      "title": "PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models",
      "authors": "A. B. M. Ashikur Rahman, Saeed Anwar, Muhammad Usman, Irfan Ahmad, Ajmal Mian",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19350",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22263a25ed189a880450b2a3c8562fb8b0c96c8ec9fefce093363fc2ea2fb8f6_w640_q70.webp",
      "contributions": "",
      "summary": "PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models",
      "mindmap": ""
    },
    {
      "title": "Learning General Policies with Policy Gradient Methods",
      "authors": "Simon Ståhlberg, Blai Bonet, Hector Geffner",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19366",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c0ac6fc8f7779ac0ec6d29400a2e745e9a17133a7ceb22854a52d83825eca4e_w640_q70.webp",
      "contributions": "",
      "summary": "Learning General Policies with Policy Gradient Methods",
      "mindmap": ""
    },
    {
      "title": "Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture",
      "authors": "Christian Hägg, Kathlén Kohn, Giovanni Luca Marchetti, Boris Shapiro",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19367",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fa84d344152205d90f463c26b0bb9356b71dde7f412bdd03d7fd7f036a0b845_w640_q70.webp",
      "contributions": "",
      "summary": "Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture",
      "mindmap": ""
    },
    {
      "title": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
      "authors": "Xueming Yan, Boyan Xu, Yaochu Jin, Lixian Xiao, Wenlong Ye, Runyang Cai, Zeqi Zheng, Jingfa Liu, Aimin Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19379",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1619f2062f011937d18aa7e18ebf2c490f57bd39c04a7d06493e15df8a15ae19_w640_q70.webp",
      "contributions": "",
      "summary": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
      "mindmap": ""
    },
    {
      "title": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration",
      "authors": "Runze Li, Yuwen Zhai, Bo Xu, LiWu Xu, Nian Shi, Wei Zhang, Ran Lin, Liang Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19396",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9bef52c443ab4e7eee0649ff0f53fd382c9727d2345228eb8bb9fe5ad898a8eb_w640_q70.webp",
      "contributions": "",
      "summary": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration",
      "mindmap": ""
    },
    {
      "title": "DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition",
      "authors": "Yueyao Chen, Kai-Ni Wang, Dario Tayupo, Arnaud Huaulm'e, Krystel Nyangoh Timoh, Pierre Jannin, Qi Dou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19387",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d09473d7af5f8db0e2099dcd5a18592d023dddc412cada060f4f05596921ff9_w640_q70.webp",
      "contributions": "",
      "summary": "DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition",
      "mindmap": ""
    },
    {
      "title": "Research Program: Theory of Learning in Dynamical Systems",
      "authors": "Elad Hazan, Shai Shalev Shwartz, Nathan Srebro",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19410",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/879026bb2ccb2b89c867729f9f077e32888b82173c20d950c0ed5feda6519aa9_w640_q70.webp",
      "contributions": "",
      "summary": "Research Program: Theory of Learning in Dynamical Systems",
      "mindmap": ""
    },
    {
      "title": "Attention Is Not What You Need",
      "authors": "Zhang Chong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19428",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbe1f1a463179312610994fd34a110ca4bfc5b56928e49a6749dd43948421e91_w640_q70.webp",
      "contributions": "",
      "summary": "Attention Is Not What You Need",
      "mindmap": ""
    },
    {
      "title": "MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation",
      "authors": "Fei Ge, Ying Huang, Jie Liu, Guixuan Zhang, Zhi Zeng, Shuwu Zhang, Hu Guan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19438",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c5f1cc5f1508ff7f3dac04d0e138fe9c30202bb132ef0c4725622de8caf6c5_w640_q70.webp",
      "contributions": "",
      "summary": "MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation",
      "mindmap": ""
    },
    {
      "title": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations",
      "authors": "Jinwei Chi, Ke Wang, Yu Chen, Xuanye Lin, Qiang Xu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19456",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44e2303feb430ac2c66a05d707fa59f0184a8efed477dd24968816daffaaf4a2_w640_q70.webp",
      "contributions": "",
      "summary": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations",
      "mindmap": ""
    },
    {
      "title": "An Agentic Framework for Autonomous Materials Computation",
      "authors": "Zeyu Xia, Jinzhe Ma, Congjie Zheng, Shufei Zhang, Yuqiang Li, Hang Su, P. Hu, Changshui Zhang, Xingao Gong, Wanli Ouyang, Lei Bai, Dongzhan Zhou, Mao Su",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19458",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8dc8796e6d842dffe17cec24dc175dc3c7a315afb61353b40a3cd5603693d9a1_w640_q70.webp",
      "contributions": "",
      "summary": "An Agentic Framework for Autonomous Materials Computation",
      "mindmap": ""
    },
    {
      "title": "Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications",
      "authors": "Lorenzo Capelli, Leandro de Souza Rosa, Gianluca Setti, Mauro Mangia, Riccardo Rovatti",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19472",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40e0a2de9bc90f1d512acba9f8178e3caeb11f59b899b803bcea54b876c14e2e_w640_q70.webp",
      "contributions": "",
      "summary": "Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications",
      "mindmap": ""
    },
    {
      "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis",
      "authors": "Katharina Stengg, Christian Macho, Martin Pinzger",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19481",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/83f5edcaed3e66cf570a1118c40709f58e2f28ea874fce90d48d26c98b1618e8_w640_q70.webp",
      "contributions": "",
      "summary": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis",
      "mindmap": ""
    },
    {
      "title": "Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset",
      "authors": "Nikita Volzhin, Soowhan Yoon",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19494",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d0c1cdb2dc4e7f555b6151e7dd057ebb961c3137ad65952a50508e3d588bb4a_w640_q70.webp",
      "contributions": "",
      "summary": "Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset",
      "mindmap": ""
    },
    {
      "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
      "authors": "Ziyang Song, Zelin Zang, Zuyao Chen, Xusheng Liang, Dong Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19512",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d14f0ebe1771704c1788aacd2e3db88e7c3b990ef8113a061936422f9bb95889_w640_q70.webp",
      "contributions": "",
      "summary": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
      "mindmap": ""
    },
    {
      "title": "DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast",
      "authors": "Hongliang Li, Nong Zhang, Zhewen Xu, Xiang Li, Changzheng Liu, Chongbo Zhao, Jie Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19506",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ab91ff5a253b798aeb4b7fa1cee40fb5a1319f0697a2094d06ad95557270ff9_w640_q70.webp",
      "contributions": "",
      "summary": "DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast",
      "mindmap": ""
    },
    {
      "title": "LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning",
      "authors": "Xueming Yan, Bo Yin, Yaochu Jin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19516",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/848324ae33b6180c9af25fd1e9aa4829e0fdb2a0ac2b46da8810accc276774e5_w640_q70.webp",
      "contributions": "",
      "summary": "LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning",
      "mindmap": ""
    },
    {
      "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
      "authors": "Li Puyin, Tiange Xiang, Ella Mao, Shirley Wei, Xinye Chen, Adnan Masood, Li Fei-fei, Ehsan Adeli",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19526",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2cc62413ed86e23bc11d3109dd3f6d9f5cdc6abfea9e0de269b698952b18c550_w640_q70.webp",
      "contributions": "",
      "summary": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
      "mindmap": ""
    },
    {
      "title": "Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement",
      "authors": "Hongsheng Xing, Qiuxin Si",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19530",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a398a1c7e656e700ae7d2e4b360c6e2a84d8a4f125de94406eb2fcbd1174cabf_w640_q70.webp",
      "contributions": "",
      "summary": "Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement",
      "mindmap": ""
    },
    {
      "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
      "authors": "Moritz Böhle, Amélie Royer, Juliette Marrie, Edouard Grave, Patrick Pérez",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19535",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b20ae57f683974a6959785befd45010cb87dde73d9ccc345f16e11af116951e_w640_q70.webp",
      "contributions": "",
      "summary": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
      "mindmap": ""
    },
    {
      "title": "CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal",
      "authors": "Yongxin Wang, Zhicheng Yang, Meng Cao, Mingfei Han, Haokun Lin, Yingying Zhu, Xiaojun Chang, Xiaodan Liang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19554",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67ed5402e057af7c1649865e93cc5d4cb278374f1381f91c80951d25ce4f4c0e_w640_q70.webp",
      "contributions": "",
      "summary": "CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal",
      "mindmap": ""
    },
    {
      "title": "Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios",
      "authors": "Jiawen Wang, Jingjing Wang Tianyang Chen, Min Zhang, Guodong Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19551",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cdd63f7cc3d632dfdf950fc0474c659424578038c0958a942040653588c1bbd3_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios",
      "mindmap": ""
    },
    {
      "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
      "authors": "Martin Sedlacek, Pavlo Yefanov, Georgy Ponimatkin, Jai Bardhan, Simon Pilc, Mederic Fourmy, Evangelos Kazakos, Cees G. M. Snoek, Josef Sivic, Vladimir Petrik",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19562",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cb99fa05ab76ebf7aa83d9c99a1cab512063ae25e6fb2fa7beee0f0c7246905_w640_q70.webp",
      "contributions": "",
      "summary": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
      "mindmap": ""
    },
    {
      "title": "Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations",
      "authors": "Lawrence Krukrubo, Julius Odede, Olawande Olusegun",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19557",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4cf6d654019f56b56f3c2182b2db9d82b07540ace871f6684795ebe22f1a7c35_w640_q70.webp",
      "contributions": "",
      "summary": "Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations",
      "mindmap": ""
    },
    {
      "title": "Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles",
      "authors": "Yanliang Huang, Xia Yan, Peiran Yin, Zhenduo Zhang, Zeyan Shao, Youran Wang, Haoliang Huang, Matthias Althoff",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19564",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd9b53a38761715bf725e8473ae990f27df4fc7048b67ea4fde9ad955b4ac95d_w640_q70.webp",
      "contributions": "",
      "summary": "Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles",
      "mindmap": ""
    },
    {
      "title": "BabyFlow: 3D modeling of realistic and expressive infant faces",
      "authors": "Antonia Alomar, Mireia Masias, Marius George Linguraru, Federico M. Sukno, Gemma Piella",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19560",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4bdcd1bc9dd6e1141b9b283f24968fbb4a40dc22257a719c5e16fbac178220f_w640_q70.webp",
      "contributions": "",
      "summary": "BabyFlow: 3D modeling of realistic and expressive infant faces",
      "mindmap": ""
    },
    {
      "title": "The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge",
      "authors": "Angjelin Hila",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19570",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14053cd17ec2f7fbfad183ca144e70fb650f93b135eca28453bda97a810f7db4_w640_q70.webp",
      "contributions": "",
      "summary": "The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge",
      "mindmap": ""
    },
    {
      "title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller",
      "authors": "Kirill Djebko, Tom Baumann, Erik Dilger, Frank Puppe, Sergio Montenegro",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19576",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/559c74a4e132b0428b11e1b742ace3b49e9292ec3c666ac9dd536d79ee6c2a1f_w640_q70.webp",
      "contributions": "",
      "summary": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller",
      "mindmap": ""
    },
    {
      "title": "Exploring the features used for summary evaluation by Human and GPT",
      "authors": "Zahra Sadeghi, Evangelos Milios, Frank Rudzicz",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19620",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e154f176f186b8dabd43d09d2a96579db8d3bbe3ccbf6debeb1b756642ffa2a_w640_q70.webp",
      "contributions": "",
      "summary": "Exploring the features used for summary evaluation by Human and GPT",
      "mindmap": ""
    },
    {
      "title": "MapTrace: Scalable Data Generation for Route Tracing on Maps",
      "authors": "Artemis Panagopoulou, Aveek Purohit, Achin Kulshrestha, Soroosh Yazdani, Mohit Goyal",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19609",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7cc350a173ade821ef309bec6e6155158ae8332042895a8d6f45396c8b70c06_w640_q70.webp",
      "contributions": "",
      "summary": "MapTrace: Scalable Data Generation for Route Tracing on Maps",
      "mindmap": ""
    },
    {
      "title": "Clustering with Label Consistency",
      "authors": "Diptarka Chakraborty, Hendrik Fichtenberger, Bernhard Haeupler, Silvio Lattanzi, Ashkan Norouzi-Fard, Ola Svensson",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19654",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73fc229d3b4fc51718fe8d59f9abfb97c5b0e2d37b67f22dacfc080be54754bb_w640_q70.webp",
      "contributions": "",
      "summary": "Clustering with Label Consistency",
      "mindmap": ""
    },
    {
      "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
      "authors": "Yuqiao Tan, Minzheng Wang, Shizhu He, Huanxuan Liao, Chengfeng Zhao, Qiunan Lu, Tian Liang, Jun Zhao, Kang Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19673",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a324294583c22f2459c7cd427d13db040bb89f060fd51e26bb284a001119f6d4_w640_q70.webp",
      "contributions": "",
      "summary": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
      "mindmap": ""
    },
    {
      "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
      "authors": "Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi, Tushar Mungle, Pabitra Mitra",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19663",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd268a98c5baac8b164ba733c255159614de6b969e8c6dd3f943fa74d98e5a1b_w640_q70.webp",
      "contributions": "",
      "summary": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
      "mindmap": ""
    },
    {
      "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
      "authors": "Hanyang Kong, Xingyi Yang, Xiaoxu Zheng, Xinchao Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19678",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4f113f5d19fa4ca45dd649c8c074bdcd96d439b1640d1340c9f10070d3b5a62_w640_q70.webp",
      "contributions": "",
      "summary": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
      "mindmap": ""
    },
    {
      "title": "Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight",
      "authors": "Junze Ye, Daniel Tawfik, Alex J. Goodell, Nikhil V. Kotha, Mark K. Buyyounouski, Mohsen Bayati",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19691",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3b7aa03e4ca854038444e0521aaa9d1a6b6c20e2054b4637dd76d61ecac2014_w640_q70.webp",
      "contributions": "",
      "summary": "Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight",
      "mindmap": ""
    },
    {
      "title": "Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing",
      "authors": "Christopher Regan, Ying Xie",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17923",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c0972c0f33f9a898aad22d9d466edb2d113b1f06ae271519a0310fbe4deb8326_w640_q70.webp",
      "contributions": "",
      "summary": "Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing",
      "mindmap": ""
    },
    {
      "title": "Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach",
      "authors": "Dongdong Yang, Bin Li, Jiguang He, Yicheng Yan, Xiaoyu Zhang, Chongwen Huang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17928",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52bfb16cf6da30f529d6affbb7e78493e8d297701e513afd78f81efa7c4804bd_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Beamforming Optimization for STAR-RIS-Assisted Communications: A Gradient-Based Meta Learning Approach",
      "mindmap": ""
    },
    {
      "title": "Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods",
      "authors": "Sheryl Chen, Tony Wang, Kyle Feinstein",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17929",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4501ed870b87583df869b8985dc8410b30a47654c96f943771a6c67d4279480c_w640_q70.webp",
      "contributions": "",
      "summary": "Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods",
      "mindmap": ""
    },
    {
      "title": "A Critical Review of Monte Carlo Algorithms Balancing Performance and Probabilistic Accuracy with AI Augmented Framework",
      "authors": "Ravi Prasad",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17968",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7665945961b2f7304f7c1df5cbe15f902828795ce6193b462edbd1a98af56880_w640_q70.webp",
      "contributions": "",
      "summary": "A Critical Review of Monte Carlo Algorithms Balancing Performance and Probabilistic Accuracy with AI Augmented Framework",
      "mindmap": ""
    },
    {
      "title": "Re-assessing the evidence for mental rotation abilities in children using computational models",
      "authors": "Arthur Aubret, Jochen Triesch",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17972",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04b19b13a0170f5c8d0ddc85fcabbe83baa46fd0d40efd430823288b256d119b_w640_q70.webp",
      "contributions": "",
      "summary": "Re-assessing the evidence for mental rotation abilities in children using computational models",
      "mindmap": ""
    },
    {
      "title": "The Subject of Emergent Misalignment in Superintelligence: An Anthropological, Cognitive Neuropsychological, Machine-Learning, and Ontological Perspective",
      "authors": "Muhammad Osama Imran, Roshni Lulla, Rodney Sappington",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17989",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/088c001d289b3633293d46545215e3c6c3c647164a557d5d4458682598dc319a_w640_q70.webp",
      "contributions": "",
      "summary": "The Subject of Emergent Misalignment in Superintelligence: An Anthropological, Cognitive Neuropsychological, Machine-Learning, and Ontological Perspective",
      "mindmap": ""
    },
    {
      "title": "On Efficient Adjustment in Causal Graphs",
      "authors": "Isabela Belciug, Simon Ferreira, Charles K. Assaad",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18315",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e651794ebebddd044adc102fdfd3e79c703c2f1d1bbe46f929abaf0866588f7c_w640_q70.webp",
      "contributions": "",
      "summary": "On Efficient Adjustment in Causal Graphs",
      "mindmap": ""
    },
    {
      "title": "Evolutionary BP+OSD Decoding for Low-Latency Quantum Error Correction",
      "authors": "Hee-Youl Kwak, Seong-Joon Park, Hyunwoo Jung, Jeongseok Ha, Jae-Won Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18273",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0f5d84f1158c216751ab871ccb9ff92e39812054d2d89f523c363ba1d516324f_w640_q70.webp",
      "contributions": "",
      "summary": "Evolutionary BP+OSD Decoding for Low-Latency Quantum Error Correction",
      "mindmap": ""
    },
    {
      "title": "TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition",
      "authors": "Haolong Zheng, Yekaterina Yegorova, Mark Hasegawa-Johnson",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18263",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a32dd492d99bdbfa3ef2453e70a027129c638aca8cddc500a7ae12d1a4ae23df_w640_q70.webp",
      "contributions": "",
      "summary": "TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition",
      "mindmap": ""
    },
    {
      "title": "The Illusion of Consistency: Selection-Induced Bias in Gated Kalman Innovation Statistics",
      "authors": "Barak Or",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18508",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de95b749b6e41b5ecb30b6f99c2de21906ad83985f90b86407263b2e092757dd_w640_q70.webp",
      "contributions": "",
      "summary": "The Illusion of Consistency: Selection-Induced Bias in Gated Kalman Innovation Statistics",
      "mindmap": ""
    },
    {
      "title": "Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics",
      "authors": "Yucheng Yang, Chiyuan Wang, Andreas Schaab, Benjamin Moll",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18892",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203cc3fec3a819cdd9dd9fc028622d35f3a4ba54e87d06f53a4c83240df799a4_w640_q70.webp",
      "contributions": "",
      "summary": "Structural Reinforcement Learning for Heterogeneous Agent Macroeconomics",
      "mindmap": ""
    },
    {
      "title": "Owning the Intelligence: Global AI Patents Landscape and Europe's Quest for Technological Sovereignty",
      "authors": "Lapo Santarlasci, Armando Rungi, Loredana Fattorini, Nestor Maslej",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19569",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d480c4d77b7eaba72c49770556d7fc5b5d3dfd78a176aef0d2a236de15ebeb67_w640_q70.webp",
      "contributions": "",
      "summary": "Owning the Intelligence: Global AI Patents Landscape and Europe's Quest for Technological Sovereignty",
      "mindmap": ""
    },
    {
      "title": "Navigating Taxonomic Expansions of Entity Sets Driven by Knowledge Bases",
      "authors": "Pietro Cofone, Giovanni Amendola, Marco Manna, Aldo Ricioppo",
      "institution": "University of Calabria, University of Cyprus",
      "link": "https://arxiv.org/pdf/2512.16953",
      "code": null,
      "tags": [
        "knowledge representation and reasoning",
        "entity set expansion",
        "expansion graph",
        "logical formula",
        "semantic inclusion",
        "computational complexity"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a logic-based framework using expansion graphs, which are rooted directed acyclic graphs, to support taxonomic expansions of entity sets from knowledge bases. To avoid the impracticality of fully materializing these potentially large graphs, the authors formalize efficient reasoning tasks to check relationships between entity tuples within the graph structure. Their main conclusion is that, under realistic assumptions like bounded input, these tasks can be implemented efficiently, enabling local and incremental navigation without full graph construction.",
      "mindmap": ""
    },
    {
      "title": "Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories",
      "authors": "Chayan Jain, Rishant Sharma, Archit Garg, Ishan Bhanuka, Pratik Narang, Dhruv Kumar",
      "institution": "BITS Pilani",
      "link": "https://arxiv.org/pdf/2512.16954",
      "code": null,
      "tags": [
        "multi-modal inference",
        "visual anchoring",
        "asset-first mechanism",
        "temporal bridge",
        "diffusion models",
        "large language model (LLM)",
        "text-to-video (T2V)",
        "character consistency",
        "multi-stage pipeline"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a multi-stage pipeline for generating long, character-consistent video stories. It uses an LLM to create a script, a text-to-image model to design consistent character visuals as anchors, and a video generation model to synthesize scenes individually, with a temporal bridge linking them. The method's necessity is validated by showing that removing visual anchoring causes a catastrophic drop in character consistency, and cultural biases in current models are also analyzed.",
      "mindmap": ""
    },
    {
      "title": "Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections",
      "authors": "Adrian Straker, Paul Magdon, Marco Zullich, Maximilian Freudenberg, Christoph Kleinn, Johannes Breidenbach, Stefano Puliti, Nils Nölke",
      "institution": "University of Applied Sciences and Art (HAWK), University of Groningen, University of Göttingen, Norwegian Institute of Bioeconomy Research (NIBIO)",
      "link": "https://arxiv.org/pdf/2512.16950",
      "code": null,
      "tags": [
        "computer vision",
        "YOLOv8",
        "Finer-CAM",
        "saliency maps",
        "cross-validation",
        "TLS point cloud projections"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a novel method that links Finer-CAM explanations to structural segments in TLS point cloud projections to evaluate which features drive tree species classification using YOLOv8 models. The analysis of saliency maps reveals that models primarily rely on crown features for classification, with stem features being more important for certain species, and that the models' perception of species similarity aligns with human expert judgment. The results underscore the need for explainable AI to understand model decision processes and build confidence in predictions.",
      "mindmap": ""
    },
    {
      "title": "Optimizing Text Search: A Novel Pattern Matching Algorithm Based on Ukkonen's Approach",
      "authors": "Xinyu Guan, Shaohua Zhang",
      "institution": "Not specified",
      "link": "https://arxiv.org/pdf/2512.16927",
      "code": null,
      "tags": [
        "pattern matching algorithms",
        "Ukkonen's Algorithm",
        "Suffix Trees",
        "pattern recognition",
        "text-search algorithms"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b065cfa04bf6f3a4b29a1299ffe0b7dd4f84fbabb6368c76abaa339e1a0a77c_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces a novel pattern matching algorithm that combines Ukkonen's Algorithm for constructing Suffix Trees with a new search technique using Python's dynamic link attributes. The optimized algorithm demonstrates linear time and space efficiency, outperforming traditional methods like Naive Search, KMP, and Boyer-Moore, and achieves 100% accuracy in tasks such as genomic sequence pattern recognition.",
      "mindmap": ""
    },
    {
      "title": "V-Agent: An Interactive Video Search System Using Vision-Language Models",
      "authors": "SunYoung Park, Jong-Hyeon Lee, Youngjune Kim, Daegyu Sung, Younghyun Yu, Young-rok Cha, Jeongho Ju",
      "institution": "NC AI, Kakao, Korea Advanced Institute of Science and Technology (KAIST)",
      "link": "https://arxiv.org/pdf/2512.16925",
      "code": null,
      "tags": [
        "multi-modal inference",
        "vision-language model",
        "fine-tuning",
        "retrieval vector",
        "re-ranking",
        "multi-agent system"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e827044257e239766415ac9500a06f7ddb67bfc47c517c041d42cc61ac33ad18_w640_q70.webp",
      "contributions": "",
      "summary": "V-Agent is a multi-agent video search system that fine-tunes a vision-language model with a small video preference dataset and enhances it with a retrieval vector to embed video frames and audio transcriptions into a shared multimodal space. It uses three agents—routing, search, and chat—to refine searches and interact with users, achieving state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark.",
      "mindmap": ""
    },
    {
      "title": "PAACE: A Plan-Aware Automated Agent Context Engineering Framework",
      "authors": "Kamer Ali Yuksel",
      "institution": "aiXplain Inc",
      "link": "https://arxiv.org/pdf/2512.16970",
      "code": null,
      "tags": [
        "llm inference",
        "context engineering",
        "plan-aware compression",
        "next-k-task relevance",
        "instruction co-refinement",
        "function-preserving compression",
        "synthetic data generation",
        "knowledge distillation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces PAACE, a framework for compressing the expanding context of LLM agents in multi-step workflows. It uses plan-aware techniques like next-k-task relevance modeling and function-preserving compression, trained on synthetic data and distilled into efficient models. The method improves agent accuracy while significantly reducing context load and inference costs.",
      "mindmap": ""
    },
    {
      "title": "MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval",
      "authors": "Saksham Sahai Srivastava, Haoyu He",
      "institution": "University of Georgia",
      "link": "https://arxiv.org/pdf/2512.16962",
      "code": null,
      "tags": [
        "llm inference",
        "retrieval-augmented generation",
        "long-term memory",
        "semantic imitation",
        "indirect injection attack",
        "memory poisoning",
        "MetaGPT",
        "DataInterpreter"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces MemoryGraft, a novel attack that poisons an LLM agent's long-term memory by implanting malicious successful experiences, which are then retrieved and imitated during future tasks. The method exploits the agent's semantic imitation heuristic through a poisoned RAG store, leading to persistent behavioral compromise. The authors demonstrate that this attack can cause significant and stealthy behavioral drift in agents like MetaGPT's DataInterpreter.",
      "mindmap": ""
    },
    {
      "title": "InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression",
      "authors": "Haotian Ye, Qiyuan He, Jiaqi Han, Puheng Li, Jiaojiao Fan, Zekun Hao, Fitsum Reda, Yogesh Balaji, Huayu Chen, Sheng Liu, Angela Yao, James Zou, Stefano Ermon, Haoxiang Wang, Ming-Yu Liu",
      "institution": "NVIDIA, Stanford University, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.16975",
      "code": null,
      "tags": [
        "multi-modal training",
        "discrete video tokenization",
        "transformer-based adaptive compressor",
        "evidence lower bound (ELBO)",
        "information-theoretic compression",
        "adaptive tokenization"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da83017a41e69234160f2c416b8a94507a537a66fd8a745a6bafc49c06817395_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces InfoTok, a principled framework for adaptive discrete video tokenization based on information theory, using a novel ELBO-based algorithm and a transformer-based adaptive compressor. It achieves state-of-the-art compression by allocating tokens according to informational richness, saving 20% of tokens without performance loss and outperforming prior heuristic approaches.",
      "mindmap": ""
    },
    {
      "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows",
      "authors": "Wanghan Xu, Yuhao Zhou, Yifan Zhou, Qinglong Cao, Shuo Li, Jia Bu, Bo Liu, Yixin Chen, Xuming He, Xiangyu Zhao, Xiang Zhuang, Fengxiang Wang, Zhiwang Zhou, Qiantai Feng, Wenxuan Huang, Jiaqi Wei, Hao Wu, Yuejin Yang, Guangshuai Wang, Sheng Xu, Ziyan Huang, Xinyao Liu, Jiyao Liu, Cheng Tang, Wei Li, Ying Chen, Junzhi Ning, Pengfei Jiang, Chenglong Ma, Ye Du, Changkai Ji, Huihui Xu, Ming Hu, Jiangbin Zheng, Xin Chen, Yucheng Wu, Feifei Jiang, Xi Chen, Xiangru Tang, Yuchen Fu, Yingzhou Lu, Yuanyuan Zhang, Lihao Sun, Chengbo Li, Jinzhe Ma, Wanhao Liu, Yating Liu, Kuo-Cheng Wu, Shengdu Chai, Yizhou Wang, Ouwen Zhangjin, Chen Tang, Shufei Zhang, Wenbo Cao, Junjie Ren, Taoyong Cui, Zhouheng Yao, Juntao Deng, Yijie Sun, Feng Liu, Wangxu Wei, Jingyi Xu, Zhangrui Li, Junchao Gong, Zijie Guo, Zhiyu Yao, Zaoyu Chen, Tianhao Peng, Fangchen Yu, Bo Zhang, Dongzhan Zhou, Shixiang Tang, Jiaheng Liu, Fenghua Ling, Yan Lu, Yuchen Ren, Ben Fei, Zhen Zhao, Xinyu Gu, Rui Su, Xiao-Ming Wu, Weikang Si, Yang Liu, Hao Chen, Xiangchao Yan, Xue Yang, Junchi Yan, Jiamin Wu, Qihao Zheng, Chenhui Li, Zhiqiang Gao, Hao Kong, Junjun He, Mao Su, Tianfan Fu, Peng Ye, Chunfeng Song, Nanqing Dong, Yuqiang Li, Huazhu Fu",
      "institution": "Shanghai Artificial Intelligence Laboratory",
      "link": "https://arxiv.org/pdf/2512.16969",
      "code": null,
      "tags": [
        "scientific ai evaluation",
        "Practical Inquiry Model (PIM)",
        "SGI-Bench",
        "Test-Time Reinforcement Learning (TTRL)",
        "retrieval-augmented novelty",
        "agent-based evaluation"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40fba3081819027f6af6208a55e87bd4bfc888d4ba6ce07d9baa5f158fbe6fa2_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes a framework for evaluating Scientific General Intelligence (SGI) in LLMs, grounded in the Practical Inquiry Model and operationalized through the SGI-Bench benchmark. The results reveal significant performance gaps across tasks like deep research and experimental reasoning. The authors also introduce Test-Time Reinforcement Learning (TTRL) to enhance hypothesis novelty without requiring reference answers.",
      "mindmap": ""
    },
    {
      "title": "Unexpected Knowledge: Auditing Wikipedia and Grokipedia Search Recommendations",
      "authors": "Erica Coppolillo, Simone Mungari",
      "institution": "University of Calabria, ICAR-CNR, University of Southern California",
      "link": "https://arxiv.org/pdf/2512.17027",
      "code": null,
      "tags": [
        "others",
        "search engine audit",
        "semantic alignment",
        "topical annotation",
        "trajectory analysis"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper conducts a comparative audit of search engine recommendations on Wikipedia and Grokipedia by analyzing over 70,000 results from nearly 10,000 neutral English word queries. It finds that both platforms frequently generate weakly related or unexpected results from innocuous queries, though their recommendation sets often differ substantially in topical distribution and exploration trajectories.",
      "mindmap": ""
    },
    {
      "title": "A Women's Health Benchmark for Large Language Models",
      "authors": "Victoria-Elisabeth Gruber, Razvan Marinescu, Diego Fajardo, Amin H. Nassar, Christopher Arkfeld, Alexandria Ludlow, Shama Patel, Mehrnoosh Samaei, Valerie Klug, Anna Huber, Marcel Gühner, Albert Botta i Orfila, Irene Lagoja, Kimya Tarr, Haleigh Larson, Mary Beth Howard",
      "institution": "Lumos AI, Yale Cancer Center, Harvard Medical School, UCSF, Brown University, Emory University, Clinic Ottakring, NHS, Yale School of Medicine, Johns Hopkins University School of Medicine",
      "link": "https://arxiv.org/pdf/2512.17028",
      "code": null,
      "tags": [
        "healthcare AI evaluation",
        "women's health benchmark",
        "large language models",
        "error types",
        "model stumps",
        "query types"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces the Women's Health Benchmark (WHB), a novel evaluation framework comprising 96 validated model stumps across five medical specialties, three query types, and eight error types to assess LLM performance in women's health. It finds that current LLMs have approximately 60% failure rates, with significant weaknesses in detecting urgency, indicating they are not yet reliable for providing women's health advice.",
      "mindmap": ""
    },
    {
      "title": "Security Risks of Agentic Vehicles: A Systematic Analysis of Cognitive and Cross-Layer Threats",
      "authors": "Ali Eslami, Jiangbo Yu",
      "institution": "Unknown",
      "link": "https://arxiv.org/pdf/2512.17041",
      "code": null,
      "tags": [
        "others",
        "agentic AI",
        "cross-layer threats",
        "role-based architecture",
        "severity matrix",
        "attack-chain analysis",
        "OWASP"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces a role-based architecture for Agentic Vehicles to systematically analyze security threats, including cognitive vulnerabilities and cross-layer risks. It concludes by providing a structured framework for assessing how small distortions can escalate into unsafe behavior in both human-driven and autonomous vehicles.",
      "mindmap": ""
    },
    {
      "title": "Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation",
      "authors": "Istiak Ahmed, Ripan Kumar Kundu, Khaza Anuarul Hoque",
      "institution": "University of Missouri-Columbia",
      "link": "https://arxiv.org/pdf/2512.17029",
      "code": null,
      "tags": [
        "others",
        "adversarial attacks",
        "deep learning",
        "cybersickness detection",
        "visual tunneling",
        "MI-FGSM",
        "PGD",
        "C&W",
        "DeepTCN",
        "Transformer"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a313962e09ceaa54a617d0e446a38a50ffa44d10894d76830f87cd1e74c0749_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces Adversarial-VR, an open-source Unity testbed that integrates DeepTCN and Transformer models for real-time cybersickness detection and mitigation, and evaluates their robustness against adversarial attacks like MI-FGSM, PGD, and C&W. The results show these attacks can successfully fool the system, significantly degrading model accuracy and preventing correct mitigation.",
      "mindmap": ""
    },
    {
      "title": "UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering",
      "authors": "Yinxu Tang, Chengsong Huang, Jiaxin Huang, William Yeoh",
      "institution": "Washington University in St. Louis",
      "link": "https://arxiv.org/pdf/2512.17043",
      "code": null,
      "tags": [
        "knowledge graph question answering",
        "reinforcement learning",
        "subgraph selection",
        "graph pruning",
        "llm fine-tuning",
        "relation-centric reasoning"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/069fd74faaf7500b76c5bd6958a190a707d8ccbe86484c3026a357b38657a47a_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces UniRel-R1, a framework for relation-centric knowledge graph question answering that integrates subgraph selection, multi-stage graph pruning, and an LLM fine-tuned with reinforcement learning. The method is designed to identify compact and informative subgraph answers by rewarding specific relations and lower-degree entities. Experiments show it outperforms baselines in connectivity and reward and generalizes well to unseen entities and relations.",
      "mindmap": ""
    },
    {
      "title": "Realistic threat perception drives intergroup conflict: A causal, dynamic analysis using generative-agent simulations",
      "authors": "Suhaib Abdurahman, Farzan Karimi-Malekabadi, Chenxiao Yu, Nour S. Kteily, Morteza Dehghani",
      "institution": "University of Southern California, Northwestern University",
      "link": "https://arxiv.org/pdf/2512.17066",
      "code": null,
      "tags": [
        "social simulation",
        "large language model",
        "generative agents",
        "causal analysis",
        "intergroup conflict",
        "threat perception"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper uses simulations with LLM-driven generative agents in virtual societies to causally analyze intergroup conflict. It finds that realistic threat directly increases hostility, while symbolic threat has a weaker effect mediated by ingroup bias and only increases hostility when realistic threat is absent.",
      "mindmap": ""
    },
    {
      "title": "Knowledge Distillation with Structured Chain-of-Thought for Text-to-SQL",
      "authors": "Khushboo Thaker, Yony Bresler",
      "institution": "Crater Labs",
      "link": "https://arxiv.org/pdf/2512.17053",
      "code": null,
      "tags": [
        "llm training",
        "knowledge distillation",
        "chain-of-thought",
        "structured reasoning",
        "query execution plan",
        "text-to-sql"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/906b49597857a3cad8e1c9c8d6cdbec46e7807fe819943d1e6d91facfb7f18bd_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes Struct-SQL, a knowledge distillation framework that trains a small language model using a structured chain-of-thought derived from query execution plans, rather than unstructured reasoning traces. The distilled model achieves an 8.1% absolute improvement over an unstructured baseline, primarily due to a reduction in syntactic errors. This demonstrates that structured logical blueprints are beneficial for reliable SQL generation in small models.",
      "mindmap": ""
    },
    {
      "title": "Bots Don't Sit Still: A Longitudinal Study of Bot Behaviour Change, Temporal Drift, and Feature-Structure Evolution",
      "authors": "Ohoud Alzahrani, Russell Beale, Bob Hendley",
      "institution": "University of Birmingham",
      "link": "https://arxiv.org/pdf/2512.17067",
      "code": null,
      "tags": [
        "social media analysis",
        "Augmented Dickey-Fuller test",
        "KPSS test",
        "Spearman correlation",
        "Chi-square test",
        "time series analysis",
        "stationarity testing"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper conducts a longitudinal study analyzing the temporal behavior of promotional Twitter bots using time series analysis and statistical tests on ten content-based meta-features. It finds that bot behavior is non-stationary, with individual features and their interdependencies evolving systematically over time and across bot generations. The conclusion is that bot-detection systems must account for this dynamic adaptation and avoid treating behavioral features as static.",
      "mindmap": ""
    },
    {
      "title": "On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues",
      "authors": "Monika Zamojska, Jarosław A. Chudziak",
      "institution": "Warsaw University of Technology",
      "link": "https://arxiv.org/pdf/2512.17060",
      "code": null,
      "tags": [
        "others",
        "multi-agent system",
        "transactional analysis",
        "ego states",
        "information retrieval",
        "vector stores",
        "ablation test"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ea02a6d58821ccc3cc89deee5daf014a7e650e133502a2a64e5cd69e54c8596_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes a multi-agent system architecture that integrates Transactional Analysis theory, dividing each agent into Parent, Adult, and Child ego states, and enhances their responses with contextual information retrieval from vector stores. The system is evaluated through ablation tests in a simulated dialogue scenario. The results show that this psychologically grounded structure improves the realism of LLM-based agent behavior.",
      "mindmap": ""
    },
    {
      "title": "Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?",
      "authors": "Saraswathy Amjith, Mihika Dusad, Neha Muramalla, Shweta Shah",
      "institution": "MIT",
      "link": "https://arxiv.org/pdf/2512.17079",
      "code": null,
      "tags": [
        "mathematical reasoning",
        "chain-of-thought prompting",
        "reinforcement learning",
        "GRPO",
        "fine-tuning",
        "error recovery"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9501255b38adfbd4ed3cb05e9a136df6cf358b6420281716744f14c17554a871_w640_q70.webp",
      "contributions": "",
      "summary": "The paper fine-tunes the Qwen3-4B model using GRPO reinforcement learning on intentionally flawed chain-of-thought reasoning traces to improve error detection and recovery. It finds that this mixed training on both calculation and reasoning errors improves robustness to misleading prefills without sacrificing accuracy on clean problems, unlike standard fine-tuning which degrades robustness.",
      "mindmap": ""
    },
    {
      "title": "When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation",
      "authors": "Michael H. Coen",
      "institution": "Independent Researcher",
      "link": "https://arxiv.org/pdf/2512.17083",
      "code": null,
      "tags": [
        "dialogue topic segmentation",
        "window-tolerant F1",
        "boundary density",
        "segment coherence",
        "granularity-aware evaluation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces a new evaluation framework for dialogue topic segmentation that emphasizes boundary density and segment coherence alongside window-tolerant F1. It demonstrates through cross-dataset experiments that reported performance differences are often artifacts of annotation granularity mismatches, not model quality. The core conclusion is that topic segmentation should be viewed as selecting an appropriate granularity rather than predicting a single correct boundary set.",
      "mindmap": ""
    },
    {
      "title": "Value Under Ignorance in Universal Artificial Intelligence",
      "authors": "Cole Wyeth, Marcus Hutter",
      "institution": "University of Waterloo, Google DeepMind, Australian National University",
      "link": "https://arxiv.org/pdf/2512.17086",
      "code": null,
      "tags": [
        "reinforcement learning",
        "AIXI",
        "Choquet integrals",
        "imprecise probability theory",
        "semimeasure loss",
        "utility functions"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper generalizes the AIXI reinforcement learning agent to handle a wider class of utility functions, confronting the ambiguity of finite history predictions by interpreting belief distributions as imprecise probabilities. It explores computing expected utilities using Choquet integrals from imprecise probability theory and investigates their computability. The authors show that the standard recursive value function is a special case, but the most general utilities under a \"death interpretation\" cannot be characterized by these integrals.",
      "mindmap": ""
    },
    {
      "title": "How to Square Tensor Networks and Circuits Without Squaring Them",
      "authors": "Lorenzo Loconte, Adrián Javaloy, Antonio Vergari",
      "institution": "University of Edinburgh",
      "link": "https://arxiv.org/pdf/2512.17090",
      "code": null,
      "tags": [
        "probabilistic modeling",
        "tensor networks",
        "squared circuits",
        "probabilistic circuits",
        "marginalization",
        "canonical forms",
        "unitary matrices",
        "distribution estimation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a method to parameterize squared circuits (a generalization of squared tensor networks) using conditions inspired by orthogonality and determinism, enabling efficient marginalization without squaring. This approach overcomes computational overhead while maintaining expressiveness for distribution estimation. Experiments confirm the method allows more efficient learning without loss of expressiveness.",
      "mindmap": ""
    },
    {
      "title": "Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making",
      "authors": "Toshiaki Hori, Jonathan DeCastro, Deepak Gopinath, Avinash Balachandran, Guy Rosman",
      "institution": "Toyota Research Institute",
      "link": "https://arxiv.org/pdf/2512.17091",
      "code": null,
      "tags": [
        "reinforcement learning",
        "reinforcement learning",
        "model predictive control",
        "MPPI",
        "hierarchical planning",
        "adaptive sampling"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a method that fuses reinforcement learning and model-predictive control (MPC) into an adaptive hierarchical framework. It uses RL actions to guide the MPPI sampler and adaptively aggregates MPPI samples to improve value estimation, leading to more robust and sample-efficient policies. The approach demonstrates improved data efficiency, performance, and convergence speed in domains like race driving and Lunar Lander.",
      "mindmap": ""
    },
    {
      "title": "UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data",
      "authors": "Justin Li, Efe Sencan, Jasper Zheng Duan, Vitus J. Leung, Stephan Tsaur, Ayse K. Coskun",
      "institution": "Boston University, Sandia National Laboratories, Boston Medical Center",
      "link": "https://arxiv.org/pdf/2512.17100",
      "code": null,
      "tags": [
        "interpretability",
        "counterfactual explanations",
        "model-agnostic",
        "time series",
        "ECG",
        "LIME",
        "SHAP"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces UniCoMTE, a universal, model-agnostic framework for generating counterfactual explanations for time series classifiers by modifying input samples to identify influential temporal features. It is evaluated on an ECG classifier and shown to produce more concise, stable, and human-aligned explanations than established methods like LIME and SHAP, thereby improving model interpretability for real-world applications.",
      "mindmap": ""
    },
    {
      "title": "A Solver-in-the-Loop Framework for Improving LLMs on Answer Set Programming for Logic Puzzle Solving",
      "authors": "Timo Pierre Schrader, Lukas Lange, Tobias Kaminski, Simon Razniewski, Annemarie Friedrich",
      "institution": "Bosch Center for AI, University of Augsburg, ScaDS.AI & TU Dresden",
      "link": "https://arxiv.org/pdf/2512.17093",
      "code": null,
      "tags": [
        "llm training",
        "solver-in-the-loop",
        "instruction-tuning",
        "supervised fine-tuning",
        "best-of-N sampling",
        "answer set programming",
        "semantic parsing"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/38c83df2ce552270bc09f323934a96a0aad16af58e736a7049ccfd73afeed0d4_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces a solver-in-the-loop framework that uses an ASP solver to provide feedback on LLM-generated code, creating a dataset of chosen and rejected instances for supervised fine-tuning. The method improves LLM performance on generating Answer Set Programming code for logic puzzles, demonstrating consistent gains across different prompting settings and datasets.",
      "mindmap": ""
    },
    {
      "title": "Reinforcement Learning for Self-Improving Agent with Skill Library",
      "authors": "Jiongxiao Wang, Qiaojing Yan, Yawei Wang, Yijun Tian, Soumya Smruti Mishra, Zhichao Xu, Megha Gandhi, Panpan Xu, Lin Lee Cheong",
      "institution": "University of Wisconsin–Madison, AWS Agentic AI",
      "link": "https://arxiv.org/pdf/2512.17102",
      "code": null,
      "tags": [
        "post-training",
        "reinforcement learning",
        "skill library",
        "sequential rollout",
        "skill-integrated reward",
        "GRPO",
        "self-improving agent"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09fdb48e8df3d2e43c1e8301082bd3478f7894eef19c7194ccd54fb1c77738ef_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes SAGE, a reinforcement learning framework that enhances LLM-based agents by integrating a skill library through sequential rollouts and a skill-integrated reward. This approach enables agents to accumulate and reuse skills across tasks for continual self-improvement. Experiments show SAGE improves task completion accuracy while significantly reducing interaction steps and token usage compared to existing methods.",
      "mindmap": ""
    },
    {
      "title": "Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs",
      "authors": "Aaron Defazio, Konstantin Mishchenko, Parameswaran Raman, Hao-Jun Michael Shi, Lin Xiao",
      "institution": "Meta Superintelligence Labs",
      "link": "https://arxiv.org/pdf/2512.17131",
      "code": null,
      "tags": [
        "llm training",
        "Generalized Primal Averaging (GPA)",
        "DiLoCo",
        "Schedule-Free",
        "AdamW",
        "Nesterov's method",
        "primal averaging",
        "optimizer",
        "iterate averaging"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes Generalized Primal Averaging (GPA), a new optimizer that extends Nesterov's method to perform smooth, per-step averaging of model iterates, addressing limitations of periodic averaging methods like single-worker DiLoCo. It demonstrates that GPA outperforms single-worker DiLoCo, simplifies hyperparameter tuning, reduces memory overhead, and achieves significant speedups in training LLMs and vision models compared to AdamW.",
      "mindmap": ""
    },
    {
      "title": "SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction",
      "authors": "Puyang Wang, Pengfei Guo, Keyi Chai, Jinyuan Zhou, Daguang Xu, Shanshan Jiang",
      "institution": "Johns Hopkins University, NVIDIA",
      "link": "https://arxiv.org/pdf/2512.17137",
      "code": null,
      "tags": [
        "others",
        "deep unrolled model",
        "Restormer",
        "learned coil sensitivity map estimator",
        "sampling-aware weighted data consistency",
        "universal conditioning",
        "progressive cascade expansion training"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13a0f6637b571493e14f364092f2d39a108d211bbddd588a88df25d1db4a8e1c_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces SDUM, a scalable deep unrolled model for universal MRI reconstruction that integrates a Restormer-based reconstructor, learned coil sensitivity estimation, and sampling-aware data consistency. It demonstrates predictable performance scaling with model depth and achieves state-of-the-art results across diverse clinical MRI protocols without task-specific fine-tuning. The work establishes a practical framework for a single, universally applicable reconstruction model in clinical environments.",
      "mindmap": ""
    },
    {
      "title": "Solomonoff-Inspired Hypothesis Ranking with LLMs for Prediction Under Uncertainty",
      "authors": "Josh Barber, Rourke Young, Cameron Coombe, Will Browne",
      "institution": "Queensland University of Technology, CSIRO",
      "link": "https://arxiv.org/pdf/2512.17145",
      "code": null,
      "tags": [
        "algorithmic information theory",
        "Solomonoff induction",
        "Bayesian Model Averaging",
        "hypothesis ranking",
        "systematic generalisation",
        "uncertainty estimation"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fb373087fc2ec93e8e3a1729cbb4c2df71ab2a3e5c7a3936acdba21fa6ee2c9_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes a method that uses a Solomonoff-inspired scoring to weight hypotheses generated by a Large Language Model based on their simplicity and predictive fit. The method, applied to Mini-ARC tasks, produces uncertainty-aware predictions by spreading probability across multiple hypotheses, contrasting with Bayesian Model Averaging which tends to concentrate weight on a single candidate. The results highlight the value of algorithmic information-theoretic priors for robust, interpretable reasoning under uncertainty.",
      "mindmap": ""
    },
    {
      "title": "Conservative Bias in Multi-Teacher Learning: Why Agents Prefer Low-Reward Advisors",
      "authors": "Maher Mesto, Francisco Cruz",
      "institution": "University of New South Wales, Universidad Central de Chile",
      "link": "https://arxiv.org/pdf/2512.17180",
      "code": null,
      "tags": [
        "reinforcement learning",
        "interactive reinforcement learning",
        "multi-teacher learning",
        "Q-learning",
        "teacher selection",
        "concept drift"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces a multi-teacher interactive reinforcement learning framework where agents can select advice from teachers with different reward structures. The core finding is that agents exhibit a strong conservative bias, overwhelmingly preferring low-reward but consistent teachers over high-reward ones, which challenges traditional reward-maximization assumptions in RL.",
      "mindmap": ""
    },
    {
      "title": "PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases",
      "authors": "Ripan Kumar Kundu, Istiak Ahmed, Khaza Anuarul Hoque",
      "institution": "University of Missouri-Columbia",
      "link": "https://arxiv.org/pdf/2512.17172",
      "code": null,
      "tags": [
        "llm inference",
        "large language model",
        "explainable ai",
        "augmented reality",
        "personalized explanations",
        "real-time object detection",
        "user study"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7767a7bc851a49f82148423782803913a5c377f60c4ce3a9cc7383c22a6d08a4_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes PILAR, a framework that uses a pre-trained large language model (LLM) to generate unified, context-aware, and personalized explanations for AI-driven augmented reality systems. A user study on a recipe recommendation prototype showed that the LLM-based explanation interface significantly improved user task performance and perceived transparency compared to a traditional template-based approach.",
      "mindmap": ""
    }
  ]
}