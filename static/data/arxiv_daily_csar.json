{
  "label": "cs.AR",
  "slug": "csar",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "Power Side-Channel Analysis of the CVA6 RISC-V Core at the RTL Level Using VeriSide",
      "authors": "Behnam Farnaghinejad, Antonio Porsia, Annachiara Ruospo, Alessandro Savino, Stefano Di Carlo, Ernesto Sanchez",
      "institution": "Politecnico di Torino",
      "link": "https://arxiv.org/pdf/2512.21362",
      "code": null,
      "tags": [
        "side-channel analysis",
        "RISC-V",
        "CVA6",
        "Correlation Power Analysis (CPA)",
        "RTL simulation",
        "power side-channel"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f1045d9314e37006547d2c94a7c4490ff95449687fd13d7c467003b4c095bac_w640_q70.webp",
      "contributions": "1. Presents the first side-channel vulnerability evaluation of the CVA6 RISC-V processor core. 2. Demonstrates the application of the VeriSide RTL-level power profiling framework for efficient power trace extraction without waveform files. 3. Shows that Correlation Power Analysis (CPA) on the CVA6 during software-based AES encryption enables key recovery, highlighting the need for early-stage RTL security assessments.",
      "summary": "This paper analyzes the power side-channel vulnerability of the CVA6 RISC-V core using the VeriSide RTL simulation framework. By applying Correlation Power Analysis (CPA) to power traces during software AES execution, the authors successfully recover the secret key. The findings demonstrate significant leakage in the CVA6 design, emphasizing the importance of pre-silicon RTL-level security evaluation.",
      "mindmap": "graph TB\n        A[Power Side-Channel Analysis of the CVA6 RISC-V Core at the RTL Level Using VeriSide] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现代RISC-V处理器需要抗侧信道攻击能力 / Modern RISC-V processors require resilience to side-channel attacks]\n        C --> C1[使用VeriSide框架在RTL级进行功耗分析 / Use VeriSide framework for RTL-level power analysis]\n        C --> C2[对软件AES执行进行相关性功耗分析(CPA) / Perform Correlation Power Analysis (CPA) on software AES execution]\n        D --> D1[CVA6设计存在显著泄漏 / CVA6 design exhibits significant leakage]\n        D --> D2[成功恢复AES密钥 / Successful AES key recovery]"
    },
    {
      "title": "Online Learning Extreme Learning Machine with Low-Complexity Predictive Plasticity Rule and FPGA Implementation",
      "authors": "Zhenya Zang, Xingda Li, David Day Uei Li",
      "institution": "University of Strathclyde",
      "link": "https://arxiv.org/pdf/2512.21777",
      "code": null,
      "tags": [
        "on-device ai",
        "predictive plasticity rule",
        "extreme learning machine",
        "FPGA implementation",
        "online learning",
        "low-complexity training"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fd9efea1b6faa314e48c1a0c90ad6c7bf79112ed59bfb1db6f14146d142848_w640_q70.webp",
      "contributions": "1. Proposed a simplified, biologically inspired predictive local learning rule that eliminates global backpropagation and membrane integration, using sparse, binary-driven vector additions triggered only by prediction errors. 2. Integrated this rule into an Extreme Learning Machine (ELM), replacing the conventional matrix inversion, thereby reducing training complexity from O(M^3) to O(M) for M hidden nodes. 3. Demonstrated an FPGA implementation showing significant reductions in computational and memory requirements, highlighting its potential for energy-efficient online learning on low-cost edge devices.",
      "summary": "This paper proposes a low-complexity online learning method by integrating a simplified predictive plasticity rule into an Extreme Learning Machine (ELM), which reduces training complexity from cubic to linear order. The approach is implemented on FPGA, showing reduced computational and memory needs while maintaining comparable accuracy. The work demonstrates strong potential for enabling efficient online learning on resource-constrained edge devices.",
      "mindmap": "graph TB\n        Root[”Online Learning ELM with Low-Complexity Predictive Plasticity Rule and FPGA Implementation<br/>在线学习ELM与低复杂度预测可塑性规则及FPGA实现”]\n        Root --> Problem[”Problem: High complexity of online learning for edge devices<br/>问题：面向边缘设备的在线学习复杂度高”]\n        Root --> Method[”Method: Simplified predictive plasticity rule + ELM<br/>方法：简化的预测可塑性规则 + ELM”]\n        Root --> Results[”Results: O(M) training, FPGA implementation, low resource use<br/>结果：O(M)训练，FPGA实现，低资源消耗”]"
    },
    {
      "title": "Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling",
      "authors": "Hannah Atmer, Yuan Yao, Thiemo Voigt, Stefanos Kaxiras",
      "institution": "Uppsala University",
      "link": "https://arxiv.org/pdf/2512.22066",
      "code": null,
      "tags": [
        "llm inference",
        "SRAM",
        "frequency scaling",
        "energy-delay product",
        "systolic array",
        "memory bandwidth"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp",
      "contributions": "1. Quantified the distinct energy and performance impacts of SRAM size and operating frequency on the compute-bound prefill and memory-bound decode phases of LLM inference. 2. Demonstrated a counter-intuitive result: high compute frequency can reduce total energy by shortening execution time and reducing static energy more than the dynamic power increase. 3. Identified an optimal hardware configuration (high frequency, small SRAM buffer) that minimizes the energy-delay product, providing concrete design insights for energy-efficient accelerators.",
      "summary": "This paper investigates how on-chip SRAM size and operating frequency affect the energy efficiency of LLM inference. Using a simulation methodology combining OpenRAM, LLMCompass, and ScaleSIM, it finds that a high-frequency, small-SRAM configuration optimizes the energy-delay product, as memory bandwidth caps decode phase improvements. The analysis provides architectural guidance for designing efficient LLM accelerators.",
      "mindmap": "graph TB\n        Root[”Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>LLM推理能耗高，Prefill与Decode阶段瓶颈不同”] --> Problem_Sub1[”SRAM大小与频率如何影响能效？”]\n        Problem --> Problem_Sub2[”内存带宽如何限制性能？”]\n        Method[”主要方法/Method<br>结合OpenRAM, LLMCompass, ScaleSIM的模拟方法”] --> Method_Sub1[”能耗建模/Energy Modeling”]\n        Method --> Method_Sub2[”延迟模拟/Latency Simulation”]\n        Method --> Method_Sub3[”操作强度分析/Operational Intensity”]\n        Results[”关键结果/Results”] --> Results_Sub1[”总能耗主要由SRAM大小决定<br>大缓存增加静态能耗”]\n        Results --> Results_Sub2[”高频可降低总能耗<br>（减少静态能耗）”]\n        Results --> Results_Sub3[”最优配置：高频(1200-1400MHz) + 小缓存(32-64KB)”]"
    },
    {
      "title": "NotSoTiny: A Large, Living Benchmark for RTL Code Generation",
      "authors": "Razine Moundir Ghorab, Emanuele Parisi, Cristian Gutierrez, Miquel Alberti-Binimelis, Miquel Moreto, Dario Garcia-Gasulla, Gokcen Kestor",
      "institution": "Barcelona Supercomputing Center, Universitat Politecnica de Catalunya",
      "link": "https://arxiv.org/pdf/2512.20823",
      "code": null,
      "tags": [
        "llm training",
        "RTL code generation",
        "benchmark",
        "hardware design",
        "data contamination",
        "verification"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ee59b5723cdae955454bd94bdc8872b40c0eaccf59a4e54b86951d040529325_w640_q70.webp",
      "contributions": "1. Introduces NotSoTiny, a large-scale, living benchmark for evaluating LLMs on RTL code generation, built from real hardware designs. 2. Proposes an automated pipeline to ensure benchmark quality by removing duplicates, verifying correctness, and periodically updating to mitigate data contamination. 3. Demonstrates that NotSoTiny presents more challenging tasks than prior benchmarks, effectively highlighting current LLM limitations in hardware design.",
      "summary": "This paper introduces NotSoTiny, a benchmark for evaluating LLMs on generating Register-Transfer Level (RTL) code, addressing limitations of prior benchmarks by using real, complex hardware designs and a pipeline to ensure correctness and reduce data contamination. The results show that NotSoTiny tasks are more challenging, effectively guiding the improvement of LLMs for hardware design.",
      "mindmap": "graph LR\n    A[NotSoTiny: A Large, Living Benchmark for RTL Code Generation] --> B(核心问题/Problem: LLM RTL代码生成评估挑战 / LLM RTL Code Generation Evaluation Challenge)\n    A --> C(主要方法/Method: 基于真实硬件设计的自动化基准测试 / Automated Benchmark from Real Hardware Designs)\n    A --> D(关键结果/Results: 任务更具挑战性，有效指导改进 / Tasks More Challenging, Effectively Guides Improvement)"
    },
    {
      "title": "ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update",
      "authors": "Zhe Su, Giacomo Indiveri",
      "institution": "Institute of Neuroinformatics, University of Zurich and ETH Zurich",
      "link": "https://arxiv.org/pdf/2512.21153",
      "code": null,
      "tags": [
        "on-device ai",
        "spiking neural network",
        "self-supervised learning",
        "structured sparsity",
        "activity-dependent update",
        "event-driven processing"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d2912d0f31a3824ddb49ba116a841b201d285560ef2177125f8dac188572270_w640_q70.webp",
      "contributions": "1. A local online self-supervised learning engine enabling multi-layer temporal learning without labeled data. 2. A dynamic structured sparse training engine supporting high-accuracy sparse-to-sparse learning. 3. An activity-dependent sparse weight update mechanism that gates updates based on input activity and network dynamics.",
      "summary": "This paper introduces ElfCore, a 28nm digital spiking neural network processor designed for event-driven sensory processing. It uniquely integrates online self-supervised learning, dynamic structured sparse training, and activity-dependent weight updates. The chip demonstrates significant improvements in power consumption, memory efficiency, and network capacity across various tasks like gesture and speech recognition.",
      "mindmap": "graph LR\n    A[ElfCore] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[边缘设备需要高效、自适应的稀疏事件处理/Edge devices need efficient, adaptive sparse event processing]\n    C --> C1[集成在线自监督学习/Integrate Online Self-Supervised Learning]\n    C --> C2[动态结构化稀疏训练/Dynamic Structured Sparse Training]\n    C --> C3[活动依赖权重更新/Activity-Dependent Weight Update]\n    D --> D1[功耗降低16倍/16x Lower Power]\n    D --> D2[片上内存减少3.8倍/3.8x Less On-Chip Memory]\n    D --> D3[网络容量效率提升5.9倍/5.9x Greater Network Capacity]"
    },
    {
      "title": "3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras",
      "authors": "Hongyang Shang, Shuai Dong, Ye Ke, Arindam Basu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20073",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32bc75b2d6c95f1b2dc70a4aff51224520c7f222f61a22edbcd56ede5fa299a5_w640_q70.webp",
      "contributions": "",
      "summary": "3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras",
      "mindmap": ""
    },
    {
      "title": "Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling",
      "authors": "Huizheng Wang, Taiquan Wei, Hongbin Wang, Zichuan Wang, Xinru Tang, Zhiheng Yue, Shaojun Wei, Yang Hu, Shouyi Yin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20198",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e2bc2f765accf0fbb24e05e708bd3a2b62c961a86def9137c9976aa8b753f85_w640_q70.webp",
      "contributions": "",
      "summary": "Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling",
      "mindmap": ""
    },
    {
      "title": "Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization",
      "authors": "He Zhu, Zheng Liu, Xingyang Li, Anbang Wu, Jieru Zhao, Fangxin Liu, Yiming Gan, Jingwen Leng, Yu Feng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20495",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1ba8a7f58cc0b9807b31889823cb7497448ae885dece28021d7b9eda1828c58_w640_q70.webp",
      "contributions": "",
      "summary": "Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization",
      "mindmap": ""
    },
    {
      "title": "Composing Mini Oscilloscope on Embedded Systems",
      "authors": "Brennan Romero, D.G. Perera",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20571",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c64819e1e69202dbb50470d8750cc2e3fb023b25140bd80e5d0c7dddb4b62612_w640_q70.webp",
      "contributions": "",
      "summary": "Composing Mini Oscilloscope on Embedded Systems",
      "mindmap": ""
    },
    {
      "title": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs",
      "authors": "Rupanshu Soi, Rohan Yadav, Fredrik Kjolstad, Alex Aiken, Maryam Mehri Dehnavi, Michael Garland, Michael Bauer",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18134",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cfd081b538bd7f4d1c91e06ba3fae36d2a230ad65cf3fc2e5160c3bdd9d98b3_w640_q70.webp",
      "contributions": "",
      "summary": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs",
      "mindmap": ""
    },
    {
      "title": "Making Strong Error-Correcting Codes Work Effectively for HBM in AI Inference",
      "authors": "Rui Xie, Yunhua Fang, Asad Ul Haq, Linsen Ma, Sanchari Sen, Swagath Venkataramani, Liu Liu, Tong Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18152",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9099b3e60f5155323e33ca2663b734d9691284bb3f33ff23ded103fa2caa02e8_w640_q70.webp",
      "contributions": "",
      "summary": "Making Strong Error-Correcting Codes Work Effectively for HBM in AI Inference",
      "mindmap": ""
    },
    {
      "title": "PermuteV: A Performant Side-channel-Resistant RISC-V Core Securing Edge AI Inference",
      "authors": "Nuntipat Narkthong, Xiaolin Xu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18132",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5249dae70a6ea951da7229a4606840eb6080d2d0b091828439fd7dc722b3fe4e_w640_q70.webp",
      "contributions": "",
      "summary": "PermuteV: A Performant Side-channel-Resistant RISC-V Core Securing Edge AI Inference",
      "mindmap": ""
    },
    {
      "title": "PIM-FW: Hardware-Software Co-Design of All-pairs Shortest Paths in DRAM",
      "authors": "Tsung-Han Lu, Zheyu Li, Minxuan Zhou, Tajana Rosing",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18158",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c0e2dda9beb3cd09d48e66d5bd1e2e0d25dcf0ff68f729ae40eea17980532c9_w640_q70.webp",
      "contributions": "",
      "summary": "PIM-FW: Hardware-Software Co-Design of All-pairs Shortest Paths in DRAM",
      "mindmap": ""
    },
    {
      "title": "BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism",
      "authors": "Suhas Vittal, Moinuddin Qureshi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18300",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d48040e52a6d8197d1665d09704221d54a7d6dab03a13f7863e398d9cb759275_w640_q70.webp",
      "contributions": "",
      "summary": "BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism",
      "mindmap": ""
    },
    {
      "title": "Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration",
      "authors": "Wonseok Choi, Hyunah Yu, Jongmin Kim, Hyesung Ji, Jaiyoung Park, Jung Ho Ahn",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18345",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e4c078425f57860f1303fe0449ae500adae2d4acf07294f16f8ed22a154bad_w640_q70.webp",
      "contributions": "",
      "summary": "Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration",
      "mindmap": ""
    },
    {
      "title": "Weight Transformations in Bit-Sliced Crossbar Arrays for Fault Tolerant Computing-in-Memory: Design Techniques and Evaluation Framework",
      "authors": "Akul Malhotra, Sumeet Kumar Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18459",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97e8d699ae3dc2331ca317ecfd53fe4db9ae0d861417dcb1143bdade9d3ecfa0_w640_q70.webp",
      "contributions": "",
      "summary": "Weight Transformations in Bit-Sliced Crossbar Arrays for Fault Tolerant Computing-in-Memory: Design Techniques and Evaluation Framework",
      "mindmap": ""
    },
    {
      "title": "DNA-HHE: Dual-mode Near-network Accelerator for Hybrid Homomorphic Encryption on the Edge",
      "authors": "Yifan Zhao, Xinglong Yu, Yi Sun, Honglin Kuang, Jun Han",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18589",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1feca70dc6f2a05e35a48f6366ccdcc1accb232952639d7388e6316ca2ff652_w640_q70.webp",
      "contributions": "",
      "summary": "DNA-HHE: Dual-mode Near-network Accelerator for Hybrid Homomorphic Encryption on the Edge",
      "mindmap": ""
    },
    {
      "title": "Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA",
      "authors": "Emir Devlet Ertörer, Cem Ünsalan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19304",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f352565f0530b4ee02ed15da3cf6f6c724c3dee282c3ce8dcec4f2bb7a8bed00_w640_q70.webp",
      "contributions": "",
      "summary": "Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA",
      "mindmap": ""
    },
    {
      "title": "Sensitivity-Aware Mixed-Precision Quantization for ReRAM-based Computing-in-Memory",
      "authors": "Guan-Cheng Chen, Chieh-Lin Tsai, Pei-Hsuan Tsai, Yuan-Hao Chang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19445",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/731a9d0dfb14219a43a9ad49271f63aef43ed484776bc0682810b6c431e5f06e_w640_q70.webp",
      "contributions": "",
      "summary": "Sensitivity-Aware Mixed-Precision Quantization for ReRAM-based Computing-in-Memory",
      "mindmap": ""
    },
    {
      "title": "LLM-based Behaviour Driven Development for Hardware Design",
      "authors": "Rolf Drechsler, Qian Liu",
      "institution": "University of Bremen, DFKI",
      "link": "https://arxiv.org/pdf/2512.17814",
      "code": null,
      "tags": [
        "others",
        "Behavior Driven Development (BDD)",
        "Large Language Models (LLMs)",
        "hardware design",
        "test and verification",
        "natural language processing",
        "Electronic Design Automation (EDA)"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper investigates the use of Large Language Models (LLMs) to automate the generation of behavioral scenarios from textual specifications for Behavior Driven Development (BDD) in hardware design. The core method involves applying LLM-based techniques to interpret specifications and produce high-level behavioral descriptions. The main conclusion is that LLMs offer a promising opportunity to support and automate BDD workflows in hardware design, addressing the manual effort and complexity of current verification practices.",
      "mindmap": ""
    },
    {
      "title": "AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines",
      "authors": "Dimitrios Danopoulos, Enrico Lupi, Chang Sun, Sebastian Dittmeier, Michael Kagan, Vladimir Loncar, Maurizio Pierini",
      "institution": "European Organization for Nuclear Research (CERN), Institute of Physics Belgrade, Heidelberg University",
      "link": "https://arxiv.org/pdf/2512.15946",
      "code": null,
      "tags": [
        "llm inference",
        "AI Engine (AIE)",
        "GEMV",
        "quantization",
        "on-chip data movement",
        "graph placement and search",
        "low-latency inference",
        "fused bias addition and ReLU"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper presents AIE4ML, an end-to-end framework that automatically compiles neural networks into optimized firmware for AMD's AI Engine-ML accelerators. It achieves high efficiency through a structured parallelization method that scales across the 2D fabric and uses a novel graph placement algorithm for on-chip execution. The framework delivers GPU-class throughput with microsecond latency, making it suitable for ultra-low-latency applications like particle physics trigger systems.",
      "mindmap": ""
    }
  ]
}