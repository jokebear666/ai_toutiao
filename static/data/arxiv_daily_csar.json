{
  "label": "cs.AR",
  "slug": "csar",
  "week": "20260105-20260111",
  "items": [
    {
      "title": "Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation",
      "authors": "Elham Cheshmikhani, Hamed Farbeh, Hossein Asadi",
      "institution": "Sharif University of Technology, Amirkabir University of Technology",
      "link": "https://arxiv.org/pdf/2601.00450",
      "code": null,
      "tags": [
        "memory & caching",
        "STT-MRAM",
        "read disturbance",
        "cache reliability",
        "ECC",
        "REAP-cache"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e7f78ea49ca100ac89e593165b5879dec1063a4cf12be706bf9dd1db3c8f455_w640_q70.webp",
      "contributions": "1. Introduces and formulates the phenomenon of read disturbance accumulation in STT-MRAM caches caused by speculative parallel reads during tag comparison. 2. Proposes the REAP-cache (Read Error Accumulation Preventer) scheme to eliminate this accumulation without compromising cache performance. 3. Demonstrates that REAP-cache significantly improves reliability (extending MTTF by 171x) with minimal overhead (less than 1% area and 2.7% energy increase).",
      "summary": "This paper identifies that the conventional parallel read of all blocks in a cache set for tag comparison leads to the accumulation of uncorrected read disturbance errors in STT-MRAM caches, degrading reliability. To solve this, the authors propose the REAP-cache scheme, which prevents this error accumulation. Their evaluation shows REAP-cache dramatically improves cache reliability (171x longer MTTF) with very low area and energy overheads.",
      "mindmap": "graph TB\n        A[Enhancing Reliability of STT-MRAM Caches<br>提升STT-MRAM缓存可靠性] --> B\n        A --> C\n        A --> D\n        B[问题: 读取干扰累积降低可靠性<br>Problem: Read Disturbance Accumulation Degrades Reliability]\n        C[方法: 提出REAP-cache方案<br>Method: Propose REAP-cache Scheme]\n        D[结果: MTTF提升171倍，开销极小<br>Results: 171x MTTF Improvement, Minimal Overhead]"
    },
    {
      "title": "ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches",
      "authors": "Elham Cheshmikhani, Hamed Farbeh, Hossein Asadi",
      "institution": "Sharif University of Technology, Amirkabir University of Technology",
      "link": "https://arxiv.org/pdf/2601.00456",
      "code": null,
      "tags": [
        "memory & caching",
        "STT-MRAM",
        "Error-Correcting Codes (ECC)",
        "cache reliability",
        "write failure",
        "interleaving"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/220a026abe75f2e65296ee9d7d5c4aebb0b4aac8cebaceddd46efd3084ecae49_w640_q70.webp",
      "contributions": "1. Conducted a comprehensive analysis revealing the inefficiency of conventional ECC configurations (per-word and interleaved) in STT-MRAM caches due to non-uniform distribution of bit transitions across codewords. 2. Proposed a novel ECC configuration called ROBIN (Incremental Oblique Interleaved ECC) designed to uniformly distribute bit transitions among codewords to maximize error correction capability. 3. Demonstrated significant reliability improvement, showing that ROBIN reduces the increased cache error rate caused by conventional ECC inefficiency by more than 28.6 times.",
      "summary": "This paper addresses the high error rate problem in STT-MRAM caches by identifying that conventional Error-Correcting Codes (ECCs) are inefficient due to data-dependent error patterns. The authors propose a new ECC configuration called ROBIN, which uses an incremental oblique interleaving technique to uniformly distribute bit transitions and improve correction capability. Evaluations show ROBIN reduces the cache error rate increase by over 28.6x compared to conventional ECCs.",
      "mindmap": "graph TB\n        A[ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[STT-MRAM缓存错误率高 / High error rate in STT-MRAM caches]\n        B --> B2[传统ECC因数据依赖错误模式效率低 / Conventional ECC inefficient due to data-dependent error patterns]\n        C --> C1[提出ROBIN ECC配置 / Propose ROBIN ECC configuration]\n        C --> C2[增量斜交错分布比特翻转 / Incremental oblique interleaving to distribute bit transitions uniformly]\n        D --> D1[降低缓存错误率提升超过28.6倍 / Reduces cache error rate increase by >28.6x]"
    },
    {
      "title": "Democratizing Electronic-Photonic AI Systems: An Open-Source AI-Infused Cross-Layer Co-Design and Design Automation Toolflow",
      "authors": "Hongjian Zhou, Ziang Yin, Jiaqi Gu",
      "institution": "Arizona State University",
      "link": "https://arxiv.org/pdf/2601.00130",
      "code": null,
      "tags": [
        "compiler & ir",
        "electronic-photonic design automation",
        "cross-layer co-design",
        "inverse photonic design",
        "AI-accelerated Maxwell solvers",
        "photonic AI system"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76312a14ab1d9b01be6967c891d86f1c36fb6eebdf382d27578721d3ff3e1c24_w640_q70.webp",
      "contributions": "1. Proposed a cross-layer co-design framework for scalable photonic edge AI and Transformer inference architectures. 2. Introduced SimPhony, an open-source modeling tool for rapid EPIC AI system evaluation and design-space exploration. 3. Developed AI-enabled photonic design automation techniques, including physical AI-based Maxwell solvers and a fabrication-aware inverse design framework.",
      "summary": "This paper addresses the challenge of designing electronic-photonic AI systems by proposing an open-source, AI-infused cross-layer co-design and automation framework. The method includes architecture designs for photonic AI, a modeling tool called SimPhony, and AI-powered design automation tools. The work aims to democratize and accelerate the development of next-generation photonic AI systems.",
      "mindmap": "graph TB\n        A[Democratizing Electronic-Photonic AI Systems<br>电子-光子AI系统民主化] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Challenging EPIC AI System Design<br>EPIC AI系统设计挑战]\n        B --> B2[Lack of Mature EPDA Toolchain<br>缺乏成熟的EPDA工具链]\n        C --> C1[Cross-Layer Co-Design Framework<br>跨层协同设计框架]\n        C --> C2[SimPhony Modeling Tool<br>SimPhony建模工具]\n        C --> C3[AI-Enabled EPDA Stack<br>AI赋能的EPDA堆栈]\n        D --> D1[Democratizes Development<br>民主化开发]\n        D --> D2[Enables Scalable EPDA<br>实现可扩展EPDA]"
    },
    {
      "title": "Toward Large-Scale Photonics-Empowered AI Systems: From Physical Design Automation to System-Algorithm Co-Exploration",
      "authors": "Ziang Yin, Hongjian Zhou, Nicholas Gangi, Meng Zhang, Jeff Zhang, Zhaoran Rena Huang, Jiaqi Gu",
      "institution": "Arizona State University, Rensselaer Polytechnic Institute",
      "link": "https://arxiv.org/pdf/2601.00129",
      "code": null,
      "tags": [
        "others",
        "photonic AI",
        "electronic-photonic design automation (EPDA)",
        "system-algorithm co-exploration",
        "cross-layer toolchain",
        "physical design automation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bdd0ce87d1ffb64d07fd82b197f024f167052c6fb76c061ecf0885e18f056796_w640_q70.webp",
      "contributions": "1. Identified three essential considerations for scaling practical photonic AI systems: dynamic tensor operation support, systematic management of overheads, and robustness under hardware non-idealities. 2. Built a cross-layer toolchain (SimPhony, ADEPT, ADEPT-Z, Apollo, LiDAR) for quantitative, physically-grounded co-design from system exploration to physical layout. 3. Established a co-design loop that bridges architectural intent and deployable photonic hardware by translating physical costs into system-level metrics.",
      "summary": "This paper addresses the challenge of scaling photonic AI systems by identifying key design considerations and developing a cross-layer toolchain for system-algorithm co-exploration. The proposed method uses tools like SimPhony and Apollo to model physical costs and automate design, enabling quantitative trade-off analysis under real implementation constraints. The main conclusion is that this approach creates a physically-grounded co-design loop essential for realizing large-scale, deployable photonic AI hardware.",
      "mindmap": "graph TB\n        A[Toward Large-Scale Photonics-Empowered AI Systems<br/>大规模光子赋能AI系统] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>Scaling AI constrained by data movement & efficiency<br/>AI扩展受限于数据移动与能效] --> B1[挑战1: 动态张量操作支持<br/>Dynamic tensor operation support]\n        B --> B2[挑战2: 开销系统管理<br/>Systematic overhead management]\n        B --> B3[挑战3: 硬件非理想性鲁棒性<br/>Robustness under hardware non-idealities]\n        C[主要方法/Method<br/>Cross-layer toolchain for co-design<br/>跨层工具链协同设计] --> C1[SimPhony: 实现感知建模<br/>Implementation-aware modeling]\n        C --> C2[ADEPT/ADEPT-Z: 电路与拓扑探索<br/>Circuit & topology exploration]\n        C --> C3[Apollo/LiDAR: 物理设计自动化<br/>Physical design automation]\n        D[关键结果/Results<br/>Quantitative & physically-grounded co-design loop<br/>定量且物理基础的协同设计循环] --> D1[连接系统目标与可行硬件<br/>Connects system objectives to feasible hardware]\n        D --> D2[产生可制造布局<br/>Produces manufacturable layouts]"
    },
    {
      "title": "Splitting Precoding with Subspace Selection and Quantized Refinement for Massive MIMO",
      "authors": "Yasaman Khorsandmanesh, Emil Bjornson, Joakim Jalden",
      "institution": "KTH Royal Institute of Technology",
      "link": "https://arxiv.org/pdf/2601.00616",
      "code": null,
      "tags": [
        "wireless communication systems",
        "splitting precoding",
        "subspace selection",
        "quantized refinement",
        "massive MIMO",
        "fronthaul capacity"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f66e91bfdb08909724d80ba8bae4b67df9824da0d5fae8eb15678f2e7897b484_w640_q70.webp",
      "contributions": "1. Proposes a novel splitting precoding architecture that separates precoding computation between the Advanced Antenna System (AAS) and Baseband Unit (BBU) to address fronthaul bottlenecks. 2. Introduces a local subspace selection method at the AAS to reduce channel dimensionality before fronthaul transmission. 3. Develops a quantization-aware refinement precoding algorithm at the BBU that operates on the reduced effective channel to optimize performance under limited fronthaul capacity.",
      "summary": "This paper addresses the limited fronthaul capacity problem in massive MIMO 5G systems by proposing a splitting precoding architecture. The method separates processing between the antenna system (which performs subspace selection) and the baseband unit (which computes quantized refinement precoding), achieving higher spectral efficiency than conventional one-stage precoding approaches.",
      "mindmap": "graph TB\n    Root[”Splitting Precoding with Subspace Selection and Quantized Refinement for Massive MIMO”] --> Problem[”核心问题/Problem: Limited fronthaul capacity in massive MIMO 5G architectures”]\n    Root --> Method[”主要方法/Method: Splitting architecture with AAS subspace selection and BBU quantized refinement precoding”]\n    Root --> Results[”关键结果/Results: Achieves higher sum spectral efficiency than conventional one-stage precoding”]"
    },
    {
      "title": "Enabling Physical AI at the Edge: Hardware-Accelerated Recovery of System Dynamics",
      "authors": "Bin Xu, Ayan Banerjee, Sandeep Gupta",
      "institution": "Arizona State University",
      "link": "https://arxiv.org/pdf/2512.23767",
      "code": null,
      "tags": [
        "on-device ai",
        "FPGA acceleration",
        "model recovery",
        "hardware-software co-design",
        "GRU",
        "Neural ODE"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e286e0d5a9672c23140099a1b5fd5c7ad7e56f56cb1c276735170b95ad29fd47_w640_q70.webp",
      "contributions": "1. Proposed MERINDA, a hardware-friendly FPGA-accelerated framework for model recovery that replaces Neural ODEs with a formulation combining GRU-based discretized dynamics, dense inverse-ODE layers, sparsity-driven dropout, and lightweight solvers. 2. Designed the framework for streaming parallelism, enabling critical computational kernels to be fully parallelized on FPGA hardware. 3. Demonstrated transformative efficiency gains over GPU implementations, including 114x lower energy, 28x smaller memory footprint, and 1.68x faster training while maintaining state-of-the-art accuracy.",
      "summary": "This paper addresses the challenge of deploying physical AI for model recovery on resource-constrained edge devices, where state-of-the-art methods using Neural ODEs are inefficient. The authors propose MERINDA, an FPGA-accelerated framework that uses a hardware-friendly architecture to replace expensive Neural ODE components. The results show that MERINDA achieves substantial improvements in energy, memory, and speed over GPU implementations while matching model recovery accuracy.",
      "mindmap": "graph TB\n        A[Enabling Physical AI at the Edge<br>在边缘实现物理人工智能] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Model recovery methods (Neural ODEs) are inefficient for edge hardware<br>模型恢复方法在边缘硬件上效率低下]\n        C[主要方法/Method<br>MERINDA: FPGA-accelerated, hardware-friendly framework<br>MERINDA: FPGA加速的硬件友好框架]\n        D[关键结果/Results<br>114x lower energy, 28x smaller memory, 1.68x faster training<br>能耗降低114倍, 内存占用减少28倍, 训练速度提升1.68倍]"
    },
    {
      "title": "HERO-Sign: Hierarchical Tuning and Efficient Compiler-Time GPU Optimizations for SPHINCS+ Signature Generation",
      "authors": "Yaoyun Zhou, Qian Wang",
      "institution": "University of California, Merced",
      "link": "https://arxiv.org/pdf/2512.23969",
      "code": null,
      "tags": [
        "gpu kernels",
        "SPHINCS+",
        "GPU Optimization",
        "Tree Fusion",
        "Adaptive Compilation",
        "Kernel Overlapping"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/29ca58021f9583efba2be9c3f10082df113be98d3f93c0f5ef9cc70c6fa8e481_w640_q70.webp",
      "contributions": "1. Introduces a Tree Fusion strategy for the FORS component, guided by an automated Tree Tuning search algorithm to adapt to different GPU architectures. 2. Employs an adaptive compilation strategy that automatically selects between PTX and native code paths for different SPHINCS+ kernels to maximize efficiency. 3. Optimizes batched signature generation using a task graph-based construction to reduce multi-stream idle time and kernel launch overhead.",
      "summary": "This paper proposes HERO-Sign, a GPU-accelerated implementation for the post-quantum signature scheme SPHINCS+. It uses hierarchical tuning, including a Tree Fusion strategy and adaptive compilation, to better exploit GPU parallelism and reduce overhead. The method achieves significant throughput improvements and latency reduction compared to state-of-the-art GPU implementations across multiple architectures.",
      "mindmap": "graph TB\n        Root[”HERO-Sign: SPHINCS+签名生成优化 / HERO-Sign: SPHINCS+ Signature Generation Optimization”]\n        Root --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”SPHINCS+签名生成慢 / Slow SPHINCS+ Signature Generation”]\n        Problem --> P2[”现有GPU优化未充分利用并行性 / Existing GPU Optimizations Underutilize Parallelism”]\n        Method --> M1[”树融合策略 / Tree Fusion Strategy”]\n        Method --> M2[”自适应编译 / Adaptive Compilation”]\n        Method --> M3[”任务图构建 / Task Graph Construction”]\n        Results --> R1[”吞吐量提升1.24-3.13倍 / Throughput Improvement 1.24-3.13x”]\n        Results --> R2[”内核启动延迟降低两个数量级 / Kernel Launch Latency Reduced by Two Orders of Magnitude”]"
    },
    {
      "title": "FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference",
      "authors": "Fen-Yu Hsieh, Yun-Chang Teng, Ding-Yong Hong, Jan-Jan Wu",
      "institution": "Institute of Information Science, Academia Sinica",
      "link": "https://arxiv.org/pdf/2512.24713",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "N:M structured pruning",
        "4-bit quantization",
        "systolic array",
        "FPGA accelerator",
        "hardware-software co-design"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a594f5eb4f19e15f5f182ee786cc270613c6a3d07553a78731a54b9a3ae90ea_w640_q70.webp",
      "contributions": "1. Proposes an automation framework and unified pipeline for applying N:M structured pruning and 4-bit integer quantization to compress LLMs. 2. Presents a hardware-software co-design method that generates a custom systolic-array-based FPGA accelerator for efficient inference. 3. Demonstrates the synergy of fine-grained sparsity and quantization, achieving significant reductions in storage and latency while offering flexibility beyond fixed hardware sparsity patterns.",
      "summary": "This paper addresses the high computational and memory demands of LLMs by proposing a hardware-software co-design framework. The method combines N:M structured pruning and 4-bit quantization to compress models, and implements a custom FPGA accelerator for efficient inference. The results show significant reductions in storage and latency, demonstrating the effectiveness of the approach for deployable LLM inference.",
      "mindmap": "graph TB\n        Root(”FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”LLM部署困难/LLM Deployment Challenge”)\n        P1 --> P2(”高计算与内存需求/High Computation & Memory Requirements”)\n        Method --> M1(”模型压缩/Model Compression”)\n        M1 --> M2(”N:M结构化剪枝与4-bit量化/N:M Structured Pruning & 4-bit Quantization”)\n        Method --> M3(”软硬件协同设计/Hardware-Software Co-Design”)\n        M3 --> M4(”生成基于脉动阵列的FPGA加速器/Generating Systolic-Array-based FPGA Accelerator”)\n        Results --> R1(”存储减少4倍/4x Weight Storage Reduction”)\n        Results --> R2(”矩阵乘法加速1.71倍/1.71x Matrix Multiplication Speedup”)\n        Results --> R3(”端到端延迟降低1.29倍/1.29x End-to-End Latency Reduction”)"
    },
    {
      "title": "Advances in Agentic AI: Back to the Future",
      "authors": "Sergio Alvarez-Telena, Marta Diez-Fernandez",
      "institution": "University College London, SciTheWorld",
      "link": "https://arxiv.org/pdf/2512.24856",
      "code": null,
      "tags": [
        "agent system",
        "Agentic AI",
        "Algorithmization",
        "M1 (first Machine in Machine Learning)",
        "M2 (second Machine in Machine Learning)",
        "Strategies-based Agentic AI"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/840ae0fbf1973bad5d93eb708489774eab0a0df7bdc273f111d764666bd471bc_w640_q70.webp",
      "contributions": "1. Proposes precise definitions and a structured analytical framework for Agentic AI and its convergence with Algorithmization. 2. Introduces and distinguishes the concepts of the first Machine in Machine Learning (M1) as the platform for LLM-based Agentic AI and the second Machine (M2) as the architectural prerequisite for production-grade B2B transformation. 3. Provides conceptual and technical insight into what is claimed to be the first fully realized implementation of an M2 system and outlines a forward-looking research agenda.",
      "summary": "This paper addresses the fragmented discourse around Agentic AI by proposing a clarifying framework and distinguishing between two key architectural concepts: M1 for current LLM-based systems and M2 for future, robust B2B transformation. It positions M2, or Strategies-based Agentic AI, as the necessary evolution to overcome operational barriers. The conclusion outlines a research agenda for the next two decades based on the authors' prior work in Algorithmization.",
      "mindmap": "graph TB\n        Root[”Advances in Agentic AI: Back to the Future<br/>Agentic AI进展：回到未来”] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题/Problem<br/>Fragmented discourse on Agentic AI<br/>Agentic AI领域论述碎片化”] --> P1[”缺乏清晰定义/Lack of clear definitions”]\n        Problem --> P2[”需要B2B转型框架/Need for B2B transformation framework”]\n    \n        Method[”主要方法/Method<br/>Propose analytical framework<br/>提出分析框架”] --> M1[”区分M1与M2/Distinguish M1 & M2”]\n        M1 --> M1a[”M1: LLM-based Agentic AI platform<br/>M1: 基于LLM的Agentic AI平台”]\n        M1 --> M1b[”M2: Strategies-based Agentic AI<br/>M2: 基于策略的Agentic AI”]\n        Method --> M2[”回顾算法化工作/Review Algorithmization work”]\n    \n        Results[”关键结果/Results<br/>Framework & Future Agenda<br/>框架与未来议程”] --> R1[”提供清晰定义与框架/Provide clear definitions & framework”]\n        Results --> R2[”介绍首个M2实现/Introduce first M2 implementation”]\n        Results --> R3[”提出未来20年议程/Propose 20-year future agenda”]"
    },
    {
      "title": "An Energy-Efficient RFET-Based Stochastic Computing Neural Network Accelerator",
      "authors": "Sheng Lu, Qianhou Qu, Sungyong Jung, Qilian Liang, Chenyun Pan",
      "institution": "University of Texas at Arlington (inferred from IEEE affiliation and author \"Qilian Liang, Fellow, IEEE\" known association)",
      "link": "https://arxiv.org/pdf/2512.22131",
      "code": null,
      "tags": [
        "on-device ai",
        "RFET",
        "stochastic computing",
        "SCNN",
        "stochastic number generator",
        "accelerator"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d718d7962f59eda2ed3430de0579c28b976cb6dd1e7da5e6fb355787c2b15ee_w640_q70.webp",
      "contributions": "1. Proposes a novel SCNN architecture leveraging Reconfigurable Field-Effect Transistors (RFETs) for device-level reconfigurability. 2. Designs highly efficient and compact core modules (e.g., SNGs, APCs) enabled by RFET technology. 3. Develops and evaluates a dedicated RFET-based SCNN accelerator, showing significant improvements in area, latency, and energy over a FinFET baseline.",
      "summary": "This paper addresses the high resource consumption of Stochastic Computing Neural Networks (SCNNs) by proposing a novel accelerator architecture based on Reconfigurable Field-Effect Transistors (RFETs). The inherent reconfigurability of RFETs enables the design of compact and efficient core components like stochastic number generators. Experimental results demonstrate that the proposed RFET-based accelerator achieves substantial reductions in area, latency, and energy consumption compared to a FinFET-based design at the same technology node.",
      "mindmap": "graph TB\n        A[An Energy-Efficient RFET-Based Stochastic Computing Neural Network Accelerator] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: SCNN资源消耗高/High SCNN Resource Usage]\n        C[主要方法/Method: 基于RFET的架构/RFET-Based Architecture]\n        D[关键结果/Results: 面积、延迟、能耗降低/Reduced Area, Latency, Energy]"
    },
    {
      "title": "HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA",
      "authors": "Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn",
      "institution": "National University of Sciences and Technology (Pakistan), RPTU Kaiserslautern-Landau (Germany)",
      "link": "https://arxiv.org/pdf/2512.22139",
      "code": "https://github.com/dll-ncai/HLS4PC",
      "tags": [
        "on-device ai",
        "FPGA",
        "HLS",
        "Point Cloud",
        "Model Compression",
        "Fixed-Point"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21433bfda0767bbdfcee46f13fb3acd9373d13bb741d87a755643767c9ad74f9_w640_q70.webp",
      "contributions": "1. Proposed HLS4PC, a parameterizable HLS framework for accelerating point-based 3D point cloud models on FPGA. 2. Introduced PointMLP-Lite, a 4x less complex model variant created via hardware-aware compression techniques (URS, quantization, pruning, fusion). 3. Demonstrated FPGA acceleration achieving 3.56x higher throughput than prior work and outperforming GPU/CPU implementations.",
      "summary": "This paper addresses the challenge of real-time 3D point cloud processing by proposing HLS4PC, a parameterizable FPGA acceleration framework. The method combines algorithmic optimizations and hardware-aware model compression to create an efficient fixed-point implementation, which significantly outperforms previous accelerators and GPU/CPU baselines in throughput.",
      "mindmap": "graph TB\n        Root[HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[GPU under-utilization due to sparse, unstructured point cloud data]\n        P1 --> P2[High memory/computation demand hinders real-time performance]\n        Method[主要方法/Method] --> M1[Parameterizable HLS framework for FPGA]\n        M1 --> M2[Hardware-aware compression: URS, quantization, pruning, fusion]\n        M2 --> M3[Creates PointMLP-Lite model]\n        Results[关键结果/Results] --> R1[PointMLP-Lite: 4x less complex, ~2% accuracy drop]\n        R1 --> R2[3.56x higher throughput vs. prior work]\n        R2 --> R3[2.3x (GPU) and 22x (CPU) higher throughput]"
    },
    {
      "title": "AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience",
      "authors": "Zining Wang, Jian Gao, Weimin Fu, Xiaolong Guo, Xuan Zhang",
      "institution": "Northeastern University, Kansas State University",
      "link": "https://arxiv.org/pdf/2512.22435",
      "code": null,
      "tags": [
        "agent system",
        "analog circuit design",
        "multi-agent framework",
        "stratified memory",
        "simulation-grounded feedback",
        "self-evolving"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf0a7cad048acb4f93f29281281d9e76543f81e5aa1dd6e183e005cda30a7c56_w640_q70.webp",
      "contributions": "1. Proposes AnalogSAGE, an open-source self-evolving multi-agent framework for analog circuit design that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. 2. Introduces a stratified context mechanism to selectively preserve stage-relevant information, enhancing long-horizon reasoning and reliability under stringent specifications. 3. Demonstrates significant improvements in pass rates and search space reduction through a benchmark of ten operational amplifier design problems using the open-source SKY130 PDK and ngspice.",
      "summary": "The paper addresses the challenge of automating analog circuit design, which traditionally relies heavily on human intuition, by introducing AnalogSAGE, a self-evolving multi-agent framework with stratified memory and simulation-grounded feedback. This approach enables iterative refinement across topology selection, refinement, and parameter optimization stages. Evaluations show it achieves a 10x overall pass rate and 4x reduction in parameter search space compared to existing methods, enhancing reliability and autonomy in analog design automation.",
      "mindmap": "graph TB\n        A[AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Analog circuit design is knowledge-intensive and relies on human intuition]\n        B --> B2[Existing LLM-based methods lack feedback and generalization]\n        C --> C1[Self-evolving multi-agent framework]\n        C --> C2[Three-stage agent explorations with stratified memory]\n        C --> C3[Iterative refinement via simulation-grounded feedback]\n        D --> D1[10x overall pass rate improvement]\n        D --> D2[48x Pass@1 improvement]\n        D --> D3[4x reduction in parameter search space]"
    },
    {
      "title": "TYTAN: Taylor-series based Non-Linear Activation Engine for Deep Learning Accelerators",
      "authors": "Soham Pramanik, Vimal William, Arnab Raha, Debayan Das, Amitava Mukherjee, Janet L. Paluh",
      "institution": "Jadavpur University, SandLogic Technologies, Intel Corporation, Indian Institute of Science, Amrita University, SUNY Polytechnic Institute",
      "link": "https://arxiv.org/pdf/2512.23062",
      "code": null,
      "tags": [
        "on-device ai",
        "activation function",
        "hardware accelerator",
        "taylor series",
        "energy efficiency",
        "edge inference"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/233e926fc1401824c658e53f6ee69cc2fa91152f36cad6ff674b919cf9a3aa0e_w640_q70.webp",
      "contributions": "1. Proposes TYTAN, a Taylor-series based Generalized Non-linear Approximation Engine (G-NAE) for accelerating non-linear activation functions in deep learning. 2. Integrates a re-configurable hardware design with a specialized algorithm to dynamically estimate approximations, aiming for minimal accuracy deviation. 3. Demonstrates significant performance gains and efficiency improvements, including ~2x performance, ~56% power reduction, and ~35x lower area compared to a baseline NVDLA implementation.",
      "summary": "This paper proposes TYTAN, a hardware-software co-designed engine that uses Taylor series approximations to accelerate non-linear activation functions for energy-efficient AI inference at the edge. The system dynamically configures the approximation to maintain accuracy. Evaluations show it achieves high frequency operation (&gt;950 MHz) with substantial improvements in performance, power, and area compared to a standard accelerator.",
      "mindmap": "graph TB\n        A[TYTAN: Taylor-series based Non-Linear Activation Engine] --> B[核心问题/Problem: Edge AI inference requires acceleration and energy efficiency, limited by high-power operations like activation functions.]\n        A --> C[主要方法/Method: Proposes a Generalized Non-linear Approximation Engine (G-NAE) using Taylor series and re-configurable hardware with a dynamic approximation algorithm.]\n        A --> D[关键结果/Results: >950 MHz operation, ~2x performance, ~56% power reduction, ~35x lower area vs. NVDLA baseline.]"
    },
    {
      "title": "KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta",
      "authors": "Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang, Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner, Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Abdul Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu",
      "institution": "Meta Platforms",
      "link": "https://arxiv.org/pdf/2512.23236",
      "code": null,
      "tags": [
        "gpu kernels",
        "agentic kernel coding",
        "heterogeneous accelerators",
        "retrieval-augmented prompt synthesis",
        "graph-based search",
        "Triton/CuTe DSL"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp",
      "contributions": "1. Proposes KernelEvolve, an agentic framework that automates kernel generation and optimization for DLRMs across heterogeneous hardware (NVIDIA/AMD GPUs, Meta accelerators) by operating at multiple programming abstractions. 2. Introduces a kernel optimization process modeled as a graph-based search with dynamic adaptation to runtime context via retrieval-augmented prompt synthesis and a persistent hardware knowledge base. 3. Demonstrates the system's effectiveness by achieving 100% correctness on benchmark suites and substantial performance speedups (up to 17x) in production, reducing development time from weeks to hours and lowering the programmability barrier for new AI hardware.",
      "summary": "This paper presents KernelEvolve, an agentic framework that automates the generation and optimization of compute kernels for deep learning recommendation models to address challenges posed by model, kernel, and hardware heterogeneity. The method uses a graph-based search process enhanced with retrieval-augmented prompts and operates across multiple programming abstractions. The system was validated on production models and benchmarks, showing significant performance improvements and reduced development time, effectively mitigating the programmability barrier for new AI accelerators.",
      "mindmap": "graph TB\n        A[KernelEvolve: Scaling Agentic Kernel Coding] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[DLRM训练/推理效率<br/>DLRM Training/Inference Efficiency]\n        B --> B2[模型、内核、硬件异构性<br/>Model, Kernel, Hardware Heterogeneity]\n        C --> C1[智能内核编码框架<br/>Agentic Kernel Coding Framework]\n        C --> C2[多抽象层: Triton, CuTe DSL<br/>Multi-Abstraction: Triton, CuTe DSL]\n        C --> C3[图搜索与检索增强提示<br/>Graph Search & Retrieval-Augmented Prompt]\n        D --> D1[100%正确率, 17倍加速<br/>100% Correctness, 17x Speedup]\n        D --> D2[开发时间: 数周->数小时<br/>Dev Time: Weeks->Hours]\n        D --> D3[降低新硬件编程壁垒<br/>Reduces New Hardware Programmability Barrier]"
    },
    {
      "title": "Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space",
      "authors": "Sergey Salishev",
      "institution": "Saint Petersburg State University",
      "link": "https://arxiv.org/pdf/2512.22676",
      "code": null,
      "tags": [
        "digital signal processing",
        "computer arithmetic",
        "high-performance computing",
        "energy-efficient computing",
        "integer-friendly approximation",
        "conflict-free memory access",
        "fast Fourier transform",
        "fast Schur algorithm"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fe50589b6f9e67a8e1acb929b6cfc7dcaecddcc5c1837d218c89e297ca79994_w640_q70.webp",
      "contributions": "1. A power/energy consumption model for clocked CMOS logic to select optimal parallelism. 2. Integer-friendly approximation methods for elementary functions using constrained piecewise-polynomials to reduce lookup-table size. 3. Provably conflict-free data placement and execution order schemes for mixed-radix streaming FFT on multi-bank/single-port memories.",
      "summary": "This thesis develops signal-processing algorithms and implementation schemes under constraints of minimal parallelism and memory space to improve energy efficiency. It proposes a power model, approximation methods, and conflict-free memory access schemes for FFT and fast Schur algorithms. The results provide constructive theorems and design trade-offs for building efficient specialized accelerators.",
      "mindmap": "graph TB\n        Root[”Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space<br>信号处理算法在最小并行度和内存空间约束下的综合”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Improving energy efficiency of low-power computing hardware<br>提高低功耗计算硬件的能效”]\n        Method[”主要方法/Method<br>1. Power/energy model for CMOS logic<br>CMOS逻辑功耗/能耗模型<br>2. Integer-friendly function approximation<br>整数友好函数近似<br>3. Conflict-free FFT schedules<br>无冲突FFT调度<br>4. Parallelism/memory analysis for fast Schur algorithm<br>快速Schur算法的并行度/内存分析”]\n        Results[”关键结果/Results<br>Constructive theorems, schedules, and design trade-offs for efficient specialized accelerators<br>为高效专用加速器提供构造性定理、调度方案和设计权衡”]"
    },
    {
      "title": "Power Side-Channel Analysis of the CVA6 RISC-V Core at the RTL Level Using VeriSide",
      "authors": "Behnam Farnaghinejad, Antonio Porsia, Annachiara Ruospo, Alessandro Savino, Stefano Di Carlo, Ernesto Sanchez",
      "institution": "Politecnico di Torino",
      "link": "https://arxiv.org/pdf/2512.21362",
      "code": null,
      "tags": [
        "side-channel analysis",
        "RISC-V",
        "CVA6",
        "Correlation Power Analysis (CPA)",
        "RTL simulation",
        "power side-channel"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f1045d9314e37006547d2c94a7c4490ff95449687fd13d7c467003b4c095bac_w640_q70.webp",
      "contributions": "1. Presents the first side-channel vulnerability evaluation of the CVA6 RISC-V processor core. 2. Demonstrates the application of the VeriSide RTL-level power profiling framework for efficient power trace extraction without waveform files. 3. Shows that Correlation Power Analysis (CPA) on the CVA6 during software-based AES encryption enables key recovery, highlighting the need for early-stage RTL security assessments.",
      "summary": "This paper analyzes the power side-channel vulnerability of the CVA6 RISC-V core using the VeriSide RTL simulation framework. By applying Correlation Power Analysis (CPA) to power traces during software AES execution, the authors successfully recover the secret key. The findings demonstrate significant leakage in the CVA6 design, emphasizing the importance of pre-silicon RTL-level security evaluation.",
      "mindmap": "graph TB\n        A[Power Side-Channel Analysis of the CVA6 RISC-V Core at the RTL Level Using VeriSide] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现代RISC-V处理器需要抗侧信道攻击能力 / Modern RISC-V processors require resilience to side-channel attacks]\n        C --> C1[使用VeriSide框架在RTL级进行功耗分析 / Use VeriSide framework for RTL-level power analysis]\n        C --> C2[对软件AES执行进行相关性功耗分析(CPA) / Perform Correlation Power Analysis (CPA) on software AES execution]\n        D --> D1[CVA6设计存在显著泄漏 / CVA6 design exhibits significant leakage]\n        D --> D2[成功恢复AES密钥 / Successful AES key recovery]"
    },
    {
      "title": "Online Learning Extreme Learning Machine with Low-Complexity Predictive Plasticity Rule and FPGA Implementation",
      "authors": "Zhenya Zang, Xingda Li, David Day Uei Li",
      "institution": "University of Strathclyde",
      "link": "https://arxiv.org/pdf/2512.21777",
      "code": null,
      "tags": [
        "on-device ai",
        "predictive plasticity rule",
        "extreme learning machine",
        "FPGA implementation",
        "online learning",
        "low-complexity training"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1fd9efea1b6faa314e48c1a0c90ad6c7bf79112ed59bfb1db6f14146d142848_w640_q70.webp",
      "contributions": "1. Proposed a simplified, biologically inspired predictive local learning rule that eliminates global backpropagation and membrane integration, using sparse, binary-driven vector additions triggered only by prediction errors. 2. Integrated this rule into an Extreme Learning Machine (ELM), replacing the conventional matrix inversion, thereby reducing training complexity from O(M^3) to O(M) for M hidden nodes. 3. Demonstrated an FPGA implementation showing significant reductions in computational and memory requirements, highlighting its potential for energy-efficient online learning on low-cost edge devices.",
      "summary": "This paper proposes a low-complexity online learning method by integrating a simplified predictive plasticity rule into an Extreme Learning Machine (ELM), which reduces training complexity from cubic to linear order. The approach is implemented on FPGA, showing reduced computational and memory needs while maintaining comparable accuracy. The work demonstrates strong potential for enabling efficient online learning on resource-constrained edge devices.",
      "mindmap": "graph TB\n        Root[”Online Learning ELM with Low-Complexity Predictive Plasticity Rule and FPGA Implementation<br/>在线学习ELM与低复杂度预测可塑性规则及FPGA实现”]\n        Root --> Problem[”Problem: High complexity of online learning for edge devices<br/>问题：面向边缘设备的在线学习复杂度高”]\n        Root --> Method[”Method: Simplified predictive plasticity rule + ELM<br/>方法：简化的预测可塑性规则 + ELM”]\n        Root --> Results[”Results: O(M) training, FPGA implementation, low resource use<br/>结果：O(M)训练，FPGA实现，低资源消耗”]"
    },
    {
      "title": "Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling",
      "authors": "Hannah Atmer, Yuan Yao, Thiemo Voigt, Stefanos Kaxiras",
      "institution": "Uppsala University",
      "link": "https://arxiv.org/pdf/2512.22066",
      "code": null,
      "tags": [
        "llm inference",
        "SRAM",
        "frequency scaling",
        "energy-delay product",
        "systolic array",
        "memory bandwidth"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp",
      "contributions": "1. Quantified the distinct energy and performance impacts of SRAM size and operating frequency on the compute-bound prefill and memory-bound decode phases of LLM inference. 2. Demonstrated a counter-intuitive result: high compute frequency can reduce total energy by shortening execution time and reducing static energy more than the dynamic power increase. 3. Identified an optimal hardware configuration (high frequency, small SRAM buffer) that minimizes the energy-delay product, providing concrete design insights for energy-efficient accelerators.",
      "summary": "This paper investigates how on-chip SRAM size and operating frequency affect the energy efficiency of LLM inference. Using a simulation methodology combining OpenRAM, LLMCompass, and ScaleSIM, it finds that a high-frequency, small-SRAM configuration optimizes the energy-delay product, as memory bandwidth caps decode phase improvements. The analysis provides architectural guidance for designing efficient LLM accelerators.",
      "mindmap": "graph TB\n        Root[”Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>LLM推理能耗高，Prefill与Decode阶段瓶颈不同”] --> Problem_Sub1[”SRAM大小与频率如何影响能效？”]\n        Problem --> Problem_Sub2[”内存带宽如何限制性能？”]\n        Method[”主要方法/Method<br>结合OpenRAM, LLMCompass, ScaleSIM的模拟方法”] --> Method_Sub1[”能耗建模/Energy Modeling”]\n        Method --> Method_Sub2[”延迟模拟/Latency Simulation”]\n        Method --> Method_Sub3[”操作强度分析/Operational Intensity”]\n        Results[”关键结果/Results”] --> Results_Sub1[”总能耗主要由SRAM大小决定<br>大缓存增加静态能耗”]\n        Results --> Results_Sub2[”高频可降低总能耗<br>（减少静态能耗）”]\n        Results --> Results_Sub3[”最优配置：高频(1200-1400MHz) + 小缓存(32-64KB)”]"
    },
    {
      "title": "NotSoTiny: A Large, Living Benchmark for RTL Code Generation",
      "authors": "Razine Moundir Ghorab, Emanuele Parisi, Cristian Gutierrez, Miquel Alberti-Binimelis, Miquel Moreto, Dario Garcia-Gasulla, Gokcen Kestor",
      "institution": "Barcelona Supercomputing Center, Universitat Politecnica de Catalunya",
      "link": "https://arxiv.org/pdf/2512.20823",
      "code": null,
      "tags": [
        "llm training",
        "RTL code generation",
        "benchmark",
        "hardware design",
        "data contamination",
        "verification"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3ee59b5723cdae955454bd94bdc8872b40c0eaccf59a4e54b86951d040529325_w640_q70.webp",
      "contributions": "1. Introduces NotSoTiny, a large-scale, living benchmark for evaluating LLMs on RTL code generation, built from real hardware designs. 2. Proposes an automated pipeline to ensure benchmark quality by removing duplicates, verifying correctness, and periodically updating to mitigate data contamination. 3. Demonstrates that NotSoTiny presents more challenging tasks than prior benchmarks, effectively highlighting current LLM limitations in hardware design.",
      "summary": "This paper introduces NotSoTiny, a benchmark for evaluating LLMs on generating Register-Transfer Level (RTL) code, addressing limitations of prior benchmarks by using real, complex hardware designs and a pipeline to ensure correctness and reduce data contamination. The results show that NotSoTiny tasks are more challenging, effectively guiding the improvement of LLMs for hardware design.",
      "mindmap": "graph LR\n    A[NotSoTiny: A Large, Living Benchmark for RTL Code Generation] --> B(核心问题/Problem: LLM RTL代码生成评估挑战 / LLM RTL Code Generation Evaluation Challenge)\n    A --> C(主要方法/Method: 基于真实硬件设计的自动化基准测试 / Automated Benchmark from Real Hardware Designs)\n    A --> D(关键结果/Results: 任务更具挑战性，有效指导改进 / Tasks More Challenging, Effectively Guides Improvement)"
    },
    {
      "title": "ElfCore: A 28nm Neural Processor Enabling Dynamic Structured Sparse Training and Online Self-Supervised Learning with Activity-Dependent Weight Update",
      "authors": "Zhe Su, Giacomo Indiveri",
      "institution": "Institute of Neuroinformatics, University of Zurich and ETH Zurich",
      "link": "https://arxiv.org/pdf/2512.21153",
      "code": null,
      "tags": [
        "on-device ai",
        "spiking neural network",
        "self-supervised learning",
        "structured sparsity",
        "activity-dependent update",
        "event-driven processing"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d2912d0f31a3824ddb49ba116a841b201d285560ef2177125f8dac188572270_w640_q70.webp",
      "contributions": "1. A local online self-supervised learning engine enabling multi-layer temporal learning without labeled data. 2. A dynamic structured sparse training engine supporting high-accuracy sparse-to-sparse learning. 3. An activity-dependent sparse weight update mechanism that gates updates based on input activity and network dynamics.",
      "summary": "This paper introduces ElfCore, a 28nm digital spiking neural network processor designed for event-driven sensory processing. It uniquely integrates online self-supervised learning, dynamic structured sparse training, and activity-dependent weight updates. The chip demonstrates significant improvements in power consumption, memory efficiency, and network capacity across various tasks like gesture and speech recognition.",
      "mindmap": "graph LR\n    A[ElfCore] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[边缘设备需要高效、自适应的稀疏事件处理/Edge devices need efficient, adaptive sparse event processing]\n    C --> C1[集成在线自监督学习/Integrate Online Self-Supervised Learning]\n    C --> C2[动态结构化稀疏训练/Dynamic Structured Sparse Training]\n    C --> C3[活动依赖权重更新/Activity-Dependent Weight Update]\n    D --> D1[功耗降低16倍/16x Lower Power]\n    D --> D2[片上内存减少3.8倍/3.8x Less On-Chip Memory]\n    D --> D3[网络容量效率提升5.9倍/5.9x Greater Network Capacity]"
    },
    {
      "title": "3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras",
      "authors": "Hongyang Shang, Shuai Dong, Ye Ke, Arindam Basu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20073",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32bc75b2d6c95f1b2dc70a4aff51224520c7f222f61a22edbcd56ede5fa299a5_w640_q70.webp",
      "contributions": "",
      "summary": "3D Stack In-Sensor-Computing (3DS-ISC): Accelerating Time-Surface Construction for Neuromorphic Event Cameras",
      "mindmap": ""
    },
    {
      "title": "Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling",
      "authors": "Huizheng Wang, Taiquan Wei, Hongbin Wang, Zichuan Wang, Xinru Tang, Zhiheng Yue, Shaojun Wei, Yang Hu, Shouyi Yin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20198",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e2bc2f765accf0fbb24e05e708bd3a2b62c961a86def9137c9976aa8b753f85_w640_q70.webp",
      "contributions": "",
      "summary": "Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling",
      "mindmap": ""
    },
    {
      "title": "Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization",
      "authors": "He Zhu, Zheng Liu, Xingyang Li, Anbang Wu, Jieru Zhao, Fangxin Liu, Yiming Gan, Jingwen Leng, Yu Feng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20495",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b1ba8a7f58cc0b9807b31889823cb7497448ae885dece28021d7b9eda1828c58_w640_q70.webp",
      "contributions": "",
      "summary": "Nebula: Enable City-Scale 3D Gaussian Splatting in Virtual Reality via Collaborative Rendering and Accelerated Stereo Rasterization",
      "mindmap": ""
    },
    {
      "title": "Composing Mini Oscilloscope on Embedded Systems",
      "authors": "Brennan Romero, D.G. Perera",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20571",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c64819e1e69202dbb50470d8750cc2e3fb023b25140bd80e5d0c7dddb4b62612_w640_q70.webp",
      "contributions": "",
      "summary": "Composing Mini Oscilloscope on Embedded Systems",
      "mindmap": ""
    },
    {
      "title": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs",
      "authors": "Rupanshu Soi, Rohan Yadav, Fredrik Kjolstad, Alex Aiken, Maryam Mehri Dehnavi, Michael Garland, Michael Bauer",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18134",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cfd081b538bd7f4d1c91e06ba3fae36d2a230ad65cf3fc2e5160c3bdd9d98b3_w640_q70.webp",
      "contributions": "",
      "summary": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs",
      "mindmap": ""
    },
    {
      "title": "Making Strong Error-Correcting Codes Work Effectively for HBM in AI Inference",
      "authors": "Rui Xie, Yunhua Fang, Asad Ul Haq, Linsen Ma, Sanchari Sen, Swagath Venkataramani, Liu Liu, Tong Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18152",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9099b3e60f5155323e33ca2663b734d9691284bb3f33ff23ded103fa2caa02e8_w640_q70.webp",
      "contributions": "",
      "summary": "Making Strong Error-Correcting Codes Work Effectively for HBM in AI Inference",
      "mindmap": ""
    },
    {
      "title": "PermuteV: A Performant Side-channel-Resistant RISC-V Core Securing Edge AI Inference",
      "authors": "Nuntipat Narkthong, Xiaolin Xu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18132",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5249dae70a6ea951da7229a4606840eb6080d2d0b091828439fd7dc722b3fe4e_w640_q70.webp",
      "contributions": "",
      "summary": "PermuteV: A Performant Side-channel-Resistant RISC-V Core Securing Edge AI Inference",
      "mindmap": ""
    },
    {
      "title": "PIM-FW: Hardware-Software Co-Design of All-pairs Shortest Paths in DRAM",
      "authors": "Tsung-Han Lu, Zheyu Li, Minxuan Zhou, Tajana Rosing",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18158",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c0e2dda9beb3cd09d48e66d5bd1e2e0d25dcf0ff68f729ae40eea17980532c9_w640_q70.webp",
      "contributions": "",
      "summary": "PIM-FW: Hardware-Software Co-Design of All-pairs Shortest Paths in DRAM",
      "mindmap": ""
    },
    {
      "title": "BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism",
      "authors": "Suhas Vittal, Moinuddin Qureshi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18300",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d48040e52a6d8197d1665d09704221d54a7d6dab03a13f7863e398d9cb759275_w640_q70.webp",
      "contributions": "",
      "summary": "BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism",
      "mindmap": ""
    },
    {
      "title": "Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration",
      "authors": "Wonseok Choi, Hyunah Yu, Jongmin Kim, Hyesung Ji, Jaiyoung Park, Jung Ho Ahn",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18345",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9e4c078425f57860f1303fe0449ae500adae2d4acf07294f16f8ed22a154bad_w640_q70.webp",
      "contributions": "",
      "summary": "Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration",
      "mindmap": ""
    },
    {
      "title": "Weight Transformations in Bit-Sliced Crossbar Arrays for Fault Tolerant Computing-in-Memory: Design Techniques and Evaluation Framework",
      "authors": "Akul Malhotra, Sumeet Kumar Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18459",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97e8d699ae3dc2331ca317ecfd53fe4db9ae0d861417dcb1143bdade9d3ecfa0_w640_q70.webp",
      "contributions": "",
      "summary": "Weight Transformations in Bit-Sliced Crossbar Arrays for Fault Tolerant Computing-in-Memory: Design Techniques and Evaluation Framework",
      "mindmap": ""
    },
    {
      "title": "DNA-HHE: Dual-mode Near-network Accelerator for Hybrid Homomorphic Encryption on the Edge",
      "authors": "Yifan Zhao, Xinglong Yu, Yi Sun, Honglin Kuang, Jun Han",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18589",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1feca70dc6f2a05e35a48f6366ccdcc1accb232952639d7388e6316ca2ff652_w640_q70.webp",
      "contributions": "",
      "summary": "DNA-HHE: Dual-mode Near-network Accelerator for Hybrid Homomorphic Encryption on the Edge",
      "mindmap": ""
    },
    {
      "title": "Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA",
      "authors": "Emir Devlet Ertörer, Cem Ünsalan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19304",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f352565f0530b4ee02ed15da3cf6f6c724c3dee282c3ce8dcec4f2bb7a8bed00_w640_q70.webp",
      "contributions": "",
      "summary": "Binary Neural Network Implementation for Handwritten Digit Recognition on FPGA",
      "mindmap": ""
    },
    {
      "title": "Sensitivity-Aware Mixed-Precision Quantization for ReRAM-based Computing-in-Memory",
      "authors": "Guan-Cheng Chen, Chieh-Lin Tsai, Pei-Hsuan Tsai, Yuan-Hao Chang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19445",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/731a9d0dfb14219a43a9ad49271f63aef43ed484776bc0682810b6c431e5f06e_w640_q70.webp",
      "contributions": "",
      "summary": "Sensitivity-Aware Mixed-Precision Quantization for ReRAM-based Computing-in-Memory",
      "mindmap": ""
    },
    {
      "title": "LLM-based Behaviour Driven Development for Hardware Design",
      "authors": "Rolf Drechsler, Qian Liu",
      "institution": "University of Bremen, DFKI",
      "link": "https://arxiv.org/pdf/2512.17814",
      "code": null,
      "tags": [
        "others",
        "Behavior Driven Development (BDD)",
        "Large Language Models (LLMs)",
        "hardware design",
        "test and verification",
        "natural language processing",
        "Electronic Design Automation (EDA)"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper investigates the use of Large Language Models (LLMs) to automate the generation of behavioral scenarios from textual specifications for Behavior Driven Development (BDD) in hardware design. The core method involves applying LLM-based techniques to interpret specifications and produce high-level behavioral descriptions. The main conclusion is that LLMs offer a promising opportunity to support and automate BDD workflows in hardware design, addressing the manual effort and complexity of current verification practices.",
      "mindmap": ""
    },
    {
      "title": "AIE4ML: An End-to-End Framework for Compiling Neural Networks for the Next Generation of AMD AI Engines",
      "authors": "Dimitrios Danopoulos, Enrico Lupi, Chang Sun, Sebastian Dittmeier, Michael Kagan, Vladimir Loncar, Maurizio Pierini",
      "institution": "European Organization for Nuclear Research (CERN), Institute of Physics Belgrade, Heidelberg University",
      "link": "https://arxiv.org/pdf/2512.15946",
      "code": null,
      "tags": [
        "llm inference",
        "AI Engine (AIE)",
        "GEMV",
        "quantization",
        "on-chip data movement",
        "graph placement and search",
        "low-latency inference",
        "fused bias addition and ReLU"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper presents AIE4ML, an end-to-end framework that automatically compiles neural networks into optimized firmware for AMD's AI Engine-ML accelerators. It achieves high efficiency through a structured parallelization method that scales across the 2D fabric and uses a novel graph placement algorithm for on-chip execution. The framework delivers GPU-class throughput with microsecond latency, making it suitable for ultra-low-latency applications like particle physics trigger systems.",
      "mindmap": ""
    }
  ]
}