{
  "label": "cs.CC",
  "slug": "cscc",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds",
      "authors": "Aritra Banik, Sujoy Bhore, Palash Dey, Abhishek Sahu",
      "institution": "National Institute of Science Education and Research Bhubaneswar, Indian Institute of Technology Bombay, Indian Institute of Technology Kharagpur",
      "link": "https://arxiv.org/pdf/2512.24037",
      "code": null,
      "tags": [
        "parameterized complexity",
        "kidney exchange",
        "FPT algorithm",
        "W1-hardness",
        "pathwidth",
        "treewidth"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00da3e6d7b1c46c83d5812783ee99ea1cfff1a2c3a708bddcff4c69f9ab7902c_w640_q70.webp",
      "contributions": "1. A new deterministic FPT algorithm for the kidney exchange problem parameterized by the number of patients receiving a kidney, improving the runtime from O*(14^t) to O*((4e)^t) ≈ O*(10.88^t). 2. A proof that the kidney exchange problem is W[1]-hard when parameterized by the pathwidth of the underlying graph, answering a natural question about the parameter's tractability. 3. Additional parameterized intractability results that improve the overall understanding of the problem's complexity landscape.",
      "summary": "This paper studies the computationally hard kidney exchange problem, where patient-donor pairs and altruistic donors exchange kidneys via cycles and paths. The authors present a faster deterministic parameterized algorithm for the standard parameter (number of patients receiving a kidney) and prove that the problem remains intractable (W[1]-hard) even when parameterized by pathwidth, a more restrictive structural parameter than treewidth.",
      "mindmap": "graph TB\n        Root[”Kidney Exchange: Faster Parameterized Algorithms and Tighter Lower Bounds<br>肾脏交换：更快的参数化算法与更紧的下界”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Kidney exchange is NP-complete<br>肾脏交换问题是NP完全问题”] --> P1[”限制/Constraint<br>Exchange via small cycles & paths<br>通过小环和路径交换”]\n        Method[”主要方法/Method<br>Parameterized Complexity<br>参数化复杂度”] --> M1[”参数/Parameter<br>Number of patients (t)<br>患者数量(t)”]\n        Method --> M2[”参数/Parameter<br>Graph pathwidth<br>图路径宽度”]\n        Results[”关键结果/Results”] --> R1[”算法改进/Algorithmic Improvement<br>FPT algorithm: O*((4e)^t)<br>FPT算法: O*((4e)^t)”]\n        Results --> R2[”下界/Lower Bound<br>W[1]-hard for pathwidth<br>对路径宽度是W[1]-难的”]"
    },
    {
      "title": "From FPT Decision to FPT Enumeration",
      "authors": "Nadia Creignou, Timo Camillo Merkl, Reinhard Pichler, Daniel Unterberger",
      "institution": "Aix Marseille Université, TU Wien",
      "link": "https://arxiv.org/pdf/2512.24137",
      "code": null,
      "tags": [
        "parameterized complexity",
        "fixed-parameter tractable (FPT)",
        "enumeration",
        "DelayP",
        "kernelization",
        "branching"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9ea5a12796e6b1045900cdb4da84a92c69ff6ce730801a1cf14697d7fae1b84_w640_q70.webp",
      "contributions": "1. Proposes a framework for studying how to extend FPT decision algorithms to FPT enumeration algorithms. 2. Inspects fundamental FPT design approaches (e.g., kernelization, branching) for their potential to yield enumeration algorithms. 3. Presents ideas and methodologies for transforming decision/optimization FPT techniques into ones suitable for enumerating all solutions.",
      "summary": "This paper addresses the gap in applying fixed-parameter tractable (FPT) algorithms to enumeration problems. It investigates how fundamental techniques for designing FPT decision algorithms, such as kernelization and branching, can be adapted to create FPT algorithms for enumerating all solutions. The main conclusion is that a systematic methodology can be developed to bridge FPT decision and FPT enumeration.",
      "mindmap": "graph TB\n        A[From FPT Decision to FPT Enumeration] --> B[核心问题/Problem: Intractable enumeration problems lack FPT algorithmic focus.]\n        A --> C[主要方法/Method: Inspect and extend fundamental FPT design approaches for enumeration.]\n        A --> D[关键结果/Results: Presents ideas and a framework for turning FPT decision into FPT enumeration algorithms.]"
    },
    {
      "title": "Diffusion Language Models are Provably Optimal Parallel Samplers",
      "authors": "Haozhe Jiang, Nika Haghtalab, Lijie Chen",
      "institution": "University of California, Berkeley",
      "link": "https://arxiv.org/pdf/2512.25014",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion language models",
        "parallel sampling",
        "chain-of-thought",
        "remasking",
        "revision"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43c65e3d030ce4b9471215a4735f2217f9be018da8e7b5ecc092a62d1394440b_w640_q70.webp",
      "contributions": "1. Formalized a model of parallel sampling and proved that DLMs with CoT can simulate any parallel sampling algorithm with an optimal number of sequential steps. 2. Showed that enabling remasking or revision with CoT allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. 3. Established a strict expressivity gap, proving DLMs with revision or remasking are strictly more expressive than those without.",
      "summary": "This paper provides a theoretical foundation for the efficiency of Diffusion Language Models (DLMs) as parallel samplers. It proves that DLMs augmented with chain-of-thought reasoning can simulate any parallel sampling algorithm with optimal sequential steps and, when further equipped with token remasking or revision, also achieve optimal space complexity. The results theoretically justify DLMs as highly efficient parallel samplers and advocate for enabling revision capabilities in such models.",
      "mindmap": "graph TB\n        Root[”Diffusion Language Models are Provably Optimal Parallel Samplers<br>扩散语言模型是可证明最优的并行采样器”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>DLMs的理论优势与效率极限未明<br>Theoretical advantages and efficiency limits of DLMs are unclear”] --> P1[”并行采样效率/Parallel Sampling Efficiency”]\n        Problem --> P2[”空间复杂度/Space Complexity”]\n        Method[”主要方法/Method<br>形式化并行采样模型与电路复杂度<br>Formalize parallel sampling model & circuit complexity”] --> M1[”增强CoT/Augment with CoT”]\n        Method --> M2[”引入重掩码或修订/Introduce Remasking or Revision”]\n        Results[”关键结果/Results”] --> R1[”最优顺序步骤/Optimal Sequential Steps”]\n        Results --> R2[”最优空间复杂度/Optimal Space Complexity”]\n        Results --> R3[”严格表达能力差距/Strict Expressivity Gap”]"
    },
    {
      "title": "Thin Tree Verification is coNP-Complete",
      "authors": "Alice Moayyedi",
      "institution": "University of Waterloo",
      "link": "https://arxiv.org/pdf/2512.25043",
      "code": null,
      "tags": [
        "computational complexity",
        "thin tree",
        "coNP-complete",
        "spanning tree",
        "graph cuts",
        "edge-connectivity"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/57fee7ec29f3a9f8d8859080b76ea70b44eeb72ddf00dece29f07228a749a454_w640_q70.webp",
      "contributions": "1. Proves that the problem of verifying if a given spanning tree is α-thin is coNP-hard, resolving an open question about its computational complexity. 2. Provides a formal proof for a problem previously speculated to be NP-hard or lacking a polynomially-checkable certificate. 3. Establishes a negative result (coNP-hardness) that impacts the algorithmic prospects related to the Thin Tree Conjecture and its applications, such as approximating the Asymmetric Travelling Salesman Problem (ATSP).",
      "summary": "This paper investigates the computational complexity of verifying whether a given spanning tree in a graph is α-thin, meaning it contains at most an α proportion of edges in any graph cut. The authors prove that this verification problem is coNP-hard. This result shows that efficiently checking a candidate solution is computationally difficult, which has implications for the Thin Tree Conjecture and related approximation algorithms for problems like ATSP.",
      "mindmap": "graph TB\n        A[Thin Tree Verification is coNP-Complete] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[验证给定生成树是否为α-thin树/Verify if a given spanning tree is α-thin]\n        C --> C1[证明复杂度下界/Prove complexity lower bound]\n        D --> D1[验证问题是coNP-hard的/Verification problem is coNP-hard]"
    },
    {
      "title": "Syndrome aware mitigation of logical errors",
      "authors": "Dorit Aharonov, Yosi Atia, Eyal Bairey, Zvika Brakerski, Itsik Cohen, Omri Golan, Ilya Gurwich, Netanel H. Lindner, Maor Shutman",
      "institution": "Qedma Quantum Computing, Hebrew University, Weizmann Institute of Science, Technion",
      "link": "https://arxiv.org/pdf/2512.23810",
      "code": null,
      "tags": [
        "quantum error correction and mitigation",
        "logical error mitigation",
        "syndrome-aware",
        "fault-tolerance",
        "runtime overhead",
        "quantum error correction"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28c546f1791a48c23de235687f3df199f96775597dcac09ee65b2ab0f41d74e5_w640_q70.webp",
      "contributions": "1. Introduces Syndrome-Aware Logical Error Mitigation (SALEM), a novel method that leverages syndrome data from error correction to mitigate logical errors. 2. Demonstrates that SALEM achieves an exponentially lower runtime overhead compared to prior logical error mitigation schemes, enabling the execution of significantly larger logical circuits. 3. Shows that SALEM can outperform physical error mitigation even above the standard fault-tolerance threshold, making error correction useful in physical error rate regimes where it was previously considered ineffective.",
      "summary": "This paper introduces Syndrome-Aware Logical Error Mitigation (SALEM), a method that uses syndrome data from quantum error correction to more efficiently mitigate residual logical errors. SALEM drastically reduces the runtime overhead compared to previous approaches, allowing for the accurate execution of much larger quantum circuits. The work demonstrates that this tight integration of error correction and mitigation can be beneficial even when physical error rates are above the conventional fault-tolerance threshold.",
      "mindmap": "graph TB\n        A[Syndrome aware mitigation of logical errors] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[有限物理比特导致残留逻辑错误/Limited physical qubits cause residual logical errors]\n        C --> C1[利用纠错伴随的综合征数据进行缓解/Use syndrome data from error correction for mitigation]\n        D --> D1[运行时开销指数级降低/Runtime overhead reduced exponentially]\n        D --> D2[可执行电路规模数量级提升/Executable circuit volume increased by orders of magnitude]\n        D --> D3[可在容错阈值以上超越物理错误缓解/Can outperform physical error mitigation above fault-tolerance threshold]"
    },
    {
      "title": "Proper colorings of a graph in linear time using a number of colors linear in the maximum degree of the graph",
      "authors": "Kritika Bhandari, Mark Huber",
      "institution": "None (Inferred from arXiv submission; no explicit affiliation provided on first page)",
      "link": "https://arxiv.org/pdf/2512.24522",
      "code": null,
      "tags": [
        "graph algorithms",
        "proper coloring",
        "graph sampling",
        "linear-time algorithm",
        "maximum degree",
        "Markov chain Monte Carlo"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2df3f6d8f9edf2c385953da77f4effcefb8b3b532fac0ef601829b8505fab4b3_w640_q70.webp",
      "contributions": "1. A new algorithm for exact sampling from the set of proper colorings of a graph., 2. The algorithm achieves an expected running time linear in the graph size for graphs with maximum degree Δ., 3. It is the first algorithm with this guarantee when the number of colors exceeds 3.637Δ + 1.",
      "summary": "This paper presents a new algorithm for exactly sampling proper colorings of a graph. It is the first algorithm whose expected running time is guaranteed to be linear in the graph size when the number of colors is greater than 3.637 times the graph's maximum degree plus one.",
      "mindmap": "graph TB\n        A[Proper colorings of a graph in linear time<br/>图的线性时间正常着色] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Exact sampling from proper colorings<br/>从正常着色中精确采样]\n        C --> C1[New sampling algorithm<br/>新的采样算法]\n        D --> D1[Linear expected runtime<br/>线性期望运行时间]\n        D --> D2[Colors > 3.637Δ + 1<br/>颜色数 > 3.637Δ + 1]"
    },
    {
      "title": "Approximate Computation via Le Cam Simulability",
      "authors": "Deniz Akdemir",
      "institution": "Not explicitly stated in the provided content. The author is Deniz Akdemir, but no affiliation or email domain is given.",
      "link": "https://arxiv.org/pdf/2512.24860",
      "code": null,
      "tags": [
        "computational complexity theory",
        "Le Cam deficiency",
        "computational deficiency",
        "LeCam-P",
        "approximate reduction",
        "statistical experiments"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8eb21a7edcb8e5cb8e1788fcdd488db8897d0be53c937e14e03661081a0b7652_w640_q70.webp",
      "contributions": "1. Proposes a decision-theoretic framework for computational complexity based on Le Cam simulability and statistical experiments, shifting focus from syntactic exactness to semantic approximation. 2. Defines computational deficiency (δ_poly) and uses it to construct the complexity class LeCam-P (Decision-Robust Polynomial Time) for problems that are semantically easy to approximate. 3. Establishes the No-Free-Transfer Inequality, showing that strictly invariant representations inevitably destroy decision-relevant information.",
      "summary": "This paper proposes a new framework for computational complexity that views computation as the efficient simulation of a target statistical experiment with bounded risk distortion (Le Cam deficiency). It introduces the concept of computational deficiency and the class LeCam-P to characterize problems that are hard to solve exactly but easy to approximate for decision-making. The main conclusion is that this framework bridges algorithmic complexity and decision theory, showing that classical exact reductions are a special case of zero-deficiency simulations.",
      "mindmap": "graph TB\n        A[Approximate Computation via Le Cam Simulability] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[经典精确计算理论对现代近似需求限制/Classical exactness paradigm is restrictive for modern approximate needs]\n        C --> C1[基于Le Cam缺陷的统计实验模拟框架/Framework based on Le Cam deficiency for simulating statistical experiments]\n        C --> C2[定义计算缺陷与LeCam-P类/Define computational deficiency and LeCam-P class]\n        D --> D1[Karp归约是零缺陷模拟的特例/Karp reductions are special cases of zero-deficiency simulations]\n        D --> D2[提出无免费转移不等式/Propose No-Free-Transfer Inequality]\n        D --> D3[连接算法复杂度与决策理论/Bridge algorithmic complexity and decision theory]"
    },
    {
      "title": "A Study of NP-Completeness and Undecidable Word Problems in Semigroups",
      "authors": "Duaa Abdullah, Jasem Hamoud",
      "institution": "Moscow Institute of Physics and Technology",
      "link": "https://arxiv.org/pdf/2512.22123",
      "code": null,
      "tags": [
        "computational complexity theory",
        "NP-completeness",
        "undecidable word problem",
        "associative calculus",
        "polynomial reducibility",
        "Turing machines"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9ade75d90324f1eb4f6ee92ec609932e78897f10c8103547d100966beb841a9_w640_q70.webp",
      "contributions": "1. Explores the relationship between complexity classes P and NP and the concept of polynomial reducibility. 2. Demonstrates the construction of an associative calculus (semigroup) with an algorithmically undecidable word problem. 3. Establishes a direct connection between a Turing machine computing a non-recursive function and the equivalence condition in the constructed calculus, linking computational complexity and algebraic undecidability.",
      "summary": "This paper investigates fundamental limits in computation by studying NP-completeness and undecidable problems. It constructs an associative calculus whose word problem is undecidable, linking it to a Turing machine that computes a non-recursive function. The work highlights the intrinsic boundaries of algorithmic solutions by connecting computational complexity theory with algebraic undecidability.",
      "mindmap": "graph TB\n        Root[”A Study of NP-Completeness and Undecidable Word Problems in Semigroups”] --> Problem[”核心问题/Problem<br>Computational complexity & decidability limits”]\n        Root --> Method[”主要方法/Method<br>Polynomial reducibility & Associative calculus construction”]\n        Root --> Results[”关键结果/Results<br>Undecidable word problem linked to non-recursive Turing machine”]"
    },
    {
      "title": "Lower bounds on pure dynamic programming for connectivity problems on graphs of bounded path-width",
      "authors": "Kacper Kluk, Jesper Nederlof",
      "institution": "University of Warsaw, Utrecht University",
      "link": "https://arxiv.org/pdf/2512.23121",
      "code": null,
      "tags": [
        "parameterized complexity",
        "tropical circuits",
        "pathwidth",
        "communication complexity",
        "Traveling Salesperson Problem",
        "dynamic programming"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86590df9819e19ad7303e5df124f5b2189c5e6ab6d542206119582e1e6c83135_w640_q70.webp",
      "contributions": "1. Proves unconditional lower bounds on the size of tropical circuits (modeling pure dynamic programming) for solving connectivity problems like TSP on graphs of bounded pathwidth. 2. Establishes a connection between tropical circuit complexity and the nondeterministic communication complexity of compatibility matrices. 3. Shows that any tropical circuit for TSP on a certain graph of pathwidth k requires at least 2^Ω(k log log k) gates, which is higher than known algebraic algorithms, suggesting algebra is necessary for competitive worst-case times.",
      "summary": "This paper studies the limitations of pure dynamic programming, modeled by tropical circuits, for solving connectivity problems like the Traveling Salesperson Problem on graphs with small pathwidth. It proves an unconditional lower bound of 2^Ω(k log log k) gates for any tropical circuit solving TSP on a specific graph of pathwidth k. This result, established via a link to communication complexity, suggests that algebraic techniques are unavoidable for achieving the fastest known worst-case running times for these problems.",
      "mindmap": "graph TB\n        A[Lower bounds on pure dynamic programming for connectivity problems on graphs of bounded path-width] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[评估纯动态规划对连通性问题的能力/Assess capability of pure DP for connectivity problems]\n        C --> C1[将热带电路复杂度与通信复杂性关联/Link tropical circuit complexity to communication complexity]\n        D --> D1[证明下界高于已知代数算法/Prove lower bound higher than known algebraic algorithms]"
    },
    {
      "title": "Pseudodeterministic Algorithms for Minimum Cut Problems",
      "authors": "Aryan Agarwala, Nithin Varma",
      "institution": "Max-Planck-Institut für Informatik, Saarland Informatics Campus; University of Cologne",
      "link": "https://arxiv.org/pdf/2512.23468",
      "code": null,
      "tags": [
        "graph algorithms",
        "pseudodeterministic algorithms",
        "global minimum cut",
        "minimum s-t cut",
        "streaming algorithms",
        "cut-query models"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c06a5f7252dc9f1337880cbdb49ac90f3b180fa63fe8a5b0eb2c22a487cedbbd_w640_q70.webp",
      "contributions": "1. Presents efficient pseudodeterministic algorithms for the global minimum cut and minimum s-t cut problems. 2. Achieves an asymptotic running time for global minimum cut that is better than the fastest known sequential deterministic algorithm. 3. Implements the algorithm in multiple computational models (sequential, streaming, PRAM, cut-query) where efficient deterministic algorithms were previously unknown.",
      "summary": "This paper introduces pseudodeterministic algorithms for finding global minimum cuts and minimum s-t cuts in graphs. The proposed method offers replicability by consistently outputting the same answer with high probability, while being faster than the best known deterministic algorithm for global minimum cut. The algorithms are also successfully adapted to work in sequential, streaming, PRAM, and cut-query models.",
      "mindmap": "graph TB\n        A[Pseudodeterministic Algorithms for Minimum Cut Problems] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(确定性算法效率低 / Deterministic algorithms are inefficient)\n        B --> B2(随机算法输出不一致 / Randomized algorithms lack replicability)\n        C --> C1(伪确定性算法 / Pseudodeterministic Algorithms)\n        C --> C2(高概率输出相同解 / Outputs same solution with high probability)\n        D --> D1(渐近更快 / Asymptotically faster)\n        D --> D2(多模型实现 / Implemented in multiple models)"
    },
    {
      "title": "Coloring Hardness on Low Twin-Width Graphs",
      "authors": "Édouard Bonnet",
      "institution": "Univ Lyon, CNRS, ENS de Lyon, Université Claude Bernard Lyon 1, LIP UMR5668",
      "link": "https://arxiv.org/pdf/2512.23680",
      "code": null,
      "tags": [
        "graph theory",
        "computational complexity",
        "twin-width",
        "graph coloring",
        "NP-hardness",
        "computational complexity",
        "graph classes"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f77a33f460e61bf2610b2bc5a51fb237df8e78440e7279dd902d4d1e4a4dbc60_w640_q70.webp",
      "contributions": "1. Proves that the Min Coloring problem is NP-hard on the class of graphs with twin-width at most 3 (T3). 2. Proves that for every k &gt;= 3, the k-Coloring problem is NP-hard on the class of graphs with twin-width at most 4 (T4). 3. Provides structural observations about the T3 and T4 classes, highlighting their distinct properties and raising open questions about complexity transitions.",
      "summary": "This paper studies the computational complexity of graph coloring problems on graphs with low twin-width. It proves that Min Coloring is NP-hard on graphs of twin-width at most 3, and that k-Coloring is NP-hard on graphs of twin-width at most 4 for all k&gt;=3. These results establish the first hardness for a problem on T3 that is easy on simpler graph classes like trees and cographs.",
      "mindmap": "graph TB\n    A[Coloring Hardness on Low Twin-Width Graphs] --> B[核心问题/Problem: Coloring complexity on bounded twin-width graphs]\n    A --> C[主要方法/Method: NP-hardness proofs via reductions]\n    A --> D[关键结果/Results: Min Coloring hard on T3, k-Coloring hard on T4]"
    },
    {
      "title": "A Note on the NP-Hardness of PARTITION Via First-Order Projections",
      "authors": "Paúl Risco Iturralde",
      "institution": "Independent researcher",
      "link": "https://arxiv.org/pdf/2512.21448",
      "code": null,
      "tags": [
        "computational complexity theory",
        "NP-hardness",
        "first-order reductions",
        "AC0 reductions",
        "PARTITION problem",
        "descriptive complexity"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b5d9f8d4b42755b482904c0cf03df329316dc9a10936a82fe27b6ce034a1e56_w640_q70.webp",
      "contributions": "1. Demonstrates NP-hardness of the PARTITION problem via first-order projections, 2. Overcomes the obstacle of requiring large sums in the standard reduction by using descriptive complexity techniques, 3. Fills a gap in the literature regarding the hardness of PARTITION under restricted reductions like AC0.",
      "summary": "This note addresses the open question of whether the PARTITION problem is NP-hard under restricted reductions like AC0. It modifies classic reductions from 3SAT to SUBSET-SUM to PARTITION, defining them using first-order logical formulas (first-order projections). The main conclusion is that PARTITION is indeed NP-hard via first-order projections, which implies hardness under polynomial-size AC0 reductions, thereby resolving the gap mentioned in prior work.",
      "mindmap": "graph TB\n        A[论文标题: A Note on the NP-Hardness of PARTITION Via First-Order Projections] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[PARTITION的NP-hardness在受限归约下是否成立?/Is PARTITION NP-hard under restricted reductions?]\n        C --> C1[使用一阶逻辑公式定义归约/Define reductions using first-order logic formulas]\n        C --> C2[修改经典归约(3SAT到SUBSET-SUM到PARTITION)/Modify classic reductions (3SAT to SUBSET-SUM to PARTITION)]\n        D --> D1[PARTITION对一阶投影是NP-hard的/PARTITION is NP-hard via first-order projections]\n        D --> D2[暗示对多项式大小AC0归约也是NP-hard的/Implies NP-hard under polynomial-size AC0 reductions]\n        D --> D3[填补了文献中的空白/Fills a gap in the literature]"
    },
    {
      "title": "A Note on Avoid vs MCSP",
      "authors": "Edward A. Hirsch, Ilya Volkovich",
      "institution": "Ariel University, Boston College",
      "link": "https://arxiv.org/pdf/2512.21764",
      "code": null,
      "tags": [
        "computational complexity theory",
        "Range Avoidance Problem",
        "Minimal Circuit Size Problem",
        "AM ∩ coAM",
        "Turing reductions"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/918e4b50a4df8fcc5a358172ac9f315b25fd82440914a13e94eb45224863b78a_w640_q70.webp",
      "contributions": "1. Presents an alternative approach to a known result linking languages reducible to the Range Avoidance Problem (Avoid) to the complexity class AM ∩ coAM. 2. Proposes using the Minimal Circuit Size Problem (MCSP) as a potential avenue to derive this containment result. 3. Highlights the connection between two central problems in complexity theory (Avoid and MCSP) for understanding the power of reductions.",
      "summary": "This note explores the complexity of the Range Avoidance Problem (Avoid). It proposes a new potential method, using the Minimal Circuit Size Problem (MCSP), to show that any language reducible to Avoid via deterministic or randomized Turing reductions is contained in the complexity class AM ∩ coAM, offering an alternative to a recent proof.",
      "mindmap": "graph TB\n        Root[”A Note on Avoid vs MCSP”] --> Problem[”核心问题/Problem<br>Complexity of Range Avoidance (Avoid)”]\n        Root --> Method[”主要方法/Method<br>Using Minimal Circuit Size Problem (MCSP)”]\n        Root --> Results[”关键结果/Results<br>Languages reducible to Avoid are in AM ∩ coAM”]"
    },
    {
      "title": "Conserved active information",
      "authors": "Yanchen Chen, Daniel Andrés Díaz-Pachón",
      "institution": "University of Miami",
      "link": "https://arxiv.org/pdf/2512.21834",
      "code": null,
      "tags": [
        "information theory",
        "conserved active information",
        "No-Free-Lunch",
        "KL divergence",
        "search space",
        "information conservation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1371f9cf2f5be050a2495fc2f2b19865fddd5c4a8834df1cd98fc2da7eea7111_w640_q70.webp",
      "contributions": "1. Introduces conserved active information (I⊕), a symmetric measure of net information gain/loss across a search space that respects No-Free-Lunch conservation. 2. Demonstrates that I⊕ can reveal regimes (e.g., strong knowledge reducing global disorder) that are hidden from traditional measures like KL divergence. 3. Applies the framework to resolve a longstanding critique of active information and illustrates its utility in domains like Markov chains and cosmological fine-tuning.",
      "summary": "This paper proposes a new information-theoretic measure called conserved active information (I⊕) to quantify net information change in search problems while respecting conservation laws. It shows that I⊕ uncovers scenarios, such as strong knowledge imposing order, which are missed by standard divergence measures. The work resolves a key critique of active information and enables applications in search and optimization.",
      "mindmap": "graph TB\n        Root[Conserved active information] --> Problem[核心问题/Problem: Limitations of average-focused information measures like KL divergence]\n        Root --> Method[主要方法/Method: Introduce conserved active information I⊕, a symmetric extension respecting No-Free-Lunch]\n        Root --> Results[关键结果/Results: I⊕ reveals hidden regimes (e.g., strong knowledge reduces disorder), resolves critique of active information]"
    },
    {
      "title": "Poincaré Duality and Multiplicative Structures on Quantum Codes",
      "authors": "Yiming Li, Zimu Li, Zi-Wen Liu, Quynh T. Nguyen",
      "institution": "Tsinghua University, Harvard University",
      "link": "https://arxiv.org/pdf/2512.21922",
      "code": null,
      "tags": [
        "quantum error correction",
        "sheaf codes",
        "Poincaré duality",
        "quantum LDPC codes",
        "transversal gates",
        "cup/cap product"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76fcb4213084226768ada35e1a65f3fff10489254aaaafade43391ca7096cc47_w640_q70.webp",
      "contributions": "1. Generalizing Poincaré duality from manifolds to sheaf-based classical and quantum codes, establishing a rigorous duality relationship between chain and cochain complexes. 2. Constructing multiplicative structures (cup and cap products) on sheaved chain complexes, leading to an explicit isomorphism between (co)homology groups. 3. Applying the framework to obtain transversal logical gates (CZ, CCZ, higher-order controlled-Z) on families of good qLDPC and quantum locally testable codes, pointing towards fault-tolerant non-Clifford gates.",
      "summary": "This paper generalizes Poincaré duality and multiplicative structures from topology to sheaf-based quantum codes. The authors rigorously prove duality relationships and construct cup/cap products, leading to an isomorphism between homology groups. As an application, they demonstrate how to construct transversal logical non-Clifford gates on good quantum LDPC codes, advancing fault-tolerant quantum computing.",
      "mindmap": "graph TB\n        Root(”Poincaré Duality and Multiplicative Structures on Quantum Codes<br>量子代码的庞加莱对偶与乘法结构”)\n        Root --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”Generalize Poincaré duality to codes<br>将对偶性推广至编码”)\n        Method --> M1(”Sheaf theory on cell complexes<br>胞腔复形上的层理论”)\n        Method --> M2(”Build cup/cap products<br>构建杯积/卡积”)\n        Results --> R1(”Duality & isomorphism proven<br>证明对偶与同构”)\n        Results --> R2(”Transversal logical gates<br>横截逻辑门”)"
    },
    {
      "title": "Shifted Partial Derivative Polynomial Rank and Codimension",
      "authors": "Darren J. Edwards",
      "institution": "Swansea University",
      "link": "https://arxiv.org/pdf/2512.20729",
      "code": null,
      "tags": [
        "computational complexity theory",
        "shifted partial derivatives",
        "circuit lower bounds",
        "algebraic complexity",
        "polynomial rank",
        "codimension"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/838d2ea35d688f3271f22d39361a703e1bf3cca6d055420f190d38fc6357ee14_w640_q70.webp",
      "contributions": "1. Introduced the Shifted Partial Derivative Polynomial (SPDP) framework, packaging classical SPD methods into an explicit coefficient-matrix formalism. 2. Proved structural properties of the framework, including monotonicity, invariance under symmetries, and robustness across embeddings. 3. Provided generic width-to-rank upper-bound templates for local circuit models via combinatorial profile counting, separating the model-agnostic toolkit from specific refinements.",
      "summary": "This paper develops a new algebraic framework called Shifted Partial Derivative Polynomial (SPDP) to formalize the study of circuit lower bounds. It packages the shifted partial derivative method into a concrete linear-algebraic matrix formalism, defining dual measures of rank and codimension. The framework is proven to have robust structural properties and provides generic templates for bounding circuit complexity.",
      "mindmap": "graph LR\n    A[Shifted Partial Derivative Polynomial Rank and Codimension] --> B(核心问题/Problem: Measuring circuit complexity via algebraic dimension)\n    A --> C(主要方法/Method: SPDP framework - coefficient-matrix formalism for shifted derivatives)\n    A --> D(关键结果/Results: Structural properties proven, generic upper-bound templates provided)"
    },
    {
      "title": "Stochastic well-structured transition systems",
      "authors": "James Aspnes",
      "institution": "Yale University",
      "link": "https://arxiv.org/pdf/2512.20939",
      "code": null,
      "tags": [
        "distributed computing theory",
        "well-structured transition systems",
        "population protocols",
        "probabilistic scheduling",
        "computational complexity",
        "BPP"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6b86e00331b0080766e8bd0e99c46088867daf6771188ff7f0462c5c277cb00_w640_q70.webp",
      "contributions": "1. Defines a new class of stochastic well-structured transition systems (SWSTSs) that unifies models like population protocols and chemical reaction networks under a probabilistic scheduling rule. 2. Proves fundamental limitations on phase clocks in SWSTSs, showing they either stop or tick too fast in expected polynomial time. 3. Provides an exact characterization of computational power, showing augmented SWSTSs compute exactly BPP languages, while unaugmented ones compute symmetric BPL languages.",
      "summary": "This paper extends the theory of well-structured transition systems by incorporating probabilistic scheduling, creating a new class called stochastic well-structured transition systems (SWSTSs). It proves that any phase clock implementation in these systems has polynomial expected duration, and that terminating computations finish in expected polynomial time. These results lead to an exact characterization of computational power, showing that augmented SWSTSs compute exactly the languages in BPP.",
      "mindmap": "graph LR\n    A[Stochastic well-structured transition systems] --> B(核心问题/Problem: Extend WSTS theory to probabilistic scheduling models)\n    A --> C(主要方法/Method: Define SWSTS class unifying population protocols, CRNs, gossip models)\n    A --> D(关键结果/Results: Phase clock limitations; Polynomial expected termination; Computational power = BPP/BPL)"
    },
    {
      "title": "Adjusted Kolmogorov Complexity of Binary Words with Empirical Entropy Normalization",
      "authors": "Brani Vidakovic",
      "institution": "Texas A&M University",
      "link": "https://arxiv.org/pdf/2512.21193",
      "code": null,
      "tags": [
        "algorithmic information theory",
        "Kolmogorov complexity",
        "empirical entropy",
        "algorithmic randomness",
        "entropy normalization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7d7810b075904c2bbfd2f2bea9585a558beed86eb55fe1de2591efdd870e8c51_w640_q70.webp",
      "contributions": "1. Introduces a new entropy-normalized complexity measure for binary words, defined as the ratio of Kolmogorov complexity to empirical entropy, to separate intrinsic descriptive complexity from symbol imbalance. 2. Proves that for Martin-Löf random sequences under constructive exchangeable measures, the adjusted complexity grows linearly and converges to one. 3. Demonstrates through a pathological construction that the regularity of the underlying measure is essential for this convergence result.",
      "summary": "This paper introduces a new measure of complexity for binary words by normalizing Kolmogorov complexity with the word's empirical entropy, aiming to isolate intrinsic algorithmic structure from the combinatorial effect of imbalanced symbol frequencies. The authors prove that for random sequences under certain measures, this adjusted complexity converges to one, and show that the regularity of the measure is crucial. The framework connects Kolmogorov complexity, entropy, and randomness, with potential applications in randomness testing and structured data analysis.",
      "mindmap": "graph LR\n    A[Adjusted Kolmogorov Complexity<br>调整后的柯尔莫哥洛夫复杂度] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[标准复杂度受符号分布影响<br>Standard complexity affected by symbol distribution]\n    C --> C1[引入熵归一化度量<br>Introduce entropy-normalized measure]\n    D --> D1[调整后复杂度收敛于1<br>Adjusted complexity converges to 1]\n    D --> D2[度量正则性至关重要<br>Measure regularity is essential]"
    },
    {
      "title": "On the complexity of computing Strahler numbers",
      "authors": "Moses Ganardi, Markus Lohrey",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19060",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/766e6fd373709e1db6e154a2587de62f4f4e26874de556b197f9b8ce5acbd6c2_w640_q70.webp",
      "contributions": "",
      "summary": "On the complexity of computing Strahler numbers",
      "mindmap": ""
    },
    {
      "title": "Negations are powerful even in small depth",
      "authors": "Bruno Cavalar, Théo Borém Fabris, Partha Mukhopadhyay, Srikanth Srinivasan, Amir Yehudayoff",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19515",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aac7e20e03b10f3eec41314aef3ae6875b189b18ff94a23b98b224f3e0816d64_w640_q70.webp",
      "contributions": "",
      "summary": "Negations are powerful even in small depth",
      "mindmap": ""
    },
    {
      "title": "Assembly Addition Chains",
      "authors": "Leroy Cronin, Juan Carlos Morales Parra, Keith Y. Patarroyo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18030",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/10ea7d39b17a64fe0479e71c1b2c378efd134d84b95b0ee41ddb001f5c7ab6cd_w640_q70.webp",
      "contributions": "",
      "summary": "Assembly Addition Chains",
      "mindmap": ""
    },
    {
      "title": "Classical billiards can compute",
      "authors": "Eva Miranda, Isaac Ramos",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19156",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7724a2d94526af4f01b67bbf44e70aa6699225be8d35d64024a661c94737371b_w640_q70.webp",
      "contributions": "",
      "summary": "Classical billiards can compute",
      "mindmap": ""
    }
  ]
}