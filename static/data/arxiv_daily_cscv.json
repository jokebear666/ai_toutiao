{
  "label": "cs.CV",
  "slug": "cscv",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "Understanding Virality: A Rubric based Vision-Language Model Framework for Short-Form Edutainment Evaluation",
      "authors": "Arnav Gupta, Gurekas Singh Sahney, Hardik Rathi, Abhishek Chandwani, Ishaan Gupta, Pratik Narang, Dhruv Kumar",
      "institution": "Birla Institute of Technology and Science, Pilani; GenimeLabs",
      "link": "https://arxiv.org/pdf/2512.21402",
      "code": null,
      "tags": [
        "video understanding",
        "Vision-Language Models",
        "engagement prediction",
        "multimodal features",
        "YouTube Shorts",
        "regression-based evaluator"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bb4f5ae92f21f2e03de0476c0d49f5d4facf563d20f5bbb707cb9ed5960cd88_w640_q70.webp",
      "contributions": "1. A large-scale curated dataset of 11,000 YouTube Shorts videos for edutainment engagement modeling. 2. An unsupervised multimodal framework using VLMs to extract audiovisual features without handcrafted selection. 3. An explainable, regression-based evaluator that predicts engagement with strong correlation and provides feature-level interpretability.",
      "summary": "This paper addresses the problem of evaluating short-form edutainment videos by moving beyond traditional quality metrics to predict human engagement. The proposed method uses Vision-Language Models to extract unsupervised audiovisual features, clusters them, and trains a regression model to predict engagement. The results show strong correlation with actual engagement, offering a scalable and interpretable evaluation framework.",
      "mindmap": "graph TB\n        Root[Understanding Virality: A Rubric based Vision-Language Model Framework] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[传统指标无法评估真实观众参与度/Traditional metrics fail to assess real viewer engagement]\n        Method[主要方法/Method] --> M1[使用VLM提取无监督视听特征/Use VLM to extract unsupervised audiovisual features]\n        Method --> M2[聚类特征并训练回归评估器/Cluster features and train regression evaluator]\n        Results[关键结果/Results] --> R1[预测与真实参与度强相关/Strong correlation between predicted and actual engagement]\n        Results --> R2[提供可解释且可扩展的评估/Provides interpretable and scalable assessment]"
    },
    {
      "title": "A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding",
      "authors": "Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli",
      "institution": "California Institute of Technology, Stanford University",
      "link": "https://arxiv.org/pdf/2512.21414",
      "code": "https://github.com/christinaliu2020/tool-bottleneck-framework",
      "tags": [
        "medical image analysis",
        "tool-use framework",
        "vision-language model",
        "interpretability",
        "data-efficiency",
        "tool bottleneck model"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp",
      "contributions": "1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios.",
      "summary": "This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions.",
      "mindmap": "graph TB\n        A[Tool Bottleneck Framework for Medical Image Understanding] --> B[核心问题/Problem: Text-based tool composition fails for medical images with localized features]\n        A --> C[主要方法/Method: TBF uses VLM to select tools, TBM (neural network) to fuse outputs]\n        A --> D[关键结果/Results: Matches or beats baselines, more interpretable, data-efficient]"
    },
    {
      "title": "Scalable Deep Subspace Clustering Network",
      "authors": "Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami",
      "institution": "University of Quebec at Montreal",
      "link": "https://arxiv.org/pdf/2512.21434",
      "code": null,
      "tags": [
        "subspace clustering",
        "landmark-based approximation",
        "self-expression",
        "spectral clustering",
        "linear complexity",
        "convolutional auto-encoder"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp",
      "contributions": "1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n×n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering.",
      "summary": "The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient.",
      "mindmap": "graph TB\n        Root[”Scalable Deep Subspace Clustering Network”] --> Problem[”核心问题/Problem: O(n^3) 计算复杂度 / O(n^3) Computational Complexity”]\n        Root --> Method[”主要方法/Method: 地标近似与联合优化 / Landmark Approximation & Joint Optimization”]\n        Root --> Results[”关键结果/Results: 线性复杂度与可比性能 / Linear Complexity & Comparable Performance”]"
    },
    {
      "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
      "authors": "Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh",
      "institution": "Simon Fraser University, AIP Labs",
      "link": "https://arxiv.org/pdf/2512.21472",
      "code": "/githubsfu-mial/IMAplusplus",
      "tags": [
        "medical image segmentation",
        "multi-annotator segmentation",
        "skin lesion segmentation",
        "dermoscopy",
        "consensus masks",
        "annotator metadata"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c892cf532aaf3e65ad92c26d2ef6701dc5d629cb5bc4b453893916992cd6089_w640_q70.webp",
      "contributions": "1. Introduces IMA++, the largest publicly available multi-annotator skin lesion segmentation dataset with 17,684 masks across 14,967 dermoscopic images. 2. Provides rich annotator metadata (e.g., skill level, tool) enabling research on annotator-specific modeling and metadata analysis. 3. Offers curated data partitions, consensus segmentation masks, and a detailed dataset analysis.",
      "summary": "This paper introduces IMA++, a large-scale public dataset to address the lack of multi-annotator data for dermoscopic skin lesion segmentation. The dataset contains thousands of images with multiple segmentation masks and annotator metadata, facilitating research into modeling annotator variability. It is presented as the largest publicly available resource of its kind for this medical imaging domain.",
      "mindmap": "graph TB\n        Root[”IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset”] --> Problem[”核心问题/Problem: Lack of large-scale public multi-annotator skin lesion segmentation datasets”]\n        Root --> Method[”主要方法/Method: Introduce ISIC MultiAnnot++ dataset with multiple masks & annotator metadata”]\n        Root --> Results[”关键结果/Results: Largest public SLS dataset (17,684 masks, 14,967 images), enables new research”]"
    },
    {
      "title": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism",
      "authors": "Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang",
      "institution": "Harbin Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.21452",
      "code": null,
      "tags": [
        "object detection",
        "Ground Penetrating Radar (GPR)",
        "Multi-modal Chain Feature Fusion (MCFF)",
        "Global Attention Mechanism (GAM)",
        "DCGAN",
        "transfer learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp",
      "contributions": "1. Proposed a DCGAN-based data augmentation strategy to synthesize high-fidelity GPR images, mitigating data scarcity. 2. Designed a novel Multi-modal Chain and Global Attention Network (MCGA-Net) integrating Multi-modal Chain Feature Fusion (MCFF) and a Global Attention Mechanism (GAM) for enhanced defect representation. 3. Utilized MS COCO transfer learning to fine-tune the backbone network, accelerating convergence and improving model generalization.",
      "summary": "This paper addresses the subjective and inefficient interpretation of Ground Penetrating Radar (GPR) images for road defect detection by proposing a comprehensive framework. The method combines DCGAN-based data augmentation, a novel MCGA-Net architecture with feature fusion and attention mechanisms, and transfer learning. The proposed model achieves high precision, recall, and robustness, establishing a new paradigm for automated GPR-based defect detection.",
      "mindmap": "graph TB\n        Root(”Intelligent recognition of GPR road hidden defect images <br/> GPR道路隐蔽病害图像智能识别”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n    \n        Problem --> P1(”Subjective & inefficient GPR interpretation <br/> GPR图像解释主观且低效”)\n        Problem --> P2(”Data scarcity <br/> 数据稀缺”)\n    \n        Method --> M1(”DCGAN-based Data Augmentation <br/> 基于DCGAN的数据增强”)\n        Method --> M2(”MCGA-Net (MCFF + GAM) <br/> MCGA-Net网络”)\n        Method --> M3(”MS COCO Transfer Learning <br/> MS COCO迁移学习”)\n    \n        Results --> R1(”High Performance (Precision 92.8%, mAP@50 95.9%) <br/> 高性能”)\n        Results --> R2(”Robust to noise & weak signals <br/> 对噪声和弱信号鲁棒”)\n        Results --> R3(”New paradigm for automated detection <br/> 自动化检测新范式”)"
    },
    {
      "title": "CCAD: Compressed Global Feature Conditioned Anomaly Detection",
      "authors": "Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu",
      "institution": "Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC",
      "link": "https://arxiv.org/pdf/2512.21459",
      "code": "https://github.com/chloeqxq/CCAD",
      "tags": [
        "anomaly detection",
        "global feature conditioning",
        "adaptive compression",
        "reconstruction-based anomaly detection"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp",
      "contributions": "1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method's effectiveness.",
      "summary": "This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence.",
      "mindmap": "graph TB\n        A[CCAD: Compressed Global Feature Conditioned Anomaly Detection] --> B[核心问题/Problem: 异常检测在有限异常数据下的挑战，现有方法在泛化性、效率和约束上的不足]\n        A --> C[主要方法/Method: 提出CCAD，融合重建与表征方法，使用压缩的全局特征作为重建模型的条件]\n        A --> D[关键结果/Results: 在AUC上超越SOTA，收敛更快，贡献了重新标注的DAGM 2007数据集]"
    },
    {
      "title": "GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification",
      "authors": "Suncheng Xiang, Xiaoyang Wang, Junjie Jiang, Hejia Wang, Dahong Qian",
      "institution": "Shanghai Jiao Tong University, Peking University, Shanghai Fifth People's Hospital",
      "link": "https://arxiv.org/pdf/2512.21476",
      "code": "https://github.com/JeremyXSC/GPF-Net",
      "tags": [
        "medical image retrieval",
        "polyp re-identification",
        "gated progressive fusion",
        "multimodal feature fusion"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp",
      "contributions": "1) Proposes a novel multimodal feature fusion framework named GPF-Net for polyp re-identification. 2) Introduces a gated progressive fusion strategy for layer-wise refinement of semantic information through multi-level feature interactions. 3) Demonstrates state-of-the-art performance on standard benchmarks, showing the benefit of multimodal fusion over unimodal methods.",
      "summary": "This paper addresses the challenge of colonoscopic polyp re-identification, where coarse high-level features harm small object matching. The authors propose GPF-Net, a Gated Progressive Fusion network that selectively fuses multi-level features using gates. Experiments show this multimodal approach outperforms state-of-the-art unimodal ReID models.",
      "mindmap": "graph TB\n        A[GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification] --> B[核心问题/Problem: Coarse high-level features lead to inferior results for small polyps]\n        A --> C[主要方法/Method: Gated Progressive Fusion network for selective, multi-level feature fusion]\n        A --> D[关键结果/Results: Outperforms unimodal models, benefits of multimodal fusion strategy]"
    },
    {
      "title": "Generative Multi-Focus Image Fusion",
      "authors": "Xinzhe Xie, Buyu Guo, Bolin Li, Shuangyan He, Yanzhen Gu, Qingyan Jiang, Peiliang Li",
      "institution": "Zhejiang University, Donghai Laboratory",
      "link": "https://arxiv.org/pdf/2512.21495",
      "code": "https://github.com/Xinzhe99/StackMFF-Series",
      "tags": [
        "image fusion",
        "multi-focus image fusion",
        "latent diffusion models",
        "generative restoration",
        "StackMFF",
        "IFControlNet"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d197875e157fe73427be3804a8f00aec57f7112b871c69141003d4a9a92477_w640_q70.webp",
      "contributions": "1. Proposes a novel two-stage generative framework (GMFF) for multi-focus image fusion, combining deterministic fusion with generative restoration. 2. Introduces a generative restoration stage using IFControlNet, a latent diffusion model, to reconstruct missing content and eliminate edge artifacts. 3. Demonstrates state-of-the-art performance, particularly in handling complex scenarios where not all scene areas are in focus in any input image.",
      "summary": "The paper proposes GMFF, a two-stage framework for multi-focus image fusion. It first uses StackMFF V4 for deterministic fusion and then employs a latent diffusion model (IFControlNet) for generative restoration to recover missing details and remove artifacts. Experiments show GMFF achieves state-of-the-art performance, especially for complex multi-focal content.",
      "mindmap": "graph TB\n        A[Generative Multi-Focus Image Fusion] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法假设总有对焦图像/Existing methods assume always an in-focus image]\n        B --> B2[边缘伪影/Edge artifacts]\n        C --> C1[阶段一: 确定性融合/Stage 1: Deterministic Fusion (StackMFF V4)]\n        C --> C2[阶段二: 生成式恢复/Stage 2: Generative Restoration (IFControlNet)]\n        D --> D1[SOTA性能/State-of-the-art performance]\n        D --> D2[处理复杂多焦内容/Handles complex multi-focal content]"
    },
    {
      "title": "Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering",
      "authors": "Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu",
      "institution": "University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University",
      "link": "https://arxiv.org/pdf/2512.21510",
      "code": null,
      "tags": [
        "multi-view clustering",
        "incomplete multi-view clustering",
        "missing pattern tree",
        "decision ensemble",
        "knowledge distillation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp",
      "contributions": "1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules.",
      "summary": "This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness.",
      "mindmap": "graph TB\n        Root[”Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering”] --> Problem[”核心问题/Problem: Inconsistent missing patterns in multi-view data limit clustering performance.”]\n        Root --> Method[”主要方法/Method: TreeEIC framework with missing-pattern tree grouping, decision ensemble, and knowledge distillation.”]\n        Root --> Results[”关键结果/Results: Achieves state-of-the-art IMVC performance and superior robustness.”]"
    },
    {
      "title": "Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art",
      "authors": "Md Ashik Khan, Arafat Alam Jion",
      "institution": "Indian Institute of Technology Kharagpur, Chittagong University of Engineering and Technology",
      "link": "https://arxiv.org/pdf/2512.21512",
      "code": null,
      "tags": [
        "image forensics",
        "fixed-threshold evaluation",
        "CNN-ViT hybrid",
        "gated fusion",
        "frequency-domain features",
        "cross-domain detection"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca89224e09c82fcbaf5bf9db1a5d9fb5df38a1947dd8ae3ddcb0e9c385e126aa_w640_q70.webp",
      "contributions": "1. Introduced a fixed-threshold evaluation protocol for AI-generated image detection to provide deployment-relevant robustness estimates by preventing per-condition threshold retuning. 2. Proposed a lightweight CNN-ViT hybrid model with gated fusion and optional frequency enhancement for cross-domain detection across photos and art. 3. Conducted a systematic analysis revealing a forensic-semantic spectrum, showing CNNs are fragile to compression while ViTs are robust, and that semantic patterns in artistic content are more reliable detection cues.",
      "summary": "This paper addresses the problem of misleading robustness estimates in AI-generated image detection by proposing a fixed-threshold evaluation protocol. The authors develop a hybrid CNN-ViT model with gated fusion and evaluate it across photos and art, finding that ViTs are more robust to post-processing like compression due to semantic pattern recognition, and that detection is fundamentally easier on artistic content than photorealistic images.",
      "mindmap": "graph TB\n        Root[”Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem: Misleading robustness estimates from per-condition threshold retuning in AI-generated image detection.”]\n        Method[”主要方法/Method: Fixed-threshold evaluation protocol & a lightweight CNN-ViT hybrid with gated fusion.”]\n        Results[”关键结果/Results: ViTs robust to compression; detection easier on art; hybrid offers balanced cross-domain performance.”]"
    },
    {
      "title": "SVBench: Evaluation of Video Generation Models on Social Reasoning",
      "authors": "Wenshuo Peng, Gongxuan Wang, Tianmeng Yang, Chuanhao Li, Xiaojie Xu, Hui He, Kaipeng Zhang",
      "institution": "Tsinghua University, Shanghai AI Laboratory, Peking University, Harbin Institute of Technology, The Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.21507",
      "code": "https://github.com/Gloria2tt/SVBench-Evaluation",
      "tags": [
        "video generation",
        "social reasoning",
        "benchmark",
        "agent-based pipeline",
        "VLM judge",
        "multi-agent interaction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415eaf890354c8803ae31303b38b5679ebed73bd981f8860ef00ba636922568d_w640_q70.webp",
      "contributions": "1. Introduces the first benchmark (SVBench) for evaluating social reasoning in video generation, grounded in developmental and social psychology. 2. Develops a fully training-free, agent-based pipeline to synthesize and evaluate diverse social reasoning scenarios. 3. Conducts a large-scale evaluation of seven state-of-the-art video generation models, revealing their systematic failures in social cognition.",
      "summary": "This paper introduces SVBench, the first benchmark for evaluating social reasoning in video generation models. It uses an agent-based pipeline to create and assess videos based on psychological paradigms, finding that while current models are visually realistic, they fail at understanding intentions, beliefs, and social norms.",
      "mindmap": "graph TB\n        A[SVBench: 视频生成模型的社会推理评估<br>SVBench: Evaluation of Video Generation Models on Social Reasoning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有视频生成模型缺乏社会推理能力<br>Current models lack social reasoning]\n        C --> C1[构建基于社会心理学范式的基准<br>Build benchmark based on social psychology paradigms]\n        C --> C2[使用基于智能体的训练免费流程进行评估<br>Use training-free agent-based pipeline for evaluation]\n        D --> D1[模型在表面合理性上表现良好<br>Models perform well on surface-level plausibility]\n        D --> D2[模型在社会推理维度上系统性失败<br>Models fail systematically on social reasoning dimensions]"
    },
    {
      "title": "Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification",
      "authors": "Md Ashik Khan, Md Nahid Siddique",
      "institution": "Indian Institute of Technology Kharagpur, Florida International University",
      "link": "https://arxiv.org/pdf/2512.21508",
      "code": null,
      "tags": [
        "multi-modal training",
        "parameter-efficient training",
        "frozen encoders",
        "adapters",
        "LoRA",
        "BitFit"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3511bf13e05579029e8458f9b63afdf7f5fb3ed3a02050a524e53a4fcb570084_w640_q70.webp",
      "contributions": "1. Demonstrated that parameter-efficient training (PET) methods with frozen encoders achieve superior performance (AUROC 0.892-0.908) compared to full fine-tuning (AUROC 0.770) for multimodal chest X-Ray classification using only 2.51% of trainable parameters. 2. Conducted controlled, budget-matched experiments revealing that performance gains stem primarily from efficient parameter allocation rather than cross-modal synergy, as vision-only models outperformed multimodal models under the same parameter budget. 3. Identified and quantified a tractable limitation of PET methods—degraded model calibration (ECE: 0.29-0.34)—and suggested post-hoc calibration as a solution for clinical deployment.",
      "summary": "This paper investigates parameter-efficient training (PET) strategies for multimodal chest X-Ray classification to reduce computational costs. By freezing pre-trained encoders and training only small modules like adapters or LoRA under a fixed parameter budget, the methods significantly outperform full fine-tuning. The main conclusion is that frozen encoder PET provides superior discrimination at a fraction of the cost, though calibration correction is needed for clinical use.",
      "mindmap": "graph TB\n        A[Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 多模态胸部X光分析的计算成本高，泛化性差 / Multimodal chest X-Ray analysis is computationally costly and has poor generalization]\n        C[主要方法/Method: 使用冻结编码器和参数高效训练策略 / Use frozen encoders and PET strategies (Adapters, LoRA, BitFit)]\n        D[关键结果/Results: PET方法性能优于全微调，但校准性差 / PET methods outperform full fine-tuning but have degraded calibration]"
    },
    {
      "title": "DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO",
      "authors": "Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji",
      "institution": "Tsinghua University, Kuaishou Technology (Kling Team), Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.21514",
      "code": null,
      "tags": [
        "diffusion models",
        "GRPO",
        "mode collapse",
        "diversity-aware reward",
        "spectral clustering",
        "structure-aware regularization"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp",
      "contributions": "1. Identifies and analyzes the mode collapse problem in GRPO-based image generation from both reward modeling and generation dynamics perspectives. 2. Proposes a distributional creativity bonus reward based on semantic grouping via spectral clustering to encourage novel visual modes. 3. Introduces a structure-aware regularization that applies stronger constraints during early-stage denoising to preserve diversity without sacrificing quality optimization.",
      "summary": "This paper addresses the mode collapse problem in GRPO-based image generation, where models produce homogenized outputs. The proposed DiverseGRPO method introduces a diversity-aware reward based on semantic clustering and a structure-aware regularization to preserve generation diversity. Experiments show the method significantly improves semantic diversity while maintaining image quality, establishing a better quality-diversity trade-off.",
      "mindmap": "graph TB\n        A[DiverseGRPO: Mitigating Mode Collapse] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[GRPO导致模式崩溃/GRPO causes mode collapse]\n        B1 --> B2[缺乏视觉多样性/Lacks visual diversity]\n        C --> C1[奖励层面: 分布创造力奖励/Reward Level: Distributional Creativity Bonus]\n        C --> C2[生成层面: 结构感知正则化/Generation Level: Structure-Aware Regularization]\n        C1 --> C3[基于语义分组的谱聚类/Spectral Clustering for Semantic Grouping]\n        D --> D1[语义多样性提升13%-18%/13%-18% Semantic Diversity Improvement]\n        D --> D2[建立新的帕累托前沿/Establishes New Pareto Frontier]"
    },
    {
      "title": "MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions",
      "authors": "Puyun Wang, Kaimin Yu, Huayang He, Xianyu Wu",
      "institution": "Fuzhou University, Research Institute of Highway, Ministry of Transport",
      "link": "https://arxiv.org/pdf/2512.21513",
      "code": "https://github.com/WangPuyun/MuS-Polar3D",
      "tags": [
        "3D reconstruction",
        "polarization imaging",
        "computational imaging",
        "underwater imaging",
        "benchmark dataset",
        "multi-scattering"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c66fd636f62e6b85894934c19824730a7d5f2125cecd8c2495b556ef0c4c42_w640_q70.webp",
      "contributions": "1. Introduces MuS-Polar3D, the first publicly available benchmark dataset for quantitative turbidity underwater polarization-based 3D imaging, featuring 42 objects captured under seven controlled scattering conditions and five viewpoints. 2. Proposes a two-stage computational imaging pipeline that decouples underwater 3D reconstruction into descattering followed by 3D reconstruction. 3. Provides extensive evaluations using multiple baseline methods, demonstrating the dataset's effectiveness for fair algorithm comparison under complex scattering conditions.",
      "summary": "This paper addresses the lack of diverse public datasets for polarization-based underwater 3D imaging by introducing MuS-Polar3D, a benchmark dataset captured under controlled multi-scattering conditions. The authors also propose a two-stage computational imaging pipeline (descattering then 3D reconstruction) for this task. The dataset enables accurate reconstruction and fair evaluation, with experiments achieving a best mean angular error of 15.49 degrees.",
      "mindmap": "graph TB\n        A[MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(现有数据集缺乏多样性的散射和观测条件/Existing datasets lack diversity in scattering and observation conditions)\n        C --> C1(构建包含42个物体、7种散射条件、5个视角的偏振图像基准数据集/Construct a benchmark dataset with 42 objects, 7 scattering conditions, 5 viewpoints)\n        C --> C2(提出解耦的两阶段成像流程：去散射后3D重建/Propose a decoupled two-stage pipeline: descattering then 3D reconstruction)\n        D --> D1(实现最佳平均角度误差15.49度/Achieve best mean angular error of 15.49 degrees)\n        D --> D2(首个公开的定量浑浊水下偏振3D成像基准数据集/First publicly available benchmark for quantitative turbidity underwater polarization-based 3D imaging)"
    },
    {
      "title": "Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data",
      "authors": "Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu",
      "institution": "Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University",
      "link": "https://arxiv.org/pdf/2512.21516",
      "code": null,
      "tags": [
        "multi-view clustering",
        "contrastive learning",
        "incomplete multi-view data",
        "noise-robust clustering",
        "graph-guided learning",
        "imputation-free"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp",
      "contributions": "1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data.",
      "summary": "This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings.",
      "mindmap": "graph TB\n        A[Global-Graph Guided and Local-Graph Weighted Contrastive Learning<br/>全局-局部图引导对比学习] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>Rare-paired & Mis-paired Samples<br/>样本配对稀少与错误] --> B1[Incomplete & Noise Multi-View Data<br/>不完整与噪声多视图数据]\n        C[主要方法/Method<br/>Unified Contrastive Learning Framework<br/>统一对比学习框架] --> C1[Global-Graph Guided CL<br/>全局图引导对比学习]\n        C --> C2[Local-Graph Weighted CL<br/>局部图加权对比学习]\n        C1 --> C1a[Construct Global Affinity Graph<br/>构建全局亲和力图]\n        C2 --> C2a[Generate Adaptive Weights<br/>生成自适应权重]\n        D[关键结果/Results<br/>Superior Clustering Performance<br/>优越的聚类性能] --> D1[Outperforms SOTA Methods<br/>超越现有最佳方法]\n        D --> D2[Effective on Incomplete & Noise Data<br/>在不完整与噪声数据上有效]"
    },
    {
      "title": "Hierarchy-Aware Fine-Tuning of Vision-Language Models",
      "authors": "Jiayu Li, Rajesh Gangireddy, Samet Akcay, Wei Cheng, Juhua Hu",
      "institution": "University of Washington, Intel",
      "link": "https://arxiv.org/pdf/2512.21529",
      "code": null,
      "tags": [
        "multimodal learning",
        "hierarchical classification",
        "vision-language models",
        "efficient fine-tuning",
        "LoRA",
        "Tree-Path KL Divergence"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp",
      "contributions": "1. Proposes an efficient hierarchy-aware fine-tuning framework for Vision-Language Models (VLMs) that updates only a few parameters. 2. Introduces two novel loss functions: Tree-Path KL Divergence (TP-KL) for vertical consistency along label paths and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency among sibling classes. 3. Demonstrates consistent improvements in Full-Path Accuracy and reduced Tree-based Inconsistency Error across multiple hierarchical benchmarks with minimal parameter overhead.",
      "summary": "This paper addresses the problem of adapting large Vision-Language Models (VLMs) to hierarchical classification tasks efficiently. The proposed method combines two novel hierarchy-aware loss functions (TP-KL and HiSCE) with lightweight LoRA adaptation to enforce structural consistency in predictions. Experiments show the approach improves accuracy and reduces inconsistency across taxonomy levels with minimal computational cost.",
      "mindmap": "graph TB\n        Root(”Hierarchy-Aware Fine-Tuning of Vision-Language Models”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”VLMs适应层级分类效率低/VLMs inefficient for hierarchical classification”)\n        Problem --> P2(”标准方法预测不一致/Standard methods produce inconsistent predictions”)\n        Method --> M1(”提出层级感知微调框架/Propose hierarchy-aware fine-tuning framework”)\n        Method --> M2(”结合TP-KL与HiSCE损失/Combine TP-KL and HiSCE losses”)\n        Method --> M3(”集成轻量级LoRA适配/Integrate lightweight LoRA adaptation”)\n        Results --> R1(”提升全路径精度/Improves Full-Path Accuracy”)\n        Results --> R2(”降低不一致性错误/Reduces Tree-based Inconsistency Error”)\n        Results --> R3(”参数开销最小/Minimal parameter overhead”)"
    },
    {
      "title": "Vision Transformers are Circulant Attention Learners",
      "authors": "Dongchen Han, Tianyu Li, Ziyi Wang, Gao Huang",
      "institution": "Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.21542",
      "code": "https://github.com/LeapLabTHU/Circulant-Attention",
      "tags": [
        "vision transformers",
        "circulant attention",
        "block circulant matrix with circulant blocks (BCCB)",
        "computational complexity",
        "vision transformers",
        "fast Fourier transform (FFT)"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14a3b6c919de65b8d25da885c52023b90262e94f46956f6aed612d5e62b5f19b_w640_q70.webp",
      "contributions": "1. Identifies that self-attention matrices in vision Transformers often approximate Block Circulant matrices with Circulant Blocks (BCCB). 2. Proposes a novel Circulant Attention paradigm that explicitly models the attention map as its nearest BCCB matrix. 3. Introduces an efficient computation algorithm using 2D Discrete Fourier Transform (DFT) to achieve O(N log N) complexity while largely preserving the modeling capacity of standard self-attention.",
      "summary": "This paper addresses the quadratic computational complexity of self-attention in vision Transformers by proposing Circulant Attention. The method models the attention matrix as a Block Circulant matrix with Circulant Blocks (BCCB) and leverages Fast Fourier Transform for efficient O(N log N) computation. Experiments show it effectively reduces computation cost while maintaining model performance, offering a promising alternative to standard self-attention.",
      "mindmap": "graph TB\n        A[Vision Transformers are Circulant Attention Learners] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Self-attention quadratic complexity O(N²) is computationally heavy for high-resolution vision tasks.]\n        C[主要方法/Method<br>Propose Circulant Attention, modeling attention map as nearest BCCB matrix for O(N log N) computation via FFT.]\n        D[关键结果/Results<br>Reduces complexity to O(N log N), maintains model capacity, validated on diverse vision tasks.]"
    },
    {
      "title": "Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration",
      "authors": "Unnati Saraswat, Tarun Rao, Namah Gupta, Shweta Swami, Shikhar Sharma, Prateek Narang, Dhruv Kumar",
      "institution": "Birla Institute of Technology and Science, Pilani",
      "link": "https://arxiv.org/pdf/2512.21560",
      "code": null,
      "tags": [
        "image editing",
        "context-aware object insertion",
        "sponsor-product logo augmentation",
        "vision-language models",
        "diffusion models"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0959c405e8f65b70f4fa16189d8d2817513788b4aa2715490a9136daff2672b7_w640_q70.webp",
      "contributions": "1. Introduces two new tasks for advertising: context-aware object insertion and sponsor-product logo augmentation. 2. Builds two new datasets with annotations for category, placement, and sponsor-product labels to support these tasks. 3. Identifies and addresses a research gap in jointly performing category reasoning, placement prediction, object generation, and seamless compositing for scene augmentation.",
      "summary": "The paper introduces two new tasks for intelligent image editing in advertising: context-aware object insertion and sponsor-product logo augmentation. It proposes to address these tasks using vision-language models and diffusion models, and builds new datasets to support them. The main conclusion is that a significant research gap exists in systems that can jointly reason about context, placement, generation, and compositing for realistic scene augmentation.",
      "mindmap": "graph TB\n        A[Toward Intelligent Scene Augmentation<br>智能场景增强] --> B(Problem: Existing image editing lacks contextual appropriateness<br>核心问题: 现有图像编辑缺乏上下文合理性)\n        A --> C(Method: Introduce two new tasks & build datasets<br>主要方法: 提出两个新任务并构建数据集)\n        A --> D(Results: Identifies research gap for joint reasoning & generation<br>关键结果: 指出了联合推理与生成的研究空白)\n        B --> E(Task 1: Context-aware object insertion<br>任务1: 上下文感知物体插入)\n        B --> F(Task 2: Sponsor-product logo augmentation<br>任务2: 赞助商产品商标增强)\n        C --> G(Utilize VLMs and diffusion models<br>利用视觉语言模型和扩散模型)\n        C --> H(Build annotated datasets<br>构建带标注的数据集)"
    },
    {
      "title": "EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal",
      "authors": "Sanghyun Jo, Donghwan Lee, Eunji Jung, Seong Je Oh, Kyungsu Kim",
      "institution": "Seoul National University, OGQ",
      "link": "https://arxiv.org/pdf/2512.21545",
      "code": null,
      "tags": [
        "image inpainting",
        "object removal",
        "dataset-free",
        "test-time adaptation",
        "multimodal large-language model",
        "background-aware reasoning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f86516075c3f56dc02ca42051c7afa7154583c986a395447b42dd8dc4bf354_w640_q70.webp",
      "contributions": "1. Proposes a novel dataset-free framework, EraseLoRA, that replaces attention manipulation with background-aware reasoning and test-time adaptation for object removal. 2. Introduces Background-aware Foreground Exclusion (BFE), which uses an MLLM to separate target foreground, non-target foregrounds, and clean background from a single image-mask pair without supervision. 3. Introduces Background-aware Reconstruction with Subtype Aggregation (BRSA), a test-time optimization that treats background subtypes as complementary pieces and enforces their consistent integration through reconstruction and alignment objectives.",
      "summary": "The paper addresses the challenge of object removal in inpainting, where existing dataset-free methods often regenerate unwanted objects or disrupt details. It proposes EraseLoRA, a framework that uses an MLLM to separate image components and performs test-time optimization to reconstruct the background coherently. The method shows consistent improvements over dataset-free baselines and competitive results against dataset-driven approaches.",
      "mindmap": "graph TB\n        A[EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Object removal must prevent target reappearance and reconstruct occluded background with fidelity, unlike common inpainting. Existing attention-redirecting methods regenerate unwanted objects and disrupt details.]\n        C[主要方法/Method: 1. Background-aware Foreground Exclusion (BFE): Uses MLLM to separate target, non-target foregrounds, and clean background. 2. Background-aware Reconstruction with Subtype Aggregation (BRSA): Test-time optimization for consistent background integration.]\n        D[关键结果/Results: Consistent improvements over dataset-free baselines; competitive results against dataset-driven methods; validated as a plug-in to pretrained diffusion models.]"
    },
    {
      "title": "Exploration of Reproducible Generated Image Detection",
      "authors": "Yihang Duan",
      "institution": "Not explicitly stated in the provided content. (Author name only, no affiliation or email domain provided)",
      "link": "https://arxiv.org/pdf/2512.21562",
      "code": null,
      "tags": [
        "image forensics",
        "AIGC detection",
        "reproducibility",
        "generalizability",
        "diffusion models",
        "binary classification"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp",
      "contributions": "1. Identifies and analyzes the root causes of poor reproducibility in AIGC image detection research, citing omitted experimental details and overfitting to generator-specific features. 2. Provides empirical evidence for the reproducibility issue by constructing a test dataset and reproducing a representative detection method, demonstrating performance drops under cross-generator testing. 3. Proposes reference directions for the research community to improve reproducibility and generalizability, such as more comprehensive disclosure of experimental details.",
      "summary": "This paper investigates the reproducibility and generalizability challenges in AI-Generated Content (AIGC) image detection. By reviewing key literature, building a test dataset, and reproducing a detection method, it identifies causes like omitted experimental details and model overfitting. The study concludes that while basic performance can be reproduced, detection fails when preprocessing disrupts key features or when testing across different generators, highlighting the need for better methodological disclosure and robustness.",
      "mindmap": "graph TB\n        A[Exploration of Reproducible Generated Image Detection] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Poor Reproducibility & Generalizability]\n        C[主要方法/Method<br>Literature Review, Dataset Construction, Method Reproduction]\n        D[关键结果/Results<br>Performance Drops with Preprocessing/Cross-Generator Tests]"
    },
    {
      "title": "Towards Long-window Anchoring in Vision-Language Model Distillation",
      "authors": "Haoyi Zhou, Shuo Li, Tianyu Chen, Qi Song, Chonghan Gao, Jianxin Li",
      "institution": "Beihang University, Zhongguancun Laboratory",
      "link": "https://arxiv.org/pdf/2512.21576",
      "code": null,
      "tags": [
        "multi-modal training",
        "knowledge distillation",
        "long-context",
        "rotary position embeddings (RoPE)",
        "attention mechanism",
        "vision-language models"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp",
      "contributions": "1. Identifies the problem of limited effective context windows in small, distilled vision-language models despite using identical positional embeddings and architectures as their larger counterparts. 2. Proposes LAid, a novel distillation method featuring progressive distance-weighted attention matching and learnable RoPE response gain modulation to transfer long-range attention mechanisms. 3. Demonstrates that LAid-distilled models achieve significantly longer effective context windows (up to 3.2x) while maintaining performance on standard benchmarks, and provides spectral analysis showing successful transfer of low-frequency attention components.",
      "summary": "This paper addresses the problem that small, distilled vision-language models have much shorter effective context windows than their large teacher models. The authors propose LAid, a new distillation method that transfers long-range attention capabilities via progressive attention matching and learnable RoPE modulation. Their method successfully extends the context window of small models by up to 3.2 times while preserving performance on standard benchmarks.",
      "mindmap": "graph TB\n        A[Towards Long-window Anchoring in Vision-Language Model Distillation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Small distilled VLMs have limited effective context windows]\n        C[主要方法/Method: LAid - Progressive attention matching & learnable RoPE modulation]\n        D[关键结果/Results: Achieves up to 3.2x longer context, maintains benchmark performance]"
    },
    {
      "title": "LLM-Free Image Captioning Evaluation in Reference-Flexible Settings",
      "authors": "Shinnosuke Hirano, Yuiga Wada, Kazuki Matsuda, Seitaro Otsuki, Komei Sugiura",
      "institution": "Keio University",
      "link": "https://arxiv.org/pdf/2512.21582",
      "code": null,
      "tags": [
        "image captioning evaluation",
        "LLM-free evaluation",
        "reference-flexible",
        "supervised metric",
        "image-caption similarity",
        "human-annotated dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad4cf942f6b913d144878df5c7f41a2b4ea396044f5286c9aeed1071cfa693a1_w640_q70.webp",
      "contributions": "1. Proposes Pearl, a novel LLM-free supervised metric for image captioning that works in both reference-based and reference-free settings. 2. Introduces a novel mechanism to learn representations of image-caption and caption-caption similarities. 3. Constructs a large-scale human-annotated dataset for metric evaluation, comprising ~333k judgments from over 2k annotators across 75k+ images.",
      "summary": "This paper addresses the issues of bias in LLM-based and the limited performance of existing LLM-free metrics for evaluating image captions. It proposes Pearl, a fast, LLM-free supervised metric that learns joint image-caption and caption-caption similarity representations and is applicable to both reference-based and reference-free settings. Pearl outperforms other LLM-free metrics on multiple benchmarks, demonstrating higher alignment with human judgment without the neutrality and speed concerns of LLM-based approaches.",
      "mindmap": "graph TB\n        Root(”LLM-Free Image Captioning Evaluation in Reference-Flexible Settings”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”LLM-based metrics lack neutrality/LLM指标缺乏中立性”)\n        Problem --> P2(”LLM-free metrics lack performance/无LLM指标性能不足”)\n        Method --> M1(”Propose Pearl metric/提出Pearl指标”)\n        Method --> M2(”Learn image-caption & caption-caption similarity/学习图像-描述与描述-描述相似性”)\n        Method --> M3(”Construct large human-annotated dataset/构建大规模人工标注数据集”)\n        Results --> R1(”Outperforms LLM-free metrics on benchmarks/在基准测试中超越其他无LLM指标”)\n        Results --> R2(”Works in reference-based & reference-free settings/适用于有参考和无参考设置”)\n        Results --> R3(”Fast & aligned with human judgment/快速且与人类判断一致”)"
    },
    {
      "title": "UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation",
      "authors": "Linxuan Fan, Juntao Jiang, Weixuan Liu, Zhucun Xue, Jiajun Lv, Jiangning Zhang, Yong Liu",
      "institution": "Not explicitly stated in the provided content. Could be inferred from author names but no affiliations/domains are given.",
      "link": "https://arxiv.org/pdf/2512.21584",
      "code": "this",
      "tags": [
        "medical image segmentation",
        "Mamba",
        "state-space model",
        "knowledge distillation",
        "lightweight model",
        "U-Net"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089b376dfef7caeeae8830bbdef8bf95a5c6ff909a643481b728550d7ff3d937_w640_q70.webp",
      "contributions": "1. Proposes a lightweight Global–Local Multi-branch Perception Module integrating global context and local features. 2. Demonstrates a bidirectional Mamba with shared weights for enhanced information flow without extra parameters. 3. Introduces a hybrid knowledge distillation strategy to create an ultra-compact, high-performance variant.",
      "summary": "This paper proposes UltraLBM-UNet, a lightweight model for skin lesion segmentation that combines a bidirectional Mamba for global modeling with multi-branch local feature perception. It achieves state-of-the-art accuracy on benchmark datasets with minimal parameters and computational cost, and a distilled variant further compresses the model, making it suitable for point-of-care deployment.",
      "mindmap": "graph TB\n        A[UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Existing methods have limitations: low performance, high complexity for skin lesion segmentation.]\n        C[主要方法/Method<br>Lightweight U-Net variant with bidirectional Mamba for global modeling and multi-branch local feature perception.]\n        D[关键结果/Results<br>Achieves SOTA accuracy with only 0.034M params; distilled variant has 0.011M params. Suitable for point-of-care.]"
    },
    {
      "title": "From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement",
      "authors": "Jian Lang, Rongpei Hong, Ting Zhong, Leiting Chen, Qiang Gao, Fan Zhou",
      "institution": "University of Electronic Science and Technology of China, Southwestern University of Finance and Economics",
      "link": "https://arxiv.org/pdf/2512.21598",
      "code": null,
      "tags": [
        "multimodal learning",
        "harmful meme detection",
        "large multimodal model",
        "agent self-improvement",
        "label-free adaptation",
        "contrastive learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c54d55d69468e93b3a2275f2a3f89c24499aa76dbe87423ea8069fb10f1df9f1_w640_q70.webp",
      "contributions": "1. Proposes ALARM, the first label-free harmful meme detection framework using LMM agent self-improvement. 2. Introduces a Confidence-based Explicit Meme Identification mechanism to isolate and pseudo-label explicit memes. 3. Introduces a Pairwise Learning Guided Agent Self-Improvement paradigm that uses contrastive pairs to refine the agent's ability to handle complex memes.",
      "summary": "This paper proposes ALARM, a label-free framework for detecting harmful memes by using a Large Multimodal Model agent that self-improves by learning from easy, explicit examples to handle complex, subtle ones. The method outperforms label-driven approaches on three datasets, demonstrating strong adaptability to evolving online content.",
      "mindmap": "graph TB\n        A[From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Harmful meme detection requires costly labeled data and struggles to adapt to evolving content.]\n        C[主要方法/Method: ALARM framework uses LMM agent self-improvement via confidence-based explicit meme identification and pairwise contrastive learning.]\n        D[关键结果/Results: Superior performance on three datasets, outperforms label-driven methods, shows strong adaptability.]"
    },
    {
      "title": "GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians",
      "authors": "Bintao He, Yiran Cheng, Hongjia Li, Xiang Gao, Xin Gao, Fa Zhang, Renmin Han",
      "institution": "Shandong University, Ningxia Medical University, Beijing Institute of Technology, King Abdullah University of Science and Technology (KAUST)",
      "link": "https://arxiv.org/pdf/2512.21599",
      "code": null,
      "tags": [
        "cryo-EM image analysis",
        "3D Gaussians",
        "conformational heterogeneity",
        "two-encoder-one-decoder",
        "pseudo-atomic model",
        "cryo-EM"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b95f9f3fcbc3c6429b2ee462e30233f1b388c03440a44273a62612612e5e7244_w640_q70.webp",
      "contributions": "1. Proposes GaussianEM, a novel Gaussian pseudo-atomic framework for modeling compositional and conformational heterogeneity from cryo-EM images. 2. Introduces a two-encoder-one-decoder architecture to map images to individual Gaussian components and represent variability through Gaussian parameter changes. 3. Bridges the gap between density-based models and atomic models, providing an intuitive and interpretable description of conformational changes while preserving local structural consistency.",
      "summary": "The paper introduces GaussianEM, a method that uses 3D Gaussians to model both compositional and conformational heterogeneity in proteins from cryo-EM images. It employs a two-encoder-one-decoder architecture to map images to Gaussian components, representing structural changes through parameter variations. The method is shown to be effective on both simulated and experimental datasets, offering an interpretable bridge between density maps and atomic models.",
      "mindmap": "graph TB\n        A[GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians] --> B(核心问题/Problem: Analyzing cryo-EM datasets with continuous motions and discrete states is challenging.)\n        A --> C(主要方法/Method: A Gaussian pseudo-atomic framework with a two-encoder-one-decoder architecture to map images to Gaussians.)\n        A --> D(关键结果/Results: Provides interpretable conformational change description, bridges density-atomic model gap, and demonstrates effectiveness.)"
    },
    {
      "title": "Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care",
      "authors": "Yusuf Brima, Marcellin Atemkeng",
      "institution": "Osnabrück University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)",
      "link": "https://arxiv.org/pdf/2512.21602",
      "code": null,
      "tags": [
        "imbalanced classification",
        "XGBoost",
        "TabNet",
        "TabResNet",
        "Bayesian hyperparameter search",
        "class imbalance"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp",
      "contributions": "1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments.",
      "summary": "This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments.",
      "mindmap": "graph TB\n        A[Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data<br>不平衡临床数据中机器学习的鲁棒性与可扩展性] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Imbalanced clinical data in emergency/critical care<br>急诊/重症监护中的不平衡临床数据]\n        C[主要方法/Method<br>Systematic evaluation of tree-based models, TabNet, and proposed TabResNet<br>系统评估树模型、TabNet及提出的TabResNet]\n        D[关键结果/Results<br>XGBoost most robust & scalable; deep models degrade under imbalance<br>XGBoost最鲁棒且可扩展；深度学习模型在不平衡下性能下降]"
    },
    {
      "title": "TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant",
      "authors": "Rongpei Hong, Jian Lang, Ting Zhong, Yong Wang, Fan Zhou",
      "institution": "University of Electronic Science and Technology of China, Aiwen Technology Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.21616",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "MLLM personalization",
        "long-context",
        "training-free",
        "state-aware",
        "retrieval-augmented generation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0916d89499cb78545669c049322f7cc673c2b69f516444a870dd8869bb13521_w640_q70.webp",
      "contributions": "1. Proposes LCMP, the first Long-Context MLLM Personalization benchmark for evaluating personalized dialogues. 2. Introduces TAME, a novel training-free and state-aware framework using double memories to manage concept variations. 3. Presents the RA2G (Retrieve-then-Align Augmented Generation) paradigm to align retrieved knowledge with current queries for better interaction.",
      "summary": "This paper addresses the limitation of existing MLLM personalization methods in handling long-context dialogues. It proposes a training-free, state-aware framework called TAME, which uses double memories and a novel retrieval-augmentation paradigm (RA2G) to manage personalized concepts over time. Experiments on the new LCMP benchmark show TAME achieves the best performance, enabling evolving interactions in long-context scenarios.",
      "mindmap": "graph TB\n        A[TAMEing Long Contexts in Personalization<br>论文标题] --> B[Problem: Existing MLLM personalization lacks long-context support<br>核心问题: 现有MLLM个性化方法缺乏长上下文支持]\n        A --> C[Method: Proposes TAME framework with double memory & RA2G<br>主要方法: 提出TAME框架, 包含双重记忆和RA2G范式]\n        A --> D[Results: TAME achieves best performance on LCMP benchmark<br>关键结果: TAME在LCMP基准测试中取得最佳性能]"
    },
    {
      "title": "SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration",
      "authors": "Zhiyuan Liu, Daocheng Fu, Pinlong Cai, Lening Wang, Ying Liu, Yilong Ren, Botian Shi, Jianqiang Wang",
      "institution": "Tsinghua University, Shanghai Artificial Intelligence Laboratory, Beihang University",
      "link": "https://arxiv.org/pdf/2512.21618",
      "code": null,
      "tags": [
        "novel view synthesis",
        "diffusion models",
        "3D Gaussian Splatting",
        "auto-regressive restoration",
        "context-aware inpainting",
        "view synthesis"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp",
      "contributions": "1. Proposes SymDrive, a unified diffusion-based framework for joint high-quality rendering and scene editing in autonomous driving simulation. 2. Introduces a Symmetric Auto-regressive Online Restoration paradigm for recovering fine-grained details and generating consistent lateral views. 3. Leverages the restoration capability for a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure lighting and shadow consistency.",
      "summary": "SymDrive addresses the challenge of creating high-fidelity and controllable 3D driving simulators by proposing a unified diffusion-based framework. It uses a symmetric auto-regressive online restoration method to enhance novel-view synthesis and a training-free harmonization mechanism for realistic vehicle insertion. The method achieves state-of-the-art performance in both rendering quality and scene editing realism.",
      "mindmap": "graph TB\n        A[SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration] --> B[核心问题/Problem: 现有方法难以同时实现高保真渲染和交互式交通编辑 / Existing methods struggle with joint photorealistic rendering and interactive traffic editing.]\n        A --> C[主要方法/Method: 提出对称自回归在线修复范式与免训练协调机制 / Proposes Symmetric Auto-regressive Online Restoration paradigm and training-free harmonization mechanism.]\n        A --> D[关键结果/Results: 在新视角增强和3D车辆插入中实现最先进性能 / Achieves SOTA performance in novel-view enhancement and realistic 3D vehicle insertion.]"
    },
    {
      "title": "CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective",
      "authors": "Zhiwen Yang, Jinglin Xu, Yuxin Pen",
      "institution": "Peking University, University of Science and Technology Beijing",
      "link": "https://arxiv.org/pdf/2512.21617",
      "code": "https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM",
      "tags": [
        "fine-grained visual categorization",
        "causal intervention",
        "structural causal model",
        "few-shot learning",
        "interventional multi-scale encoder",
        "interventional masked feature reconstruction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdf9390290fc2d16f99be55ee74ab9d2d794c0c3343cf9ea92ae99e679f1d456_w640_q70.webp",
      "contributions": "1. Proposes a causal perspective to address the confounding effect of support samples in FS-FGVC, framing the task as a causal inference problem. 2. Introduces a novel CausalFSFG method with two key components: an Interventional Multi-Scale Encoder (IMSE) for sample-level intervention and an Interventional Masked Feature Reconstruction (IMFR) module for feature-level intervention. 3. Demonstrates state-of-the-art performance on standard FS-FGVC datasets (CUB-200-2011, Stanford Dogs, Stanford Cars) through extensive experiments.",
      "summary": "This paper identifies that the support sample set acts as a confounding variable in Few-Shot Fine-Grained Visual Categorization (FS-FGVC), introducing bias and spurious correlations. To address this, the authors propose CausalFSFG, a method that uses causal intervention via a sample-level and a feature-level module to reveal true causal relationships. The approach achieves new state-of-the-art results on multiple benchmark datasets.",
      "mindmap": "graph TB\n        Root[CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective]\n        Root --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[Support set as confounder/支持集作为混淆变量]\n        Problem --> P2[Biased data distribution/有偏数据分布]\n        Problem --> P3[Spurious correlations/虚假关联]\n        Method --> M1[Causal Intervention/因果干预]\n        Method --> M2[IMSE: Sample-level/IMSE: 样本层面]\n        Method --> M3[IMFR: Feature-level/IMFR: 特征层面]\n        Results --> R1[SOTA Performance/最优性能]\n        Results --> R2[Datasets: CUB, Dogs, Cars/数据集: CUB, Dogs, Cars]"
    },
    {
      "title": "Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints",
      "authors": "Mutiara Shabrina, Nova Kurnia Putri, Jefri Satria Ferdiansyah, Sabita Khansa Dewi, Novanto Yudistira",
      "institution": "Universitas Brawijaya",
      "link": "https://arxiv.org/pdf/2512.21637",
      "code": null,
      "tags": [
        "image editing",
        "StyleGAN2",
        "CLIP",
        "L1 regularization",
        "latent space manipulation",
        "attribute disentanglement"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d3d2b6fa78e88b5a6e668468f53e16660057860c14bd7c81ab11e3e9b86d455_w640_q70.webp",
      "contributions": "1. An empirical analysis identifying that L2-based regularization in the PPE framework leads to dense latent updates and attribute leakage. 2. The introduction of a sparsity-based constraint using L1 regularization (referred to as an ultra-strict layer masking strategy) for latent space manipulation. 3. Demonstration that the proposed approach enforces more focused edits, reducing unintended changes and better preserving facial identity.",
      "summary": "This paper addresses the problem of attribute entanglement in text-guided image editing. It analyzes the PPE framework and proposes a new method using L1 regularization to enforce sparsity in latent space updates. The results show this approach achieves more controlled edits with less unintended semantic leakage.",
      "mindmap": "graph TB\n        A[Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints] --> B(核心问题/Problem: Attribute Entanglement in Text-Driven Image Editing)\n        A --> C(主要方法/Method: Sparse Latent Constraints via L1 Regularization)\n        A --> D(关键结果/Results: More Focused Edits, Reduced Unintended Changes)"
    },
    {
      "title": "Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding",
      "authors": "Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, Junchao Gong, Xiang Zhuang, Wanghan Xu, Qinglong Cao, Shixiang Tang, Yihao Liu, Wenlong Zhang, Lei Bai",
      "institution": "Shanghai AI Laboratory, Tongji University, Shanghai Jiao Tong University, Zhejiang University, University of Science and Technology of China, UCLA",
      "link": "https://arxiv.org/pdf/2512.21643",
      "code": "https://github.com/Zhouzone/OmniWeather",
      "tags": [
        "multimodal weather modeling",
        "multimodal foundation model",
        "weather generation",
        "weather understanding",
        "Chain-of-Thought",
        "self-attention"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df6816c446631f92d0b9ba66f4d2f4ddda08200edc161b65555c257b6a7b7a85_w640_q70.webp",
      "contributions": "1. Proposes Omni-Weather, the first unified multimodal foundation model that integrates weather generation and understanding in a single architecture. 2. Introduces a Chain-of-Thought dataset for causal reasoning in weather generation to improve interpretability and perceptual quality. 3. Demonstrates through experiments that generative and understanding tasks in weather can mutually enhance each other, achieving state-of-the-art performance.",
      "summary": "This paper addresses the gap between weather prediction and interpretation by proposing Omni-Weather, a unified multimodal foundation model that combines generation and understanding using a shared self-attention mechanism and a novel Chain-of-Thought dataset. The model achieves state-of-the-art results in both tasks, showing that they can mutually benefit each other.",
      "mindmap": "graph TB\n        A[Omni-Weather: 统一多模态天气基础模型 / Unified Multimodal Foundation Model for Weather] --> B\n        A --> C\n        A --> D\n        B[核心问题 / Problem: 天气建模中生成与理解任务分离 / Separation of generation and understanding in weather modeling]\n        C[主要方法 / Method: 统一架构，共享自注意力，思维链数据集 / Unified architecture, shared self-attention, Chain-of-Thought dataset]\n        D[关键结果 / Results: SOTA性能，任务互增强 / State-of-the-art performance, mutual enhancement of tasks]"
    },
    {
      "title": "TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References",
      "authors": "Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng",
      "institution": "Zhejiang University, Fudan University, Huawei Technologies Ltd.",
      "link": "https://arxiv.org/pdf/2512.21641",
      "code": null,
      "tags": [
        "3D object grounding",
        "temporal multimodal grounding",
        "LiDAR-image fusion",
        "language-conditioned decoding",
        "UniScene representation",
        "NuPrompt benchmark"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp",
      "contributions": "1. Proposes TrackTeller, a unified temporal multimodal framework for 3D grounding that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning. 2. Introduces a shared UniScene representation aligned with textual semantics to generate language-aware 3D proposals. 3. Demonstrates significant performance improvements on the NuPrompt benchmark, including a 70% relative gain in Average Multi-Object Tracking Accuracy and a 3.15-3.4x reduction in False Alarm Frequency.",
      "summary": "This paper addresses the problem of grounding natural language references to objects in dynamic 3D driving scenes, which often depend on recent motion or behavior. The authors propose TrackTeller, a framework that fuses LiDAR and camera data with language, builds a unified scene representation, and uses temporal reasoning to refine object identification. Experiments show that TrackTeller significantly outperforms existing baselines in language-grounded tracking accuracy.",
      "mindmap": "graph TB\n        A[TrackTeller: Temporal Multimodal 3D Grounding] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[动态3D场景中的行为依赖语言指代/Dynamic 3D Behavior-Dependent Language Grounding]\n        C --> C1[统一多模态时序框架/Unified Temporal Multimodal Framework]\n        C1 --> C2[LiDAR-图像融合与语言解码/LiDAR-Image Fusion & Language Decoding]\n        C1 --> C3[构建UniScene表示/Build UniScene Representation]\n        C1 --> C4[利用运动历史推理/Reason with Motion History]\n        D --> D1[在NuPrompt上显著提升性能/Significant Improvement on NuPrompt]\n        D1 --> D2[AMOTA提升70%/70% AMOTA Gain]\n        D1 --> D3[误报率降低3.15-3.4倍/3.15-3.4x FA Reduction]"
    },
    {
      "title": "The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds",
      "authors": "Subramanyam Sahoo, Jared Junkin",
      "institution": "University of California, Berkeley, Johns Hopkins University",
      "link": "https://arxiv.org/pdf/2512.21670",
      "code": "https://github.com/SubramanyamSahoo/The-Deepfake-Detective",
      "tags": [
        "deepfake detection",
        "mechanistic interpretability",
        "sparse autoencoder",
        "forensic manifold analysis",
        "feature selectivity",
        "vision-language model"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp",
      "contributions": "1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model's feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues.",
      "summary": "This paper addresses the \"black box\" nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model's internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model's features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors.",
      "mindmap": "graph TB\n        A[The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[深度伪造检测器是黑盒模型/Deepfake detectors are black boxes]\n        C --> C1[稀疏自编码器分析/Sparse Autoencoder (SAE) Analysis]\n        C --> C2[法证流形分析/Forensic Manifold Analysis]\n        D --> D1[潜在特征稀疏使用/Latent features are sparsely used]\n        D --> D2[流形几何特性揭示伪影/Manifold geometry reveals artifacts]"
    },
    {
      "title": "Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles",
      "authors": "Jalal Khan",
      "institution": "United Arab Emirates University",
      "link": "https://arxiv.org/pdf/2512.21673",
      "code": null,
      "tags": [
        "object detection",
        "YOLO-NAS",
        "YOLOv8",
        "perception",
        "autonomous vehicles",
        "custom dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp",
      "contributions": "1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.",
      "summary": "This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.",
      "mindmap": "graph TB\n    A[Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[评估自动驾驶感知中深度学习模型的性能/Evaluate DL model performance for AV perception]\n    C --> C1[使用自定义数据集比较YOLO-NAS与YOLOv8/Compare YOLO-NAS and YOLOv8 using a custom dataset]\n    D --> D1[YOLOv8s训练时间减少75%/YOLOv8s saves 75% training time]\n    D --> D2[YOLOv8s准确率更高(83% vs 81%)/YOLOv8s has higher accuracy (83% vs 81%)]"
    },
    {
      "title": "UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture",
      "authors": "Shuo Cao, Jiayang Li, Xiaohui Li, Yuandong Pu, Kaiwen Zhu, Yuanting Gao, Siqi Luo, Yi Xin, Qi Qin, Yu Zhou, Xiangyu Chen, Wenlong Zhang, Bin Fu, Yu Qiao, Yihao Liu",
      "institution": "Shanghai AI Laboratory, University of Science and Technology of China, Peking University, Shanghai Jiao Tong University, Tsinghua University, Nanjing University, Sun Yat-sen University, Tele-AI",
      "link": "https://arxiv.org/pdf/2512.21675",
      "code": "https://github.com/thunderbolt215/UniPercept",
      "tags": [
        "multimodal understanding",
        "perceptual-level image understanding",
        "multimodal large language models",
        "domain-adaptive pre-training",
        "task-aligned reinforcement learning",
        "unified benchmark"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e93f4678ef30567f8d2c7b2034256f3795d2cf297d9c6c013681b974accd77a7_w640_q70.webp",
      "contributions": "1. Proposes UniPercept-Bench, a unified benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture with hierarchical definitions and large-scale datasets. 2. Develops a strong baseline model, UniPercept, trained via Domain-Adaptive Pre-Training and Task-Aligned Reinforcement Learning for robust generalization. 3. Demonstrates that UniPercept outperforms existing MLLMs on perceptual tasks and can serve as a plug-and-play reward model for text-to-image generation.",
      "summary": "This paper addresses the limited ability of Multimodal Large Language Models (MLLMs) to perceive perceptual-level image features. It introduces UniPercept, a unified framework and benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture, trained with Domain-Adaptive Pre-Training and Task-Aligned RL. The proposed model outperforms existing MLLMs and can be used as a reward model for image generation.",
      "mindmap": "graph TB\n        Root[”UniPercept: 统一感知级图像理解 / Unified Perceptual-Level Image Understanding”] --> Problem[”MLLMs感知能力有限 / MLLMs' Perceptual Ability is Limited”]\n        Root --> Method[”提出统一基准与模型 / Proposes Unified Benchmark & Model”]\n        Root --> Results[”性能超越现有模型 / Outperforms Existing Models”]\n        Problem --> P1[”感知级特征理解不足 / Limited Perceptual-Level Feature Understanding”]\n        Method --> M1[”UniPercept-Bench基准 / UniPercept-Bench Benchmark”]\n        Method --> M2[”DAPT与Task-Aligned RL训练 / DAPT & Task-Aligned RL Training”]\n        Results --> R1[”在VR与VQA任务上泛化 / Generalizes on VR & VQA Tasks”]\n        Results --> R2[”可作为图像生成奖励模型 / Serves as Reward Model for Generation”]"
    },
    {
      "title": "SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration",
      "authors": "Md Motaleb Hossen Manik, Md Zabirul Islam, Ge Wang",
      "institution": "Rensselaer Polytechnic Institute",
      "link": "https://arxiv.org/pdf/2512.21684",
      "code": null,
      "tags": [
        "multi-modal inference",
        "blockchain provenance",
        "vision-language models",
        "semantic extraction",
        "reproducibility",
        "educational AI"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369d8533474f19127055b5c4bf7fad048caff38e6ab3e9aca56aa3a6039a79a4_w640_q70.webp",
      "contributions": "1. Introduces SlideChain, a blockchain-backed provenance framework for verifiable integrity in multimodal semantic extraction from educational content. 2. Presents the SlideChain Slides Dataset, a curated corpus of 1,117 medical imaging lecture slides, and conducts the first systematic analysis of semantic disagreement across four state-of-the-art VLMs. 3. Demonstrates practical scalability, perfect tamper detection, and deterministic reproducibility through evaluation of gas usage, throughput, and auditability on a local EVM-compatible blockchain.",
      "summary": "This paper introduces SlideChain, a framework that uses blockchain to provide verifiable provenance for semantic outputs from vision-language models (VLMs) on educational slides. It analyzes discrepancies across VLMs and shows that SlideChain ensures tamper-evident auditability and reproducibility for AI-assisted instructional systems. The results indicate it is a practical step toward trustworthy multimodal educational pipelines.",
      "mindmap": "graph TB\n        A[SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration] --> B[核心问题/Problem: VLM语义输出难以验证、复现和审计，存在不一致性 / VLM semantic outputs are hard to verify, reproduce, and audit, with inconsistencies]\n        A --> C[主要方法/Method: 基于区块链的溯源框架，从幻灯片中提取概念和关系三元组，哈希上链 / Blockchain-backed provenance framework extracting concepts and triples, hashing to chain]\n        A --> D[关键结果/Results: 揭示模型间显著差异，实现完美篡改检测和确定性复现，提供可扩展的完整性 / Reveals cross-model discrepancies, achieves perfect tamper detection and reproducibility, provides scalable integrity]"
    },
    {
      "title": "Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation",
      "authors": "Yuntian Bo, Tao Zhou, Zechao Li, Haofeng Zhang, Ling Shao",
      "institution": "Nanjing University of Science and Technology, University of Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.21683",
      "code": "https://github.com/primebo1/C-Graph",
      "tags": [
        "medical image segmentation",
        "graph neural network",
        "contrastive learning",
        "few-shot learning",
        "cross-domain",
        "structural consistency"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9571022ab0a47d7b3509a2620b8081c3b3ecf53a3d4371337ce5b1b8a68a57c_w640_q70.webp",
      "contributions": "1. Proposes a Structural Prior Graph (SPG) layer to capture and transfer target-category node dependencies for global structure modeling. 2. Introduces a Subgraph Matching Decoding (SMD) mechanism that leverages semantic node relations to guide prediction. 3. Designs a Confusion-minimizing Node Contrast (CNC) loss to reduce node ambiguity and subgraph heterogeneity via contrastive learning.",
      "summary": "This paper addresses the problem of cross-domain few-shot medical image segmentation, where existing methods degrade performance by filtering out domain-specific information. The proposed C-Graph framework leverages the structural consistency of medical images, modeling features as graphs with novel SPG layers, SMD decoding, and a CNC loss. It achieves state-of-the-art cross-domain performance while preserving strong source-domain accuracy.",
      "mindmap": "graph TB\n        A[Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法过滤域特定信息，损害性能/Existing methods filter domain-specific info, harming performance]\n        C --> C1[利用结构一致性作为先验/Use structural consistency as prior]\n        C --> C2[图建模: SPG层, SMD解码, CNC损失/Graph Modeling: SPG layer, SMD decoding, CNC loss]\n        D --> D1[跨域性能SOTA/Cross-domain SOTA performance]\n        D --> D2[保持源域精度/Preserves source-domain accuracy]"
    },
    {
      "title": "Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective",
      "authors": "Huan Li, Longjun Luo, Yuling Shi, Xiaodong Gu",
      "institution": "Huazhong University of Science and Technology, Guangdong University of Technology, Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.21691",
      "code": null,
      "tags": [
        "3D reconstruction",
        "attention collapse",
        "degenerate diffusion",
        "token-merging",
        "mean-field PDE",
        "VGGT"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81a116424b9dbb97e00548c11b9b6393d53b336e80df62d4caf4add021d7d599_w640_q70.webp",
      "contributions": "1. Provides a rigorous mathematical explanation for the attention collapse phenomenon in VGGT by modeling it as a degenerate diffusion process. 2. Derives a closed-form mean-field partial differential equation that quantitatively predicts the observed rank profile and convergence rate of token features. 3. Explains why the token-merging remedy delays the collapse by slowing the effective diffusion coefficient, offering a principled lens for future scalable 3D-vision transformers.",
      "summary": "This paper analyzes the attention collapse problem in Visual Geometry Grounded Transformers (VGGT) for 3D reconstruction. It models the global self-attention iteration as a degenerate diffusion process, proving that token features converge to a Dirac measure and deriving a mean-field PDE to predict the collapse. The theory explains the empirical observations and shows how token-merging mitigates the issue by reducing the diffusion rate.",
      "mindmap": "graph TB\n        A[Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[VGGT中的注意力崩溃现象/Attention Collapse in VGGT]\n        C --> C1[将注意力迭代建模为退化扩散过程/Model Attention Iteration as Degenerate Diffusion]\n        D --> D1[推导出预测崩溃的均值场PDE/Derive Mean-Field PDE Predicting Collapse]\n        D --> D2[理论解释令牌合并的缓解作用/Theory Explains Token-Merging Remedy]"
    },
    {
      "title": "ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields",
      "authors": "Albert Barreiro, Roger Marí, Rafael Redondo, Gloria Haro, Carles Bosch",
      "institution": "Eurecat, Centre Tecnològic de Catalunya; Universitat Pompeu Fabra; Universitat de Vic - UCC",
      "link": "https://arxiv.org/pdf/2512.21692",
      "code": null,
      "tags": [
        "neural rendering",
        "Neural Radiance Fields",
        "anisotropic specular reflections",
        "Anisotropic Spherical Gaussian",
        "von Mises-Fisher distribution",
        "material editing"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad12a4dd31b4ec9b89dafef4a6d4d851fe0aa09b5e3d8c6f918d085ee2acda50_w640_q70.webp",
      "contributions": "1. Introduces ShinyNeRF, a novel NeRF framework capable of modeling both isotropic and anisotropic specular reflections. 2. Proposes a method to jointly estimate physical surface properties (normals, tangents, specular concentration, anisotropy) by approximating outgoing radiance with a mixture of isotropic von Mises-Fisher distributions. 3. Achieves state-of-the-art performance in digitizing anisotropic materials and enables interpretable material property editing.",
      "summary": "This paper introduces ShinyNeRF, a novel Neural Radiance Fields framework designed to accurately model anisotropic specular reflections, such as those on brushed metals, which previous methods struggled with. The method learns an approximation of outgoing radiance using a mixture of isotropic von Mises-Fisher distributions to jointly estimate physical surface properties. Experimental results show it achieves state-of-the-art performance and enables plausible material editing.",
      "mindmap": "graph TB\n        A[ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法难以建模各向异性高光/Existing methods struggle with anisotropic specular reflections]\n        C --> C1[提出ShinyNeRF框架/Propose ShinyNeRF framework]\n        C1 --> C2[使用各向同性vMF混合近似出射辐射度/Use isotropic vMF mixture to approximate outgoing radiance]\n        C2 --> C3[联合估计法线、切线、高光参数/Jointly estimate normals, tangents, specular parameters]\n        D --> D1[实现SOTA性能/Achieves SOTA performance]\n        D --> D2[提供物理解释和材质编辑/Provides physical interpretation and material editing]"
    },
    {
      "title": "Prior-AttUNet: Retinal OCT Fluid Segmentation Based on Normal Anatomical Priors and Attention Gating",
      "authors": "Li Yang, Yuting Liu",
      "institution": "Wannan Medical College",
      "link": "https://arxiv.org/pdf/2512.21693",
      "code": null,
      "tags": [
        "medical image segmentation",
        "anatomical prior",
        "attention mechanism",
        "variational autoencoder",
        "densely connected blocks",
        "spatial pyramid pooling"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85dc59e9646562af5e8c3595b08a9f5064375d7976961a2ab7b04fe287a243d1_w640_q70.webp",
      "contributions": "1. Proposes a hybrid dual-path architecture integrating a generative prior pathway (using a VAE) with a segmentation network to provide multi-scale normative anatomical priors. 2. Introduces an anatomical prior-guided triple-attention mechanism to dynamically modulate feature importance across decoding stages for improved boundary delineation. 3. Embeds densely connected blocks and spatial pyramid pooling modules in the segmentation backbone to enhance contextual feature extraction.",
      "summary": "This paper introduces Prior-AttUNet, a model for segmenting macular edema fluid in OCT images. It uses a dual-path architecture with generative anatomical priors and a novel triple-attention mechanism to improve accuracy, especially at boundaries, and achieves high Dice scores across multiple imaging devices while maintaining low computational cost.",
      "mindmap": "graph TB\n        A[Prior-AttUNet: 视网膜OCT液体分割 / Prior-AttUNet: Retinal OCT Fluid Segmentation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[模糊边界与设备差异 / Ambiguous Boundaries & Device Heterogeneity]\n        C --> C1[双路径架构 / Dual-Path Architecture]\n        C --> C2[变分自编码器先验 / VAE Priors]\n        C --> C3[三重注意力机制 / Triple-Attention Mechanism]\n        D --> D1[高Dice分数 / High Dice Scores]\n        D --> D2[低计算成本 / Low Computational Cost]"
    },
    {
      "title": "BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks",
      "authors": "Md. Rakibul Islam, Md. Kamrozzaman Bhuiyan, Safwan Muntasir, Arifur Rahman Jawad, Most. Sharmin Sultana Samu",
      "institution": "Not explicitly stated in provided text. Affiliation/domain cannot be reliably inferred.",
      "link": "https://arxiv.org/pdf/2512.21694",
      "code": null,
      "tags": [
        "handwritten text generation",
        "Generative Adversarial Networks",
        "Handwritten Text Generation",
        "Bengali",
        "Dataset",
        "Pre-processing"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp",
      "contributions": "1. Proposed a method for generating Bengali handwritten words from plain text using GANs, addressing a significant research gap. 2. Developed and used a novel, self-collected dataset of Bengali handwriting from approximately 500 diverse individuals. 3. Demonstrated the ability to produce diverse and realistic handwritten outputs through the described approach.",
      "summary": "This paper addresses the lack of research on Bengali handwritten text generation by proposing a GAN-based method to generate words from plain text. The authors created a new dataset of Bengali handwriting samples from hundreds of contributors. The work successfully generates diverse handwritten outputs and contributes to advancing research in this area for the Bengali language.",
      "mindmap": "graph TB\n        A[BeHGAN: Bengali Handwritten Word Generation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[HTG is challenging & understudied for Bengali<br/>孟加拉语手写文本生成研究不足且困难]\n        C --> C1[Propose GAN-based method<br/>提出基于GAN的方法]\n        C --> C2[Use self-collected dataset<br/>使用自收集数据集]\n        C --> C3[Pre-process images<br/>预处理图像]\n        D --> D1[Generates diverse handwritten words<br/>生成多样化手写词]\n        D --> D2[Contributes to Bengali HTG research<br/>推动孟加拉语手写文本生成研究]"
    },
    {
      "title": "FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection",
      "authors": "Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman, Md. Rakibul Islam",
      "institution": "Not explicitly stated in provided content. Affiliation/email domain not present.",
      "link": "https://arxiv.org/pdf/2512.21695",
      "code": null,
      "tags": [
        "ai-generated image detection",
        "Fast Fourier Transform",
        "CLIP",
        "hybrid system",
        "spectral features",
        "semantic features"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9517071d67c92839f8173192ae8096327651e4417f7ed69aefae08dc78c34838_w640_q70.webp",
      "contributions": "1. Proposes FUSE, a novel hybrid detection system that fuses spectral (FFT-based) and semantic (CLIP-based) features into a joint representation. 2. Introduces a progressive two-stage training strategy to effectively learn the fused representation. 3. Demonstrates state-of-the-art robustness and generalization across multiple high-fidelity image generators and diverse benchmarks, particularly on challenging datasets like Chameleon.",
      "summary": "This paper addresses the challenge of robustly detecting AI-generated images by proposing FUSE, a hybrid method that combines spectral features from Fast Fourier Transform with semantic features from CLIP's vision encoder. The fused features are trained progressively in two stages. The approach achieves strong generalization and state-of-the-art performance across multiple datasets and generators, highlighting the benefit of integrating spectral and semantic cues.",
      "mindmap": "graph TB\n        Root[”FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection”] --> Problem[”核心问题/Problem: Reliable detection of AI-generated images”]\n        Root --> Method[”主要方法/Method: Hybrid system fusing FFT spectral features & CLIP semantic features with two-stage training”]\n        Root --> Results[”关键结果/Results: SOTA on Chameleon, strong generalization across datasets”]"
    },
    {
      "title": "Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction",
      "authors": "Zheng Yin, Chengjian Li, Xiangbo Shu, Meiqi Cao, Rui Yan, Jinhui Tang",
      "institution": "Nanjing University of Science and Technology, Nanjing Forestry University",
      "link": "https://arxiv.org/pdf/2512.21707",
      "code": "https://github.com/alanyz106/ST-MoE",
      "tags": [
        "human motion prediction",
        "Mixture of Experts",
        "Mamba",
        "spatiotemporal dependencies",
        "computational efficiency"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfad49ae18e732f3537bb8bddb97086ffd94c3c545ca0a735b6cfe245b14f6f0_w640_q70.webp",
      "contributions": "1. Proposes a Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE) model to flexibly capture complex spatio-temporal dependencies in multi-person motion. 2. Introduces four distinct spatiotemporal experts based on bidirectional spatiotemporal Mamba to achieve parameter sharing and efficiency. 3. Demonstrates superior accuracy, a 41.38% reduction in model parameters, and a 3.6x training speedup on benchmark datasets.",
      "summary": "This paper addresses the limitations of existing multi-person motion prediction methods, which suffer from inflexible spatiotemporal representations and high computational costs. The authors propose the ST-MoE model, which uses a mixture of Mamba-based experts to adaptively capture spatiotemporal patterns. The method achieves higher accuracy while being more parameter-efficient and faster to train than state-of-the-art approaches.",
      "mindmap": "graph TB\n        Root(”Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”不灵活的时空表示/Inflexible spatiotemporal representation”)\n        Problem --> P2(”高计算成本/High computational cost”)\n        Method --> M1(”提出ST-MoE模型/Propose ST-MoE model”)\n        M1 --> M2(”四种时空专家/Four spatiotemporal experts”)\n        M2 --> M3(”双向时空Mamba/Bidirectional spatiotemporal Mamba”)\n        Results --> R1(”精度超越SOTA/Outperforms SOTA in accuracy”)\n        Results --> R2(”参数减少41.38%/Reduces parameters by 41.38%”)\n        Results --> R3(”训练加速3.6倍/3.6x training speedup”)"
    },
    {
      "title": "RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention",
      "authors": "Zhan Chen, Zile Guo, Enze Zhu, Peirong Zhang, Xiaoxuan Liu, Lei Wang, Yidan Zhang",
      "institution": "Aerospace Information Research Institute, Chinese Academy of Sciences; University of Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.21710",
      "code": "https://github.com/Thelegendzz/RAPTOR",
      "tags": [
        "video prediction",
        "Efficient Video Attention (EVA)",
        "spatiotemporal factorization",
        "real-time inference",
        "on-device AI",
        "training curriculum"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca32784d3d86e53d15479ff35e2982c66656147309d47e2856d7c2334de35f97_w640_q70.webp",
      "contributions": "1. Introduces RAPTOR, a single-pass video prediction architecture that avoids the latency and error accumulation of iterative models like diffusion., 2. Proposes Efficient Video Attention (EVA), a novel module that factorizes spatiotemporal modeling to achieve linear time and memory complexity, enabling high-resolution (512^2) processing., 3. Presents a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp, temporally coherent details.",
      "summary": "This paper addresses the trilemma in video prediction between speed, resolution, and quality by introducing RAPTOR, a model featuring a novel Efficient Video Attention (EVA) module for linear-complexity spatiotemporal modeling. RAPTOR achieves real-time, high-resolution prediction on edge hardware, setting new state-of-the-art results on benchmark datasets and significantly improving UAV navigation success rates.",
      "mindmap": "graph TB\n        A[RAPTOR: 实时高分辨率无人机视频预测<br>RAPTOR: Real-Time High-Resolution UAV Video Prediction] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>视频预测的”三难困境”: 速度、分辨率、质量<br>Video Prediction Trilemma: Speed, Resolution, Quality]\n        C[主要方法/Method<br>高效视频注意力 (EVA) 模块<br>Efficient Video Attention (EVA) Module]\n        D[关键结果/Results<br>首个在Jetson上512^2视频>30 FPS<br>First >30 FPS for 512^2 video on Jetson]"
    },
    {
      "title": "AstraNav-World: World Model for Foresight Control and Consistency",
      "authors": "Junjun Hu, Jintao Chen, Haochen Bai, Minghua Luo, Shichao Xie, Ziyi Chen, Fei Liu, Zedong Chu, Xinda Xue, Botao Ren, Xiaolong Wu, Mu Xu, Shanghang Zhang",
      "institution": "Amap Alibaba, Peking University (PKU), Tsinghua University (THU)",
      "link": "https://arxiv.org/pdf/2512.21714",
      "code": "https://astra-amap.github.io/AstraNav-World.github.io/",
      "tags": [
        "world models",
        "world model",
        "diffusion model",
        "embodied navigation",
        "foresight control",
        "vision-language policy"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15f89f809a2e6654a0c9645caeda1ab33306e62b8276228d05e7467ef63fc5b8_w640_q70.webp",
      "contributions": "1. Proposes AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. 2. Introduces a training paradigm with bidirectional constraints, optimizing for both action-conditioned visual prediction and visual-conditioned trajectory derivation to ensure executability and physical consistency. 3. Demonstrates improved navigation performance and exceptional zero-shot generalization in real-world testing, showing the model captures transferable spatial and dynamic understanding.",
      "summary": "The paper proposes AstraNav-World, a unified world model that integrates a diffusion-based video generator with a vision-language policy to jointly predict future visual scenes and plan action sequences. This bidirectional, synchronized approach mitigates cumulative errors from decoupled pipelines. Experiments show it improves navigation accuracy and success rates, and exhibits strong zero-shot generalization to real-world scenarios.",
      "mindmap": "graph TB\n        A[AstraNav-World] --> B[核心问题/Problem: Embodied navigation lacks foresight, leading to error accumulation in open, dynamic environments.]\n        A --> C[主要方法/Method: Unified world model with diffusion video generator & vision-language policy for synchronized vision-action rollouts.]\n        A --> D[关键结果/Results: Improved trajectory accuracy, higher success rates, and exceptional zero-shot real-world adaptation.]"
    },
    {
      "title": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation",
      "authors": "Steven Xiao, XIndi Zhang, Dechao Meng, Qi Wang, Peng Zhang, Bang Zhang",
      "institution": "Tongyi Lab, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.21734",
      "code": "https://humanaigc.github.io/knot_forcing_demo_page/",
      "tags": [
        "diffusion models",
        "autoregressive video generation",
        "KV caching",
        "sliding window attention",
        "temporal knot",
        "chunk-wise generation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3db0cbfa541f28563a8f65a9f7c0b4cb0b909e1c62fbec822e091b0ad44811c7_w640_q70.webp",
      "contributions": "1. A chunk-wise generation strategy with global identity preservation via cached KV states and local temporal modeling using sliding window attention. 2. A temporal knot module that overlaps adjacent chunks and uses image-to-video conditioning to smooth inter-chunk motion transitions. 3. A \"running ahead\" mechanism that dynamically updates the reference frame's temporal coordinate to maintain long-term coherence.",
      "summary": "This paper proposes Knot Forcing, a streaming framework for real-time, infinite portrait animation. It addresses error accumulation and motion discontinuities in autoregressive video diffusion models through chunk-wise generation, a temporal knot module, and a running-ahead mechanism. The method achieves high-fidelity, temporally consistent animation with real-time performance on consumer GPUs.",
      "mindmap": "graph TB\n        A[”Knot Forcing: Taming Autoregressive Video Diffusion Models<br>Knot Forcing: 驯服自回归视频扩散模型”] --> B[”核心问题/Problem: Real-time portrait animation needs low latency & consistency, but autoregressive models suffer from error accumulation and motion discontinuities.<br>实时肖像动画需要低延迟和一致性，但自回归模型存在误差累积和运动不连续问题。”]\n        A --> C[”主要方法/Method: Chunk-wise generation with KV caching, Temporal Knot module for smooth transitions, and 'Running Ahead' mechanism.<br>分块生成与KV缓存，用于平滑过渡的时序结模块，以及'超前运行'机制。”]\n        A --> D[”关键结果/Results: Enables high-fidelity, infinite, interactive portrait animation with real-time performance on consumer GPUs.<br>在消费级GPU上实现高保真、无限、交互式的实时肖像动画。”]"
    },
    {
      "title": "SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild",
      "authors": "Xindi Zhang, Dechao Meng, Steven Xiao, Qi Wang, Peng Zhang, Bang Zhang",
      "institution": "Tongyi Lab, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.21736",
      "code": "https://humanaigc.github.io/sync_anyone_demo_page/",
      "tags": [
        "video generation",
        "lip-syncing",
        "diffusion transformer",
        "two-stage learning",
        "inpainting",
        "self-correction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/825b387e09612033170e3bd5c2f7bedd0f32c2a644514ee397fd09b64088653e_w640_q70.webp",
      "contributions": "1. A novel two-stage learning framework (SyncAnyone) for lip-syncing that decouples accurate motion modeling from high-fidelity generation. 2. A Stage 2 mask-free tuning pipeline that uses a self-generated pseudo-dataset to correct artifacts from the initial mask-based training. 3. Demonstrates state-of-the-art performance in visual quality, temporal coherence, and identity preservation for in-the-wild scenarios.",
      "summary": "This paper addresses the problem of generating high-quality, audio-synchronized lip movements in videos without disrupting the spatiotemporal context or background. The proposed method, SyncAnyone, uses a two-stage framework: first training a diffusion-based video transformer for masked mouth inpainting, and then fine-tuning it mask-free on self-generated data to correct artifacts. Experiments show it achieves state-of-the-art results in lip-syncing for challenging, real-world videos.",
      "mindmap": "graph TB\n        A[SyncAnyone] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[”现有方法破坏时空上下文 / Existing methods disrupt spatiotemporal context”]\n        C --> C1[”阶段1: 基于掩码的扩散变换器训练 / Stage 1: Mask-based DiT training”]\n        C --> C2[”阶段2: 无掩码调优与自校正 / Stage 2: Mask-free tuning & self-correction”]\n        D --> D1[”SOTA视觉质量与时间一致性 / SOTA visual quality & temporal coherence”]\n        D --> D2[”更好的身份与背景保持 / Better identity & background preservation”]"
    },
    {
      "title": "Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning",
      "authors": "Hengyi Wu, Zhenyi Wang, Heng Huang",
      "institution": "University of Maryland, College Park, University of Central Florida",
      "link": "https://arxiv.org/pdf/2512.21743",
      "code": null,
      "tags": [
        "continual learning",
        "entropy scaling",
        "catastrophic forgetting",
        "stability-plasticity dilemma",
        "dynamic feedback",
        "layer-wise control"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp",
      "contributions": "1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones.",
      "summary": "The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines.",
      "mindmap": "graph TB\n        Root[Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Catastrophic forgetting in continual learning due to uniform layer treatment] --> P1[高熵层欠拟合/High-entropy layers underfit]\n        Problem --> P2[低熵层过拟合/Low-entropy layers overfit]\n        Method[主要方法/Method: Entropy-aware dynamic feedback for layer-wise control] --> M1[减少高熵层熵值/Reduce entropy in high-entropy layers]\n        Method --> M2[增加低熵层熵值/Increase entropy in low-entropy layers]\n        Results[关键结果/Results: Improved generalization and performance] --> R1[收敛到更宽的局部极小值/Converge to wider local minima]\n        Results --> R2[超越现有基线方法/Outperforms state-of-the-art baselines]"
    },
    {
      "title": "Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG",
      "authors": "Gourav Siddhad, Anurag Singh, Rajkumar Saini, Partha Pratim Roy",
      "institution": "Indian Institute of Technology Roorkee, OP Jindal University, Luleå University of Technology, Indian Institute of Technology Dhanbad",
      "link": "https://arxiv.org/pdf/2512.21747",
      "code": null,
      "tags": [
        "brain-computer interface (BCI)",
        "EEG",
        "TSception",
        "Adaptive Average Pooling",
        "spatiotemporal features",
        "drowsiness detection"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp",
      "contributions": "1. Proposed a Modified TSception architecture with a five-layer temporal refinement strategy to capture multi-scale brain dynamics. 2. Introduced Adaptive Average Pooling for structural flexibility to handle varying EEG input dimensions and a two-stage fusion mechanism for optimized spatiotemporal feature integration. 3. Demonstrated improved performance stability (reduced confidence interval) on the SEED-VIG dataset and state-of-the-art generalizability on the STEW mental workload dataset.",
      "summary": "This paper proposes a Modified TSception deep learning model for robust EEG-based detection of driver drowsiness and mental workload. The key modifications include a multi-layer temporal refinement strategy and Adaptive Average Pooling, which improve the model's stability and ability to handle varying input sizes. The model achieves comparable accuracy with significantly better stability on a drowsiness dataset and state-of-the-art results on a mental workload dataset, demonstrating its effectiveness for reliable cognitive state monitoring.",
      "mindmap": "graph TB\n        A[Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG] --> B(核心问题/Problem: Driver drowsiness detection for road safety)\n        A --> C(主要方法/Method: Modified TSception with temporal refinement & Adaptive Average Pooling)\n        A --> D(关键结果/Results: Improved stability on SEED-VIG, SOTA on STEW)"
    },
    {
      "title": "A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets",
      "authors": "Arunkumar V, Firos V M, Senthilkumar S, Gangadharan G R",
      "institution": "Anna University, National Institute of Technology Tiruchirappalli",
      "link": "https://arxiv.org/pdf/2512.21760",
      "code": null,
      "tags": [
        "medical image segmentation",
        "Quaternion Neural Networks",
        "Cross-Attention",
        "Unpaired Data",
        "Multimodal Learning",
        "Explainable AI"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp",
      "contributions": "1. Proposes an Adaptive Quaternion Cross-Fusion (A-QCF) block for bidirectional knowledge transfer between unpaired CT and MRI data streams., 2. Introduces a unified segmentation model (A-QCF-Net) that leverages Quaternion Neural Networks to build a shared feature space from separate datasets., 3. Demonstrates significant performance gains over strong unimodal baselines and validates clinical relevance through explainability analysis.",
      "summary": "This paper addresses the challenge of training multimodal segmentation models with unpaired datasets. It proposes A-QCF-Net, which uses Quaternion Neural Networks and an adaptive cross-fusion block to enable knowledge transfer between separate CT and MRI data. The method significantly outperforms unimodal baselines and provides a viable paradigm for leveraging large, unpaired medical image archives.",
      "mindmap": "graph TB\n        A[A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network<br>for Multimodal Liver Tumor Segmentation from Unpaired Datasets] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Scarcity of paired & aligned multimodal medical datasets]\n        C[主要方法/Method<br>Adaptive Quaternion Cross-Fusion (A-QCF) block<br>Quaternion Neural Networks for shared feature space]\n        D[关键结果/Results<br>Significant Dice score improvement over nnU-Net<br>Validated by explainability analysis]"
    },
    {
      "title": "BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization",
      "authors": "Evgeny Alves Limarenko, Anastasiia Studenikina",
      "institution": "Moscow Institute of Physics and Technology",
      "link": "https://arxiv.org/pdf/2512.21769",
      "code": null,
      "tags": [
        "3d medical image analysis",
        "Masked Autoencoder",
        "Swin Transformer",
        "Self-Supervised Learning",
        "3D Vision Transformer",
        "Structural Priority Loss"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp",
      "contributions": "1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs.",
      "summary": "The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost.",
      "mindmap": "graph TB\n        Root(”BertsWin: 3D MAE优化”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”3D MAE拓扑稀疏性/Topological Sparsity in 3D MAE”)\n        Problem --> P2(”破坏空间关系/Destroys Spatial Context”)\n        Method --> M1(”BertsWin混合架构/BertsWin Hybrid Architecture”)\n        Method --> M2(”完整3D令牌网格/Full 3D Token Grid”)\n        Method --> M3(”Swin窗口 & 结构损失/Swin Windows & Structural Loss”)\n        Results --> R1(”5.8x语义收敛加速/5.8x Faster Convergence”)\n        Results --> R2(”15倍训练轮次减少/15x Fewer Epochs”)\n        Results --> R3(”FLOPs持平，总资源减少/FLOP Parity, Net Resource Reduction”)"
    },
    {
      "title": "Inference-based GAN Video Generation",
      "authors": "Jingbo Yang, Adrian G. Bors",
      "institution": "University of York",
      "link": "https://arxiv.org/pdf/2512.21776",
      "code": null,
      "tags": [
        "video generation",
        "VAE-GAN",
        "Markov chain",
        "long video generation",
        "temporal consistency",
        "encoder-decoder"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp",
      "contributions": "1. Proposes a new video generator, Encoder GAN3 (EncGAN3), which is a VAE-GAN hybrid structure that incorporates inference capabilities into an adversarial-based unconditional video generator. 2. Introduces a novel, memory-efficient approach to generate long videos (hundreds/thousands of frames) by extending the base VAE-GAN model. 3. Leverages a Markov chain framework with a recall mechanism, where each state is a short VAE-GAN generator, to sequentially connect video sub-sequences and ensure temporal continuity.",
      "summary": "This paper addresses the challenge of generating long, high-quality videos, a task where existing models suffer from quality degradation. The proposed method first introduces a VAE-GAN hybrid video generator (EncGAN3) and then extends it using a Markov chain framework to sequentially generate short video clips, enabling the creation of temporally consistent long videos. The main conclusion is that this approach overcomes the temporal scaling limitation and allows for memory-efficient generation of long video sequences.",
      "mindmap": "graph TB\n        Root[Inference-based GAN Video Generation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有模型难以生成长视频/Existing models struggle with long video generation]\n        P1 --> P2[视频长度增加导致质量下降/Increased length degrades quality]\n        Method[主要方法/Method] --> M1[提出VAE-GAN混合视频生成器/Propose VAE-GAN hybrid video generator]\n        M1 --> M2[使用马尔可夫链框架扩展/Extend with Markov chain framework]\n        M2 --> M3[状态代表短视频生成器/Each state is a short video generator]\n        Results[关键结果/Results] --> R1[能够生成长视频序列/Can generate long video sequences]\n        R1 --> R2[确保时序连续性与一致性/Ensures temporal continuity and consistency]"
    },
    {
      "title": "Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models",
      "authors": "Nimrod Berman, Adam Botach, Emanuel Ben-Baruch, Shunit Haviv Hakimi, Asaf Gendler, Ilan Naiman, Erez Yosef, Igor Kviatkovsky",
      "institution": "Ben-Gurion University, Amazon Prime Video, Tel-Aviv University",
      "link": "https://arxiv.org/pdf/2512.21778",
      "code": null,
      "tags": [
        "video scene segmentation",
        "vision-language model",
        "multimodal reasoning",
        "context-focus window",
        "confidence score extraction",
        "explainable AI"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d88d54ab6607412e3eb29cafcea4e88a9b83dcabc3bd75f5b4eca365e6ada2b5_w640_q70.webp",
      "contributions": "1. Introduces Scene-VLM, the first fine-tuned vision-language model framework for video scene segmentation, enabling multimodal reasoning across visual and textual cues. 2. Proposes a method to extract confidence scores from VLM token-level logits, enabling controllable precision-recall trade-offs. 3. Demonstrates the model can be aligned to generate coherent natural-language rationales for its boundary decisions with minimal supervision.",
      "summary": "This paper proposes Scene-VLM, a novel vision-language model framework for segmenting long videos into coherent scenes by jointly processing visual frames, dialogue, and metadata. It predicts boundaries sequentially with a context-focus window and can provide confidence scores and explanations. The method achieves state-of-the-art performance on benchmarks like MovieNet, significantly outperforming previous methods.",
      "mindmap": "graph TB\n        Root(”Scene-VLM: Multimodal Video Scene Segmentation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”现有方法限制/Limitations of existing methods”)\n        P1 --> P1_1(”视觉中心偏见/Visual-centric bias”)\n        P1 --> P1_2(”孤立分类/Isolated shot classification”)\n        P1 --> P1_3(”缺乏可解释性/Lack explainability”)\n        Method --> M1(”微调VLM框架/Fine-tuned VLM framework”)\n        M1 --> M1_1(”多模态推理/Multimodal reasoning”)\n        M1 --> M1_2(”序列预测/Sequential prediction”)\n        M1 --> M1_3(”上下文聚焦窗口/Context-focus window”)\n        Method --> M2(”置信度提取/Confidence score extraction”)\n        Method --> M3(”生成解释/Generate rationales”)\n        Results --> R1(”SOTA性能/State-of-the-art performance”)\n        Results --> R2(”显著提升/Significant improvement on MovieNet”)"
    },
    {
      "title": "Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning",
      "authors": "Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles",
      "institution": "The Pennsylvania State University, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.21789",
      "code": null,
      "tags": [
        "image captioning",
        "scientific figure captioning",
        "large-scale dataset",
        "domain-specific training",
        "human evaluation",
        "large language models (LLMs)"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp",
      "contributions": "1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions.",
      "summary": "This paper reviews the SciCap project's first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field.",
      "mindmap": "graph TB\n        Root[Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[科学图表说明质量差/Poor quality of scientific figure captions]\n        Problem --> P2[缺乏大规模真实数据集/Lack of large-scale real-world dataset]\n        Method --> M1[构建arXiv图表-说明对数据集/Construct arXiv figure-caption dataset]\n        Method --> M2[领域特定训练与评估/Domain-specific training & evaluation]\n        Method --> M3[应对大语言模型兴起/Navigate rise of LLMs]\n        Results --> R1[总结技术方法经验/Summarize technical & methodological lessons]\n        Results --> R2[提出未来挑战与方向/Outline future challenges & directions]"
    },
    {
      "title": "InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation",
      "authors": "Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang, Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan",
      "institution": "ByteDance Inc., Rutgers University",
      "link": "https://arxiv.org/pdf/2512.21788",
      "code": "https://github.com/yanq095/InstructMoLE",
      "tags": [
        "diffusion models",
        "Parameter-Efficient Fine-Tuning",
        "Mixture of Low-rank Experts",
        "Instruction-Guided Routing",
        "Multi-Conditional Generation",
        "Diffusion Transformers"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp",
      "contributions": "1. Introduces InstructMoLE, a framework using an Instruction-Guided Mixture of Low-Rank Experts for multi-conditional image generation., 2. Proposes Instruction-Guided Routing (IGR), a global routing signal derived from user instructions to select a coherent expert council for all tokens, addressing spatial fragmentation and semantic drift., 3. Introduces an output-space orthogonality loss to promote expert functional diversity and prevent representational collapse.",
      "summary": "This paper addresses the problem of task interference in multi-conditional image generation when using monolithic PEFT adapters like LoRA. It proposes InstructMoLE, a novel framework that uses global instruction-guided routing to select a consistent mixture of low-rank experts, combined with an orthogonality loss for diversity, which outperforms existing methods on benchmarks.",
      "mindmap": "graph TB\n        A[InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation] --> B[核心问题/Problem: Task interference & spatial fragmentation in multi-conditional DiT fine-tuning]\n        A --> C[主要方法/Method: Global Instruction-Guided Routing (IGR) & output-space orthogonality loss]\n        A --> D[关键结果/Results: Outperforms LoRA & MoLE variants on benchmarks]"
    },
    {
      "title": "AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge",
      "authors": "Hyam Omar Ali, Sahar Alhesseen, Lamis Elkhair, Adrian Galdran, Ming Feng, Zhixiang Xiong, Zengming Lin, Kele Xu, Liang Hu, Benjamin Keel, Oliver Mills, James Battye, Akshay Kumar, Asra Aslam, Prasad Dutande, Ujjwal Baid, Bhakti Baheti, Suhas Gajre, Aravind Shrenivas Murali, Eung-Joo Lee, Ahmed Fahal, Rachid Jennane",
      "institution": "University of Khartoum, Orleans University, Tongji University, National University of Defense Technology, University of Leeds",
      "link": "https://arxiv.org/pdf/2512.21792",
      "code": null,
      "tags": [
        "medical image analysis",
        "histopathological images",
        "image segmentation",
        "grain detection",
        "deep learning",
        "mycetoma classification"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2608e467b615afdd887a03b7ca003d5d2989f1640e4dc84304a9ee0f5a6e979_w640_q70.webp",
      "contributions": "1. Organized the mAIcetoma challenge to advance AI solutions for mycetoma diagnosis. 2. Provided the standardized Mycetoma database (MyData) for model development. 3. Demonstrated the effectiveness of automated deep learning models for segmenting grains and classifying mycetoma types from histopathological images.",
      "summary": "This paper presents the mAIcetoma challenge, which aimed to develop AI models for automating the diagnosis of mycetoma by segmenting grains and classifying disease types from histopathological images. Multiple teams proposed various deep learning architectures, which were evaluated on a provided standardized dataset. The results showed that the models achieved high segmentation accuracy and significant performance in classification, highlighting the potential of AI to assist in diagnosing this neglected tropical disease.",
      "mindmap": "graph TB\n        Root(”AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge<br>AI用于组织病理学图像中的足菌肿诊断：MICCAI 2024挑战赛”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”足菌肿诊断困难，尤其在资源匮乏地区<br>Mycetoma diagnosis is challenging, especially in low-resource settings”)\n        Method --> M1(”组织mAIcetoma挑战赛，开发AI模型<br>Organized mAIcetoma challenge to develop AI models”)\n        Method --> M2(”提供MyData数据集，用于分割和分类<br>Provided MyData dataset for segmentation and classification”)\n        Results --> R1(”模型实现高精度分割<br>Models achieved high segmentation accuracy”)\n        Results --> R2(”顶级模型分类性能显著<br>Top models showed significant classification performance”)"
    },
    {
      "title": "Diffusion Posterior Sampling for Super-Resolution under Gaussian Measurement Noise",
      "authors": "Abu Hanif Muhammad Syarubany",
      "institution": "Korea Advanced Institute of Science & Technology (KAIST)",
      "link": "https://arxiv.org/pdf/2512.21797",
      "code": null,
      "tags": [
        "image super-resolution",
        "diffusion posterior sampling",
        "single-image super-resolution",
        "inverse problems",
        "measurement consistency",
        "unconditional diffusion prior"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc3596dd62ab28bf4cee2aec80e4147a24811ace57a7d9d76802c0ea0767bbdd_w640_q70.webp",
      "contributions": "1. Implementation and evaluation of diffusion posterior sampling (DPS) for single-image super-resolution under a known degradation model. 2. An ablation study analyzing the impact of guidance scale and noise level on reconstruction quality, identifying an optimal configuration. 3. Demonstration that balancing the diffusion prior and measurement-gradient strength enables stable, high-quality reconstructions without model retraining.",
      "summary": "This paper studies the use of diffusion posterior sampling (DPS) for single-image super-resolution under Gaussian noise. The method combines an unconditional diffusion prior with a likelihood-guided update to enforce measurement consistency during sampling. The main finding is that moderate guidance, at a specific scale and noise level, yields the best reconstruction quality, highlighting the importance of balancing prior and data fidelity for stable results.",
      "mindmap": "graph TB\n        Root[”Diffusion Posterior Sampling for Super-Resolution<br>扩散后验采样用于超分辨率”] --> Problem[”核心问题/Problem<br>Single-image super-resolution under Gaussian noise<br>高斯噪声下的单图像超分辨率”]\n        Root --> Method[”主要方法/Method<br>Diffusion Posterior Sampling (DPS)<br>扩散后验采样<br>Unconditional prior + likelihood-guided conditioning<br>无条件先验 + 似然引导的条件化”]\n        Root --> Results[”关键结果/Results<br>Optimal PS scale 0.95, noise σ=0.01<br>最佳PS尺度0.95, 噪声σ=0.01<br>Balancing prior and data yields sharp details<br>平衡先验与数据得到清晰细节”]"
    },
    {
      "title": "CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection",
      "authors": "Ruochen Liu, Yi Tian, Jiahao Wang, Hongbin Liu, Xianxu Hou, Jingxin Liu",
      "institution": "University of Liverpool, National University of Singapore, Xi'an Jiaotong-Liverpool University",
      "link": "https://arxiv.org/pdf/2512.21803",
      "code": null,
      "tags": [
        "object detection",
        "Mamba",
        "Triple-Mapping Adaptive Coupling (TMAC)",
        "Adaptive Mamba Head",
        "biomedical instance detection",
        "VSSD backbone"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp",
      "contributions": "1. Proposes a novel Triple-Mapping Adaptive Coupling (TMAC) module that splits channels into parallel branches with dual idiosyncratic and one consensus attention map for enhanced spatial discriminability. 2. Designs an Adaptive Mamba Head that fuses multi-scale features via learnable weights to handle varying object sizes robustly. 3. Introduces CellMamba, a lightweight one-stage detector built on a VSSD backbone with CellMamba Blocks, achieving superior accuracy and efficiency on biomedical cell detection datasets.",
      "summary": "This paper proposes CellMamba, a lightweight one-stage detector for cell detection in pathological images. It introduces a novel Triple-Mapping Adaptive Coupling (TMAC) module and an Adaptive Mamba Head to improve spatial discriminability and multi-scale feature fusion. Experiments show CellMamba outperforms CNN, Transformer, and Mamba baselines in accuracy while being more efficient in model size and inference speed.",
      "mindmap": "graph TB\n        A[CellMamba: Adaptive Mamba for Cell Detection] --> B[核心问题/Problem: Cell detection challenges in pathological images]\n        A --> C[主要方法/Method: CellMamba with TMAC module & Adaptive Mamba Head]\n        A --> D[关键结果/Results: Outperforms baselines, lightweight & efficient]"
    },
    {
      "title": "S&P 500 Stock's Movement Prediction using CNN",
      "authors": "Rahul Gupta",
      "institution": "None (No affiliation or email domain provided in the given content)",
      "link": "https://arxiv.org/pdf/2512.21804",
      "code": null,
      "tags": [
        "financial time series forecasting",
        "Convolutional Neural Network (CNN)",
        "multivariate raw data",
        "stock movement prediction",
        "historical data matrices",
        "S&P 500"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp",
      "contributions": "1. Proposes the application of Convolutional Neural Networks (CNNs), typically used for image classification, to the problem of stock movement prediction by treating multivariate historical stock data as image-like matrices. 2. Utilizes raw, unprocessed market data including events like stock splits and dividends, instead of relying on pre-engineered financial features. 3. Demonstrates a flexible prediction framework that can be applied at different levels: individual stocks, sectors, or entire portfolios.",
      "summary": "This paper tackles the problem of predicting stock price movements for the S&P 500 index. The core method involves using a Convolutional Neural Network (CNN) to analyze multivariate historical stock data, which is structured as image-like matrices, without extensive feature engineering. The approach shows promising results and offers a flexible model for stock, sector, or portfolio-level predictions.",
      "mindmap": "graph TB\n        Root[”S&P 500 Stock's Movement Prediction using CNN<br>使用CNN预测标普500股票走势”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Predicting stock price movement<br>预测股票价格走势”] --> P1[”传统方法依赖特征工程<br>Traditional methods rely on engineered features”]\n        Problem --> P2[”现有研究多使用单维数据<br>Existing research often uses single-dimension data”]\n        Method[”主要方法/Method<br>Use CNN on raw multivariate data<br>对原始多变量数据使用CNN”] --> M1[”将历史数据矩阵视为图像<br>Treat historical data matrices as images”]\n        Method --> M2[”包含原始市场事件(如拆股)<br>Include raw market events (e.g., splits)”]\n        Results[”关键结果/Results<br>Model achieves promising results<br>模型取得有希望的结果”] --> R1[”支持股票/行业/组合级别预测<br>Supports stock/sector/portfolio prediction”]"
    },
    {
      "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
      "authors": "Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang",
      "institution": "Australian National University, The University of Queensland, GE Research",
      "link": "https://arxiv.org/pdf/2512.21815",
      "code": null,
      "tags": [
        "adversarial attacks",
        "entropy-guided attacks",
        "vision-language models",
        "adversarial robustness",
        "harmful content generation",
        "transferability"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp",
      "contributions": "1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses.",
      "summary": "This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms.",
      "mindmap": "graph TB\n        Root[Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[VLMs易受对抗攻击/VLMs are vulnerable to adversarial attacks]\n        Problem --> P2[先验攻击假设所有token同等重要/Prior attacks assume all tokens are equally important]\n        Method[主要方法/Method] --> M1[识别高熵关键决策点/Identify high-entropy critical decision points]\n        Method --> M2[提出熵库引导对抗攻击(EGA)/Propose Entropy-bank Guided Adversarial attack (EGA)]\n        Results[关键结果/Results] --> R1[高效攻击:小预算实现强语义退化/Efficient attack: strong degradation with small budget]\n        Results --> R2[高有害转化率:35-49%/High harmful conversion: 35-49%]\n        Results --> R3[可行迁移性:17-26%/Feasible transferability: 17-26%]"
    },
    {
      "title": "End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration",
      "authors": "Zhenwei Yang, Yibo Ai, Weidong Zhang",
      "institution": "University of Science and Technology Beijing, National Center for Materials Service Safety",
      "link": "https://arxiv.org/pdf/2512.21831",
      "code": null,
      "tags": [
        "autonomous driving perception",
        "multimodal fusion",
        "multi-view cooperative perception",
        "spatiotemporal modeling",
        "V2X communication",
        "deformable attention"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2257ca524cd2e350a5c2abf7059415ea35045bbf6cb45991c73d32410116cb9_w640_q70.webp",
      "contributions": "1. Proposes XET-V2X, an end-to-end tracking framework that unifies multi-view multimodal sensing within a shared spatiotemporal representation for V2X collaboration. 2. Introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention to efficiently align heterogeneous viewpoints and modalities. 3. Demonstrates robust performance on real-world and simulated V2X datasets, showing consistent improvements in detection and tracking under varying communication delays.",
      "summary": "This paper addresses the challenge of reliable 3D perception in autonomous driving under occlusions and communication delays by proposing XET-V2X, an end-to-end framework that fuses multi-view images and point clouds using a novel dual-layer spatial cross-attention module. The method achieves effective cross-modal interaction and reduces computational overhead. Experiments show it delivers robust and temporally stable detection and tracking performance in complex V2X scenarios.",
      "mindmap": "graph TB\n    A[End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[感知挑战:遮挡,视角限制,通信延迟/Perception Challenges: Occlusions, Limited Viewpoints, Communication Delays]\n    C --> C1[提出XET-V2X框架/Proposes XET-V2X Framework]\n    C1 --> C2[双层级空间交叉注意力模块/Dual-layer Spatial Cross-Attention Module]\n    C2 --> C3[多视图图像特征聚合/Multi-view Image Feature Aggregation]\n    C2 --> C4[点云融合/Point Cloud Fusion]\n    D --> D1[在V2X-Seq-SPD等数据集上性能提升/Performance Improvements on V2X-Seq-SPD, etc.]\n    D1 --> D2[检测与跟踪性能增强/Enhanced Detection & Tracking Performance]\n    D2 --> D3[鲁棒且时序稳定的感知/Robust & Temporally Stable Perception]"
    },
    {
      "title": "Scalable Class-Incremental Learning Based on Parametric Neural Collapse",
      "authors": "Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen",
      "institution": "Not explicitly stated in the provided content. Affiliation information is not included.",
      "link": "https://arxiv.org/pdf/2512.21845",
      "code": "this",
      "tags": [
        "class-incremental learning",
        "parametric neural collapse",
        "equiangular tight frame",
        "knowledge distillation",
        "adaptive expansion",
        "feature alignment"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp",
      "contributions": "1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules.",
      "summary": "The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient.",
      "mindmap": "graph TB\n        Root[”Scalable Class-Incremental Learning Based on Parametric Neural Collapse”] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题 / Problem”] --> P1[”过拟合新数据 / Overfitting to new data”]\n        Problem --> P2[”灾难性遗忘旧数据 / Catastrophic forgetting of old data”]\n        Problem --> P3[”特征差异与类别错位 / Feature difference & Class misalignment”]\n    \n        Method[”主要方法 / Method”] --> M1[”SCL-PNC方法 / SCL-PNC Method”]\n        M1 --> M1_1[”自适应层扩展主干 / Adapt-layer for backbone expansion”]\n        M1 --> M1_2[”动态参数化ETF分类器 / Dynamic Parametric ETF Classifier”]\n        M1 --> M1_3[”并行扩展与知识蒸馏 / Parallel expansion & Knowledge distillation”]\n    \n        Results[”关键结果 / Results”] --> R1[”高效处理类别增长 / Efficiently handles increasing categories”]\n        Results --> R2[”解决类别错位 / Addresses class misalignment”]\n        Results --> R3[”确保特征一致性 / Ensures feature consistency”]"
    },
    {
      "title": "Breaking Alignment Barriers: TPS-Driven Semantic Correlation Learning for Alignment-Free RGB-T Salient Object Detection",
      "authors": "Lupiao Hu, Fasheng Wang, Fangmei Chen, Fuming Sun, Haojie Li",
      "institution": "Dalian Minzu University, Shandong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.21856",
      "code": "https://github.com/HTUTU2/TPS-SCL",
      "tags": [
        "salient object detection",
        "RGB-T",
        "unaligned images",
        "Thin-Plate Spline",
        "MobileViT",
        "Mamba"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcfc4f61262c973564dfeb3e9591537a78f5165f01a7fece4325e5961bd3fe89_w640_q70.webp",
      "contributions": "1. Proposes a Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies between unaligned RGB-T modalities, addressing local deformations better than homography estimation. 2. Designs a Semantic Correlation Constraint Module (SCCM) to hierarchically constrain salient features and suppress background interference during alignment. 3. Introduces a Cross-Modal Correlation Module (CMCM) to fully explore and integrate inter-modal dependencies, enhancing detection performance in a lightweight architecture using MobileViT and Mamba.",
      "summary": "This paper addresses the performance degradation of RGB-T salient object detection (SOD) methods on real-world, unaligned image pairs. It proposes TPS-SCL, a lightweight network that uses a Thin-Plate Spline module for alignment, semantic correlation constraints, and cross-modal fusion. Experiments show the method achieves state-of-the-art performance among lightweight SOD methods and outperforms mainstream RGB-T SOD approaches.",
      "mindmap": "graph TB\n        A[Breaking Alignment Barriers: TPS-SCL<br>突破对齐壁垒: TPS-SCL] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法依赖对齐数据集<br>Existing methods rely on aligned datasets]\n        B --> B2[真实场景图像未对齐<br>Real-world images are unaligned]\n        C --> C1[双流MobileViT编码器<br>Dual-stream MobileViT encoder]\n        C --> C2[TPS对齐模块<br>TPS Alignment Module]\n        C --> C3[语义关联约束模块<br>Semantic Correlation Constraint Module]\n        C --> C4[跨模态关联模块<br>Cross-Modal Correlation Module]\n        D --> D1[轻量级SOTA性能<br>Lightweight SOTA performance]\n        D --> D2[超越主流RGB-T方法<br>Outperforms mainstream RGB-T methods]"
    },
    {
      "title": "Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models",
      "authors": "Masayuki Kawarada, Kosuke Yamada, Antonio Tejero-de-Pablos, Naoto Inoue",
      "institution": "CyberAgent",
      "link": "https://arxiv.org/pdf/2512.21860",
      "code": "https://github.com/CyberAgentAILab/DIOR_conditional_image_embeddings",
      "tags": [
        "image embedding",
        "conditional image embedding",
        "large vision-language model",
        "training-free",
        "image similarity",
        "hidden state"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc3046fafcb5e9e27821f45ec139d58a5fbb5098fe33ea6a51f89167ca9df74_w640_q70.webp",
      "contributions": "1. Proposes DIOR, a novel training-free framework for generating conditional image embeddings using Large Vision-Language Models (LVLMs)., 2. Introduces a method that prompts an LVLM to describe an image with a single word based on a given condition and uses the last token's hidden state as the embedding., 3. Demonstrates superior performance over existing training-free baselines (e.g., CLIP) and even methods requiring additional training on conditional similarity tasks.",
      "summary": "The paper addresses the problem of generating image embeddings that focus on specific textual conditions, which standard models like CLIP cannot do. It proposes DIOR, a training-free method that uses a Large Vision-Language Model to produce a one-word description based on a condition and extracts its hidden state as the conditional embedding. Experiments show DIOR outperforms both training-free and training-required baselines on conditional image similarity tasks.",
      "mindmap": "graph TB\n        A[Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 全局图像嵌入无法聚焦于特定文本条件]\n        C[主要方法/Method: DIOR - 利用LVLM生成单字描述并提取最后token的隐藏状态]\n        D[关键结果/Results: 在条件图像相似性任务上超越现有方法]"
    },
    {
      "title": "Fast Inference of Visual Autoregressive Model with Adjacency-Adaptive Dynamical Draft Trees",
      "authors": "Haodong Lei, Hongsong Wang, Xin Geng, Liang Wang, Pan Zhou",
      "institution": "Southeast University, Institute of Automation Chinese Academy of Sciences, Singapore Management University",
      "link": "https://arxiv.org/pdf/2512.21857",
      "code": "https://github.com/Haodong-Lei-Ray/ADT-Tree",
      "tags": [
        "multi-modal inference",
        "speculative decoding",
        "draft tree",
        "inference acceleration",
        "autoregressive image generation",
        "dynamic tree structure"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b9ff13149fa2d6733868b2125e7af2ae06239a529152a057b80d0d6f357ccf3_w640_q70.webp",
      "contributions": "1. Identified the key obstacle of applying speculative decoding to visual AR models: inconsistent acceptance rates across draft trees due to spatially varying token prediction difficulty. 2. Proposed ADT-Tree, an adjacency-adaptive dynamic draft tree that dynamically adjusts tree depth and width based on adjacent token states and prior acceptance rates. 3. Demonstrated significant speedups (over 3x) on benchmarks and seamless integration with relaxed sampling methods for further acceleration.",
      "summary": "This paper addresses the slow inference of visual autoregressive models by proposing ADT-Tree, a dynamic draft tree method that adapts its structure to image region complexity. It achieves over 3x speedup on standard benchmarks and can be combined with other sampling techniques for additional gains.",
      "mindmap": "graph TB\n        Root[”Fast Inference of Visual AR Model with ADT-Tree<br>视觉自回归模型快速推理与ADT-Tree”]\n        Root --> Problem[”核心问题/Problem<br>Visual AR models have slow sequential inference.<br>视觉AR模型推理慢”]\n        Root --> Method[”主要方法/Method<br>Propose Adjacency-Adaptive Dynamical Draft Trees (ADT-Tree).<br>提出邻接自适应动态草稿树”]\n        Root --> Results[”关键结果/Results<br>Achieves 3.13x/3.05x speedup on benchmarks.<br>在基准测试上实现3.13x/3.05x加速”]\n        Problem --> P1[”Spatially varying token prediction difficulty.<br>空间变化的token预测难度”]\n        Method --> M1[”Dynamically adjusts tree depth & width.<br>动态调整树深度与宽度”]\n        Method --> M2[”Leverages adjacency & prior acceptance rates.<br>利用邻接关系和先验接受率”]\n        Results --> R1[”Integrates with relaxed sampling.<br>可与松弛采样方法结合”]"
    },
    {
      "title": "Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening",
      "authors": "Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan",
      "institution": "North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh",
      "link": "https://arxiv.org/pdf/2512.21861",
      "code": null,
      "tags": [
        "medical image classification",
        "feature-level fusion",
        "convolutional neural networks",
        "diabetic retinopathy screening",
        "EfficientNet",
        "DenseNet"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp",
      "contributions": "1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.",
      "summary": "This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.",
      "mindmap": "graph TB\n        A[Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Large-scale DR screening is constrained by limited specialists and variable image quality.]\n        C[主要方法/Method<br>Feature-level fusion of complementary CNN backbones (e.g., EfficientNet, DenseNet) on pooled fundus images.]\n        D[关键结果/Results<br>Fusion outperforms single models. Eff+Den fusion offers best accuracy-latency balance.]"
    },
    {
      "title": "EasyOmnimatte: Taming Pretrained Inpainting Diffusion Models for End-to-End Video Layered Decomposition",
      "authors": "Yihan Hu, Xuelin Chen, Xiaodong Cun",
      "institution": "Great Bay University, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.21865",
      "code": "https://github.com/GVCLab/EasyOmnimatte",
      "tags": [
        "video matting",
        "video omnimatte",
        "diffusion models",
        "LoRA",
        "DiT blocks",
        "dual-expert"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/200afe63944dc4d9793ba3dadc45f6ca120880dcbf65f86982757bb158120f60_w640_q70.webp",
      "contributions": "1. Proposes the first unified, end-to-end video omnimatte method by finetuning a pretrained video inpainting diffusion model. 2. Introduces a Dual-Expert strategy (Effect Expert and Quality Expert) that selectively applies LoRA to specific DiT blocks to capture foreground-associated effects and refine alpha mattes. 3. Achieves state-of-the-art performance in quality and efficiency by using experts at different denoising steps, eliminating the need for two full diffusion passes.",
      "summary": "The paper introduces EasyOmnimatte, a method for video layered decomposition that finetunes a pretrained video inpainting diffusion model with a novel Dual-Expert strategy to efficiently produce high-quality alpha mattes with associated effects. It significantly outperforms existing methods in both speed and quality, enabling various downstream video editing tasks.",
      "mindmap": "graph TB\n        A[EasyOmnimatte] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法慢且次优/Existing methods are slow and suboptimal]\n        C --> C1[双专家微调/Dual-Expert Finetuning]\n        C1 --> C2[效果专家/Effect Expert]\n        C1 --> C3[质量专家/Quality Expert]\n        D --> D1[高质量分解/High-quality decomposition]\n        D --> D2[高效快速/Efficient and fast]"
    },
    {
      "title": "DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation",
      "authors": "Divyansh Srivastava, Akshay Mehra, Pranav Maneriker, Debopam Sanyal, Vishnu Raj, Vijay Kamarshi, Fan Du, Joshua Kimball",
      "institution": "University of California, San Diego, Dolby Laboratories",
      "link": "https://arxiv.org/pdf/2512.21867",
      "code": null,
      "tags": [
        "multi-modal training",
        "autoregressive image generation",
        "dynamic tokenization",
        "next-token prediction entropy",
        "patch merging",
        "training efficiency"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dac1942675af4bd3638ebec23734e3fed0239b066b673633b094708fa3d2d26f_w640_q70.webp",
      "contributions": "1. Introduces DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into variable-sized patches for efficient generation, using next-token prediction entropy as a merging criterion. 2. Demonstrates that training with dynamic patches yields patch-boundary-robust representations, enabling inference with larger patches for further efficiency. 3. Achieves significant reductions in token count (up to 2.06x) and training FLOPs (up to 40%) while improving image quality (FID) by up to 27.1% compared to fixed-tokenization baselines.",
      "summary": "The paper addresses the computational inefficiency of fixed tokenization in autoregressive image generation, where token count grows quadratically with resolution. It proposes DPAR, a method that dynamically merges image tokens into larger patches based on the information content predicted by a lightweight model's entropy, allocating more compute to complex regions. This approach reduces training costs by up to 40% and improves generated image quality (FID) by up to 27.1% compared to standard models.",
      "mindmap": "graph TB\n        Root[DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Fixed tokenization leads to quadratic token growth and high computational cost in autoregressive image generation.]\n        Method[主要方法/Method: Dynamic patch merging using next-token prediction entropy as a criterion for token aggregation.]\n        Results[关键结果/Results: Reduces token count (1.81x-2.06x), cuts training FLOPs by up to 40%, and improves FID by up to 27.1%.]"
    },
    {
      "title": "Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer",
      "authors": "Tianchen Deng, Wenhua Wu, Kunzhen Wu, Guangming Wang, Siting Zhu, Shenghai Yuan, Xun Chen, Guole Shen, Zhe Liu, Hesheng Wang",
      "institution": "Shanghai Jiao Tong University, Nanyang Technological University, Cambridge University",
      "link": "https://arxiv.org/pdf/2512.21883",
      "code": "https://github.com/dtc111111/Reloc-VGGT",
      "tags": [
        "visual localization",
        "early-fusion",
        "sparse mask attention",
        "pose tokenizer",
        "VGGT backbone",
        "multi-view geometry"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9b7693fa823e25dd55dc558d5c57f7d31f7adebc941d9ff45f7ef2ad7c6508_w640_q70.webp",
      "contributions": "1. Introduces the first visual localization framework with an early-fusion mechanism for multi-view spatial integration. 2. Proposes a novel sparse mask attention strategy to reduce computational complexity and enable real-time performance. 3. Presents a pose tokenizer and projection module to better exploit spatial relationships from multiple database views.",
      "summary": "This paper proposes Reloc-VGGT, a visual re-localization framework that uses an early-fusion mechanism and a VGGT backbone to integrate multi-view 3D geometry for robust pose estimation. It introduces a sparse mask attention strategy for efficiency and is trained on millions of image pairs. The method achieves strong accuracy, generalization, and real-time performance across diverse datasets.",
      "mindmap": "graph TB\n        A[Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer] --> B[核心问题/Problem: Late-fusion in visual localization is insufficient, degrading accuracy in complex environments.]\n        A --> C[主要方法/Method: Early-fusion framework with VGGT backbone, pose tokenizer, and sparse mask attention.]\n        A --> D[关键结果/Results: Strong accuracy, generalization, and real-time performance validated on public datasets.]"
    },
    {
      "title": "SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis",
      "authors": "Mo Wang, Junfeng Xia, Wenhao Ye, Enyu Liu, Kaining Peng, Jianfeng Feng, Quanying Liu, Hongkai Wen",
      "institution": "Southern University of Science and Technology, University of Warwick, Fudan University",
      "link": "https://arxiv.org/pdf/2512.21881",
      "code": null,
      "tags": [
        "multi-modal training",
        "fMRI",
        "foundation model",
        "data-efficient",
        "training-efficient",
        "hierarchical encoder"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03335d41a7bea8c473f46251d91847fc35908f4f15a206682290034deaf6ef92_w640_q70.webp",
      "contributions": "1. Proposes SLIM-Brain, a novel atlas-free foundation model for fMRI analysis that addresses the dual bottlenecks of data- and training-efficiency. 2. Introduces a two-stage adaptive design featuring a lightweight temporal extractor for saliency ranking and a 4D hierarchical encoder (Hiera-JEPA) that learns from selected windows and uses aggressive masking. 3. Demonstrates state-of-the-art performance across seven benchmarks while requiring significantly less pre-training data (4k sessions) and GPU memory (~30% of traditional methods).",
      "summary": "The paper addresses the data- and training-efficiency bottlenecks in fMRI foundation models. It proposes SLIM-Brain, a two-stage model that uses a temporal extractor to select salient data windows and a hierarchical encoder to learn from them efficiently. The method achieves state-of-the-art results on multiple tasks using far less pre-training data and computational resources than traditional approaches.",
      "mindmap": "graph TB\n        A[SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法瓶颈/Bottlenecks of Existing Methods]\n        B1 --> B1_1[图谱方法: 丢失细节, 需大数据/Atlas-based: lose details, need big data]\n        B1 --> B1_2[无图谱方法: 内存计算成本高/Atlas-free: high memory & compute cost]\n        C --> C1[两阶段自适应设计/Two-stage Adaptive Design]\n        C1 --> C1_1[轻量时序提取器: 全局上下文与显著性排序/Lightweight Temporal Extractor: global context & saliency ranking]\n        C1 --> C1_2[4D分层编码器: 从Top-k窗口学习/4D Hierarchical Encoder: learn from top-k windows]\n        D --> D1[性能/Performance]\n        D --> D2[效率/Efficiency]\n        D1 --> D1_1[在七个基准上达到SOTA/Achieves SOTA on seven benchmarks]\n        D2 --> D2_1[仅需4千次预训练会话/Only 4k pre-training sessions]\n        D2 --> D2_2[GPU内存降至30%/GPU memory reduced to ~30%]"
    },
    {
      "title": "CrownGen: Patient-customized Crown Generation via Point Diffusion Model",
      "authors": "Juyoung Bae, Moo Hyun Son, Jiale Peng, Wanting Qu, Wener Chen, Zelin Qiu, Kaixin Li, Xiaojuan Chen, Yifan Lin, Hao Chen",
      "institution": "Hong Kong University of Science and Technology, University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.21890",
      "code": null,
      "tags": [
        "3D shape generation",
        "diffusion model",
        "point cloud",
        "dental crown",
        "generative framework",
        "boundary prediction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2baf8cabd0677f151bf313fb7831de809fe9ecc0a70533ba9d2cffabe95b064b_w640_q70.webp",
      "contributions": "1. A novel generative framework (CrownGen) that automates patient-customized dental crown design. 2. A method using a denoising diffusion model on a tooth-level point cloud representation with a boundary prediction module for spatial priors. 3. Validation through a large-scale quantitative benchmark and a clinical study showing the generated crowns are non-inferior to manual designs and reduce design time.",
      "summary": "This paper presents CrownGen, a framework that automates dental crown design using a point diffusion model. It uses a boundary prediction module and a generative module to create patient-customized crowns in a single pass. Clinical validation shows the generated crowns match manual quality while significantly reducing design time.",
      "mindmap": "graph TB\n        A[CrownGen: Patient-customized Crown Generation<br>基于点扩散模型的个性化牙冠生成] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Digital crown design is labor-intensive<br>数字化牙冠设计费时费力]\n        C --> C1[Uses a point diffusion model<br>使用点扩散模型]\n        C --> C2[Has a boundary prediction module<br>包含边界预测模块]\n        D --> D1[Surpasses SOTA in geometric fidelity<br>几何保真度超越现有方法]\n        D --> D2[Reduces active design time<br>减少主动设计时间]\n        D --> D3[Crowns are clinically non-inferior<br>临床质量不劣于人工设计]"
    },
    {
      "title": "High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer",
      "authors": "Shen Zheng, Jiaran Cai, Yuansheng Guan, Shenneng Huang, Xingpei Ma, Junjie Cao, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang, Xiao-Ping Zhang",
      "institution": "Guangzhou Quwan Network Technology, Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.21905",
      "code": null,
      "tags": [
        "human image animation",
        "diffusion transformer",
        "hybrid implicit guidance",
        "position shift adaptive module",
        "skeleton alignment"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dcd7ed4538aa6140dc40b848f4f018648b9407793f582523c563f79c97253f1_w640_q70.webp",
      "contributions": "1. Designed a set of hybrid implicit guidance signals and a sharpness guidance factor to incorporate detailed facial and hand features for high-fidelity generation. 2. Proposed a Position Shift Adaptive Module (time-aware position shift fusion) to enable video generation of arbitrary length. 3. Introduced a novel data augmentation strategy and a skeleton alignment model to mitigate the impact of human shape variations across identities.",
      "summary": "This paper addresses the challenges of generating high-fidelity and long-duration human animation videos by proposing a Diffusion Transformer (DiT)-based framework. The method introduces novel guidance mechanisms for facial/hand details, a module for arbitrary-length generation, and techniques to handle identity shape variations. Experimental results show it outperforms existing state-of-the-art approaches.",
      "mindmap": "graph TB\n        A[”HIGH-FIDELITY AND LONG-DURATION HUMAN IMAGE ANIMATION WITH DIFFUSION TRANSFORMER<br>基于扩散Transformer的高保真长时人体图像动画”] --> B[”核心问题/Problem”]\n        A --> C[”主要方法/Method”]\n        A --> D[”关键结果/Results”]\n        B --> B1[”长视频生成挑战<br>Long-duration Video Generation”]\n        B --> B2[”面部与手部细节合成不足<br>Lack of Fine-grained Facial/Hand Details”]\n        C --> C1[”混合隐式引导信号<br>Hybrid Implicit Guidance”]\n        C --> C2[”位置偏移自适应模块<br>Position Shift Adaptive Module”]\n        C --> C3[”数据增强与骨架对齐<br>Data Augmentation & Skeleton Alignment”]\n        D --> D1[”超越现有SOTA方法<br>Outperforms SOTA”]\n        D --> D2[”实现超1分钟动画<br>Exceeds 1-minute Animation”]"
    },
    {
      "title": "Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition",
      "authors": "Zeyu Liang, Hailun Xia, Naichuan Zheng",
      "institution": "Beijing University of Posts and Telecommunications",
      "link": "https://arxiv.org/pdf/2512.21916",
      "code": null,
      "tags": [
        "multimodal action recognition",
        "human-centric graph representation learning",
        "attention-based post calibration",
        "spatiotemporal graph",
        "multimodal fusion",
        "skeleton-guided sampling"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ada463935a334d688b0ec740d70f3f4578fc9000b1c19263667ad6c53ea2b18_w640_q70.webp",
      "contributions": "1. Proposes PAN, the first human-centric graph representation learning framework for multimodal action recognition, which models RGB patches containing human joints as spatiotemporal graphs to align with skeleton data and suppress redundancy. 2. Introduces an attention-based post calibration module to reduce the framework's dependency on high-quality skeletal data with minimal performance cost. 3. Presents two model variants, PAN-Ensemble (dual-path with late fusion) and PAN-Unified (single-network unified learning), both achieving state-of-the-art performance on three benchmark datasets.",
      "summary": "This paper addresses the challenge of effectively fusing heterogeneous RGB and skeleton data for action recognition by proposing PAN, a human-centric graph learning framework. PAN represents RGB patches containing human joints as spatiotemporal graphs, enabling semantically coherent multimodal fusion and reducing RGB redundancy. The proposed method achieves state-of-the-art performance on three datasets through its two variants, PAN-Ensemble and PAN-Unified.",
      "mindmap": "graph TB\n        A[Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[RGB与骨架模态异构融合困难/RGB-Skeleton Heterogeneous Fusion Difficulty]\n        C --> C1[以人为中心的图表示学习/Human-Centric Graph Representation Learning]\n        C1 --> C2[基于注意力的后校准/Attention-Based Post Calibration]\n        C1 --> C3[双变体: PAN-Ensemble与PAN-Unified/Two Variants: PAN-Ensemble & PAN-Unified]\n        D --> D1[三个数据集上SOTA性能/SOTA Performance on Three Datasets]"
    },
    {
      "title": "Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning",
      "authors": "Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.21924",
      "code": null,
      "tags": [
        "medical image analysis",
        "unsupervised anomaly detection",
        "disentangled representation",
        "pseudo-healthy image reconstruction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp",
      "contributions": "1. Proposed a disentangled representation module to decouple brain MRI into imaging-invariant anatomical information and imaging-specific information, improving model generalizability across multi-modality and multi-center data. 2. Designed an edge-to-image restoration module that reconstructs high-quality pseudo-healthy images from edge information, suppressing the propagation of abnormal residuals. 3. Introduced brain anatomical priors and a differentiable one-hot encoding operator to constrain and stabilize the disentanglement learning process.",
      "summary": "This paper addresses the limitations of generalizability and performance in unsupervised anomaly detection for brain MRI. It proposes a new framework that disentangles anatomical from imaging information and reconstructs pseudo-healthy images from edges, achieving state-of-the-art results on multi-center datasets.",
      "mindmap": "graph TB\n        A[Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[泛化性差与异常残留/Generalizability & Residuals]\n        C --> C1[解耦表示模块/Disentangled Representation Module]\n        C --> C2[边缘到图像恢复模块/Edge-to-Image Restoration Module]\n        D --> D1[性能超越17种SOTA方法/Outperforms 17 SOTA Methods]"
    },
    {
      "title": "AutoPP: Towards Automated Product Poster Generation and Optimization",
      "authors": "Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law",
      "institution": "JD.COM",
      "link": "https://arxiv.org/pdf/2512.21921",
      "code": "https://github.com/JD-GenX/AutoPP",
      "tags": [
        "image generation",
        "product poster generation",
        "click-through rate optimization",
        "isolated direct preference optimization",
        "AutoPP1M dataset",
        "unified design module"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp",
      "contributions": "1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback.",
      "summary": "The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations.",
      "mindmap": "graph TB\n        A[AutoPP: Towards Automated Product Poster Generation and Optimization] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[人工制作与优化海报耗时耗力/Manual poster creation and optimization is laborious]\n        C --> C1[自动化生成与优化管道/Automated generation and optimization pipeline]\n        C1 --> C1_1[生成器: 统一设计模块与元素渲染/Generator: Unified design & element rendering]\n        C1 --> C1_2[优化器: 元素替换与IDPO/Optimizer: Element replacement & IDPO]\n        C --> C2[数据集: AutoPP1M/Dataset: AutoPP1M]\n        D --> D1[离线和在线SOTA结果/Offline and online SOTA results]\n        D --> D2[代码与数据集公开/Code & dataset released]"
    },
    {
      "title": "Data relativistic uncertainty framework for low-illumination anime scenery image enhancement",
      "authors": "Yiquan Gao, John See",
      "institution": "Heriot-Watt University",
      "link": "https://arxiv.org/pdf/2512.21944",
      "code": null,
      "tags": [
        "low-light image enhancement",
        "Data Relativistic Uncertainty",
        "Unsupervised Learning",
        "EnlightenGAN",
        "Anime Scenery",
        "Domain Gap"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp",
      "contributions": "1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.",
      "summary": "This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.",
      "mindmap": "graph TB\n        Root(”Data relativistic uncertainty framework for low-illumination anime scenery image enhancement”) --> Problem\n        Root --> Method\n        Root --> Results\n        Problem(”核心问题/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.”)\n        Method(”主要方法/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.”)\n        Results(”关键结果/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.”)"
    },
    {
      "title": "Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials",
      "authors": "Ali Lotfi, Adam Carter, Thuan Ha, Mohammad Meysami, Kwabena Nketia, Steve Shirtliffe",
      "institution": "University of Saskatchewan, New Mexico State University",
      "link": "https://arxiv.org/pdf/2512.21948",
      "code": null,
      "tags": [
        "remote sensing",
        "vegetation classification",
        "normalized difference polynomials",
        "spectral indices",
        "feature selection",
        "Sentinel-2",
        "illumination invariance"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/511f8067b02514f2cb415c41eb77aefa341affb7b0beba8fc78dcc37ebff9508_w640_q70.webp",
      "contributions": "1. Introduces an automated framework for generating a structured search space of spectral indices using pairwise normalized differences and polynomial combinations up to a fixed degree. 2. Employs feature selection methods (ANOVA filtering, recursive elimination, L1-regularized SVM) to select small, interpretable sets of indices that maintain high classification accuracy. 3. Demonstrates the effectiveness of the approach on a real-world task (Kochia detection), showing that simple degree-2 product indices achieve high accuracy and are deployable on platforms like Google Earth Engine.",
      "summary": "This paper introduces an automated method to discover simple spectral indices for vegetation classification by generating polynomial combinations of pairwise normalized differences and applying feature selection. The method was tested on detecting Kochia with Sentinel-2 imagery, where a single degree-2 index achieved over 96% accuracy. The results show that discriminative signals often come from spectral interactions, and the resulting indices are simple enough for direct deployment in cloud platforms.",
      "mindmap": "graph TB\n        A[Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[自动化发现用于植被分类的紧凑光谱指数/Automated discovery of compact spectral indices for vegetation classification]\n        C --> C1[生成归一化差异多项式候选特征/Generate candidate features via normalized difference polynomials]\n        C --> C2[使用特征选择方法挑选指数/Use feature selection methods to pick indices]\n        D --> D1[单个二阶指数达到96.26%准确率/Single degree-2 index achieves 96.26% accuracy]\n        D --> D2[指数简单，可直接部署于GEE/Indices are simple and deployable on GEE]"
    },
    {
      "title": "Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models",
      "authors": "Dunyuan XU, Xikai Yang, Yaoqian Li, Juzheng Miao, Jinpeng Li, Pheng-Ann Heng",
      "institution": "The Chinese University of Hong Kong (CUHK)",
      "link": "https://arxiv.org/pdf/2512.21964",
      "code": null,
      "tags": [
        "multi-modal robustness",
        "multi-modal large language model",
        "input perturbation",
        "training-free calibration",
        "denoising",
        "benchmark"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252aad7806c9998b636d3335e0a4e90e437f57a51e2a942eb6d7b871a984ef2b_w640_q70.webp",
      "contributions": "1. A systematic analysis of the impact of various visual and textual perturbations on medical MLLMs, revealing their sensitivity. 2. A novel training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs' own capabilities for robustness enhancement. 3. The introduction of two specific methods within IMC: Perturbation-aware Denoising Calibration (PDC) for visual noise and a Self-instantiated Multi-agent System (SMS) for textual noise.",
      "summary": "This paper addresses the vulnerability of Medical Multi-modal Large Language Models (MLLMs) to input noise like imaging artifacts and text errors. It proposes a training-free framework called Inherent-enhanced Multi-modal Calibration (IMC) that uses the model's own vision encoder and self-assessment capabilities to denoise inputs. Experiments on a new benchmark show the method achieves state-of-the-art robustness, enhancing clinical applicability.",
      "mindmap": "graph TB\n        A[Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models] --> B(核心问题/Problem: MLLMs are sensitive to input noise, undermining clinical use)\n        A --> C(主要方法/Method: Training-free IMC framework with PDC for vision and SMS for text)\n        A --> D(关键结果/Results: SOTA performance on new multi-modal noise benchmark)"
    },
    {
      "title": "LVLM-Aided Alignment of Task-Specific Vision Models",
      "authors": "Alexander Koebler, Lukas Kuhn, Ingo Thon, Florian Buettner",
      "institution": "Goethe University Frankfurt, Siemens AG, German Cancer Research Center (DKFZ)",
      "link": "https://arxiv.org/pdf/2512.21985",
      "code": null,
      "tags": [
        "model alignment and interpretability",
        "LVLM-VA",
        "spurious correlations",
        "explainable AI (XAI)",
        "vision-language model",
        "human-in-the-loop"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp",
      "contributions": "1. Introduces LVLM-Aided Visual Alignment (LVLM-VA), a novel method for aligning small task-specific vision models with human domain knowledge using a Large Vision Language Model (LVLM). 2. Proposes a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling efficient expert-model interaction. 3. Demonstrates that the method effectively reduces the model's dependence on spurious features and group-specific biases without requiring fine-grained, instance-level feedback.",
      "summary": "This paper addresses the problem of small task-specific vision models relying on spurious correlations, which leads to brittle real-world performance. The authors propose LVLM-Aided Visual Alignment (LVLM-VA), a method that uses a Large Vision Language Model to create a bidirectional interface between human domain knowledge and the model, translating explanations and critiques. The method is shown to significantly improve model alignment with human specifications and reduce dependence on spurious features across synthetic and real-world datasets.",
      "mindmap": "graph TB\n        A[LVLM-Aided Alignment of Task-Specific Vision Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[小规模任务专用视觉模型依赖虚假相关性/Small task-specific vision models rely on spurious correlations]\n        B --> B2[导致部署时行为脆弱/Leads to brittle behavior when deployed]\n        C --> C1[利用LVLM进行视觉对齐/Leverage LVLM for visual alignment]\n        C --> C2[双向接口: 行为转语言, 规范转评估/Bidirectional interface: behavior to language, specs to critiques]\n        D --> D1[模型行为与人类规范更好对齐/Better alignment of model behavior with human specifications]\n        D --> D2[减少对虚假特征和偏见的依赖/Reduced dependence on spurious features and biases]"
    },
    {
      "title": "A Lightweight Multi-Scale Attention Framework for Real-Time Spinal Endoscopic Instance Segmentation",
      "authors": "Qi Lai, JunYan Li, Qiang Cai, Lei Wang, Tao Yan, XiaoKun Liang",
      "institution": "The affiliations include IEEE members, suggesting an academic or research institution, but specific names are not fully listed in the provided text. Based on the GitHub URL pattern, a likely institution is not explicitly stated in the given content.",
      "link": "https://arxiv.org/pdf/2512.21984",
      "code": "https://github.com/hhwmortal/PELD-Instance-segmentation",
      "tags": [
        "instance segmentation",
        "re-parameterized convolution",
        "efficient multi-scale attention",
        "lightweight multi-task head"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ddb0a22c2e72d9e15e574413f3c78c60ca63b7152d6320583b76bc00a64110f_w640_q70.webp",
      "contributions": "1. Proposed LMSF-A, a lightweight multi-scale attention framework co-designed across backbone, neck, and head for real-time instance segmentation. 2. Introduced the C2f-Pro backbone module combining RepViT-style re-parameterized convolution with efficient multi-scale attention for multi-branch training and fast inference. 3. Released a new clinically reviewed PELD dataset for spinal endoscopic instance segmentation.",
      "summary": "This paper addresses the challenge of real-time instance segmentation in spinal endoscopy, where factors like a narrow field of view and hardware constraints make deployment difficult. The authors propose LMSF-A, a lightweight framework featuring a re-parameterized backbone, a feature fusion neck, and a shared head, which achieves competitive accuracy with only 1.8M parameters. The method is validated on a new clinical dataset and shows good generalization.",
      "mindmap": "graph TB\n        A[LMSF-A: Real-Time Spinal Endoscopic Instance Segmentation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[狭窄视野, 伪影, 硬件限制/Narrow FOV, Artifacts, Hardware Constraints]\n        C --> C1[轻量级多尺度注意力框架/Lightweight Multi-scale Attention Framework]\n        C1 --> C2[主干: C2f-Pro (重参数化卷积+EMA)/Backbone: C2f-Pro (Rep Conv+EMA)]\n        C1 --> C3[颈部: SSFF与TFE/Neck: SSFF and TFE]\n        C1 --> C4[头部: 轻量共享头 (LMSH)/Head: Lightweight Shared Head (LMSH)]\n        D --> D1[性能优越, 参数量少 (1.8M)/High Performance, Few Params (1.8M)]\n        D --> D2[发布PELD数据集/Release PELD Dataset]\n        D --> D3[良好泛化性/Good Generalization]"
    },
    {
      "title": "Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs",
      "authors": "Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He",
      "institution": "Chongqing University, Xinjiang University",
      "link": "https://arxiv.org/pdf/2512.21999",
      "code": "https://github.com/hujiayu1223/ALEAHallu",
      "tags": [
        "multi-modal training",
        "hallucination mitigation",
        "adversarial parametric editing",
        "parameter clustering",
        "visual-language models",
        "activation dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp",
      "contributions": "1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework's effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors.",
      "summary": "This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks.",
      "mindmap": "graph TB\n        A[Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: VLM幻觉问题/VLM Hallucination Issue]\n        C[主要方法/Method: ALEAHallu框架/ALEAHallu Framework]\n        D[关键结果/Results: 有效缓解幻觉/Effectively Mitigates Hallucinations]\n        C --> C1[激活数据集/Activation Dataset]\n        C --> C2[定位关键参数/Locate Critical Parameters]\n        C --> C3[对抗性编辑/Adversarial Editing]"
    },
    {
      "title": "iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception",
      "authors": "Sarthak Mehrotra, Sairam V C Rebbapragada, Mani Hemanth Reddy Bonthu, Vineeth N Balasubramanian",
      "institution": "Indian Institute of Technology, Bombay, Indian Institute of Technology, Hyderabad",
      "link": "https://arxiv.org/pdf/2512.22009",
      "code": null,
      "tags": [
        "agent system",
        "multimodal large language models (MLLMs)",
        "slow-fast inference",
        "adaptive perception",
        "visual grounding",
        "lightweight agent"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6067c294acdf08cb7927ad962f2d0d2c4f3efd938837b5cdb9ca1bdad4019422_w640_q70.webp",
      "contributions": "1. Introduces iSHIFT, a lightweight (2.5B parameter) agent framework that integrates implicit chain-of-thought reasoning with a perception control module. 2. Proposes a novel slow-fast hybrid inference mechanism, allowing the model to adaptively switch between a detailed, high-precision \"slow mode\" and an efficient \"fast mode\" based on task demands. 3. Employs special perception tokens to dynamically guide the model's visual attention to relevant screen regions, enabling precise visual grounding for GUI interaction.",
      "summary": "The paper addresses the challenge of building efficient yet precise GUI agents by proposing iSHIFT, a lightweight MLLM agent. iSHIFT uses an adaptive slow-fast inference mechanism and perception tokens to dynamically balance efficiency and accuracy for GUI tasks. Despite its small 2.5B size, it achieves state-of-the-art performance on multiple benchmarks.",
      "mindmap": "graph TB\n        A[iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception] --> B[核心问题/Problem: Building efficient and precise GUI agents is challenging]\n        A --> C[主要方法/Method: Slow-fast hybrid inference with adaptive perception tokens]\n        A --> D[关键结果/Results: Matches SOTA performance with compact 2.5B size]"
    },
    {
      "title": "LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration",
      "authors": "Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, Xiangyang Ji",
      "institution": "Beijing Institute of Technology, Tsinghua University, Dalian University of Technology, Xidian University",
      "link": "https://arxiv.org/pdf/2512.22010",
      "code": null,
      "tags": [
        "vision-and-language navigation",
        "spatiotemporal context modeling",
        "slot-based compression",
        "prompt-guided multimodal integration"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp",
      "contributions": "1. A slot-based historical image compression module to distill multi-view historical observations into fixed-length contextual representations. 2. A spatiotemporal trajectory encoding module to capture the temporal dynamics and spatial structure of UAV trajectories. 3. A prompt-guided multimodal integration module to fuse spatiotemporal context with current observations for robust waypoint prediction.",
      "summary": "This paper proposes LongFly, a framework for long-horizon UAV vision-and-language navigation that addresses the challenge of modeling spatiotemporal context. The method integrates a history-aware modeling strategy with modules for compressing past observations, encoding trajectories, and fusing multimodal information. Experimental results show it outperforms state-of-the-art baselines in navigation success metrics across seen and unseen environments.",
      "mindmap": "graph TB\n        A[LongFly: Long-Horizon UAV Vision-and-Language Navigation] --> B[核心问题/Problem: Current UAV VLN methods struggle with long-horizon spatiotemporal context, leading to inaccurate alignment and unstable planning.]\n        A --> C[主要方法/Method: History-aware spatiotemporal modeling with slot-based image compression, trajectory encoding, and prompt-guided multimodal integration.]\n        A --> D[关键结果/Results: Outperforms SOTA baselines by 7.89% in success rate and 6.33% in SPL.]"
    },
    {
      "title": "SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching",
      "authors": "Xiangwen Zhang, Xiaowei Dai, Runnan Chen, Xiaoming Chen, Zeke Zexi Hu",
      "institution": "Beijing Technology and Business University, The University of Sydney",
      "link": "https://arxiv.org/pdf/2512.22016",
      "code": null,
      "tags": [
        "Human-Computer Interaction (HCI)",
        "Gestural Interaction",
        "Physics Simulation",
        "Air-drawn Sketches",
        "VR Content Creation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp",
      "contributions": "1. A novel VR interaction framework (SketchPlay) that combines air-drawn sketches and gestures to create dynamic, physically realistic scenes. 2. A method that uses sketches to capture object/scene structure and gestures to convey physical cues (velocity, force) for defining motion and behavior. 3. Enables the generation of complex physical phenomena (rigid body motion, elastic deformation, cloth dynamics) through an intuitive, controller-free creation process.",
      "summary": "The paper proposes SketchPlay, a VR framework that allows users to create physically realistic dynamic scenes by sketching objects in the air and using gestures to define their motion. This method combines structural and dynamic intent to simulate phenomena like rigid body and cloth dynamics. The approach is shown to be more expressive and offer a better user experience than text-driven methods, lowering the barrier for non-expert creators.",
      "mindmap": "graph TB\n        A[SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching] --> B[核心问题/Problem: Creating physically realistic VR content is complex and requires expert tools, creating barriers for non-expert users.]\n        A --> C[主要方法/Method: A novel VR framework that transforms air-drawn sketches (for structure) and gestures (for physical cues like velocity/force) into dynamic, physically realistic scenes.]\n        A --> D[关键结果/Results: Offers significant advantages in expressiveness and user experience over traditional methods, lowering the entry barrier and showing potential for education, art, and storytelling.]"
    },
    {
      "title": "Patch-Discontinuity Mining for Generalized Deepfake Detection",
      "authors": "Huanhuan Yuan, Yang Ping, Zhengqin Xu, Junyi Cao, Shuai Jia, Chao Ma",
      "institution": "Shanghai Jiao Tong University, Chinese Academy of Military Science",
      "link": "https://arxiv.org/pdf/2512.22027",
      "code": "https://gendf.github.io/",
      "tags": [
        "deepfake detection",
        "patch-discontinuity",
        "feature space redistribution",
        "classification-invariant feature augmentation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f93245fe55a81a22e8997db22f045beb4494e864df08fd145bf36088517c580_w640_q70.webp",
      "contributions": "1. Proposed a deepfake-specific representation learning (DSRL) scheme to capture patch-discontinuity patterns in fake images and continuity in real ones. 2. Introduced a feature space redistribution (FSR) scheme to separately optimize the distributions of real and fake features, mitigating domain mismatch. 3. Designed a classification-invariant feature augmentation (CIFAug) strategy to enhance generalization by expanding feature scopes orthogonally to the classification direction without adding trainable parameters.",
      "summary": "This paper proposes GenDF, a generalized deepfake detection framework that transfers a large-scale vision model to detect fake faces by mining patch-discontinuity patterns. It introduces three key components—deepfake-specific representation learning, feature space redistribution, and a parameter-free feature augmentation strategy—to improve generalization to unseen forgery methods. The method achieves state-of-the-art cross-domain performance with only 0.28M trainable parameters, demonstrating high efficiency and effectiveness.",
      "mindmap": "graph TB\n        A[”Patch-Discontinuity Mining for Generalized Deepfake Detection”] --> B[”核心问题/Problem: Existing deepfake detectors generalize poorly to unseen forgery patterns.”]\n        A --> C[”主要方法/Method: Propose GenDF framework with DSRL, FSR, and CIFAug to learn generalizable features from a pre-trained vision model.”]\n        A --> D[”关键结果/Results: Achieves SOTA generalization with only 0.28M parameters.”]"
    },
    {
      "title": "Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models",
      "authors": "Zongmin Zhang, Zhen Sun, Yifan Liao, Wenhan Dong, Xinlei He, Xingshuo Han, Shengmin Xu, Xinyi Huang",
      "institution": "Hong Kong University of Science and Technology (Guangzhou), Nanjing University of Aeronautics and Astronautics, Fujian Normal University, Jinan University",
      "link": "https://arxiv.org/pdf/2512.22046",
      "code": null,
      "tags": [
        "backdoor attacks",
        "video segmentation foundation models",
        "backdoor attack",
        "two-stage training",
        "gradient analysis",
        "attention shift"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a7e9ff0ffc63f2085d6ca65db8e87f14fed435be5d8a51fd3d1265b22b668b1_w640_q70.webp",
      "contributions": "1. Identifies the ineffectiveness of classic backdoor attacks on prompt-driven Video Segmentation Foundation Models (VSFMs) and provides analysis via gradient and attention maps. 2. Proposes BadVSFM, the first dedicated backdoor attack framework for VSFMs, using a novel two-stage training strategy to separate clean and triggered representations. 3. Demonstrates strong, controllable attack performance across multiple models and datasets while preserving clean-task accuracy, and shows the vulnerability persists against existing defenses.",
      "summary": "This paper identifies that standard backdoor attacks fail on prompt-driven Video Segmentation Foundation Models (VSFMs) like SAM2. To solve this, the authors propose BadVSFM, a two-stage backdoor attack framework that successfully implants a controllable backdoor by steering the encoder and decoder separately. Experiments show the attack is effective and evades current defenses, revealing a significant security vulnerability in VSFMs.",
      "mindmap": "graph TB\n        A[Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Classic backdoor attacks fail on VSFMs (ASR<5%)]\n        C[主要方法/Method: BadVSFM - Two-stage training (steer encoder, train decoder)]\n        D[关键结果/Results: High ASR, preserves clean performance, defenses ineffective]"
    },
    {
      "title": "StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars",
      "authors": "Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu",
      "institution": "Tsinghua University, Renmin University of China, Tencent Hunyuan, Nanjing University",
      "link": "https://arxiv.org/pdf/2512.22065",
      "code": "https://streamavatar.github.io",
      "tags": [
        "diffusion models",
        "autoregressive distillation",
        "adversarial refinement",
        "real-time streaming",
        "reference-anchored positional re-encoding",
        "consistency-aware discriminator"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp",
      "contributions": "1. A two-stage autoregressive adaptation and acceleration framework (autoregressive distillation + adversarial refinement) to adapt a non-causal human video diffusion model for real-time, interactive streaming. 2. Three novel components to ensure long-term stability and consistency: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. 3. A one-shot, interactive human avatar model capable of generating both natural talking and listening behaviors with coherent full-body gestures, surpassing existing methods in quality, efficiency, and interaction naturalness.",
      "summary": "This paper addresses the challenge of making diffusion-based human avatar generation suitable for real-time, interactive streaming. It proposes StreamAvatar, a two-stage framework that adapts a high-fidelity human video diffusion model using autoregressive distillation and adversarial refinement, incorporating novel components for long-term consistency. The method achieves state-of-the-art performance in generating high-resolution, full-body interactive avatars with natural talking/listening behaviors in real-time.",
      "mindmap": "graph TB\n        A[StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Non-causal, high-cost diffusion models unsuitable for real-time streaming; Limited to head-and-shoulder, lacking gestures.]\n        C[主要方法/Method<br>Two-stage autoregressive adaptation (distillation + refinement) with Reference Sink, RAPR, Consistency-Aware Discriminator.]\n        D[关键结果/Results<br>State-of-the-art real-time, interactive full-body avatar with natural talking/listening and gestures.]"
    },
    {
      "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents",
      "authors": "Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong, Chenglin Cai, Chen Liu, Yue Wang, Jingren Zhou, Steven Hoi",
      "institution": "Tongyi Lab, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.22047",
      "code": "https://github.com/Tongyi-MAI/MAI-UI",
      "tags": [
        "agent system",
        "GUI agent",
        "device-cloud collaboration",
        "online reinforcement learning",
        "self-evolving data pipeline",
        "foundation model"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/819d67f31c44f0847dff819fbdf82773fea54c76d001f429e9e731b2ba253b85_w640_q70.webp",
      "contributions": "1. A self-evolving data pipeline that expands GUI navigation data to include user interaction and tool calls. 2. A native device-cloud collaboration system that routes task execution based on state to improve efficiency and privacy. 3. An online RL framework with optimizations for scaling parallel environments and context length.",
      "summary": "The paper presents MAI-UI, a family of foundation GUI agents designed to address challenges in real-world deployment, such as limited interaction and dynamic environments. Its unified methodology includes a self-evolving data pipeline, a device-cloud collaboration system, and an online RL framework. MAI-UI achieves state-of-the-art performance on multiple GUI grounding and mobile navigation benchmarks.",
      "mindmap": "graph TB\n        A[MAI-UI Technical Report: Real-World Centric Foundation GUI Agents] --> B1(核心问题/Problem)\n        A --> B2(主要方法/Method)\n        A --> B3(关键结果/Results)\n        B1 --> C1(缺乏原生人机交互/Lack of Native Agent-User Interaction)\n        B1 --> C2(仅UI操作的限制/Limits of UI-Only Operation)\n        B1 --> C3(缺乏实用部署架构/Absence of Practical Deployment Architecture)\n        B1 --> C4(动态环境脆弱性/Brittleness in Dynamic Environments)\n        B2 --> D1(自演进数据管道/Self-Evolving Data Pipeline)\n        B2 --> D2(原生设备-云协作系统/Native Device-Cloud Collaboration System)\n        B2 --> D3(在线RL框架/Online RL Framework)\n        D1 --> E1(包含用户交互/Includes User Interaction)\n        D1 --> E2(包含MCP工具调用/Includes MCP Tool Calls)\n        D3 --> E3(扩展并行环境/Scales Parallel Environments)\n        D3 --> E4(扩展上下文长度/Scales Context Length)\n        B3 --> F1(GUI Grounding SOTA/GUI Grounding SOTA)\n        B3 --> F2(移动导航SOTA/Mobile Navigation SOTA)\n        B3 --> F3(系统性能提升/System Performance Gains)\n        F1 --> G1(ScreenSpot-Pro: 73.5%/ScreenSpot-Pro: 73.5%)\n        F1 --> G2(MMBench GUI L2: 91.3%/MMBench GUI L2: 91.3%)\n        F2 --> G3(AndroidWorld: 76.7%/AndroidWorld: 76.7%)\n        F2 --> G4(MobileWorld: 41.7%/MobileWorld: 41.7%)\n        F3 --> G5(设备性能提升33%/On-Device Perf. +33%)\n        F3 --> G6(云调用减少40%/Cloud Calls -40%)"
    },
    {
      "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model",
      "authors": "Xiaofeng Mao, Zhen Li, Chuanhao Li, Xiaojie Xu, Kaining Ying, Tong He, Jiangmiao Pang, Yu Qiao, Kaipeng Zhang",
      "institution": "Shanghai AI Laboratory, Fudan University, Shanghai Innovation Institute",
      "link": "https://arxiv.org/pdf/2512.22096",
      "code": "https://github.com/stdstu12/YUME",
      "tags": [
        "diffusion models",
        "interactive world generation",
        "long-video generation",
        "attention distillation",
        "context compression",
        "text-controlled generation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f7ffd3f0a90ba67551ade4e28abf8e27d5d08c106e463f85f9447011008416b_w640_q70.webp",
      "contributions": "1. A long-video generation framework integrating unified context compression with linear attention. 2. A real-time streaming acceleration strategy using bidirectional attention distillation and an enhanced text embedding scheme. 3. A text-controlled method for generating world events.",
      "summary": "This paper proposes Yume-1.5, a framework to address challenges in generating interactive, explorable worlds using diffusion models, such as large model size and slow inference. The method introduces a novel architecture combining context compression, attention distillation, and text-based event control to enable real-time, keyboard-controlled world generation from text or images. The work concludes with a public codebase demonstrating the feasibility of text-controlled interactive world creation.",
      "mindmap": "graph TB\n        Root[Yume-1.5: A Text-Controlled Interactive World Generation Model] --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1[大模型参数与慢推理/Large Model & Slow Inference]\n        Problem --> P2[缺乏文本控制/Lack of Text Control]\n        Method --> M1[长视频生成框架/Long-Video Gen Framework]\n        Method --> M2[实时流加速策略/Real-time Streaming]\n        Method --> M3[文本控制事件生成/Text-Controlled Events]\n        M1 --> M1_Sub[统一上下文压缩与线性注意力/Unified Context Compression & Linear Attention]\n        M2 --> M2_Sub[双向注意力蒸馏与文本嵌入/Bidirectional Attention Distillation & Text Embedding]\n        Results --> R1[生成交互式世界/Generates Interactive Worlds]\n        Results --> R2[支持键盘探索/Supports Keyboard Exploration]\n        Results --> R3[公开代码库/Public Codebase]"
    },
    {
      "title": "Learning Association via Track-Detection Matching for Multi-Object Tracking",
      "authors": "Momir Adžemović",
      "institution": "University of Belgrade",
      "link": "https://arxiv.org/pdf/2512.22105",
      "code": "https://github.com/Robotmurlock/TDLP",
      "tags": [
        "multi-object tracking",
        "tracking-by-detection",
        "link prediction",
        "association learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d61776d860da3a903f769a1427363e91df9be50d56b83666c73ac36008099062_w640_q70.webp",
      "contributions": "1. Proposes Track-Detection Link Prediction (TDLP), a tracking-by-detection method that learns data association via link prediction between tracks and detections, eliminating handcrafted heuristics. 2. Demonstrates that TDLP achieves state-of-the-art performance across multiple benchmarks, outperforming both traditional tracking-by-detection and end-to-end methods. 3. Provides analysis showing link prediction is more effective than metric learning for association, especially when handling heterogeneous features like bounding boxes.",
      "summary": "The paper proposes TDLP, a tracking-by-detection method that learns to associate object detections across video frames by predicting links between existing tracks and new detections. This approach avoids handcrafted rules while remaining computationally efficient. Experiments show it outperforms state-of-the-art methods, and analysis indicates link prediction is superior to metric learning for this association task.",
      "mindmap": "graph TB\n        A[Learning Association via Track-Detection Matching for Multi-Object Tracking] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Tracking-by-detection methods rely on handcrafted heuristics, end-to-end methods are computationally complex.]\n        C[主要方法/Method<br>Propose TDLP: Track-Detection Link Prediction for learning association via link prediction.]\n        D[关键结果/Results<br>TDLP surpasses SOTA performance; link prediction is more effective than metric learning.]"
    },
    {
      "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
      "authors": "Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang",
      "institution": "Microsoft Research, Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.22120",
      "code": "https://github.com/zss02/BiPS",
      "tags": [
        "visual question answering",
        "perceptual shaping",
        "KL-consistency",
        "KL-separation",
        "evidence-preserving view",
        "evidence-ablated view"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74efcdb66ae5cc2a16fd7b59382dce3e78d24b03e998520275db9a96986fa158_w640_q70.webp",
      "contributions": "1. Proposes Bi-directional Perceptual Shaping (BiPS), a novel training paradigm that uses question-conditioned masked views to generate bidirectional \"where-to-look\" signals. 2. Introduces a dual KL constraint mechanism: a KL-consistency constraint to encourage complete visual evidence coverage and a KL-separation constraint to discourage text-only shortcuts and enforce fine-grained visual reliance. 3. Demonstrates strong performance improvements and out-of-domain generalization across multiple benchmarks without adding inference-time cost.",
      "summary": "The paper addresses the problem that large vision-language models often overlook fine-grained visual evidence and rely on text shortcuts. It proposes Bi-directional Perceptual Shaping (BiPS), a training method that uses KL constraints on evidence-preserving and evidence-ablated image views to shape the model's perception. The method significantly boosts the performance of a base VLM model and shows strong generalization to unseen data.",
      "mindmap": "graph TB\n        A[See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: VLMs overlook fine-grained visual evidence, generalize poorly, and have high inference cost]\n        C[主要方法/Method: BiPS uses bidirectional KL constraints (consistency & separation) on masked views to shape perception during training]\n        D[关键结果/Results: Boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization]"
    },
    {
      "title": "ProEdit: Inversion-based Editing From Prompts Done Right",
      "authors": "Zhi Ouyang, Dian Zheng, Xiao-Ming Wu, Jian-Jian Jiang, Kun-Yu Lin, Jingke Meng, Wei-Shi Zheng",
      "institution": "Sun Yat-sen University, CUHK MMLab, Nanyang Technological University, The University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.22118",
      "code": "https://isee-laboratory.github.io/ProEdit/",
      "tags": [
        "image editing",
        "inversion-based editing",
        "KV-mix",
        "Latents-Shift",
        "plug-and-play",
        "flow inversion"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/878121cf01366a9dcec34198113a6d864c4ea37e9b41c39e221a71b26e72039f_w640_q70.webp",
      "contributions": "1. Proposes KV-mix to mix source and target KV features in the attention mechanism, reducing source influence in edited regions while preserving background. 2. Introduces Latents-Shift to perturb the source latent in the edited region, eliminating the influence of the inverted latent during sampling. 3. Presents a plug-and-play design that can be integrated into existing inversion-based editing methods like RF-Solver, FireFlow, and UniEdit.",
      "summary": "This paper addresses the problem in inversion-based visual editing where over-reliance on source image information hinders accurate attribute changes. The proposed method, ProEdit, introduces two techniques: KV-mix in the attention aspect and Latents-Shift in the latent aspect, to mitigate source influence. It achieves state-of-the-art performance on image and video editing benchmarks and is designed as a plug-and-play module compatible with existing methods.",
      "mindmap": "graph TB\n        A[ProEdit: Inversion-based Editing From Prompts Done Right] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法过度注入源图像信息/Existing methods overly inject source info]\n        B1 --> B2[阻碍属性编辑如姿态、数量、颜色/Hinders editing attributes like pose, number, color]\n        C --> C1[注意力层面: KV-mix/Attention Aspect: KV-mix]\n        C1 --> C11[混合源与目标KV特征/Mix source & target KV features]\n        C --> C2[潜在层面: Latents-Shift/Latent Aspect: Latents-Shift]\n        C2 --> C21[扰动编辑区域的源潜在表示/Perturb source latent in edited region]\n        D --> D1[SOTA性能/SOTA performance]\n        D --> D2[即插即用设计/Plug-and-play design]"
    },
    {
      "title": "A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer with Region-Aware Attention for Gastrointestinal Disease Classification with Explainable AI",
      "authors": "Md Assaduzzaman, Nushrat Jahan Oyshi, Eram Mahamud",
      "institution": "Daffodil International University",
      "link": "https://arxiv.org/pdf/2512.21372",
      "code": null,
      "tags": [
        "medical image classification",
        "Knowledge Distillation",
        "Vision Transformer",
        "Swin Transformer",
        "Explainable AI",
        "Wireless Capsule Endoscopy"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cd724a13af61d1181b015dc7b5fe86e10cc834beac172261c5bebe54e371bc3_w640_q70.webp",
      "contributions": "1. Proposed a hybrid dual-stream teacher model combining Swin Transformer for global context and Vision Transformer for local features. 2. Developed a compact Tiny-ViT student model via knowledge distillation to balance accuracy and efficiency for clinical deployment. 3. Validated the model's clinical relevance through extensive interpretability analysis using Grad-CAM, LIME, and Score-CAM.",
      "summary": "This paper addresses the challenge of classifying gastrointestinal diseases from endoscopic images by proposing a knowledge distillation framework. A high-capacity dual-stream teacher model guides a compact Tiny-ViT student, achieving near-perfect accuracy on WCE datasets while providing explainable predictions. The work offers an efficient and interpretable solution suitable for resource-constrained clinical environments.",
      "mindmap": "graph TB\n        A[论文标题 / Paper Title: A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer for GI Disease Classification] --> B(核心问题 / Problem: 胃肠道疾病图像分类挑战 / GI Disease Image Classification Challenge)\n        A --> C(主要方法 / Method: 基于知识蒸馏的双流Vision Transformer / Knowledge Distillation based Dual-Stream Vision Transformer)\n        A --> D(关键结果 / Results: 高准确率与可解释性 / High Accuracy & Explainability)\n        B --> B1[数据量大, 类间差异小 / Large Data Volume, Subtle Inter-class Variation]\n        C --> C1[教师模型: Swin + ViT / Teacher: Swin + ViT]\n        C --> C2[学生模型: Tiny-ViT / Student: Tiny-ViT]\n        C --> C3[可解释性分析: Grad-CAM等 / XAI: Grad-CAM etc.]\n        D --> D1[准确率 > 0.99 / Accuracy > 0.99]\n        D --> D2[AUC = 1.0000]\n        D --> D3[适用于临床环境 / Suitable for Clinical Settings]"
    },
    {
      "title": "Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models",
      "authors": "Takuro Kutsuna",
      "institution": "Toyota Central R&D Labs., Inc.",
      "link": "https://arxiv.org/pdf/2512.21593",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion models",
        "generative modeling",
        "evidence lower bound",
        "residual learning",
        "two-stage framework"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp",
      "contributions": "1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.",
      "summary": "The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.",
      "mindmap": "graph TB\n        Root[”Residual Prior Diffusion (RPD) / 残差先验扩散模型”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”单一扩散模型难以同时捕捉全局结构和局部细节 / Single diffusion model struggles with global structure and local details”]\n        Method --> M1[”两阶段框架: 粗粒度先验 + 残差扩散模型 / Two-stage framework: coarse prior + residual diffusion model”]\n        Method --> M2[”概率模型与可处理ELBO / Probabilistic model with tractable ELBO”]\n        Results --> R1[”在合成数据上准确捕捉细节 / Accurately captures details on synthetic data”]\n        Results --> R2[”自然图像生成匹配或超越基线 / Natural image generation matches or exceeds baselines”]\n        Results --> R3[”少步推理保持性能 / Maintains performance with few inference steps”]"
    },
    {
      "title": "RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring",
      "authors": "Zhuoyu Wu, Wenhui Ou, Qiawei Zheng, Jiayan Yang, Quanjun Wang, Wenqi Fang, Zheng Wang, Yongkui Yang, Heshan Li",
      "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Unifyware Co., Ltd.; Monash University; Hong Kong University of Science and Technology; Shenzhen Infynova Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.21975",
      "code": "https://github.com/ReaganWu/RT-Focuser",
      "tags": [
        "image deblurring",
        "lightweight network",
        "real-time inference",
        "edge deployment",
        "U-shaped architecture",
        "motion blur"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d65834719412d7909823764ddbf757cadad16b1b273fd3e67cf177cad638b9d_w640_q70.webp",
      "contributions": "1. Proposes a lightweight U-shaped network (RT-Focuser) with a Lightweight Deblurring Block (LD) for edge-aware feature extraction. 2. Introduces a Multi-Level Integrated Aggregation module (MLIA) for encoder feature integration. 3. Designs a Cross-source Fusion Block (X-Fuse) for progressive decoder refinement.",
      "summary": "This paper proposes RT-Focuser, a lightweight U-shaped neural network designed for real-time image deblurring on edge devices. It achieves a balance between speed and accuracy with only 5.85M parameters, running at over 140 FPS on both GPU and mobile platforms. The model demonstrates strong potential for deployment in real-time applications like autonomous driving and UAV perception.",
      "mindmap": "graph TB\n        A[RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[运动模糊降低图像质量 / Motion blur degrades image quality]\n        B --> B2[实时应用挑战 / Challenges for real-time applications]\n        C --> C1[轻量级U型网络 / Lightweight U-shaped network]\n        C --> C2[三个关键组件 / Three key components: LD, MLIA, X-Fuse]\n        D --> D1[30.67 dB PSNR / 30.67 dB PSNR]\n        D --> D2[5.85M参数, 15.76 GMACs / 5.85M params, 15.76 GMACs]\n        D --> D3[>140 FPS on GPU/mobile / >140 FPS on GPU/mobile]"
    },
    {
      "title": "The Color-Clinical Decoupling: Why Perceptual Calibration Fails Clinical Biomarkers in Smartphone Dermatology",
      "authors": "Sungwoo Kang",
      "institution": "Korea University",
      "link": "https://arxiv.org/pdf/2512.21988",
      "code": null,
      "tags": [
        "medical imaging",
        "colorimetric calibration",
        "clinical biomarkers",
        "Individual Typology Angle (ITA)",
        "Melanin Index",
        "intraclass correlation coefficient (ICC)"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fbc8b90f041b184cdc9f22e1063fa065b2e1f8b098825058b11e381c89ffee7_w640_q70.webp",
      "contributions": "1. Identifies and defines the \"color-clinical decoupling\" phenomenon, where perceptual color accuracy does not guarantee reliability of clinical biomarkers. 2. Demonstrates that anatomical variance (facial region) is a dominant source of color variance, significantly outweighing device effects, challenging single-patch calibration. 3. Provides a mathematical explanation for the decoupling by showing the ITA biomarker's disproportionate sensitivity to noise in the b* color channel.",
      "summary": "This paper investigates the assumption that colorimetric calibration ensures reliable clinical biomarker extraction in smartphone dermatology. Using a large dataset and standard calibration, it finds that while color error is reduced, biomarker reliability remains poor, a phenomenon termed \"color-clinical decoupling,\" primarily due to anatomical variance and formula sensitivity. The conclusion is that current calibration standards are insufficient for clinical use and require region-aware protocols.",
      "mindmap": "graph TB\n        A[The Color-Clinical Decoupling<br>颜色-临床解耦] --> B[核心问题/Problem<br>Does color calibration ensure clinical reliability?<br>色彩校准能否确保临床可靠性？]\n        A --> C[主要方法/Method<br>Analyze 43,425 images across devices with CCM<br>使用CCM分析43,425张跨设备图像]\n        A --> D[关键结果/Results<br>Color-clinical decoupling: ∆E↓ but ICC(ITA) poor<br>颜色-临床解耦：∆E下降但ITA的ICC差]\n        B --> D\n        C --> D"
    },
    {
      "title": "MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation",
      "authors": "Chi-Hsiang Hsiao, Yi-Cheng Wang, Tzung-Sheng Lin, Yi-Ren Yeh, Chu-Song Chen",
      "institution": "National Taiwan University, E.SUN Financial Holding Co., Ltd., National Kaohsiung Normal University",
      "link": "https://arxiv.org/pdf/2512.20626",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "multimodal knowledge graph",
        "cross-modal reasoning",
        "visual document understanding",
        "retrieval-augmented generation",
        "entity-centric structure"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/425d6eb853edb40749e686474d27dc018d8a86017a4cd69160f9ac2081d36385_w640_q70.webp",
      "contributions": "1. Proposes a multimodal knowledge graph-based RAG framework that integrates visual cues into KG construction, retrieval, and answer generation for cross-modal reasoning. 2. Addresses the limitation of existing text-only KG-RAG methods by automatically building KGs that capture text-to-figure and figure-to-figure relationships. 3. Demonstrates superior performance over existing RAG approaches on both textual and multimodal question-answering tasks through comprehensive experiments.",
      "summary": "The paper introduces MegaRAG, a multimodal knowledge graph-based retrieval-augmented generation method designed to overcome the limitations of text-only RAG systems in understanding complex, long-form visual documents. It integrates visual information into the knowledge graph construction and retrieval process to enable better cross-modal reasoning. Experimental results show it consistently outperforms existing RAG methods on various question-answering tasks.",
      "mindmap": "graph LR\n        A[MegaRAG: 多模态知识图谱检索增强生成 / MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有RAG方法在长文档、多模态内容上理解不足 / Existing RAG struggles with long-form, multimodal document understanding]\n        C --> C1[构建融合视觉线索的多模态知识图谱 / Construct multimodal KG incorporating visual cues]\n        C --> C2[在多模态检索与生成中利用图谱 / Utilize KG in multimodal retrieval & generation]\n        D --> D1[在全局与细粒度QA任务上超越现有方法 / Outperforms existing methods on global & fine-grained QA]"
    },
    {
      "title": "MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing",
      "authors": "Yuting Hu, Lei Zhuang, Hua Xiang, Jinjun Xiong, Gi-Joon Nam",
      "institution": "University at Buffalo, IBM Research",
      "link": "https://arxiv.org/pdf/2512.20655",
      "code": null,
      "tags": [
        "others",
        "mask optimization",
        "optical proximity correction",
        "inverse lithography technique",
        "deep learning",
        "benchmark dataset"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce9bc1ac552258257c450a9bc4d4c4ae0264c958efae291f56fc44af367bf07a_w640_q70.webp",
      "contributions": "1. Introduces MaskOpt, a large-scale benchmark dataset for AI-driven mask optimization constructed from real 45nm IC designs, addressing limitations of synthetic data. 2. The dataset preserves standard-cell hierarchy and includes varying context window sizes to enable cell- and context-aware model training. 3. Provides comprehensive benchmarks by evaluating state-of-the-art deep learning models, revealing performance trade-offs and validating the importance of contextual and cell-aware inputs.",
      "summary": "The paper presents MaskOpt, a large-scale dataset built from real integrated circuit designs to advance deep learning for mask optimization in semiconductor manufacturing. It addresses the limitations of existing synthetic datasets by including cell hierarchy and surrounding context. Benchmarking results demonstrate the dataset's utility and highlight the critical role of context and cell information for accurate mask generation.",
      "mindmap": "graph LR\n    A[MaskOpt Dataset<br/>MaskOpt数据集] --> B[核心问题/Problem<br/>Existing datasets are synthetic, lack cell hierarchy & context<br/>现有数据集为合成数据，缺乏单元层次和上下文];\n    A --> C[主要方法/Method<br/>Build large-scale dataset from real 45nm IC designs with cell-aware tiles & context windows<br/>基于真实45nm设计构建大规模数据集，包含单元感知切片和上下文窗口];\n    A --> D[关键结果/Results<br/>Benchmarks show model trade-offs, context & cell info are crucial<br/>基准测试显示模型权衡，上下文和单元信息至关重要];"
    },
    {
      "title": "HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model",
      "authors": "Yuanhao Xi, Xiaohuan Bing, Ramin Yahyapour",
      "institution": "Liaoning Technical University, University of Göttingen, Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen",
      "link": "https://arxiv.org/pdf/2512.20674",
      "code": null,
      "tags": [
        "multi-modal training",
        "Low-Rank Adaptation (LoRA)",
        "parameter-efficient fine-tuning",
        "rank adaptation",
        "mobile vision language model",
        "dynamic scheduling"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9589d36616e78e4826e61d2dbafa4ccc718075f3659352d4d01c1b6f4795a02a_w640_q70.webp",
      "contributions": "1. Proposes HyDRA, a parameter-efficient fine-tuning framework for mobile VLMs that implements hierarchical and dynamic rank scheduling. 2. Introduces hierarchical optimization with coarse-grained (layer-level) and fine-grained (intra-layer) rank assignment. 3. Employs dynamic adjustment via an end-to-end automatic optimization using a lightweight performance model to determine ranks during fine-tuning.",
      "summary": "This paper introduces HyDRA, a parameter-efficient fine-tuning framework for mobile Vision Language Models (VLMs) that addresses the computational inefficiency of standard LoRA by implementing hierarchical and dynamic rank scheduling. The method uses a two-pronged optimization strategy and a lightweight performance model to adjust ranks automatically. Experiments show HyDRA outperforms baselines, achieving a 4.7% average improvement without extra parameters and sometimes surpassing full fine-tuning.",
      "mindmap": "graph LR\n    A[HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model] --> B[核心问题/Problem: Standard LoRA with fixed rank is insufficient for training mobile VLMs]\n    A --> C[主要方法/Method: HyDRA framework with hierarchical & dynamic rank scheduling]\n    A --> D[关键结果/Results: Outperforms baseline by 4.7%, no extra parameters, sometimes beats full fine-tuning]"
    },
    {
      "title": "VL4Gaze: Unleashing Vision-Language Models for Gaze Following",
      "authors": "Shijing Wang, Chaoqun Cui, Yaping Huang, Hyung Jin Chang, Yihua Cheng",
      "institution": "Beijing Jiaotong University, Institute of Automation, Chinese Academy of Sciences, University of Birmingham",
      "link": "https://arxiv.org/pdf/2512.20735",
      "code": null,
      "tags": [
        "gaze following",
        "vision-language models",
        "gaze understanding",
        "visual question answering",
        "benchmark dataset",
        "multi-task learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48a1d18f4099c1ab4f26169d964e14f98c077ef87b0bc95c0bf4868b43217b89_w640_q70.webp",
      "contributions": "1. Introduces VL4Gaze, the first large-scale benchmark for evaluating and training vision-language models on gaze understanding. 2. Formulates gaze understanding as a unified VQA problem across four complementary tasks: gaze object description, gaze direction description, gaze point location, and ambiguous question recognition. 3. Demonstrates that targeted multi-task supervision on VL4Gaze substantially improves VLMs' gaze understanding capabilities, which do not reliably emerge from general pre-training.",
      "summary": "This paper addresses the lack of gaze understanding in vision-language models by introducing VL4Gaze, a large-scale benchmark dataset with 489K QA pairs. It evaluates VLMs on four gaze-related VQA tasks and finds that task-specific fine-tuning on this dataset is crucial for achieving reliable performance, as general-purpose VLMs struggle with gaze inference.",
      "mindmap": "graph LR\n        A[VL4Gaze: Unleashing Vision-Language Models for Gaze Following] --> B[核心问题/Problem: Gaze understanding is unexplored in VLMs, with no existing benchmark.]\n        A --> C[主要方法/Method: Introduce VL4Gaze benchmark with 489K QA pairs across four gaze VQA tasks.]\n        A --> D[关键结果/Results: VLMs struggle without supervision; fine-tuning on VL4Gaze brings substantial improvements.]"
    },
    {
      "title": "TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection",
      "authors": "Tony Tran, Bin Hu",
      "institution": "University of Houston",
      "link": "https://arxiv.org/pdf/2512.20746",
      "code": null,
      "tags": [
        "on-device ai",
        "neural architecture search",
        "hardware-aware search",
        "edge detection",
        "TinyML",
        "waste detection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dbb2f9a6c26aed837d78c4cfa78db76890d85a7fadbb49f8592bc1ae30894e1_w640_q70.webp",
      "contributions": "1. Proposes an iterative hardware-aware neural architecture search (NAS) framework that alternates between backbone and neck/head optimization for efficient object detection under TinyML constraints. 2. Introduces a population passthrough mechanism and an accuracy predictor to reduce search cost and improve the stability of the evolutionary search. 3. Delivers a family of deployment-ready detectors (TrashDets) that significantly improve efficiency (energy, latency, power) and accuracy on microcontroller hardware compared to existing TinyML baselines.",
      "summary": "This paper addresses efficient waste detection for edge/IoT devices under TinyML constraints. It proposes an iterative hardware-aware neural architecture search framework to generate a family of efficient detectors called TrashDets. The resulting models achieve higher accuracy with fewer parameters and significantly reduce energy consumption and latency on resource-constrained microcontrollers.",
      "mindmap": "graph LR\n        A[TrashDet] --> B[核心问题/Problem: 边缘设备垃圾检测<br>TinyML Constraints];\n        A --> C[主要方法/Method: 迭代硬件感知NAS<br>Iterative Hardware-aware NAS];\n        A --> D[关键结果/Results: 高效TrashDet家族<br>Efficient TrashDet Family];\n        B --> D;\n        C --> D;"
    },
    {
      "title": "OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective",
      "authors": "Markus Gross, Sai B. Matha, Aya Fahmy, Rui Song, Daniel Cremers, Henri Meess",
      "institution": "Fraunhofer IVI, TU Munich, MCML, UCLA",
      "link": "https://arxiv.org/pdf/2512.20770",
      "code": "https://github.com/markus-42/occufly",
      "tags": [
        "semantic scene completion",
        "semantic scene completion",
        "3D reconstruction",
        "aerial perspective",
        "benchmark dataset",
        "label transfer"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/292af942047832eaba7423020289566350434f49381259d1665a57faf1a20881_w640_q70.webp",
      "contributions": "1. Introduces OccuFly, the first real-world, camera-based aerial Semantic Scene Completion (SSC) benchmark dataset captured at different altitudes and seasons. 2. Proposes a LiDAR-free data generation framework that automates label transfer from 2D masks to 3D point clouds, minimizing manual annotation. 3. Benchmarks state-of-the-art methods on the new dataset and highlights unique challenges of aerial viewpoints.",
      "summary": "This paper addresses the lack of aerial datasets for Semantic Scene Completion (SSC) by introducing OccuFly, a camera-based benchmark captured from UAVs. The authors propose a framework that uses traditional 3D reconstruction and 2D mask lifting to automate 3D annotation. The resulting dataset enables benchmarking and reveals specific challenges for 3D perception from elevated viewpoints.",
      "mindmap": "graph LR\n    A[OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[缺乏空中SSC基准/Lack of Aerial SSC Benchmark]\n    B --> B2[LiDAR对UAV不友好/LiDAR Unsuitable for UAVs]\n    C --> C1[提出相机模态数据集/Propose Camera-based Dataset]\n    C --> C2[基于3D重建的标签迁移/Label Transfer via 3D Reconstruction]\n    D --> D1[覆盖多场景季节/Covers Scenarios & Seasons]\n    D --> D2[基准测试揭示挑战/Benchmark Reveals Challenges]"
    },
    {
      "title": "NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts",
      "authors": "Raja Mallina, Bryar Shareef",
      "institution": "University of Nevada, Las Vegas",
      "link": "https://arxiv.org/pdf/2512.20783",
      "code": null,
      "tags": [
        "medical image segmentation",
        "nullable prompts",
        "mixed-supervision",
        "vision-language models",
        "breast ultrasound segmentation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c68929834a07c86ac43fe9856efa137aa11c30a231a02ea87272f2b689e4f7f_w640_q70.webp",
      "contributions": "1. Proposes NullBUS, a multimodal mixed-supervision framework for BUS segmentation that can learn from images both with and without prompts in a single model. 2. Introduces nullable prompts, implemented as learnable null embeddings with presence masks, to handle missing text metadata by enabling fallback to image-only evidence. 3. Demonstrates state-of-the-art performance on a unified pool of three public BUS datasets under mixed prompt availability.",
      "summary": "The paper addresses the problem that many public breast ultrasound datasets lack reliable text or spatial prompts, which limits the training of promptable segmentation models. It proposes NullBUS, a framework that uses nullable global-local prompts to learn from both prompted and prompt-free images. The method achieves state-of-the-art segmentation performance on a unified evaluation of three public datasets, showing robustness under mixed prompt availability.",
      "mindmap": "graph LR\n    A[NULLBUS] --> B[核心问题/Problem: BUS数据集缺乏可靠提示词]\n    A --> C[主要方法/Method: 可空全局-局部提示的混合监督框架]\n    A --> D[关键结果/Results: 在混合提示下达到SOTA性能]"
    },
    {
      "title": "Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation",
      "authors": "Reeshad Khan amd John Gauch",
      "institution": "University of Arkansas",
      "link": "https://arxiv.org/pdf/2512.20815",
      "code": null,
      "tags": [
        "semantic segmentation",
        "optics-sensor co-design",
        "RAW-to-task pipeline",
        "learnable color filter array",
        "differentiable simulation",
        "autonomous driving"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98485ccaafcb09f4e55fbe828874d818d5caf3b68c7d2d4f86250cb41081aebe_w640_q70.webp",
      "contributions": "1. A task-driven, end-to-end co-design framework that jointly optimizes optics (via lens models), sensor parameters (learnable color filter arrays, noise, quantization), and a lightweight semantic segmentation network for autonomous driving. 2. Demonstrates consistent performance improvements (mIoU) on KITTI-360, particularly for challenging classes, by adapting image acquisition directly to semantic structure rather than human-viewable imagery. 3. Shows that the robustness gains are achieved with a compact, efficient model (~1M parameters, ~28 FPS), proving the deployability of the full-stack co-optimization approach on edge devices.",
      "summary": "This paper proposes a joint optics-sensor-model co-design framework for autonomous driving perception. It unifies differentiable models of optics, sensor noise, and quantization with a lightweight segmentation network into an end-to-end RAW-to-task pipeline, optimized directly for semantic accuracy. The method outperforms traditional fixed pipelines, especially in challenging conditions, while maintaining high efficiency suitable for edge deployment.",
      "mindmap": "graph LR\n    A[Learning to Sense for Driving] --> B[核心问题/Problem: Traditional camera design is decoupled from perception, losing information and introducing artifacts.]\n    A --> C[主要方法/Method: End-to-end co-design of optics, sensor, and model via differentiable simulation.]\n    A --> D[关键结果/Results: Improved mIoU, robustness, and efficiency on KITTI-360.]"
    },
    {
      "title": "Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference",
      "authors": "Putu Indah Githa Cahyani, Komang David Dananjaya Suartana, Novanto Yudistira",
      "institution": "University of Brawijaya",
      "link": "https://arxiv.org/pdf/2512.20839",
      "code": "https://github.com/kmdavidds/mlfastlm",
      "tags": [
        "multi-modal inference",
        "adaptive preprocessing",
        "visual token reduction",
        "inference efficiency",
        "content-aware cropping",
        "FastVLM"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffbed91a603765fa692e3234db5637487cb3f57c201db5e6263fed6a6c03b2ca_w640_q70.webp",
      "contributions": "1. Proposed an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content to reduce redundancy. 2. Integrated the method with FastVLM without modifying its architecture or requiring retraining, maintaining model integrity. 3. Demonstrated significant efficiency improvements, including over 50% reduction in per-image inference time and over 55% reduction in visual token count on a DocVQA subset.",
      "summary": "The paper addresses the high inference latency and computational cost of Vision-Language Models (VLMs) when processing high-resolution images by proposing an adaptive visual preprocessing method. This method dynamically adjusts input resolution and cropping based on image content to reduce visual redundancy before encoding, integrated seamlessly with FastVLM. Experimental results show over 50% reduction in per-image inference time and over 55% reduction in visual token count, proving it as an effective lightweight strategy for deployment efficiency.",
      "mindmap": "graph LR\n    A[Input-Adaptive Visual Preprocessing] --> B[核心问题/Problem: High inference latency & cost in VLMs]\n    A --> C[主要方法/Method: Adaptive resolution & cropping based on content]\n    A --> D[关键结果/Results: >50% faster inference, >55% token reduction]"
    },
    {
      "title": "CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images",
      "authors": "Vidit Agrawal, John Peters, Tyler N. Thompson, Mohammad Vali Sanian, Chau Pham, Nikita Moshkov, Arshad Kazi, Aditya Pillai, Jack Freeman, Byunguk Kang, Samouil L. Farhi, Ernest Fraenkel, Ron Stewart, Lassi Paavolainen, Bryan A. Plummer, Juan C. Caicedo",
      "institution": "Morgridge Institute for Research, University of Wisconsin-Madison, Institute for Molecular Medicine Finland (FIMM), University of Helsinki, Boston University, Institute of Computational Biology (Helmholtz Munich), Massachusetts Institute of Technology, Broad Institute of MIT and Harvard",
      "link": "https://arxiv.org/pdf/2512.20833",
      "code": null,
      "tags": [
        "bioimage analysis",
        "multi-channel microscopy",
        "cellular morphology",
        "pre-training dataset",
        "channel-adaptive models",
        "heterogeneous data"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3554315333d73be844495c47c136c201cae29e405327a25f6e11fc1488c5f4f9_w640_q70.webp",
      "contributions": "1. Introduces CHAMMI-75, a novel open-access dataset of heterogeneous, multi-channel microscopy images curated from 75 diverse biological studies. 2. Proposes the use of this dataset to investigate and develop channel-adaptive models for cellular morphology that can process any microscopy image type. 3. Demonstrates experimentally that pre-training with the diverse CHAMMI-75 dataset improves performance on multi-channel bioimaging tasks.",
      "summary": "This paper addresses the problem that models for quantifying cell morphology are typically specialized to single microscopy types, limiting their reuse. It proposes CHAMMI-75, a diverse, multi-channel microscopy image dataset, and shows that pre-training with it improves model performance by enabling channel-adaptability and handling heterogeneous modalities. The work aims to pave the way for more generalizable cellular morphology models.",
      "mindmap": "graph LR\n        A[CHAMMI-75: Pre-training Multi-channel Models] --> B[核心问题/Problem: Specialized models cannot be reused across studies]\n        A --> C[主要方法/Method: Curate CHAMMI-75, a heterogeneous multi-channel dataset]\n        A --> D[关键结果/Results: Training with CHAMMI-75 improves performance via diversity]"
    },
    {
      "title": "ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction",
      "authors": "Md Zabirul Islam, Md Motaleb Hossen Manik, Ge Wang",
      "institution": "Rensselaer Polytechnic Institute",
      "link": "https://arxiv.org/pdf/2512.20858",
      "code": null,
      "tags": [
        "on-device ai",
        "content-aware retrieval",
        "FAISS",
        "neural talking-head synthesis",
        "local deployment",
        "multimodal interaction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edf9b6fce27313fb183b0fbb9a8b32f719d83748e14517c97b10d89c253c6cac_w640_q70.webp",
      "contributions": "1. A fully local, privacy-preserving interactive video engine that integrates avatar synthesis, content retrieval, and multimodal interaction. 2. A content-aware retrieval mechanism combining semantic similarity with timestamp alignment to surface contextually relevant lecture segments. 3. A system design employing lightweight models, FAISS, and segmented synthesis with progressive preloading to achieve real-time responsiveness.",
      "summary": "The paper presents ALIVE, a system that transforms passive lecture videos into interactive experiences by using local ASR, LLMs, and neural avatars to generate lecture content, a content-aware retrieval mechanism to find relevant segments, and enabling real-time Q&A. It demonstrates that combining multimodal AI with local, content-aware retrieval can create engaging and pedagogically valuable interactive learning environments.",
      "mindmap": "graph LR\n        A[ALIVE: Avatar-Lecture Interactive Video Engine] --> B[核心问题/Problem: Passive lecture videos lack real-time clarification mechanisms]\n        A --> C[主要方法/Method: Local avatar synthesis, content-aware retrieval, real-time multimodal interaction]\n        A --> D[关键结果/Results: Provides accurate, engaging, and privacy-preserving real-time learning support]"
    },
    {
      "title": "NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder",
      "authors": "Daichi Arai, Kyohei Unno, Yasuko Sugito, Yuichi Kusakabe",
      "institution": "Science & Technology Research Laboratories, NHK",
      "link": "https://arxiv.org/pdf/2512.20871",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "360-degree video",
        "implicit neural representations",
        "viewport decoding",
        "spatial-temporal affine transform",
        "video compression"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77be849ee4606db0adeefbd5d6ebfff328fcdd9b300007aa2dc353aaea4af87e_w640_q70.webp",
      "contributions": "1. Proposes NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of the entire panoramic frame for 360-degree videos. 2. Introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. 3. Achieves significant reductions in memory consumption (7x) and increases in decoding speed (2.5x) compared to prior work HNeRV, while improving image quality.",
      "summary": "The paper addresses the high memory usage and slow decoding of applying implicit neural representations (NeRV) to high-resolution 360-degree videos. It proposes NeRV360, a framework that integrates viewport extraction directly into the decoding process using a conditional spatial-temporal affine transform module. Experiments show NeRV360 significantly reduces memory consumption and increases decoding speed while delivering better image quality compared to prior methods.",
      "mindmap": "graph LR\n    A[NeRV360] --> B[核心问题/Problem: High memory & slow decoding for 360° NeRV]\n    A --> C[主要方法/Method: Viewport decoder with spatial-temporal affine transform]\n    A --> D[关键结果/Results: 7x memory↓, 2.5x speed↑, better quality]"
    },
    {
      "title": "Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images",
      "authors": "Haotian Lv, Chao Li, Jiangbo Dai, Yuhui Zhang, Zepeng Fan, Yiqiu Tan, Dawei Wang, Binglei Xie",
      "institution": "Harbin Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.20866",
      "code": null,
      "tags": [
        "object detection",
        "YOLOv11",
        "3D-DIoU",
        "multi-view fusion",
        "GPR",
        "FDTD"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5ecaa60b98d3c92e7c85d9cd5e1f9894fba8806dbb24358189ef30201b7b93c_w640_q70.webp",
      "contributions": "1. Proposed a B/C/D-Scan three-view joint analysis strategy and a feature evaluation method validated by FDTD simulations and real data. 2. Developed the DCO-YOLO framework, integrating DySample, CGLU, and OutlookAttention into YOLOv11 to enhance small-scale pipeline feature extraction. 3. Introduced a 3D-DIoU spatial feature matching algorithm with 3D geometric constraints to automate multi-view annotation association and resolve single-view ambiguities.",
      "summary": "This paper proposes a lightweight framework for 3D underground pipeline detection using multi-view 2D GPR images. The method integrates an improved YOLO-based detection model (DCO-YOLO) with a novel 3D-DIoU spatial matching algorithm for multi-view fusion. Experiments on real urban data show the framework achieves high accuracy, recall, and mAP, outperforming the baseline and offering a reliable solution for pipeline recognition and localization.",
      "mindmap": "graph LR\n    A[Lightweight framework for underground pipeline recognition and spatial localization<br>基于多视图2D GPR图像的地下管道识别与空间定位轻量级框架] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[Weak multi-view feature correlation, low small-target accuracy<br>多视图特征关联弱，小目标识别精度低]\n    C --> C1[3D pipeline three-view feature evaluation<br>三维管道三视图特征评估]\n    C --> C2[DCO-YOLO framework<br>DCO-YOLO框架]\n    C --> C3[3D-DIoU spatial feature matching<br>3D-DIoU空间特征匹配]\n    D --> D1[Accuracy 96.2%, Recall 93.3%, mAP 96.7%<br>准确率96.2%，召回率93.3%，平均精度96.7%]"
    },
    {
      "title": "Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification",
      "authors": "Tingfeng Xian, Wenlve Zhou, Zhiheng Zhou, Zhelin Li",
      "institution": "South China University of Technology",
      "link": "https://arxiv.org/pdf/2512.20892",
      "code": "https://github.com/TingfengXian/DRI",
      "tags": [
        "cross-modal re-identification",
        "Parameter-Efficient Fine-Tuning",
        "Vision Foundation Models",
        "Domain Representation Injection",
        "feature-space adaptation",
        "cross-modality"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39fa98c450498929c15e9f360b859ef2bfff59f65e629afdd04c76cb67a0fe53_w640_q70.webp",
      "contributions": "1. Proposes a novel Parameter-Efficient Fine-Tuning (PEFT) strategy called Domain Representation Injection (DRI) that operates in the feature space instead of the weight space. 2. Introduces a lightweight Offset Encoder and a Modulator to extract and adapt domain-specific representations, which are then injected into a frozen Vision Foundation Model to bridge modality gaps. 3. Achieves state-of-the-art performance on cross-modal ship re-identification with minimal trainable parameters, demonstrating the effectiveness of feature-space adaptation for limited-capacity models.",
      "summary": "This paper addresses the challenge of cross-modal ship re-identification by proposing a feature-space adaptation method called Domain Representation Injection (DRI). Instead of fine-tuning model weights, DRI injects learned domain-specific features into a frozen Vision Foundation Model to bridge the modality gap. The method achieves state-of-the-art results with very few trainable parameters, showing its efficiency and effectiveness.",
      "mindmap": "graph LR\n    A[Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification] --> B[核心问题/Problem: 跨模态船舶重识别中的显著模态差异/Significant modality discrepancy in Cross-Modality Ship Re-ID]\n    A --> C[主要方法/Method: 提出特征空间领域表示注入(DRI)/Propose feature-space Domain Representation Injection (DRI)]\n    A --> D[关键结果/Results: 以极少的可训练参数实现SOTA性能/Achieve SOTA performance with minimal trainable parameters]"
    },
    {
      "title": "DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction",
      "authors": "Xiao Yu, Zhaojie Fang, Guanyu Zhou, Yin Shen, Huoling Luo, Ye Li, Ahmed Elazab, Xiang Wan, Ruiquan Ge, Changmiao Wang",
      "institution": "Hangzhou Dianzi University",
      "link": "https://arxiv.org/pdf/2512.20898",
      "code": "https://github.com/lcbkmm/DGSAN",
      "tags": [
        "medical image analysis",
        "spatiotemporal attention",
        "graph neural network",
        "multimodal fusion",
        "pulmonary nodule classification",
        "feature encoder"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/422fb811704c808454d49662b57428a8e4f2132f4f9eb28d4def2caf51204b49_w640_q70.webp",
      "contributions": "1. Proposed a Dual-Graph Spatiotemporal Attention Network (DGSAN) for pulmonary nodule malignancy prediction. 2. Introduced a Dual-Graph Construction method and a Hierarchical Cross-Modal Graph Fusion Module for effective multimodal feature integration. 3. Compiled a novel multimodal dataset named NLST-cmst to support related research.",
      "summary": "The paper addresses the problem of inefficient multimodal fusion in pulmonary nodule malignancy prediction. It proposes a Dual-Graph Spatiotemporal Attention Network (DGSAN) that uses a Global-Local Feature Encoder and a hierarchical graph fusion module to integrate temporal and multimodal data. Experiments show DGSAN outperforms state-of-the-art methods with high computational efficiency.",
      "mindmap": "graph LR\n    A[DGSAN: Dual-Graph Spatiotemporal Attention Network] --> B(核心问题/Problem: Inefficient multimodal fusion for nodule prediction)\n    A --> C(主要方法/Method: Dual-Graph construction & Hierarchical Cross-Modal Fusion)\n    A --> D(关键结果/Results: Outperforms SOTA on NLST-cmst/CSTL datasets)"
    },
    {
      "title": "Benchmarking and Enhancing VLM for Compressed Image Understanding",
      "authors": "Zifu Zhang, Tongda Xu, Siqi Li, Shengxi Li, Yue Zhang, Mai Xu, Yan Wang",
      "institution": "Tsinghua University, Beihang University, Beijing University of Technology",
      "link": "https://arxiv.org/pdf/2512.20901",
      "code": null,
      "tags": [
        "vision-language models",
        "image compression",
        "benchmark",
        "vision-language models",
        "adaptor",
        "generalization gap"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39011e6665de56b73804a40551ab504f1a26f1c50146d14d5c84a2c6c5586f47_w640_q70.webp",
      "contributions": "1. Introduces the first comprehensive benchmark to evaluate Vision-Language Models (VLMs) on compressed images, covering multiple codecs and tasks with over a million images. 2. Analyzes the performance gap, categorizing it into information loss and VLM generalization failure, and identifies that only the generalization gap is mitigable. 3. Proposes a universal VLM adaptor that improves model performance across various codecs and bitrates by 10%-30%.",
      "summary": "This paper addresses the performance drop of Vision-Language Models (VLMs) when processing low-bitrate compressed images. It introduces a large-scale benchmark to evaluate this issue, analyzes the sources of the performance gap, and proposes a universal adaptor to enhance VLM robustness. The proposed adaptor successfully improves VLM performance on compressed images by 10%-30%, bridging the gap between VLMs and compressed data.",
      "mindmap": "graph LR\n        A[论文标题/Paper Title: Benchmarking and Enhancing VLM for Compressed Image Understanding] --> B(核心问题/Problem: VLM在低码率压缩图像上性能下降/VLM performance drop on low-bitrate compressed images)\n        A --> C(主要方法/Method: 提出基准测试与通用适配器/Propose benchmark & universal adaptor)\n        A --> D(关键结果/Results: 性能提升10%-30%/Performance improved by 10%-30%)"
    },
    {
      "title": "PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding",
      "authors": "Seongmin Jung, Seongho Choi, Gunwoo Jeon, Minsu Cho, Jongwoo Lim",
      "institution": "Seoul National University, Pohang University of Science and Technology (POSTECH)",
      "link": "https://arxiv.org/pdf/2512.20907",
      "code": "https://choiseongho-h.github.io/PanoGrounder",
      "tags": [
        "3D visual grounding",
        "panoramic rendering",
        "vision-language model (VLM)",
        "3D scene understanding",
        "multi-modal fusion",
        "viewpoint selection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb61d36c0dc9249907d446e1e54dd427e25dfcfa7af988fd4093a0199335a6e7_w640_q70.webp",
      "contributions": "1. Proposes a novel 3D visual grounding framework that uses panoramic scene renderings as an intermediate 2D-3D representation, enabling direct use of powerful pretrained 2D Vision-Language Models (VLMs) with minimal adaptation. 2. Introduces a three-stage pipeline that strategically places panoramic viewpoints, performs per-view grounding with a VLM, and fuses predictions into a final 3D bounding box. 3. Demonstrates state-of-the-art performance on standard benchmarks (ScanRefer, Nr3D) and superior generalization to unseen datasets and text paraphrases.",
      "summary": "This paper addresses the problem of limited generalization in 3D Visual Grounding (3DVG) by introducing PanoGrounder, a framework that bridges 2D and 3D using panoramic scene renderings. The method leverages pretrained 2D Vision-Language Models for reasoning on these panoramic views and fuses predictions into a 3D bounding box. It achieves state-of-the-art results and shows strong generalization capabilities.",
      "mindmap": "graph LR\n        A[PanoGrounder] --> B[核心问题/Problem: 3DVG泛化性差/3DVG Poor Generalization]\n        A --> C[主要方法/Method: 全景表示与VLM/Panoramic Rep. & VLM]\n        A --> D[关键结果/Results: SOTA与强泛化/SOTA & Strong Generalization]"
    },
    {
      "title": "Self-supervised Multiplex Consensus Mamba for General Image Fusion",
      "authors": "Yingying Wang, Rongjin Zhuang, Hui Zheng, Xuanhua He, Ke Cao, Xiaotong Tu, Xinghao Ding",
      "institution": "Xiamen University, The Hong Kong University of Science and Technology, University of Science and Technology of China",
      "link": "https://arxiv.org/pdf/2512.20921",
      "code": null,
      "tags": [
        "image fusion",
        "Mamba",
        "Self-supervised Learning",
        "Mixture of Experts",
        "Cross-modal Scanning",
        "Contrastive Learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91945f65768f96b0613d0702a5b5373c8b78673452a1717dc1624fdddc50aa4b_w640_q70.webp",
      "contributions": "1. Proposed the SMC-Mamba framework, featuring a Modality-Agnostic Feature Enhancement (MAFE) module for detail preservation and global representation enhancement. 2. Introduced the Multiplex Consensus Cross-modal Mamba (MCCM) module for dynamic expert collaboration and efficient cross-modal information integration. 3. Designed a Bi-level Self-supervised Contrastive Learning Loss (BSCL) to preserve high-frequency information and boost downstream task performance without extra computational cost.",
      "summary": "This paper proposes SMC-Mamba, a self-supervised Mamba-based framework for general image fusion. It introduces novel modules for feature enhancement and cross-modal consensus, along with a contrastive learning loss, to efficiently integrate information from various modalities. Experiments show it outperforms state-of-the-art methods across multiple fusion tasks and downstream vision applications.",
      "mindmap": "graph LR\n        A[Self-supervised Multiplex Consensus Mamba<br>自监督多重共识Mamba] --> B(核心问题/Problem: General Image Fusion<br>通用图像融合)\n        A --> C(主要方法/Method: SMC-Mamba Framework<br>SMC-Mamba框架)\n        A --> D(关键结果/Results: Outperforms SOTA<br>超越SOTA方法)\n        B --> B1(Integrate multi-modal info<br>融合多模态信息)\n        B --> B2(Enhance downstream tasks<br>增强下游任务)\n        C --> C1(MAFE Module<br>MAFE模块)\n        C --> C2(MCCM Module<br>MCCM模块)\n        C --> C3(BSCL Loss<br>BSCL损失)\n        D --> D1(IVIF, MDIF, MFIF, MEIF<br>红外-可见光、医学、多焦点、多曝光融合)\n        D --> D2(Better downstream performance<br>更好的下游性能)"
    },
    {
      "title": "Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting",
      "authors": "Yoonwoo Jeong, Cheng Sun, Frank Wang, Minsu Cho, Jaesung Choe",
      "institution": "NVIDIA, POSTECH",
      "link": "https://arxiv.org/pdf/2512.20927",
      "code": null,
      "tags": [
        "3D scene understanding",
        "3D Gaussian Splatting",
        "open-vocabulary segmentation",
        "quantile rendering",
        "feature rendering",
        "real-time rendering"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9eadb39e7ce78a2787efbad5a7c95d49fedbdfefe4e08690405521c36873eb3_w640_q70.webp",
      "contributions": "1. Introduces Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that sparsely samples only dominant Gaussians along a ray to efficiently handle high-dimensional features. 2. Proposes Gaussian Splatting Network (GS-Net), a generalizable 3D neural network that integrates Q-Render to predict Gaussian features. 3. Achieves state-of-the-art performance on benchmarks (ScanNet, LeRF) while enabling real-time rendering with a ~43.7x speedup for 512-D feature maps.",
      "summary": "This paper addresses the challenge of efficiently rendering high-dimensional features (e.g., 512-D CLIP features) for open-vocabulary segmentation in 3D Gaussian Splatting. The authors propose Quantile Rendering (Q-Render), which sparsely samples influential Gaussians, and integrate it into a generalizable network called GS-Net. The method outperforms existing approaches on standard datasets and achieves significant speedups, enabling real-time performance.",
      "mindmap": "graph LR\n    A[Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting] --> B(核心问题/Problem: 高维特征渲染效率低/High-dimensional feature rendering is inefficient)\n    A --> C(主要方法/Method: 分位数渲染与GS-Net/Quantile Rendering & GS-Net)\n    A --> D(关键结果/Results: 性能提升与实时渲染/Performance gain & real-time rendering)"
    },
    {
      "title": "Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning",
      "authors": "Shengguang Wu, Xiaohan Wang, Yuhui Zhang, Hao Zhu, Serena Yeung-Levy",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.20934",
      "code": "https://transductive-visualprogram.github.io/",
      "tags": [
        "visual reasoning",
        "visual programming",
        "spatial reasoning",
        "tool induction",
        "transductive learning",
        "3D scene understanding"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6374488a70a5d9147002f5652452c2f63ea3698c6660c54123c36fc9deef3991_w640_q70.webp",
      "contributions": "1. Proposes Transductive Visual Programming (TVP), a novel framework that builds new tools from experiential solutions rather than speculative induction., 2. Introduces a closed-loop system with an evolving Tool Library and an Example Library, enabling self-improvement through experience., 3. Demonstrates state-of-the-art performance on spatial reasoning benchmarks and shows that transductively learned tools are used more frequently and generalize better.",
      "summary": "The paper addresses the challenge of spatial reasoning in 3D scenes by proposing Transductive Visual Programming (TVP), a framework that learns reusable higher-level tools by abstracting patterns from its own successful solutions. This experience-driven approach outperforms existing methods and GPT-4o on benchmarks, showing more effective tool discovery and strong generalization to unseen tasks.",
      "mindmap": "graph LR\n        A[Transductive Visual Programming] --> B[核心问题/Problem<br>Spatial reasoning is challenging for VLMs]\n        A --> C[主要方法/Method<br>Build tools from experience, not speculation]\n        A --> D[关键结果/Results<br>SOTA performance, better tool reuse & generalization]"
    },
    {
      "title": "Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation",
      "authors": "Hongxing Fan, Shuyu Zhao, Jiayang Ao, Lu Sheng",
      "institution": "Beihang University, The University of Melbourne",
      "link": "https://arxiv.org/pdf/2512.20936",
      "code": "https://fanhongxing.github.io/remac-page",
      "tags": [
        "amodal completion",
        "multi-agent reasoning",
        "semantic planning",
        "visual synthesis",
        "MAC-Score",
        "chain-of-thought"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70d3435136a9939f15c6fefdbf2a3700e29be4992fac1f670aa29c0aa24f6e92_w640_q70.webp",
      "contributions": "1. A Collaborative Multi-Agent Reasoning Framework that decouples Semantic Planning from Visual Synthesis for stable, single-pass amodal completion. 2. Integration of a self-correcting Verification Agent and a Diverse Hypothesis Generator to improve reasoning and handle ambiguity. 3. Introduction of the MAC-Score, a novel human-aligned evaluation metric for assessing inferred invisible content.",
      "summary": "This paper addresses the challenges of inference instability and error accumulation in amodal completion by proposing a reasoning-driven framework. The method employs specialized agents for upfront semantic planning before visual synthesis, and introduces a new evaluation metric. Experiments show the framework outperforms state-of-the-art methods on multiple datasets.",
      "mindmap": "graph LR\n    A[Reasoning-Driven Amodal Completion<br>推理驱动的模态补全] --> B[核心问题/Problem<br>Inference Instability & Error Accumulation<br>推理不稳定与误差累积];\n    A --> C[主要方法/Method<br>Collaborative Multi-Agent Reasoning Framework<br>协作多智能体推理框架];\n    C --> D[Semantic Planning & Visual Synthesis<br>语义规划与视觉合成];\n    C --> E[Verification Agent & Diverse Hypothesis Generator<br>验证智能体与多样化假设生成器];\n    A --> F[关键结果/Results<br>Outperforms SOTA & Novel MAC-Score Metric<br>超越SOTA与新MAC-Score指标];"
    },
    {
      "title": "Beyond Artifacts: Real-Centric Envelope Modeling for Reliable AI-Generated Image Detection",
      "authors": "Ruiqi Liu, Yi Han, Zhengbo Zhang, Liwei Yao, Zhiyuan Yan, Jialiang Shen, ZhiJin Chen, Boyi Sun, Lubin Weng, Jing Dong, Yan Wang, Shu Wu",
      "institution": "Institute of Automation, Chinese Academy of Sciences; Tsinghua University; Peking University; The University of Sydney; Southwest University; Shanghai Second Polytechnic University",
      "link": "https://arxiv.org/pdf/2512.20937",
      "code": null,
      "tags": [
        "ai-generated image detection",
        "real-centric envelope modeling",
        "distribution modeling",
        "chain degradations",
        "benchmark",
        "generalization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e6b1c54aadd8c3872d06b909d6de02c5d6a8a1aafa2a19375b0c325e7c62c51_w640_q70.webp",
      "contributions": "1. Proposes a new paradigm called Real-centric Envelope Modeling (REM) that shifts the detection focus from learning generator-specific artifacts to modeling the robust distribution of real images. 2. Introduces a method that uses feature-level perturbations in self-reconstruction to generate near-real samples and an envelope estimator with cross-domain consistency to learn a boundary around the real image manifold. 3. Builds a comprehensive benchmark named RealChain that covers various generators and simulates real-world degradation chains to evaluate detector robustness.",
      "summary": "The paper addresses the problem that existing AI-generated image detectors overfit to generator artifacts and fail under real-world degradations. It proposes a new method called Real-centric Envelope Modeling (REM) that learns the robust distribution of real images instead, and introduces a new benchmark, RealChain, for evaluation. The results show REM achieves significant performance improvements and maintains strong generalization on degraded images.",
      "mindmap": "graph LR\n        A[Beyond Artifacts: Real-Centric Envelope Modeling] --> B[核心问题/Problem: 现有检测器过拟合生成器痕迹，对真实世界退化敏感]\n        A --> C[主要方法/Method: 提出真实中心包络建模(REM)，建模真实图像分布而非伪造痕迹]\n        A --> D[关键结果/Results: 在八个基准上平均提升7.5%，在RealChain基准上泛化性强]"
    },
    {
      "title": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
      "authors": "Zekai Zhang, Xiao Li, Xiang Li, Lianghe Shi, Meng Wu, Molei Tao, Qing Qu",
      "institution": "University of Michigan, Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.20963",
      "code": "https://deepthink-umich.github.io",
      "tags": [
        "diffusion models",
        "representation learning",
        "denoising autoencoder",
        "memorization detection",
        "representation steering",
        "generalization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58c26c58c94d92dfa7924dfd1c702b8c3239c0a63a4e6ef6f0eaaf42d7dc410a_w640_q70.webp",
      "contributions": "1. Provides a theoretical analysis linking memorization and generalization in diffusion models to specific representation structures (\"spiky\" vs. \"balanced\") using a two-layer ReLU DAE model. 2. Validates the theoretical findings on real-world unconditional and text-to-image diffusion models, showing the practical emergence of these representation regimes. 3. Proposes a representation-based method for detecting memorization and a training-free editing technique for precise control via representation steering.",
      "summary": "This paper analyzes the distinction between memorization and generalization in diffusion models through representation learning. It theoretically proves that memorization leads to \"spiky\" representations while generalization yields \"balanced\" ones, and validates this on real models. Based on these insights, the authors propose methods for memorization detection and training-free image editing, highlighting the central role of good representations for meaningful generation.",
      "mindmap": "graph LR\n    A[Generalization of Diffusion Models Arises with a Balanced Representation Space] --> B(核心问题/Problem: Diffusion models risk memorizing training data)\n    A --> C(主要方法/Method: Analyze via representation learning using a two-layer ReLU DAE)\n    A --> D(关键结果/Results: Memorization yields spiky representations, generalization yields balanced ones; Enables detection and editing techniques)"
    },
    {
      "title": "XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping",
      "authors": "Zeqing Song, Zhongmiao Yan, Junyuan Deng, Songpengcheng Xia, Xiang Mu, Jingyi Xu, Qi Wu, Ling Pei",
      "institution": "Shanghai Jiao Tong University (inferred from author email domains, e.g., likely sjtu.edu.cn)",
      "link": "https://arxiv.org/pdf/2512.20976",
      "code": null,
      "tags": [
        "neural radiance fields",
        "hybrid grid",
        "VDB structure",
        "submap-based organization",
        "distillation-based alignment",
        "dynamic removal"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4f609d4aace0ae5130d408518726a0efd356768fd806198569bd7240de6d7ef_w640_q70.webp",
      "contributions": "1. Proposes XGrid-Mapping, a hybrid grid framework combining sparse explicit grids for geometric priors and implicit dense grids for rich scene representation to enable efficient neural LiDAR mapping. 2. Introduces a submap-based organization coupled with a VDB structure to reduce computational load and support large-scale incremental mapping. 3. Presents a distillation-based overlap alignment strategy and a dynamic removal module to ensure submap consistency and enhance robustness and sampling efficiency.",
      "summary": "The paper proposes XGrid-Mapping, a hybrid explicit-implicit grid framework for efficient incremental neural LiDAR mapping. It uses submaps with a VDB structure and a distillation-based alignment strategy to achieve real-time performance and superior mapping quality, outperforming existing state-of-the-art methods.",
      "mindmap": "graph LR\n    A[XGrid-Mapping] --> B[核心问题/Problem: 大规模增量神经LiDAR建图效率低/ Large-scale incremental neural LiDAR mapping is inefficient]\n    A --> C[主要方法/Method: 显式-隐式混合网格子图/ Explicit-Implicit Hybrid Grid Submaps]\n    A --> D[关键结果/Results: 实时性能与高质量建图/ Real-time performance & superior mapping quality]"
    },
    {
      "title": "SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking",
      "authors": "Yujin Noh, Inho Jake Park, Chigon Hwang",
      "institution": "Kwangwoon University, Gwangju Institute of Science and Technology (GIST)",
      "link": "https://arxiv.org/pdf/2512.20975",
      "code": null,
      "tags": [
        "multi-camera tracking",
        "spatial RAG",
        "beam search",
        "CARLA simulator"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28d25f4afe3b9ea5d43a1eab9aa50328a7e33a926ce2a3183f35335ab5710c43_w640_q70.webp",
      "contributions": "1. Proposes SPOT, a map-guided LLM agent for zero-shot vehicle tracking in multi-CCTV blind spots without prior training. 2. Introduces a method to structure road and CCTV data as spatial documents using chunking for real-time LLM querying. 3. Combines map data with vehicle dynamics to perform beam search at intersections for predicting the next CCTV location.",
      "summary": "This paper proposes SPOT, a method that uses a map-guided LLM agent to track vehicles across blind spots in multi-CCTV environments. It converts spatial map and camera data into a queryable format for the LLM and uses vehicle motion data with beam search to predict where a vehicle will reappear. Experiments in CARLA show it maintains continuous trajectories better than existing techniques.",
      "mindmap": "graph LR\n    A[SPOT!: Map-Guided LLM Agent<br>SPOT!: 地图引导的LLM智能体] --> B[核心问题/Problem<br>Multi-CCTV盲点导致轨迹中断<br>Multi-CCTV blind spots cause trajectory loss]\n    A --> C[主要方法/Method<br>地图引导LLM + 空间RAG + 波束搜索<br>Map-guided LLM + Spatial RAG + Beam Search]\n    A --> D[关键结果/Results<br>在CARLA中准确预测下一个CCTV<br>Accurately predicts next CCTV in CARLA]"
    },
    {
      "title": "X-ray Insights Unleashed: Pioneering the Enhancement of Multi-Label Long-Tail Data",
      "authors": "Xinquan Yang, Jinheng Xie, Yawen Huang, Yuexiang Li, Huimin Huang, Hao Zheng, Xian Wu, Yefeng Zheng, Linlin Shen",
      "institution": "Tencent, Shenzhen University, National University of Singapore, Westlake University",
      "link": "https://arxiv.org/pdf/2512.20980",
      "code": null,
      "tags": [
        "medical image synthesis",
        "diffusion model",
        "data augmentation",
        "long-tail learning",
        "chest X-ray",
        "inpainting"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0edf9e433c973b8803a686a6312cf73fb691cb44807918973120eb68c5308b2a_w640_q70.webp",
      "contributions": "1. A novel data synthesis pipeline that uses a diffusion model trained on normal X-rays to inpaint head-class lesions, thereby augmenting tail-class data. 2. The introduction of a Large Language Model Knowledge Guidance (LKG) module to aid the inpainting process. 3. The proposal of a Progressive Incremental Learning (PIL) strategy to stabilize the fine-tuning of the inpainting model.",
      "summary": "This paper addresses the challenge of diagnosing rare, long-tail pulmonary anomalies in chest X-rays by proposing a data augmentation pipeline. The method trains a diffusion model on normal X-rays and uses it to inpaint common lesions from diseased images, preserving rare lesions for training, and is guided by an LLM and a progressive learning strategy. Evaluations on MIMIC and CheXpert datasets show the method achieves state-of-the-art performance.",
      "mindmap": "graph LR\n    A[X-ray Insights Unleashed: Pioneering the Enhancement of Multi-Label Long-Tail Data] --> B[核心问题/Problem: 长尾肺异常诊断挑战/Long-tail pulmonary anomaly diagnosis challenge]\n    A --> C[主要方法/Method: 基于扩散模型的尾部数据增强/Diffusion-based tail-class data augmentation with LKG & PIL]\n    A --> D[关键结果/Results: 在公开数据集上达到新SOTA/Achieves new SOTA on public datasets]"
    },
    {
      "title": "PUFM++: Point Cloud Upsampling via Enhanced Flow Matching",
      "authors": "Zhi-Song Liu, Chenhang He, Roland Maier, Andreas Rupp",
      "institution": "Lappeenranta-Lahti University of Technology LUT, The Hong Kong Polytechnic University, Karlsruhe Institute of Technology (KIT), Saarland University",
      "link": "https://arxiv.org/pdf/2512.20988",
      "code": "https://github.com/Holmes-Alan/Enhanced_PUFM",
      "tags": [
        "point cloud upsampling",
        "flow matching",
        "two-stage strategy",
        "adaptive time scheduler",
        "on-manifold constraints",
        "recurrent interface network"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eef67ea87c98d8de86d38ef9deebb9dc41ba30f94748973adbad178bbaab09_w640_q70.webp",
      "contributions": "1. A two-stage flow-matching strategy for improved geometric fidelity and distribution approximation, 2. A data-driven adaptive time scheduler to accelerate and stabilize inference, 3. The incorporation of on-manifold constraints and a recurrent interface network (RIN) to enhance surface alignment and feature interaction.",
      "summary": "The paper presents PUFM++, an enhanced flow-matching framework for generating dense and accurate point clouds from sparse, noisy inputs. It introduces a two-stage flow strategy, an adaptive time scheduler, on-manifold constraints, and a recurrent network to improve fidelity, robustness, and efficiency. Experiments show it achieves state-of-the-art performance in point cloud upsampling.",
      "mindmap": "graph LR\n        A[PUFM++] --> B[核心问题/Problem<br>Sparse, Noisy, Partial Point Clouds]\n        A --> C[主要方法/Method<br>Enhanced Flow Matching]\n        C --> D[Two-Stage Flow]\n        C --> E[Adaptive Time Scheduler]\n        C --> F[On-Manifold Constraints]\n        C --> G[Recurrent Interface Network]\n        A --> H[关键结果/Results<br>State-of-the-Art Upsampling]"
    },
    {
      "title": "Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations",
      "authors": "Jinghan Li, Yang Jin, Hao Jiang, Yadong Mu, Yang Song, Kun Xu",
      "institution": "Peking University",
      "link": "https://arxiv.org/pdf/2512.21004",
      "code": "https://github.com/Singularity0104/NExT-Vid",
      "tags": [
        "video representation learning",
        "autoregressive pretraining",
        "next-frame prediction",
        "flow-matching",
        "context-isolated predictor",
        "generative pretraining"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/680ed2b04d0f3e4083f2df212eb836dbdb953f63f6593cd3dbdaba933611f46c_w640_q70.webp",
      "contributions": "1. Proposes NExT-Vid, a novel autoregressive visual generative pretraining framework using masked next-frame prediction to jointly model images and videos. 2. Introduces a context-isolated autoregressive predictor to decouple semantic representation learning from target decoding. 3. Employs a conditioned flow-matching decoder to enhance the quality and diversity of generated frames.",
      "summary": "This paper addresses the limitation of existing visual generative pretraining methods, which often neglect temporal information, by proposing NExT-Vid, an autoregressive framework for next-frame prediction. The method uses a context-isolated predictor and a flow-matching decoder to learn effective video representations. Experiments show it outperforms previous generative pretraining methods on downstream classification tasks.",
      "mindmap": "graph LR\n    A[Learning from Next-Frame Prediction<br>基于下一帧预测的学习] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[视觉生成预训练忽视时序信息<br>Visual Pretraining Ignores Temporal Info]\n    C --> C1[提出NExT-Vid框架<br>Propose NExT-Vid Framework]\n    C1 --> C2[上下文隔离自回归预测器<br>Context-Isolated AR Predictor]\n    C1 --> C3[条件流匹配解码器<br>Conditioned Flow-Matching Decoder]\n    D --> D1[下游分类表现更优<br>Better Downstream Classification]"
    },
    {
      "title": "Granular-ball Guided Masking: Structure-aware Data Augmentation",
      "authors": "Shuyin Xia, Fan Chen, Dawei Dai, Meng Yang, Junwei Han, Xinbo Gao, Guoyin Wang",
      "institution": "Chongqing University of Posts and Telecommunications",
      "link": "https://arxiv.org/pdf/2512.21011",
      "code": null,
      "tags": [
        "data augmentation",
        "Granular-ball Computing",
        "structure-aware masking",
        "information dropping",
        "hierarchical masking"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20fc1c10cf1817c8407a18f117844ed1bf0dd4159dfb95e173f2060df3d75c18_w640_q70.webp",
      "contributions": "1. Proposes Granular-ball Guided Masking (GBGM), a novel structure-aware data augmentation method guided by Granular-ball Computing (GBC). 2. Introduces a coarse-to-fine hierarchical masking process that adaptively preserves semantically rich regions while suppressing redundant areas. 3. Demonstrates the method's model-agnostic nature and effectiveness through extensive experiments on multiple benchmarks, showing improvements in classification and reconstruction.",
      "summary": "This paper addresses the lack of structural awareness in existing mask-based data augmentation methods, which can discard essential semantics. The authors propose Granular-ball Guided Masking (GBGM), a strategy that uses Granular-ball Computing to guide a hierarchical masking process, preserving important regions. Experiments show GBGM improves model robustness and performance across various benchmarks and architectures.",
      "mindmap": "graph LR\n    A[Granular-ball Guided Masking] --> B[核心问题/Problem: 数据增强缺乏结构感知，可能丢弃关键语义/Data augmentation lacks structural awareness, may discard key semantics]\n    A --> C[主要方法/Method: 基于粒球计算的层次化掩码/GBGM: Granular-ball Computing guided hierarchical masking]\n    A --> D[关键结果/Results: 提升分类与重建性能，模型无关/Improves classification & reconstruction, model-agnostic]"
    },
    {
      "title": "FluencyVE: Marrying Temporal-Aware Mamba with Bypass Attention for Video Editing",
      "authors": "Mingshu Cai, Yixuan Li, Osamu Yoshie, Yuya Ieiri",
      "institution": "Waseda University, Southeast University",
      "link": "https://arxiv.org/pdf/2512.21015",
      "code": null,
      "tags": [
        "video editing",
        "Mamba",
        "Stable Diffusion",
        "temporal attention",
        "low-rank approximation",
        "one-shot editing"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3419f702725d02a1b18daa40499c9c2d89fb38a4a84ff67afde4aaf53c6e943_w640_q70.webp",
      "contributions": "1. Proposes FluencyVE, a one-shot video editing method that integrates the Mamba module to replace temporal attention layers for improved temporal consistency and reduced computation. 2. Employs low-rank approximation matrices for query and key weights and a weighted averaging technique during training to preserve generative power while lowering computational burden. 3. Demonstrates effective editing of various video attributes, subjects, and locations in real-world videos through experiments.",
      "summary": "The paper addresses the challenges of temporal inconsistency and high computational cost in text-driven video editing. It proposes FluencyVE, a method that integrates the Mamba module into a Stable Diffusion-based model to replace temporal attention, and uses low-rank approximation and weighted averaging to maintain performance while reducing overhead. Experiments show the method's effectiveness in diverse video editing tasks.",
      "mindmap": "graph LR\n        A[FluencyVE: Video Editing] --> B[核心问题/Problem: Temporal inconsistency & High computational cost in video editing]\n        A --> C[主要方法/Method: Integrate Mamba, Replace temporal attention, Use low-rank approximation & weighted averaging]\n        A --> D[关键结果/Results: Effective editing of attributes, subjects, locations; Reduced computational burden]"
    },
    {
      "title": "MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds",
      "authors": "Xiangzuo Wu, Chengwei Ren, Jun Zhou, Xiu Li, Yuan Liu",
      "institution": "Tsinghua University, Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.21003",
      "code": "https://maddog241.github.io/mvinverse-page/",
      "tags": [
        "inverse rendering",
        "multi-view consistency",
        "feed-forward network",
        "attention mechanism",
        "consistency-based finetuning",
        "intrinsic decomposition"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a474ea3dff714bb84e38df3458cdbb2ec256329a9c182d4c1eeb5f20afad380_w640_q70.webp",
      "contributions": "1. A feed-forward multi-view inverse rendering framework that jointly predicts consistent geometry and material properties from RGB image sequences in a single forward pass. 2. A model architecture that uses alternating attention across views to capture both intra-view lighting and inter-view material consistency. 3. A consistency-based finetuning strategy using unlabeled real-world videos to improve generalization and robustness in real-world conditions.",
      "summary": "This paper introduces MVInverse, a fast feed-forward framework for multi-view inverse rendering that predicts consistent scene properties like albedo and normals from RGB images. It uses cross-view attention for coherent reasoning and a novel finetuning strategy on real videos to improve generalization. Experiments show it achieves state-of-the-art performance in consistency and quality while being much faster than optimization-based methods.",
      "mindmap": "graph LR\n        A[MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[单视图方法多视图不一致/Single-view methods cause multi-view inconsistency]\n        B --> B2[多视图优化方法速度慢/Multi-view optimization methods are slow]\n        B --> B3[合成数据泛化能力差/Synthetic data leads to poor real-world generalization]\n        C --> C1[前馈多视图网络/Feed-forward multi-view network]\n        C --> C2[跨视图注意力/Cross-view attention]\n        C --> C3[基于一致性的微调/Consistency-based finetuning]\n        D --> D1[多视图一致性高/High multi-view consistency]\n        D --> D2[材质与法线质量优/Superior material & normal quality]\n        D --> D3[泛化到真实图像/Generalizes to real-world imagery]"
    },
    {
      "title": "Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face",
      "authors": "Rui-qing Sun, Xingshan Yao, Tian Lan, Hui-Yang Zhao, Jia-Ling Shi, Chen-Hao Cui, Zhijing Wu, Chen Yang, Xian-Ling Mao",
      "institution": "Beijing Institute of Technology, Alibaba International Digital Commerce",
      "link": "https://arxiv.org/pdf/2512.21019",
      "code": "https://github.com/Richen7418/VDF",
      "tags": [
        "adversarial defense",
        "talking face generation",
        "3D neural field",
        "adversarial perturbation",
        "video defense",
        "spatial-frequency optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba8dbfa36170ca60f99422da322b8d87e90e6f7380199d6d5fb91ee653784372_w640_q70.webp",
      "contributions": "1. A novel video defense framework that perturbs the 3D information acquisition process of talking face generation models to protect portrait videos. 2. A similarity-guided parameter sharing mechanism to achieve high computational efficiency (47x acceleration). 3. A multi-scale dual-domain attention module to jointly optimize perturbations in both spatial and frequency domains for robustness and high visual fidelity.",
      "summary": "This paper addresses the privacy threat posed by 3D-field talking face generation models by proposing an efficient video defense framework. The method introduces a parameter-sharing mechanism for speed and a dual-domain attention module to perturb 3D information while preserving video quality. Experiments show it offers strong defense, is 47x faster than baselines, and is robust against common attacks and transformations.",
      "mindmap": "graph LR\n        A[Efficient and Robust Video Defense Framework<br>高效鲁棒的视频防御框架] --> B[Problem: 3D-field TFG models pose privacy risks<br>问题: 3D场TFG模型带来隐私风险]\n        A --> C[Method: Perturb 3D info acquisition with shared params & dual-domain attention<br>方法: 用共享参数和双域注意力扰动3D信息获取]\n        A --> D[Results: Strong defense, 47x faster, high fidelity & robust<br>结果: 强防御性, 47倍加速, 高保真且鲁棒]"
    },
    {
      "title": "Multi-Attribute guided Thermal Face Image Translation based on Latent Diffusion Model",
      "authors": "Mingshu Cai, Osamu Yoshie, Yuya Ieiri",
      "institution": "Waseda University",
      "link": "https://arxiv.org/pdf/2512.21032",
      "code": null,
      "tags": [
        "heterogeneous face recognition",
        "latent diffusion model",
        "multi-attribute classifier",
        "Self-attn Mamba"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fef179e889d0a26d06a1c9b52dedf4221a1040e9e221b0c19692c72d9430246_w640_q70.webp",
      "contributions": "1. Proposes a novel latent diffusion-based model for generating high-quality visible face images from thermal inputs while preserving identity features. 2. Incorporates a multi-attribute classifier to extract key facial attributes from visible images to mitigate feature loss during translation. 3. Introduces the Self-attn Mamba module to enhance global modeling of cross-modal features and improve inference speed.",
      "summary": "This paper addresses the problem of performance degradation in face recognition models when applied to thermal infrared images due to domain shift. It proposes a new method using a latent diffusion model guided by a multi-attribute classifier and a Self-attn Mamba module to translate thermal images into high-quality visible images that preserve identity. The approach achieves state-of-the-art performance in image quality and identity preservation on benchmark datasets.",
      "mindmap": "graph LR\n    A[Multi-Attribute guided Thermal Face Image Translation<br>基于潜在扩散模型的多属性引导热人脸图像翻译] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[域偏移导致红外识别性能下降<br>Domain shift degrades IR recognition]\n    B --> B2[现有方法存在特征失真与丢失<br>Existing methods cause feature distortion/loss]\n    C --> C1[潜在扩散模型生成可见光图像<br>Latent Diffusion Model for V image generation]\n    C --> C2[多属性分类器引导特征保留<br>Multi-attribute classifier guides feature preservation]\n    C --> C3[Self-attn Mamba模块提升建模与速度<br>Self-attn Mamba enhances modeling & speed]\n    D --> D1[图像质量高<br>High image quality]\n    D --> D2[身份特征保留好<br>Good identity preservation]\n    D --> D3[达到SOTA性能<br>Achieves SOTA performance]"
    },
    {
      "title": "Next-Scale Prediction: A Self-Supervised Approach for Real-World Image Denoising",
      "authors": "Yiwen Shan, Haiyu Zhao, Peng Hu, Xi Peng, Yuanbiao Gou",
      "institution": "Sichuan University",
      "link": "https://arxiv.org/pdf/2512.21038",
      "code": null,
      "tags": [
        "image denoising",
        "self-supervised learning",
        "blind-spot network",
        "pixel-shuffle downsampling",
        "real-world noise",
        "super-resolution"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fd7469e2ff88e84d5b9f4c335e67c32a82564aabaef8eb11c7625010499218a_w640_q70.webp",
      "contributions": "1. Proposes Next-Scale Prediction (NSP), a novel self-supervised paradigm that decouples noise decorrelation from detail preservation by constructing cross-scale training pairs for blind-spot networks. 2. Introduces a method where the network takes low-resolution, fully decorrelated sub-images as input to predict high-resolution targets, alleviating the trade-off between noise removal and detail retention. 3. Demonstrates that the proposed NSP framework naturally supports super-resolution of noisy images as a by-product, without requiring retraining or architectural modification.",
      "summary": "This paper addresses the challenge in self-supervised real-world image denoising where existing methods struggle to balance noise decorrelation and detail preservation. It introduces Next-Scale Prediction (NSP), a method that uses cross-scale training pairs to decouple these two objectives, allowing a blind-spot network to learn from decorrelated low-resolution inputs to predict clean high-resolution outputs. Experiments show NSP achieves state-of-the-art performance and can also perform super-resolution on noisy images without additional training.",
      "mindmap": "graph LR\n    A[Next-Scale Prediction: A Self-Supervised Approach for Real-World Image Denoising] --> B[核心问题/Problem: 自监督去噪中噪声去相关与细节保留的权衡/Trade-off between noise decorrelation and detail preservation in self-supervised denoising]\n    A --> C[主要方法/Method: 下一尺度预测/Next-Scale Prediction: 构建跨尺度训练对/Constructing cross-scale training pairs]\n    A --> D[关键结果/Results: 实现SOTA性能，自然支持超分辨率/Achieves SOTA performance, naturally supports super-resolution]"
    },
    {
      "title": "A Large-Depth-Range Layer-Based Hologram Dataset for Machine Learning-Based 3D Computer-Generated Holography",
      "authors": "Jaehong Lee, You Chan No, YoungWoo Kim, Duksu Kim",
      "institution": "Korea University of Technology & Education (KOREATECH)",
      "link": "https://arxiv.org/pdf/2512.21040",
      "code": null,
      "tags": [
        "computer-generated holography",
        "hologram dataset",
        "amplitude projection",
        "angular spectrum method",
        "RGB-D images",
        "super-resolution"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52ea7153ddcddf48944d10c7358fc7dea7c5e544520bd72d9c3d2634cf167b35_w640_q70.webp",
      "contributions": "1. Introduces KOREATECH-CGH, a large-scale public dataset of 6,000 RGB-D and complex hologram pairs with wide resolution and depth range for ML-CGH. 2. Proposes amplitude projection, a post-processing technique to enhance hologram reconstruction fidelity at large depth ranges by replacing amplitude components while preserving phase. 3. Validates the dataset's utility by demonstrating its effectiveness for training state-of-the-art ML models on tasks like hologram generation and super-resolution.",
      "summary": "This paper addresses the lack of high-quality datasets for machine learning-based computer-generated holography (ML-CGH) by introducing KOREATECH-CGH, a large-scale dataset of RGB-D and hologram pairs. The authors also propose a novel amplitude projection technique to improve hologram quality at large depth ranges. The dataset and method are shown to enable effective training of ML models and achieve superior reconstruction quality compared to prior work.",
      "mindmap": "graph LR\n    A[KOREATECH-CGH Dataset Paper] --> B(核心问题/Problem: ML-CGH缺乏高质量大规模数据集)\n    A --> C(主要方法/Method: 构建KOREATECH-CGH数据集并提出振幅投影技术/Amplitude Projection)\n    A --> D(关键结果/Results: 重建质量提升，验证了数据集的实用性/Improved reconstruction, validated dataset utility)"
    },
    {
      "title": "Matrix Completion Via Reweighted Logarithmic Norm Minimization",
      "authors": "Zhijie Wang, Liangtian He, Qinghua Zhang, Jifei Miao, Liang-Jian Deng, Jun Liu",
      "institution": "The affiliations are not explicitly listed in the provided content. Based on the author names (Zhijie Wang, Liangtian He, Qinghua Zhang, Jifei Miao, Liang-Jian Deng, Jun Liu), it is not possible to reliably infer a single main institution without the full paper.",
      "link": "https://arxiv.org/pdf/2512.21050",
      "code": null,
      "tags": [
        "low-rank matrix completion",
        "reweighted logarithmic norm",
        "matrix completion",
        "ADMM",
        "nonconvex optimization",
        "image inpainting"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81808ff3d4d128d0df5f11ee05651c74a2691f1ac5943d560a7091bdb5c374c8_w640_q70.webp",
      "contributions": "1. Proposes a novel reweighted matrix logarithmic norm (RMLN) as a more accurate nonconvex surrogate for the rank function. 2. Develops an ADMM-based optimization algorithm that works for any p in (0,1], removing a restriction present in prior work. 3. Demonstrates superior performance in image inpainting experiments compared to state-of-the-art low-rank matrix completion methods.",
      "summary": "This paper addresses the suboptimal solutions from nuclear norm minimization in low-rank matrix completion by proposing a novel reweighted logarithmic norm as a better nonconvex surrogate. The resulting optimization problem is solved efficiently using ADMM. Experiments on image inpainting show the proposed method outperforms existing state-of-the-art approaches.",
      "mindmap": "graph LR\n    A[Matrix Completion Via Reweighted Logarithmic Norm Minimization] --> B(核心问题/Problem: 核范数导致次优解/Nuclear norm yields suboptimal solutions)\n    A --> C(主要方法/Method: 提出重加权对数范数/Propose reweighted logarithmic norm)\n    A --> D(关键结果/Results: 图像修复性能优越/Superior image inpainting performance)"
    },
    {
      "title": "Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera",
      "authors": "Zibin Liu, Banglei Guan, Yang Shang, Shunkun Liang, Zhenbao Yu, Qifeng Yu",
      "institution": "National University of Defense Technology, Wuhan University",
      "link": "https://arxiv.org/pdf/2512.21053",
      "code": null,
      "tags": [
        "object pose tracking",
        "event camera",
        "optical flow",
        "6DoF pose estimation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9212058f75e634f43650bad542c684ec2b99117ed801e190d2e670146ed2f351_w640_q70.webp",
      "contributions": "1. Proposes a novel optical flow-guided 6DoF object pose tracking method using an event camera. 2. Introduces a 2D-3D hybrid feature extraction strategy to detect corners and edges from events and object models. 3. Establishes correlation between features by searching for corner optical flow and minimizing distances between corners and edges for iterative pose optimization.",
      "summary": "This paper proposes a new method for 6DoF object pose tracking using an event camera. The method uses optical flow to guide the association between extracted 2D-3D hybrid features (corners and edges) and iteratively optimizes the object pose. Experiments on simulated and real event data show the method outperforms existing event-based approaches in accuracy and robustness.",
      "mindmap": "graph LR\n    A[论文标题: Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera] --> B(核心问题/Problem: 传统相机位姿跟踪面临运动模糊、光照变化等挑战)\n    A --> C(主要方法/Method: 使用光流引导的2D-3D混合特征提取与关联进行位姿优化)\n    A --> D(关键结果/Results: 在模拟和真实事件数据上优于现有事件相机方法)"
    },
    {
      "title": "DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors",
      "authors": "Kaustubh Kundu, Hrishav Bakul Barua, Lucy Robertson-Bell, Zhixi Cai, Kalin Stefanov",
      "institution": "Monash University, TCS Research",
      "link": "https://arxiv.org/pdf/2512.21054",
      "code": "https://github.com/kaustesseract/DexAvatar",
      "tags": [
        "3D human pose estimation",
        "3D sign language reconstruction",
        "biomechanical accuracy",
        "hand and body pose priors",
        "monocular video",
        "SMPL-X"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594bef871fe9a00d58a9f3f12c9a0b4bf4f66d3d738bd3f02dedcbad04bdcd25_w640_q70.webp",
      "contributions": "1. A novel framework (DexAvatar) for reconstructing biomechanically accurate 3D hand and body poses from monocular sign language videos. 2. The use of learned 3D hand and body pose priors to guide the reconstruction and overcome challenges like self-occlusion and motion blur. 3. Demonstrating strong performance on the SGNify benchmark, achieving a 35.11% improvement over the state-of-the-art.",
      "summary": "The paper introduces DexAvatar, a framework that uses learned 3D hand and body pose priors to reconstruct accurate 3D sign language poses from monocular videos. It addresses the limitations of existing 2D datasets and noisy 3D estimations. The method significantly outperforms prior work on the SGNify motion capture benchmark.",
      "mindmap": "graph LR\n        A[DexAvatar] --> B[核心问题/Problem: 手语视频缺乏准确3D数据，现有3D姿态估计质量差]\n        A --> C[主要方法/Method: 利用学习到的3D手部和身体姿态先验，从单目视频重建]\n        A --> D[关键结果/Results: 在SGNify数据集上性能提升35.11%]"
    },
    {
      "title": "Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition",
      "authors": "Hongsong Wang, Heng Fei, Bingxuan Dai, Jie Gui",
      "institution": "Southeast University, Purple Mountain Laboratories",
      "link": "https://arxiv.org/pdf/2512.21064",
      "code": null,
      "tags": [
        "skeleton-based action recognition",
        "multimodal fusion",
        "self-supervised learning",
        "decomposition and composition",
        "skeleton data",
        "representation learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c579dcce18ac587c13f2fd1178f66dd2922786e4c3434ff17ad5319761f7aa1_w640_q70.webp",
      "contributions": "1. Proposes a novel self-supervised multimodal skeleton-based action representation learning framework named \"Decomposition and Composition\". 2. Introduces a Decomposition strategy to decompose fused multimodal features into distinct unimodal features and align them with ground truth unimodal counterparts. 3. Introduces a Composition strategy that integrates multiple unimodal features as self-supervised guidance to enhance multimodal representation learning.",
      "summary": "This paper proposes a self-supervised framework called \"Decomposition and Composition\" for multimodal skeleton-based action recognition. It aims to effectively utilize the complementarity of different modalities while maintaining computational efficiency. Experiments on major datasets show the method achieves a strong balance between performance and cost.",
      "mindmap": "graph LR\n    A[Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition] --> B(核心问题/Problem: 多模态互补性与模型效率的平衡/Balancing multimodal complementarity and model efficiency)\n    A --> C(主要方法/Method: 分解与组合自监督框架/Decomposition and Composition self-supervised framework)\n    A --> D(关键结果/Results: 在效率与性能间取得优秀平衡/Achieves excellent balance between efficiency and performance)"
    },
    {
      "title": "Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation",
      "authors": "Zebin Jiang, Tianle Jin, Xiangtong Yao, Alois Knoll, Hu Cao",
      "institution": "Technical University of Munich",
      "link": "https://arxiv.org/pdf/2512.21065",
      "code": null,
      "tags": [
        "robotic grasping",
        "language-guided grasping",
        "cross-modal fusion",
        "coarse-to-fine learning",
        "CLIP embeddings",
        "dynamic convolution"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2da981b4739ec7852bb087293e335e475a0b336479e9ff3e69ca100703c7262_w640_q70.webp",
      "contributions": "1. Proposes a hierarchical cross-modal fusion pipeline that progressively injects linguistic cues into visual feature reconstruction for fine-grained visual-semantic alignment., 2. Introduces a language-conditioned dynamic convolution head (LDCH) that adaptively mixes convolution experts based on sentence-level features for instruction-adaptive predictions., 3. Presents a final refinement module to enhance grasp consistency and robustness in complex scenes, validated on real robotic platforms.",
      "summary": "This paper proposes LGGD, a language-guided grasp detection method using a coarse-to-fine learning paradigm with hierarchical cross-modal fusion and a language-conditioned dynamic convolution head. It achieves superior performance on benchmark datasets and demonstrates effective real-world robotic manipulation.",
      "mindmap": "graph LR\n    A[Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation] --> B[核心问题/Problem: Weak alignment between language instructions and visual grasp reasoning in cluttered scenes.]\n    A --> C[主要方法/Method: Coarse-to-fine learning with hierarchical cross-modal fusion and a language-conditioned dynamic convolution head (LDCH).]\n    A --> D[关键结果/Results: Outperforms existing methods, shows strong generalization, and is effective on a real robot.]"
    },
    {
      "title": "Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control",
      "authors": "Minghao Han, YiChen Liu, Yizhou Liu, Zizhi Chen, Jingqun Tang, Xuecheng Wu, Dingkang Yang, Lihua Zhang",
      "institution": "Fudan University, University of Science and Technology Beijing, Xi'an Jiaotong University, ByteDance, Fysics Intelligence Technologies Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.21058",
      "code": null,
      "tags": [
        "medical image generation",
        "diagnostic semantic tokens",
        "prototype control",
        "multi-stream control",
        "pathology MLLM",
        "Patho-FID"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54421c653fc67f5d29e7359f606ed3b8f53d435c83275a71b19858309287fe45_w640_q70.webp",
      "contributions": "1. Proposed UniPath, a semantics-driven pathology image generation framework with Multi-Stream Control (Raw-Text, High-Level Semantics, and Prototype streams) for fine-grained, diagnosis-aware control. 2. Curated a large-scale 2.65M image-text corpus and a high-quality 68K subset to address data scarcity in computational pathology. 3. Established a comprehensive four-tier evaluation hierarchy tailored for pathology and demonstrated state-of-the-art performance, including a 51% improvement in Patho-FID.",
      "summary": "This paper introduces UniPath, a framework for generating pathology images with fine-grained semantic control by leveraging diagnostic understanding through multi-stream conditioning and a prototype bank. It addresses key challenges like data scarcity and terminological heterogeneity by curating large datasets and using a frozen pathology MLLM to distill robust semantic tokens. Experiments show UniPath achieves state-of-the-art performance, significantly outperforming prior methods in both image quality and semantic fidelity.",
      "mindmap": "graph LR\n    A[Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[数据稀缺/Data Scarcity]\n    B --> B2[语义控制不足/Lack of Semantic Control]\n    B --> B3[术语异质性/Terminological Heterogeneity]\n    C --> C1[多流控制/Multi-Stream Control]\n    C1 --> C1a[原始文本流/Raw-Text Stream]\n    C1 --> C1b[高层语义流/High-Level Semantics Stream]\n    C1 --> C1c[原型流/Prototype Stream]\n    C --> C2[诊断语义令牌/Diagnostic Semantic Tokens]\n    C --> C3[原型库/Prototype Bank]\n    D --> D1[Patho-FID 80.9 / Patho-FID 80.9]\n    D --> D2[语义控制达98.7% / Semantic Control 98.7%]\n    D --> D3[开源数据与代码 / Open Data & Code]"
    },
    {
      "title": "UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer",
      "authors": "Tianchen Deng, Xun Chen, Ziming Li, Hongming Shen, Danwei Wang, Javier Civera, Hesheng Wang",
      "institution": "Shanghai Jiao Tong University, University of Zaragoza, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.21078",
      "code": "https://github.com/dtc111111/UniPR-3D",
      "tags": [
        "visual place recognition",
        "multi-view",
        "transformer",
        "3D representation",
        "feature aggregation",
        "geometry-grounded"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f608df655d14ba7ae116d2888176d7d1cb60fee1d08eaf9644d0c320c42e0fe6_w640_q70.webp",
      "contributions": "1. Introduces UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. 2. Proposes a method that jointly leverages 2D and 3D tokens from a VGGT backbone and designs dedicated aggregation modules for each. 3. Incorporates both single- and multi-frame aggregation schemes with a variable-length sequence retrieval strategy to enhance generalization.",
      "summary": "This paper introduces UniPR-3D, a novel Visual Place Recognition (VPR) method that uses a Visual Geometry Grounded Transformer (VGGT) backbone to integrate multi-view information. It constructs descriptors by aggregating both 2D and 3D tokens with dedicated modules and supports variable-length sequence retrieval. Experiments show that UniPR-3D achieves state-of-the-art performance, outperforming existing single- and multi-view baselines.",
      "mindmap": "graph LR\n    A[UniPR-3D] --> B[核心问题/Problem: 传统单视图VPR泛化能力有限/Traditional single-view VPR has limited generalization]\n    A --> C[主要方法/Method: 基于VGGT的多视图2D/3D特征聚合/Multi-view 2D/3D feature aggregation with VGGT]\n    A --> D[关键结果/Results: 达到SOTA性能/Achieves state-of-the-art performance]"
    },
    {
      "title": "Hierarchical Modeling Approach to Fast and Accurate Table Recognition",
      "authors": "Takaya Kawakatsu",
      "institution": "Preferred Networks, Inc.",
      "link": "https://arxiv.org/pdf/2512.21083",
      "code": null,
      "tags": [
        "document analysis",
        "table recognition",
        "multi-task learning",
        "non-causal attention",
        "parallel inference",
        "hierarchical modeling"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd3e215f24ffc72ff43528e39e86c65289bbfb5de75955d48f0284f277f0089e_w640_q70.webp",
      "contributions": "1. A novel multi-task model utilizing non-causal attention to capture the entire table structure. 2. A parallel inference algorithm for cell content recognition that significantly speeds up inference. 3. A hierarchical modeling approach that extends the performance of multi-task models while addressing their speed and explainability limitations.",
      "summary": "This paper addresses the slow inference speed and lack of explainability in existing table recognition models. It proposes a new multi-task model with non-causal attention for global structure understanding and a parallel inference algorithm to decode cell contents simultaneously, achieving both superior accuracy and a 10x+ speedup on large public datasets.",
      "mindmap": "graph LR\n    A[Hierarchical Modeling Approach to Fast and Accurate Table Recognition] --> B(核心问题/Problem: 现有表格识别模型推理慢且有效性未充分解释/Existing models are slow and their effectiveness is not fully explained)\n    A --> C(主要方法/Method: 使用非因果注意力的多任务模型与并行推理算法/Novel multi-task model with non-causal attention & parallel inference algorithm)\n    A --> D(关键结果/Results: 在大型公开数据集上实现视觉与统计上的优越性/Superiority demonstrated visually and statistically on two large public datasets)"
    },
    {
      "title": "UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters",
      "authors": "Yongkun Du, Zhineng Chen, Yazhen Xie, Weikang Baiand Hao Feng, Wei Shi, Yuchen Su, Can Huang, Yu-Gang Jiang",
      "institution": "Fudan University, ByteDance",
      "link": "https://arxiv.org/pdf/2512.21095",
      "code": "https://github.com/Topdu/OpenOCR",
      "tags": [
        "document parsing",
        "unified recognition",
        "hierarchical supervision",
        "semantic-decoupled tokenizer",
        "lightweight model",
        "multi-level recognition"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d562a464fcb112a02846c5c1d5056088c1b9a37f7ece91f77cfeea28939b63b_w640_q70.webp",
      "contributions": "1. Proposed UniRec-0.1B, a lightweight unified model with only 0.1B parameters for multi-level text and formula recognition. 2. Introduced a hierarchical supervision training strategy to address structural variability across different recognition levels. 3. Developed a semantic-decoupled tokenizer to separate text and formula representations, mitigating semantic entanglement.",
      "summary": "This paper proposes UniRec-0.1B, a lightweight vision-language model for unified text and formula recognition across multiple document levels. To address challenges like structural variability and semantic entanglement, the method introduces hierarchical supervision training and a semantic-decoupled tokenizer, trained on a new 40M-sample dataset. Experiments show it outperforms existing general and expert models while being 2-9x faster.",
      "mindmap": "graph LR\n    A[UniRec-0.1B] --> B[核心问题/Problem: Large VLMs are computationally demanding for unified text/formula recognition]\n    A --> C[主要方法/Method: Lightweight model with hierarchical supervision & semantic-decoupled tokenizer]\n    A --> D[关键结果/Results: Outperforms SOTA models with 2-9x speedup]"
    },
    {
      "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation",
      "authors": "Zhe Cao, Tao Wang, Jiaming Wang, Yanghai Wang, Yuanxing Zhang, Jialu Chen, Miao Deng, Jiahao Wang, Yubin Guo, Chenxi Liao, Yize Zhang, Zhaoxiang Zhang, Jiaheng Liu",
      "institution": "Nanjing University, Kuaishou Technology, Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.21094",
      "code": "https://github.com/NJU-LINK/T2AV-Compass",
      "tags": [
        "multimodal generation evaluation",
        "text-to-audio-video",
        "cross-modal alignment",
        "MLLM-as-a-Judge",
        "benchmark",
        "evaluation framework"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d57ae1315c90b252f611ac78bcd0cff28f02c7dc2648194faa7e1614fe031d1_w640_q70.webp",
      "contributions": "1. Introduces T2AV-Compass, a unified benchmark with 500 diverse and complex prompts for evaluating Text-to-Audio-Video generation systems. 2. Proposes a dual-level evaluation framework combining objective signal-level metrics with a subjective MLLM-as-a-Judge protocol. 3. Provides extensive evaluation of 11 T2AV systems, revealing significant gaps in realism and cross-modal consistency, establishing a diagnostic testbed for future research.",
      "summary": "The paper addresses the fragmented evaluation of Text-to-Audio-Video (T2AV) generation by proposing T2AV-Compass, a unified benchmark and evaluation framework. It combines objective metrics for quality and alignment with a subjective MLLM-as-a-Judge protocol. The evaluation of 11 systems shows they still fall short of human-level realism and synchronization, highlighting the benchmark's value for advancing the field.",
      "mindmap": "graph LR\n    A[T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[评估碎片化/Fragmented Evaluation]\n    B --> B2[缺乏跨模态对齐/Lacks Cross-modal Alignment]\n    C --> C1[统一基准/Unified Benchmark]\n    C --> C2[双层级评估/Dual-level Evaluation]\n    C1 --> C1a[500个提示/500 Prompts]\n    C2 --> C2a[客观指标/Objective Metrics]\n    C2 --> C2b[主观MLLM评判/Subjective MLLM-as-a-Judge]\n    D --> D1[模型表现不足/Models Fall Short]\n    D --> D2[显著的改进空间/Significant Improvement Room]"
    },
    {
      "title": "TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars",
      "authors": "Jaeseong Lee, Junyeong Ahn, Taewoong Kang, Jaegul Choo",
      "institution": "KAIST, Hanyang University",
      "link": "https://arxiv.org/pdf/2512.21099",
      "code": null,
      "tags": [
        "3D avatar generation",
        "3D Gaussian Splatting",
        "analytic rigging",
        "texel-space deformation",
        "hybrid representation",
        "head reenactment"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8320fd6b1a6f131ad704b82b65143269040ab86b9b67005a1edf15fca8097f6_w640_q70.webp",
      "contributions": "1. A hybrid avatar representation (TexAvatars) that combines analytic rigging for geometric grounding with texel-space neural regression for spatial continuity. 2. A method that predicts Gaussian attributes in UV space via CNNs but drives 3D deformation using mesh-aware Jacobians, enabling smooth transitions across mesh boundaries. 3. The model demonstrates improved generalization, stability, and capture of fine-grained expression details (e.g., wrinkles, mouth cavity) under extreme poses and expressions.",
      "summary": "This paper introduces TexAvatars, a method for creating drivable 3D head avatars by hybridizing analytic rigging with texel-space neural regression to improve generalization to unseen expressions. It predicts local attributes in UV space but uses mesh-aware Jacobians for 3D deformation, separating semantic modeling from geometric control. The approach achieves state-of-the-art performance in challenging reenactment scenarios, capturing fine details with high fidelity.",
      "mindmap": "graph LR\n        A[TexAvatars] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法泛化性差/Existing methods generalize poorly]\n        B --> B2[难以处理极端表情与姿态/Struggle with extreme expressions & poses]\n        C --> C1[混合表示/Hybrid Representation]\n        C --> C2[UV空间预测，3D网格驱动/UV-space prediction, 3D mesh-driven deformation]\n        D --> D1[泛化能力提升/Improved generalization]\n        D --> D2[高保真细节/High-fidelity details]\n        D --> D3[状态领先性能/State-of-the-art performance]"
    },
    {
      "title": "FreeInpaint: Tuning-free Prompt Alignment and Visual Rationality Enhancement in Image Inpainting",
      "authors": "Chao Gong, Dong Li, Yingwei Pan, Jingjing Chen, Ting Yao, Tao Mei",
      "institution": "Fudan University, HiDream.ai Inc.",
      "link": "https://arxiv.org/pdf/2512.21104",
      "code": "https://github.com/CharlesGong12/FreeInpaint",
      "tags": [
        "image inpainting",
        "diffusion models",
        "latent optimization",
        "prompt alignment",
        "visual rationality",
        "plug-and-play"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4367afc5cacaf5baa9675d465f870aa3305d6715ef1ced7b9a233ea1cdf470b_w640_q70.webp",
      "contributions": "1. Proposes a plug-and-play, tuning-free approach (FreeInpaint) that optimizes diffusion latents during inference to enhance image faithfulness., 2. Introduces a prior-guided noise optimization method to steer model attention towards valid inpainting regions by optimizing the initial noise., 3. Designs a composite guidance objective tailored for inpainting to direct the denoising process, improving both prompt alignment and visual rationality by optimizing intermediate latents.",
      "summary": "The paper addresses the challenge in text-guided image inpainting of aligning generated content with user prompts while maintaining visual fidelity. It proposes FreeInpaint, a tuning-free method that optimizes diffusion latents during inference using a prior-guided noise optimization and a composite guidance objective. Extensive experiments demonstrate its effectiveness in enhancing both prompt alignment and visual rationality compared to existing methods.",
      "mindmap": "graph LR\n    A[FreeInpaint] --> B[核心问题/Problem: 文本引导图像修复中提示对齐与视觉合理性的平衡问题/Balancing prompt alignment and visual rationality in text-guided inpainting]\n    A --> C[主要方法/Method: 无需微调的潜在优化与复合引导目标/Tuning-free latent optimization & composite guidance objective]\n    A --> D[关键结果/Results: 提升了提示对齐与视觉合理性，验证了方法的有效性与鲁棒性/Improved prompt alignment & visual rationality, validated effectiveness & robustness]"
    },
    {
      "title": "STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting",
      "authors": "Shi Quan Foo, Chi-Ho Wong, Zhihan Gao, Dit-Yan Yeung, Ka-Hing Wong, Wai-Kin Wong",
      "institution": "The Hong Kong University of Science and Technology, Hong Kong Observatory",
      "link": "https://arxiv.org/pdf/2512.21118",
      "code": "https://github.com/sqfoo/stldm_official",
      "tags": [
        "diffusion models",
        "precipitation nowcasting",
        "latent diffusion model",
        "spatio-temporal prediction",
        "variational autoencoder",
        "conditioning network"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3f04a94a2a07676690c2f108f7a602a6b052c1463701d217a319cd81e05ecce_w640_q70.webp",
      "contributions": "1. Proposes STLDM, a novel two-stage diffusion-based architecture for precipitation nowcasting that combines deterministic forecasting with generative enhancement. 2. Introduces an end-to-end learning framework that jointly trains a Variational Autoencoder, a conditioning network, and a latent diffusion model. 3. Demonstrates superior performance and improved inference efficiency compared to state-of-the-art methods on multiple radar datasets.",
      "summary": "The paper addresses the challenge of precipitation nowcasting, where deterministic models produce blurry predictions and generative models suffer from poor accuracy. It proposes STLDM, a spatio-temporal latent diffusion model that decomposes the task into a deterministic forecasting stage and a generative enhancement stage. Experiments show STLDM outperforms state-of-the-art methods while being more efficient.",
      "mindmap": "graph LR\n    A[STLDM: 降水临近预报模型] --> B[核心问题/Problem: 确定性模型模糊，生成模型精度差]\n    A --> C[主要方法/Method: 两阶段潜扩散模型]\n    A --> D[关键结果/Results: 性能优越，推理高效]"
    },
    {
      "title": "MarineEval: Assessing the Marine Intelligence of Vision-Language Models",
      "authors": "YuK-Kwan Wong, Tuan-An To, Jipeng Zhang, Ziqiang Zheng, Sai-Kit Yeung",
      "institution": "Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.21126",
      "code": "https://marineeval.hkustvgd.com",
      "tags": [
        "vision-language models",
        "marine intelligence",
        "domain-specific evaluation",
        "benchmark dataset"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98ea3a0e94d1e540398b18f5612700e51ce9e190b312a741182b20da5ecfacac_w640_q70.webp",
      "contributions": "1. Introduces MarineEval, the first large-scale marine domain-specific dataset and benchmark for evaluating Vision-Language Models (VLMs). 2. Constructs a diverse dataset with 2,000 image-based QA pairs, covering 7 task dimensions and 20 capacity dimensions, validated by marine domain experts. 3. Provides a comprehensive benchmark evaluation of 17 existing VLMs, revealing their significant limitations in handling domain-specific marine questions and highlighting areas for future improvement.",
      "summary": "This paper investigates whether existing general-purpose Vision-Language Models (VLMs) can act as domain experts for marine science. To evaluate this, the authors construct MarineEval, a large-scale, expert-verified benchmark dataset of 2,000 marine image-question-answer pairs. The benchmark reveals that current VLMs perform poorly on this domain-specific task, indicating a significant gap and need for future research in specialized VLM capabilities.",
      "mindmap": "graph LR\n    A[MarineEval: Assessing the Marine Intelligence of Vision-Language Models] --> B(核心问题/Problem: Can VLMs act as marine domain experts?);\n    A --> C(主要方法/Method: Construct MarineEval benchmark with 2000 expert-verified marine image-QA pairs);\n    A --> D(关键结果/Results: Existing VLMs perform poorly, highlighting a large room for improvement);"
    },
    {
      "title": "ORCA: Object Recognition and Comprehension for Archiving Marine Species",
      "authors": "Yuk-Kwan Wong, Haixin Liang, Zeyu Ma, Yiwei Chen, Ziqiang Zheng, Rinaldi Gotama, Pascal Sebastian, Lauren D. Sparks, Sai-Kit Yeung",
      "institution": "Hong Kong University of Science and Technology, University of Electronic Science and Technology of China, Indo Ocean Foundation",
      "link": "https://arxiv.org/pdf/2512.21150",
      "code": "https://orca.hkustvgd.com",
      "tags": [
        "multi-modal benchmark",
        "marine visual understanding",
        "object detection",
        "instance captioning",
        "visual grounding",
        "open-vocabulary"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfd1cc706ce3b8fe57cf7dcd49a501ba304d0ea7e587a81bddd8288ab2c69be7_w640_q70.webp",
      "contributions": "1. Introduces ORCA, a comprehensive multi-modal benchmark dataset for marine species with 14,647 images, 42,217 bounding boxes, and 22,321 expert-verified instance captions. 2. Formulates and evaluates three core computer vision tasks (object detection, instance captioning, visual grounding) on the benchmark to align domain challenges with well-defined tasks. 3. Provides an extensive evaluation of 18 state-of-the-art models, highlighting key challenges like species diversity and morphological overlap, establishing a baseline for future research.",
      "summary": "This paper presents ORCA, a multi-modal benchmark dataset for marine visual understanding, addressing the lack of systematic data and task formulation in the domain. It evaluates 18 state-of-the-art models on tasks like object detection and captioning, revealing significant challenges due to species diversity and domain-specific demands. The work establishes a comprehensive benchmark to advance research in automated marine ecosystem monitoring.",
      "mindmap": "graph LR\n        A[ORCA: Object Recognition and Comprehension for Archiving Marine Species] --> B[核心问题/Problem: Limited marine training data & lack of systematic task formulation]\n        A --> C[主要方法/Method: Introduce multi-modal benchmark with images, bboxes, captions & evaluate models on 3 tasks]\n        A --> D[关键结果/Results: Highlights challenges (diversity, overlap) & establishes comprehensive benchmark]"
    },
    {
      "title": "TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation",
      "authors": "Gaoren Lin, Huangxuan Zhao, Yuan Xiong, Lefei Zhang, Bo Du, Wentao Zhu",
      "institution": "Wuhan University (Inferred from authors Gaoren Lin, Lefei Zhang, Bo Du, Wentao Zhu, who are known to be affiliated with Wuhan University. Huangxuan Zhao and Yuan Xiong are likely from the same group.)",
      "link": "https://arxiv.org/pdf/2512.21135",
      "code": null,
      "tags": [
        "medical image segmentation",
        "CLIP",
        "multimodal fusion",
        "parameter-efficient fine-tuning",
        "vision-language alignment",
        "anatomical structure"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b11d6061b6a08f8b03b2395e16fc0847c1b68ab4e388d93a886ee875211fcf44_w640_q70.webp",
      "contributions": "1. Proposes a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch to preserve fine-grained anatomical structures. 2. Introduces a Domain-Augmented Text Encoder (DATE) that injects medical knowledge from large language models to better model complex clinical descriptions. 3. Designs a Vision-Language Calibration Module (VLCM) to refine cross-modal correspondence in a unified feature space, addressing domain-specific semantic misalignment.",
      "summary": "The paper proposes TGC-Net, a parameter-efficient CLIP-based framework for text-guided medical image segmentation. It addresses CLIP's limitations in medical imaging by introducing modules for structural refinement, medical knowledge injection, and cross-modal calibration. Experiments on five datasets show state-of-the-art performance with fewer trainable parameters.",
      "mindmap": "graph LR\n        A[TGC-Net: Text-Guided Medical Image Segmentation<br>文本引导的医学图像分割] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n    \n        B --> B1[CLIP在医学领域应用受限<br>CLIP Limitations in Medical Domain]\n        B1 --> B2[结构细节丢失<br>Loss of Fine-grained Structure]\n        B1 --> B3[复杂文本建模不足<br>Inadequate Text Modeling]\n        B1 --> B4[领域语义未对齐<br>Domain Semantic Misalignment]\n    \n        C --> C1[语义-结构协同编码器 SSE<br>Semantic-Structural Synergy Encoder]\n        C --> C2[领域增强文本编码器 DATE<br>Domain-Augmented Text Encoder]\n        C --> C3[视觉-语言校准模块 VLCM<br>Vision-Language Calibration Module]\n    \n        D --> D1[在5个数据集上SOTA<br>SOTA on 5 Datasets]\n        D --> D2[参数高效<br>Parameter-Efficient]\n        D --> D3[Dice分数显著提升<br>Notable Dice Gains]"
    },
    {
      "title": "Towards Arbitrary Motion Completing via Hierarchical Continuous Representation",
      "authors": "Chenghao Xu, Guangtao Lyu, Qi Liu, Jiexi Yan, Muli Yang, Cheng Deng",
      "institution": "Hohai University, Xidian University, Institute for Infocomm Research (I2R), A*STAR",
      "link": "https://arxiv.org/pdf/2512.21183",
      "code": null,
      "tags": [
        "motion representation and generation",
        "Implicit Neural Representations",
        "hierarchical temporal encoding",
        "parametric activation",
        "continuous representation",
        "motion interpolation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d8371d82733eca85c31c35a7b6beb75b440d219ddb44ee03521161e6e3f48a6_w640_q70.webp",
      "contributions": "1. Proposes a novel hierarchical continuous representation framework (PA-HiRes) for human motion sequences based on Implicit Neural Representations (INRs). 2. Introduces a hierarchical temporal encoding mechanism to capture intricate motion patterns at multiple temporal scales. 3. Integrates a custom parametric activation function, powered by Fourier transformations, into the MLP decoder to enhance the expressiveness and accuracy of the continuous motion model.",
      "summary": "This paper addresses the limitation of fixed-frame-rate human motion data by proposing a continuous representation model called PA-HiRes. The method uses a hierarchical implicit neural representation with a novel parametric activation function to enable interpolation and extrapolation of motion at arbitrary frame rates. Extensive evaluations show the approach is effective and robust for representing complex motion behaviors.",
      "mindmap": "graph LR\n    A[Towards Arbitrary Motion Completing via Hierarchical Continuous Representation] --> B(核心问题/Problem: Fixed-frame-rate motion sequences compromise fidelity and smoothness)\n    A --> C(主要方法/Method: PA-HiRes framework with hierarchical temporal encoding and parametric Fourier activation)\n    A --> D(关键结果/Results: Enables arbitrary frame rate interpolation/extrapolation; Demonstrates effectiveness and robustness)"
    },
    {
      "title": "UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement",
      "authors": "Tanghui Jia, Dongyu Yan, Dehao Hao, Yang Li, Kaiyi Zhang, Xianyi He, Lanjiong Li, Jinnan Chen, Lutao Jiang, Qishen Yin, Long Quan, Ying-Cong Chen, Li Yuan",
      "institution": "Peking University, The Hong Kong University of Science and Technology, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.21185",
      "code": null,
      "tags": [
        "3D shape generation",
        "diffusion models",
        "geometric refinement",
        "watertight processing",
        "voxel-based refinement",
        "RoPE"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73c8cc72abe4a497351656b82b6735dbe1550a6d18e5c93d7aa031162f07f81e_w640_q70.webp",
      "contributions": "1. A comprehensive data processing pipeline for 3D datasets, featuring a novel watertight processing method and high-quality filtering to improve geometric quality. 2. A two-stage 3D diffusion framework that decouples spatial localization from geometric detail synthesis, using voxel-based refinement with RoPE-encoded positional anchors. 3. Training and evaluation demonstrating competitive performance with existing open-source methods using only publicly available datasets and limited resources.",
      "summary": "This paper introduces UltraShape 1.0, a two-stage diffusion framework for generating high-fidelity 3D shapes. It first synthesizes a coarse structure and then refines it using a novel method that decouples spatial localization from detail synthesis via voxel queries and RoPE encoding. The approach, supported by an improved data processing pipeline, achieves competitive geometry generation quality using public datasets.",
      "mindmap": "graph LR\n    A[UltraShape 1.0] --> B(核心问题/Problem: High-fidelity 3D shape generation 高保真3D形状生成)\n    A --> C(主要方法/Method: Two-stage diffusion with geometric refinement 两阶段扩散与几何细化)\n    C --> C1(Stage 1: Coarse structure synthesis 粗结构合成)\n    C --> C2(Stage 2: Voxel-based detail refinement 基于体素的细节细化)\n    A --> D(关键结果/Results: Competitive generation quality 具有竞争力的生成质量)"
    },
    {
      "title": "A Turn Toward Better Alignment: Few-Shot Generative Adaptation with Equivariant Feature Rotation",
      "authors": "Chenghao Xu, Qi Liu, Jiexi Yan, Muli Yang, Cheng Deng",
      "institution": "Hohai University, Xidian University, Institute for Infocomm Research (I2R), A*STAR",
      "link": "https://arxiv.org/pdf/2512.21174",
      "code": null,
      "tags": [
        "few-shot image generation",
        "domain adaptation",
        "feature rotation",
        "Lie Group",
        "equivariant space",
        "generative adversarial networks"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b06e02ddfdad8176f73c53632a928df39be42b51a926b7d00530f341c4b6945e_w640_q70.webp",
      "contributions": "1. Proposes Equivariant Feature Rotation (EFR), a novel adaptation strategy that aligns source and target domains within a self-rotated proxy feature space to bridge the domain gap. 2. Introduces adaptive rotations within a parameterized Lie Group to transform features into an equivariant space, preserving intra-domain structural information without distortion. 3. Demonstrates through comprehensive experiments that the method significantly enhances generative performance in the target domain compared to existing approaches.",
      "summary": "This paper addresses the challenge of few-shot image generation, where existing methods struggle to align source and target domains effectively due to strict or relaxed constraints. The authors propose Equivariant Feature Rotation (EFR), a method that uses learnable rotations in a Lie Group to map features into an equivariant proxy space for better alignment. Experiments show that EFR significantly improves generative performance in the target domain.",
      "mindmap": "graph LR\n    A[A Turn Toward Better Alignment: Few-Shot Generative Adaptation with Equivariant Feature Rotation] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[现有对齐方法效果不佳 / Existing alignment methods are ineffective]\n    C --> C1[提出等变特征旋转 / Propose Equivariant Feature Rotation]\n    C1 --> C2[在参数化李群中进行自适应旋转 / Adaptive rotations in parameterized Lie Group]\n    C2 --> C3[在代理空间中对齐 / Align in proxy space]\n    D --> D1[生成性能显著提升 / Generative performance significantly improved]"
    },
    {
      "title": "VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs",
      "authors": "Brigitta Malagurski Törtei, Yasser Dahou, Ngoc Dung Huynh, Wamiq Reyaz Para, Phúc H. Lê Khac, Ankit Singh, Sofian Chaybouti, Sanath Narayan",
      "institution": "Technology Innovation Institute, Tuebingen AI Center/University of Tuebingen",
      "link": "https://arxiv.org/pdf/2512.21194",
      "code": null,
      "tags": [
        "visual reasoning",
        "vision-language models",
        "benchmark",
        "perceptual reasoning",
        "compositional reasoning",
        "abstraction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da4a0d93325d1e003056f13f48074769efe0198717d9c263282ebe51f3848352_w640_q70.webp",
      "contributions": "1. Introduces VisRes Bench, a new benchmark for evaluating visual reasoning in naturalistic settings without language supervision. 2. Proposes a structured evaluation across three levels of complexity (perceptual, single-attribute, multi-attribute) to isolate distinct reasoning abilities. 3. Reveals that state-of-the-art VLMs perform near random under subtle perceptual perturbations, exposing their limited abstraction beyond pattern recognition.",
      "summary": "The paper introduces VisRes Bench, a benchmark designed to evaluate the visual reasoning capabilities of Vision-Language Models (VLMs) without relying on linguistic cues. It tests models across three levels of increasing complexity—perceptual completion, rule-based inference, and compositional reasoning—using over 19,000 controlled images. The main finding is that current VLMs perform poorly, near random chance, under perceptual perturbations, indicating a lack of genuine abstract visual reasoning.",
      "mindmap": "graph LR\n        A[VisRes Bench] --> B{核心问题/Problem};\n        A --> C{主要方法/Method};\n        A --> D{关键结果/Results};\n        B --> B1[VLMs依赖语言先验/Linguistic Priors];\n        B --> B2[视觉推理能力不明/Unclear Visual Reasoning];\n        C --> C1[三级评估框架/3-Level Framework];\n        C1 --> C1_1[L1: 感知补全/Perceptual Completion];\n        C1 --> C1_2[L2: 单属性推理/Single-Attribute];\n        C1 --> C1_3[L3: 组合推理/Compositional];\n        D --> D1[性能接近随机/Near Random Performance];\n        D --> D2[抽象能力有限/Limited Abstraction];"
    },
    {
      "title": "Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation",
      "authors": "Yu He, Da Huang, Zhenyang Liu, Zixiao Gu, Qiang Sun, Guangnan Ye, Yanwei Fu",
      "institution": "Fudan University, Shanghai Jiao Tong University, Shanghai University of International Business and Economics, Shanghai Innovation Institute",
      "link": "https://arxiv.org/pdf/2512.21201",
      "code": "https://heyu322.github.io/Schrodinger-Navigator.github.io/",
      "tags": [
        "robot navigation",
        "zero-shot object navigation",
        "trajectory-conditioned 3D imagination",
        "occlusion-aware planning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34e56028ea40f6f3b4a9150683288695e8b7fd2724c676c4f7f56f07367b4fb3_w640_q70.webp",
      "contributions": "1. Proposed Schrödinger's Navigator, a novel navigation framework that models unobserved space as an ensemble of plausible future worlds to handle uncertainty. 2. Introduced a trajectory-conditioned 3D world model that imagines future observations along candidate paths to see beyond occlusions and anticipate risks. 3. Developed a method to fuse imagined 3D observations into a navigation map to update a value map, guiding the policy toward safer, less-occluded routes for better object tracking.",
      "summary": "The paper addresses the challenge of zero-shot object navigation in cluttered environments with occlusions and moving targets. It proposes Schrödinger's Navigator, a framework that samples candidate trajectories and uses a 3D imagination model to predict future observations, enabling the robot to plan safer paths and locate hidden objects. Experiments on a quadruped robot show the method outperforms baselines in success rate and localization in occlusion-heavy settings.",
      "mindmap": "graph LR\n        A[Schrödinger's Navigator] --> B[核心问题/Problem: ZSON struggles with occlusions & uncertainty]\n        A --> C[主要方法/Method: Trajectory-conditioned 3D imagination of futures]\n        A --> D[关键结果/Results: Outperforms baselines on real robot]"
    },
    {
      "title": "Latent Implicit Visual Reasoning",
      "authors": "Kelvin Li, Chuyi Shang, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, Roei Herzig",
      "institution": "University of California, Berkeley, Xero, MIT-IBM Watson AI Lab",
      "link": "https://arxiv.org/pdf/2512.21218",
      "code": null,
      "tags": [
        "multimodal learning",
        "visual reasoning tokens",
        "task-agnostic",
        "implicit supervision"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42f15cd73a3268dae7d708e796f38593c4f7159611e5b226fceb4458a983b525_w640_q70.webp",
      "contributions": "1. Proposes a task-agnostic mechanism for training Large Multimodal Models (LMMs) to discover and use visual reasoning tokens without explicit supervision. 2. Enables models to adaptively re-encode images for a task, extracting relevant visual information without hand-crafted intermediate steps. 3. Achieves state-of-the-art results on diverse vision-centric tasks and generalizes well to multi-task instruction tuning.",
      "summary": "The paper addresses the limitation of text-centric Large Multimodal Models (LMMs) in handling vision-heavy reasoning tasks. It proposes a method that allows LMMs to learn and use visual reasoning tokens implicitly, without needing supervised intermediate visual steps. This approach outperforms direct fine-tuning and achieves state-of-the-art performance on a range of challenging visual reasoning tasks.",
      "mindmap": "graph LR\n    A[Latent Implicit Visual Reasoning] --> B[核心问题/Problem: LMMs are text-centric, struggle with visual reasoning tasks]\n    A --> C[主要方法/Method: Train LMMs to discover task-adaptive visual reasoning tokens without explicit supervision]\n    A --> D[关键结果/Results: Outperforms fine-tuning, SOTA on diverse vision tasks, generalizes well]"
    },
    {
      "title": "RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic",
      "authors": "Le Wang, Zonghao Ying, Xiao Yang, Quanchen Zou, Zhenfei Yin, Tianlin Li, Jian Yang, Yaodong Yang, Aishan Liu, Xianglong Liu",
      "institution": "Beihang University, Beijing University of Posts and Telecommunications, 360 AI Security Lab, The University of Sydney, Nanyang Technological University, Peking University",
      "link": "https://arxiv.org/pdf/2512.21220",
      "code": null,
      "tags": [
        "agent system",
        "runtime safety guardrail",
        "executable safety logic",
        "hybrid reasoning",
        "temporal safety predicate",
        "context-aware safety predicate"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb68be6c6803ce76bf0036925bc1f629db0f2d1ae9be491b5ab0a450b37d1e5_w640_q70.webp",
      "contributions": "1. Proposes RoboSafe, a hybrid reasoning runtime safeguard for embodied agents using executable predicate-based safety logic. 2. Introduces a Backward Reflective Reasoning module to infer temporal safety predicates from recent trajectories and trigger replanning. 3. Introduces a Forward Predictive Reasoning module to anticipate risks by generating context-aware safety predicates from long-term memory and observations.",
      "summary": "The paper addresses the vulnerability of vision-language model-driven embodied agents to hazardous instructions in dynamic environments. It proposes RoboSafe, a runtime safety system that uses hybrid reasoning with backward reflection and forward prediction to generate executable safety logic. Experiments show RoboSafe significantly reduces hazardous actions while maintaining task performance, and its practicality is validated on physical robots.",
      "mindmap": "graph LR\n        A[RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic] --> B[核心问题/Problem: Embodied agents vulnerable to hazardous instructions in dynamic environments]\n        A --> C[主要方法/Method: Hybrid reasoning runtime safeguard with Backward Reflective & Forward Predictive modules]\n        A --> D[关键结果/Results: Reduces hazardous actions (-36.8%), maintains task performance, validated on physical robots]"
    },
    {
      "title": "Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval",
      "authors": "Dao Sy Duy Minh, Huynh Trung Kiet, Nguyen Lam Phu Quy, Phu-Hoa Pham, Tran Chi Nguyen",
      "institution": "University of Science - VNUHCM",
      "link": "https://arxiv.org/pdf/2512.21221",
      "code": "https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval",
      "tags": [
        "image-text retrieval",
        "event-centric entity extraction",
        "BM25",
        "BEiT-3",
        "two-stage retrieval"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3fccaf1dfde41ddfe9c023d8a1f41a9f87c4a0899f25d8a475702d3f368a129_w640_q70.webp",
      "contributions": "1. Proposes a lightweight two-stage retrieval pipeline for event-based image retrieval. 2. Leverages event-centric entity extraction to incorporate temporal and contextual signals for efficient candidate filtering. 3. Combines BM25-based filtering with BEiT-3 reranking to achieve high accuracy on the OpenEvents benchmark.",
      "summary": "This paper addresses the challenge of retrieving images from natural language descriptions in complex, real-world scenarios. It proposes a two-stage method that first filters candidates using BM25 on extracted event entities, then reranks them with a BEiT-3 model. The approach significantly outperforms prior baselines on the OpenEvents benchmark, demonstrating the effectiveness of combining lightweight entity guidance with deep multimodal modeling.",
      "mindmap": "graph LR\n    A[Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval] --> B[核心问题/Problem: Real-world image-text retrieval is challenging due to vague queries and scalability needs.]\n    A --> C[主要方法/Method: Two-stage pipeline with event-centric entity extraction, BM25 filtering, and BEiT-3 reranking.]\n    A --> D[关键结果/Results: Achieves high mean average precision (0.559) on OpenEvents v1, outperforming baselines.]"
    },
    {
      "title": "Human Motion Estimation with Everyday Wearables",
      "authors": "Siqi Zhu, Yixuan Li, Junfu Li, Qi Wu, Zan Wang, Haozhe Ma, Wei Liang",
      "institution": "Beijing Institute of Technology, Yangtze Delta Region Academy of Beijing Institute of Technology, Shenzhen MSU-BIT University",
      "link": "https://arxiv.org/pdf/2512.21209",
      "code": "https://pie-lab.cn/EveryWear/",
      "tags": [
        "human motion capture",
        "multimodal learning",
        "teacher-student framework",
        "egocentric vision",
        "inertial measurement units (IMUs)",
        "sim-to-real gap"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ced793b8564fa6f3ddfc9f17a175ca19ac32fba9b281a74c4be9046c4484d995_w640_q70.webp",
      "contributions": "1. Proposes EveryWear, a lightweight and practical full-body motion capture system using only common consumer wearables (smartphone, smartwatch, earbuds, smart glasses) without requiring explicit calibration. 2. Introduces Ego-Elec, a large-scale, real-world dataset with 9 hours of data covering 56 daily activities in 17 environments, providing ground-truth 3D motion annotations. 3. Presents a multimodal teacher-student framework that fuses visual cues from egocentric cameras with inertial signals, trained directly on real-world data to eliminate the sim-to-real gap.",
      "summary": "The paper proposes EveryWear, a human motion estimation method that uses everyday wearables like smartphones and smartwatches in a multimodal teacher-student framework. It introduces a real-world dataset, Ego-Elec, for training and benchmarking. Experiments show the method outperforms baselines, offering a practical solution for full-body motion capture.",
      "mindmap": "graph LR\n    A[Human Motion Estimation with Everyday Wearables] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[现有方法穿戴性差、硬件昂贵、校准繁琐 / Poor wearability, expensive hardware, cumbersome calibration]\n    C --> C1[EveryWear: 基于日常可穿戴设备 / Everyday Wearables-based]\n    C --> C2[多模态师生框架 / Multimodal Teacher-Student Framework]\n    C --> C3[Ego-Elec 真实世界数据集 / Ego-Elec Real-World Dataset]\n    D --> D1[消除仿真到现实的差距 / Eliminates Sim-to-Real Gap]\n    D --> D2[性能超越基线模型 / Outperforms Baseline Models]"
    },
    {
      "title": "SegMo: Segment-aligned Text to 3D Human Motion Generation",
      "authors": "Bowen Dang, Lin Wu, Xiaohang Yang, Zheng Yuan, Zhixiang Chen",
      "institution": "University of Sheffield, University of Glasgow, Queen Mary University of London",
      "link": "https://arxiv.org/pdf/2512.21237",
      "code": null,
      "tags": [
        "human motion generation",
        "segment alignment",
        "contrastive learning",
        "text-to-motion",
        "fine-grained correspondence",
        "shared embedding space"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39d0fd9df6a0e35165dba62659aeda0ea42d69345d39516d1c7d95fc5895ac0a_w640_q70.webp",
      "contributions": "1. Proposes a novel segment-aligned framework (SegMo) for text-to-3D-motion generation, decomposing both text and motion into temporally ordered segments for fine-grained alignment. 2. Introduces a method for fine-grained text-motion alignment using contrastive learning to create a shared embedding space for text and motion segments. 3. Demonstrates improved performance on standard benchmarks and shows the framework's applicability to retrieval tasks like motion grounding and motion-to-text retrieval.",
      "summary": "This paper addresses the problem of coarse alignment in text-to-3D human motion generation by proposing SegMo, a framework that decomposes text and motion into segments and aligns them using contrastive learning. The method achieves improved performance on the HumanML3D dataset and enables retrieval-style applications. The results show that fine-grained segment alignment enhances the accuracy and realism of generated motions.",
      "mindmap": "graph LR\n    A[SegMo: Segment-aligned Text to 3D Human Motion Generation] --> B(核心问题/Problem: Coarse sequence-level text-motion alignment)\n    A --> C(主要方法/Method: Segment decomposition & fine-grained alignment via contrastive learning)\n    A --> D(关键结果/Results: Improved TOP1 score on HumanML3D; Enables retrieval tasks)"
    },
    {
      "title": "Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks",
      "authors": "Xinjie Xu, Shuyu Cheng, Dongwei Xu, Qi Xuan, Chen Ma",
      "institution": "Zhejiang University of Technology, Binjiang Institute of Artificial Intelligence, ZJUT, JQ Investments",
      "link": "https://arxiv.org/pdf/2512.21241",
      "code": "https://github.com/machanic/hard_label_attacks",
      "tags": [
        "adversarial attacks",
        "hard-label black-box attacks",
        "query efficiency",
        "ray search optimization",
        "Nesterov's Accelerated Gradient",
        "momentum-based optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/302b574932ba227c13854e5c9c2cab3d7cf8f1ed29ee145fbc73062632304cd4_w640_q70.webp",
      "contributions": "1. Proposed ARS-OPT, a momentum-based algorithm inspired by Nesterov's Accelerated Gradient to accelerate the convergence of ray search in hard-label attacks. 2. Introduced PARS-OPT, which further enhances ARS-OPT by incorporating surrogate-model priors into the gradient estimation. 3. Provided theoretical convergence guarantees for the proposed methods and demonstrated superior query efficiency over 13 state-of-the-art approaches on ImageNet and CIFAR-10.",
      "summary": "This paper addresses the high query cost of hard-label black-box adversarial attacks by optimizing ray search. The authors propose ARS-OPT, a momentum-based algorithm, and its enhanced version PARS-OPT, which uses surrogate-model priors, to accelerate convergence. Experiments show the methods outperform 13 existing approaches in query efficiency on standard datasets.",
      "mindmap": "graph LR\n    A[Improving the Convergence Rate of Ray Search Optimization<br>改进射线搜索优化的收敛率] --> B[核心问题/Problem<br>Hard-label攻击查询成本高<br>High query cost in hard-label attacks]\n    A --> C[主要方法/Method<br>提出ARS-OPT & PARS-OPT<br>Propose ARS-OPT & PARS-OPT]\n    A --> D[关键结果/Results<br>超越13种SOTA方法<br>Outperforms 13 SOTA methods]"
    },
    {
      "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation",
      "authors": "Jiawei Liu, Junqiao Li, Jiangfan Deng, Gen Li, Siyu Zhou, Zetao Fang, Shanshan Lao, Zengde Deng, Jianing Zhu, Tingting Ma, Jiayi Li, Yunqiu Wang, Qian He, Xinglong Wu",
      "institution": "ByteDance",
      "link": "https://arxiv.org/pdf/2512.21252",
      "code": "https://dreamontage.github.io/DreaMontage",
      "tags": [
        "video generation",
        "Diffusion Transformers (DiT)",
        "intermediate-conditioning",
        "Segment-wise Auto-Regressive (SAR)",
        "Direct Preference Optimization (DPO)",
        "one-shot video"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b17202b644882c3a0dbe4c4001d4778edc5131e61435a474ca0e440fd66482a_w640_q70.webp",
      "contributions": "1. Introduced a lightweight intermediate-conditioning mechanism into the DiT architecture with an Adaptive Tuning strategy for robust arbitrary-frame control. 2. Developed a Visual Expression SFT stage and a Tailored DPO scheme to enhance visual fidelity, motion rationality, and transition smoothness. 3. Designed a memory-efficient Segment-wise Auto-Regressive (SAR) inference strategy for generating long-duration videos.",
      "summary": "This paper introduces DreaMontage, a framework for generating seamless, long-duration \"one-shot\" videos guided by arbitrary user-provided frames. It achieves this by integrating an intermediate-conditioning mechanism into a Diffusion Transformer, employing specialized fine-tuning and preference optimization, and using a segment-wise auto-regressive inference strategy. The method produces visually coherent and expressive one-shot videos efficiently.",
      "mindmap": "graph LR\n        A[DreaMontage] --> B(核心问题/Problem: 传统一镜到底成本高，现有视频生成方法拼接不连贯)\n        A --> C(主要方法/Method: 中间帧条件控制、视觉表达SFT与DPO、分段自回归推理)\n        A --> D(关键结果/Results: 生成视觉震撼、无缝连贯的长一镜到底视频)"
    },
    {
      "title": "ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision",
      "authors": "Weiqi Li, Zehao Zhang, Liang Lin, Guangrun Wang",
      "institution": "Sun Yat-sen University, Yonsei University",
      "link": "https://arxiv.org/pdf/2512.21268",
      "code": null,
      "tags": [
        "video generation",
        "attention supervision",
        "conditional control",
        "video diffusion",
        "layout controlnet",
        "3D-aware layout"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb3dd27adaccdaa06447db98898dc4dfc553a4184249c20523f8f6b4642036c3_w640_q70.webp",
      "contributions": "1. Proposes ACD (Attention-Conditional Diffusion), a novel framework for direct conditional control in video diffusion models via attention supervision. 2. Introduces a sparse 3D-aware object layout as an efficient conditioning signal and a dedicated Layout ControlNet. 3. Presents an automated annotation pipeline for scalable layout integration.",
      "summary": "The paper addresses the limited controllability in existing video diffusion models by proposing ACD, a framework that directly aligns the model's attention maps with external control signals. This method uses a novel sparse 3D-aware layout and a Layout ControlNet to achieve superior alignment with conditioning inputs while maintaining video quality. Experiments show ACD establishes an effective paradigm for conditional video synthesis.",
      "mindmap": "graph LR\n    A[ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision] --> B[核心问题/Problem: Limited Controllability in Video Synthesis]\n    A --> C[主要方法/Method: Attention Supervision & Sparse 3D-Aware Layout]\n    A --> D[关键结果/Results: Superior Alignment & Preserved Fidelity]"
    },
    {
      "title": "AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI",
      "authors": "Changwei Wu, Yifei Chen, Yuxin Du, Mingxuan Liu, Jinying Zong, Beining Wu, Jie Dong, Feiwei Qin, Yunkang Cao, Qiyuan Tian",
      "institution": "Hangzhou Dianzi University, Tsinghua University, Hunan University",
      "link": "https://arxiv.org/pdf/2512.21264",
      "code": "https://github.com/wuchangw/AnyAD",
      "tags": [
        "medical image analysis",
        "anomaly detection",
        "missing modality",
        "feature alignment",
        "prototype learning",
        "multi-sequence MRI"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d759b100d62183be434837a07a310b265daec4b80f872064c9f08fd1e67db57_w640_q70.webp",
      "contributions": "1. A unified any-modality anomaly detection framework for incomplete multi-sequence MRI. 2. A dual-pathway encoder with feature distribution alignment to handle severe modality dropout. 3. An Intrinsic Normal Prototypes (INPs) extractor and guided decoder to reconstruct normal patterns and amplify anomalies.",
      "summary": "This paper proposes AnyAD, a unified framework for anomaly detection in brain MRI that works robustly with any available combination of imaging modalities, even when some are missing. The method uses a feature alignment mechanism and normal prototype-guided reconstruction to adapt to incomplete data without retraining. Experiments show it outperforms state-of-the-art methods across multiple datasets and modality combinations, establishing a scalable approach for real-world clinical use.",
      "mindmap": "graph LR\n    A[AnyAD: Unified Any-Modality Anomaly Detection] --> B[核心问题/Problem: 临床MRI模态缺失与异常样本稀缺]\n    A --> C[主要方法/Method: 特征分布对齐与正常原型引导重建]\n    A --> D[关键结果/Results: 在多种模态组合上超越SOTA方法]"
    },
    {
      "title": "GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation",
      "authors": "Snehal Singh Tomar, Alexandros Graikos, Arjun Krishna, Dimitris Samaras, Klaus Mueller",
      "institution": "Stony Brook University",
      "link": "https://arxiv.org/pdf/2512.21276",
      "code": null,
      "tags": [
        "video generation",
        "diffusion transformer",
        "factorized generation",
        "grid-based diffusion",
        "super-resolution",
        "long sequence"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b10d59f334dda58c40390950d7473806d1453d042361c2e6ae7ef6e5e1a680df_w640_q70.webp",
      "contributions": "1. Proposes a factorized generation approach for image sequences, separating coarse sequence generation at low resolution from individual frame refinement at high resolution. 2. Introduces a method to train a model on grid images of subsampled frames, effectively extending a 2D image generator to a 3D sequence generator without architectural changes. 3. Demonstrates superior synthesis quality, improved sequence coherence, high-fidelity generation of arbitrary-length sequences, and increased efficiency (at least 2x faster inference) across diverse datasets.",
      "summary": "This paper addresses inefficiencies in generating long image sequences by proposing GriDiT, a method that factorizes the process. It first generates a coarse, low-resolution sequence using a grid-based Diffusion Transformer, then super-resolves each frame individually. This approach achieves higher quality, better coherence, and at least twice the inference speed compared to state-of-the-art methods.",
      "mindmap": "graph LR\n        A[GriDiT: Factorized Grid-Based Diffusion] --> B[核心问题/Problem: Inefficient modeling of image sequences as large tensors]\n        A --> C[主要方法/Method: Factorized generation: low-res grid sequence + per-frame super-resolution]\n        A --> D[关键结果/Results: Superior quality, coherence, >2x faster inference]"
    },
    {
      "title": "Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential",
      "authors": "Shihao Zou, Jingjing Li, Wei Ji, Jincai Huang, Kai Wang, Guo Dan, Weixin Si, Yi Pan",
      "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; University of Alberta; Yale University; Southern University of Science and Technology; Nanfang Hospital Southern Medical University; Shenzhen University",
      "link": "https://arxiv.org/pdf/2512.21284",
      "code": null,
      "tags": [
        "surgical scene segmentation",
        "spiking neural networks",
        "video transformer",
        "masked autoencoding",
        "real-time inference",
        "surgical video"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67a3926b4197163c38fdd24a63b06867e674a7f4f039c4976e75df11c771f6b6_w640_q70.webp",
      "contributions": "1. Proposes SpikeSurgSeg, the first spike-driven video Transformer framework for surgical scene segmentation with real-time potential on non-GPU platforms. 2. Introduces a surgical-scene masked autoencoding pretraining strategy for SNNs using layer-wise tube masking to learn robust spatiotemporal representations from limited labeled data. 3. Designs a lightweight spike-driven segmentation head that maintains temporal consistency and the low-latency characteristics of SNNs.",
      "summary": "This paper addresses the challenge of real-time surgical scene segmentation in resource-constrained environments by proposing SpikeSurgSeg, a spike-driven video Transformer based on Spiking Neural Networks (SNNs). The method uses a masked autoencoding pretraining strategy and a lightweight segmentation head to achieve efficient inference. Experiments show it matches the accuracy of state-of-the-art ANN models while reducing latency by at least 8x and achieving over 20x acceleration compared to foundation models, demonstrating its potential for time-critical surgical applications.",
      "mindmap": "graph LR\n    A[SpikeSurgSeg<br>论文标题/Paper Title] --> B[问题: 手术场景分割需要高精度与低延迟，但现有模型计算开销大<br>Problem: Surgical scene segmentation requires high accuracy & low latency, but existing models are computationally expensive]\n    A --> C[方法: 提出首个基于脉冲神经网络的视频Transformer，使用掩码自编码预训练<br>Method: Proposes first spike-driven video Transformer with masked autoencoding pretraining]\n    A --> D[结果: 精度媲美SOTA ANN模型，推理延迟降低8倍以上<br>Results: Accuracy comparable to SOTA ANN models, inference latency reduced by >8x]"
    },
    {
      "title": "Post-Processing Mask-Based Table Segmentation for Structural Coordinate Extraction",
      "authors": "Suren Bandara",
      "institution": "University of Moratuwa",
      "link": "https://arxiv.org/pdf/2512.21287",
      "code": null,
      "tags": [
        "document image analysis",
        "table structure extraction",
        "mask-based segmentation",
        "Gaussian convolution",
        "signal processing",
        "Cell-Aware Segmentation Accuracy (CASA)"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4ec531bbcda4dfdb5e4b7b3bbe3d643d9a1e7bf829357622807bc23e628f0b58_w640_q70.webp",
      "contributions": "1. Proposes a novel multi-scale signal-processing method that models row/column transitions as 1D signals and uses Gaussian convolution with increasing variances for robust edge detection from imperfect table masks. 2. Introduces a statistical thresholding technique applied after convolution to suppress noise while preserving stable structural edges, improving boundary localization. 3. Demonstrates robustness to resolution variations through zero-padding and scaling strategies, significantly boosting layout-aware accuracy (CASA) on a standard benchmark.",
      "summary": "This paper addresses the challenge of accurately extracting table row and column boundaries from noisy or low-resolution document images. It proposes a post-processing method that converts table masks into 1D signals, applies multi-scale Gaussian convolution and statistical thresholding to detect edges, and maps peaks back to image coordinates. The approach improves Cell-Aware Segmentation Accuracy from 67% to 76% on PubLayNet-1M, showing robustness to image degradation and producing structured outputs suitable for downstream analysis.",
      "mindmap": "graph LR\n    A[Post-Processing Mask-Based Table Segmentation<br>基于掩码的后处理表格分割] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[表格边界检测不准确<br>Inaccurate table boundary detection]\n    B --> B2[噪声与低分辨率图像<br>Noisy & low-res images]\n    C --> C1[将掩码转为1D信号<br>Convert mask to 1D signal]\n    C --> C2[多尺度高斯卷积<br>Multi-scale Gaussian convolution]\n    C --> C3[统计阈值处理<br>Statistical thresholding]\n    D --> D1[CASA从67%提升至76%<br>CASA improved from 67% to 76%]\n    D --> D2[对分辨率变化鲁棒<br>Robust to resolution variations]"
    },
    {
      "title": "AndroidLens: Long-latency Evaluation with Nested Sub-targets for Android GUI Agents",
      "authors": "Yue Cao, Yingyao Wang, Pi Bu, Jingxuan Xing, Wei Jiang, Zekun Zhu, Junpeng Ma, Sashuai Zhou, Tong Lu, Jun Song, Yu Cheng, Yuning Jiang, Bo Zheng",
      "institution": "Alibaba Group, Nanjing University, Fudan University, Zhejiang University",
      "link": "https://arxiv.org/pdf/2512.21302",
      "code": "https://github.com/alibaba/AndroidLens",
      "tags": [
        "agent system",
        "GUI agents",
        "mobile automation",
        "long-latency tasks",
        "evaluation benchmark",
        "Average Task Progress (ATP)"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/662a645c6b32441afe19665c8ae67f9217e2983c70a42ab2e9430700bb7e5305_w640_q70.webp",
      "contributions": "1. Introduces AndroidLens, a challenging benchmark with 571 long-latency tasks across 38 real-world domains in Chinese and English environments. 2. Proposes a static evaluation that preserves real-world anomalies and allows multiple valid paths to reduce bias. 3. Designs a dynamic evaluation with a milestone-based scheme for fine-grained progress measurement via Average Task Progress (ATP).",
      "summary": "This paper introduces AndroidLens, a new evaluation framework designed to assess mobile GUI agents on complex, long-latency tasks derived from real-world scenarios. The framework features both static and dynamic evaluation methods, including a novel milestone-based progress metric. The evaluation results show that even state-of-the-art models perform poorly, achieving only a 12.7% success rate, highlighting significant challenges in real-world mobile automation.",
      "mindmap": "graph LR\n    A[AndroidLens] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[现有基准受限 / Existing Benchmarks Limited]\n    B1 --> B2[应用、任务、指标简单 / Simple Apps, Tasks, Metrics]\n    C --> C1[静态与动态评估 / Static & Dynamic Evaluation]\n    C1 --> C2[真实长延迟任务 / Real-world Long-latency Tasks]\n    C1 --> C3[里程碑与ATP / Milestone & ATP]\n    D --> D1[成功率低 / Low Success Rate (12.7%)]\n    D --> D2[ATP为50.47% / ATP is 50.47%]"
    },
    {
      "title": "Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks",
      "authors": "Roy Turgeman, Tom Tirer",
      "institution": "Bar-Ilan University",
      "link": "https://arxiv.org/pdf/2512.21315",
      "code": null,
      "tags": [
        "information theory",
        "statistical learning",
        "data processing inequality",
        "Bayes classifier",
        "low-level processing",
        "classification accuracy",
        "finite sample analysis"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59be928ca6eaa4e86b8cd3fb9b4f9e66ff3aed7a604e05c3f63b00a3ca0c56bd_w640_q70.webp",
      "contributions": "1. A theoretical proof that for any finite number of training samples, there exists a pre-classification processing that improves classification accuracy, even for a classifier converging to the optimal Bayes classifier. 2. An analysis of how class separation, training set size, and class balance affect the relative gain from low-level pre-processing. 3. Empirical validation of the theory on a synthetic setup and on practical deep classifiers with denoising/encoding tasks on benchmark datasets, showing consistent trends.",
      "summary": "This paper investigates the practical utility of performing low-level signal processing tasks (like denoising) before classification, which seemingly contradicts the information-theoretic Data Processing Inequality. Through a theoretical binary classification analysis and empirical studies, it demonstrates that for finite training samples, such pre-processing can improve accuracy, with the benefit influenced by dataset characteristics like size and noise level.",
      "mindmap": "graph LR\n    A[Does the Data Processing Inequality Reflect Practice?<br/>数据加工不等式反映实践吗?] --> B(核心问题/Problem: Low-level processing is common in practice but seems to contradict the Data Processing Inequality.<br/>实践中的低级处理似乎与数据加工不等式矛盾)\n    A --> C(主要方法/Method: Theoretical study of a binary classifier + empirical validation on deep networks.<br/>二元分类器的理论研究+深度网络的实证验证)\n    A --> D(关键结果/Results: For finite samples, pre-processing can improve accuracy; gain depends on dataset properties.<br/>对于有限样本，预处理可提高精度；收益取决于数据集属性)"
    },
    {
      "title": "TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning",
      "authors": "Varun Belagali, Saarthak Kapse, Pierre Marza, Srijan Das, Zilinghan Li, Sofiène Boutaj, Pushpak Pati, Srikar Yellapragada, Tarak Nath Nandi, Ravi K Madduri, Joel Saltz, Prateek Prasanna, Stergios Christodoulidis Maria Vakalopoulou, Dimitris Samaras",
      "institution": "Stony Brook University, MICS/CentraleSupélec/Université Paris-Saclay, UNC Charlotte, Argonne National Laboratory",
      "link": "https://arxiv.org/pdf/2512.21331",
      "code": "https://cvlab-stonybrook.github.io/TICON/",
      "tags": [
        "computational pathology",
        "tile contextualization",
        "transformer",
        "masked modeling",
        "whole slide image",
        "foundation model"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd2cc5f1b4233e238b13cc9ecf555914fae34743ffd9e8646b2f8300433d9479_w640_q70.webp",
      "contributions": "1. Introduces TICON, a unified transformer-based model to contextualize tile embeddings from any tile-level foundation model for computational pathology. 2. Uses a masked modeling objective during pretraining to simultaneously unify and enrich representations from diverse tile encoders. 3. Demonstrates that TICON-contextualized embeddings achieve state-of-the-art results on multiple tile-level and slide-level benchmarks and enables a highly data-efficient slide-level foundation model.",
      "summary": "The paper addresses the problem of interpreting small image tiles in computational pathology without their larger slide-level context. It proposes TICON, a transformer-based model that contextualizes embeddings from any tile-level foundation model using a masked modeling pretraining objective. The results show that TICON significantly improves performance across various tasks and enables a slide-level foundation model that outperforms prior work while using far less data.",
      "mindmap": "graph LR\n    A[TICON: Slide-Level Tile Contextualizer] --> B[核心问题/Problem: Tile embeddings lack slide-level context];\n    A --> C[主要方法/Method: Transformer-based contextualizer with masked modeling];\n    A --> D[关键结果/Results: SOTA on benchmarks, efficient slide-level foundation model];"
    },
    {
      "title": "Streaming Video Instruction Tuning",
      "authors": "Jiaer Xia, Peixian Chen, Mengdan Zhang, Xing Sun, Kaiyang Zhou",
      "institution": "Hong Kong Baptist University, Tencent Youtu Lab",
      "link": "https://arxiv.org/pdf/2512.21334",
      "code": "https://github.com/maifoundations/Streamo",
      "tags": [
        "video understanding",
        "streaming video",
        "instruction tuning",
        "real-time assistant",
        "temporal reasoning",
        "multimodal LLM"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/683a0c8f318f6ed415484df41515d10ec5ce6b48a1d38eba2ff127258aac07cd_w640_q70.webp",
      "contributions": "1. Proposes Streamo, a real-time streaming video LLM for general-purpose interactive assistance across diverse tasks. 2. Constructs Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. 3. Demonstrates strong performance in temporal reasoning and generalization across streaming benchmarks, bridging offline models and real-time assistants.",
      "summary": "This paper introduces Streamo, a real-time streaming video LLM trained on a large-scale instruction dataset (Streamo-Instruct-465K) to perform diverse tasks like narration and question answering on continuous video. The model shows strong temporal reasoning and generalization, bridging the gap between offline video models and real-time multimodal assistants.",
      "mindmap": "graph LR\n    A[Streaming Video Instruction Tuning] --> B[核心问题/Problem: Offline video models struggle with real-time, continuous video streams]\n    A --> C[主要方法/Method: Train Streamo LLM on Streamo-Instruct-465K dataset for unified streaming tasks]\n    A --> D[关键结果/Results: Streamo achieves strong temporal reasoning and bridges offline/real-time gap]"
    },
    {
      "title": "Fast SAM2 with Text-Driven Token Pruning",
      "authors": "Avilasha Mandal, Chaoning Zhang, Fachrina Dewi Puspitasari, Xudong Wang, Jiaquan Zhang, Caiyan Qin, Guoqing Wang, Yang Yang, Heng Tao Shen",
      "institution": "University of Electronic Science and Technology of China, Indian Institute of Technology Delhi, Harbin Institute of Technology (Shenzhen)",
      "link": "https://arxiv.org/pdf/2512.21333",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "token pruning",
        "video object segmentation",
        "vision transformer",
        "inference efficiency",
        "text guidance"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/135ec6d87d21a48705862639dd6d546443e4868af716ccfb3932e1ca4a9ba7d9_w640_q70.webp",
      "contributions": "1. A text-guided token pruning framework that reduces token density before temporal propagation in SAM2 without modifying the core architecture. 2. A lightweight routing mechanism for token ranking that integrates local visual context, semantic relevance from text, and uncertainty cues. 3. Demonstrates significant improvements in inference speed (up to 42.50% faster) and memory usage (37.41% lower) while maintaining competitive segmentation performance on video benchmarks.",
      "summary": "This paper addresses the high computational cost of the Segment Anything Model 2 (SAM2) for video segmentation by proposing a text-driven token pruning method. The method selectively removes less informative visual tokens before temporal processing using a routing mechanism that considers visual, textual, and uncertainty information. This achieves substantially faster inference and lower memory usage with minimal impact on segmentation accuracy.",
      "mindmap": "graph LR\n        A[Fast SAM2 with Text-Driven Token Pruning] --> B(核心问题/Problem: SAM2计算和内存成本高/High computational & memory cost)\n        A --> C(主要方法/Method: 文本引导的令牌剪枝/Text-guided token pruning)\n        A --> D(关键结果/Results: 推理更快，内存更低/Faster inference & lower memory)"
    },
    {
      "title": "Flow Gym",
      "authors": "Francesco Banelli, Antonio Terpin, Alan Bonomi, Raffaello D'Andrea",
      "institution": "ETH Zürich",
      "link": "https://arxiv.org/pdf/2512.20642",
      "code": "https://github.com/antonioterpin/flowgym",
      "tags": [
        "optical flow / particle image velocimetry",
        "flow-field quantification",
        "synthetic data generation",
        "JAX",
        "reinforcement learning environment",
        "benchmarking toolkit"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e5173f537ce94701f33cf868d525b0c8117477e440d08e96c43c778b59915a4_w640_q70.webp",
      "contributions": "1. Introduces Flow Gym, a unified toolkit for research and deployment of flow-field quantification methods, inspired by OpenAI Gym. 2. Provides a modular, stateless interface for testing, training, and deploying both learning-based and classical algorithms using a synthetic image generation engine (SynthPix). 3. Offers stable JAX re-implementations and integrations of existing algorithms for standardized benchmarking.",
      "summary": "The paper presents Flow Gym, a toolkit designed to standardize the development and evaluation of algorithms for quantifying flow fields from particle images. It provides a unified, modular interface inspired by reinforcement learning environments, enabling easy testing and training of methods using synthetic data. The main outcome is a framework that facilitates reproducible research and benchmarking in flow-field quantification.",
      "mindmap": "graph LR\n    A[Flow Gym] --> B[核心问题/Problem: 流场量化算法缺乏标准化测试框架/Lack of standardized framework for flow-field quantification algorithms];\n    A --> C[主要方法/Method: 提供受RL启发的统一接口与合成数据引擎/Provides RL-inspired unified interface & synthetic data engine];\n    A --> D[关键结果/Results: 用于算法开发与基准测试的JAX兼容工具包/JAX-compatible toolkit for algorithm dev & benchmarking];"
    },
    {
      "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
      "authors": "Li-Zhong Szu-Tu, Ting-Lin Wu, Chia-Jui Chang, He Syu, Yu-Lun Liu",
      "institution": "National Yang Ming Chiao Tung University",
      "link": "https://arxiv.org/pdf/2512.21337",
      "code": "https://sytwu.github.io/BeyondMemo/",
      "tags": [
        "vision-language models",
        "popularity bias",
        "ordinal regression",
        "multi-modal benchmark"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6c6407a1e5e9ce1a54aebfb3d9bee052114a5e56feb5e9df1910a7031677d3c6_w640_q70.webp",
      "contributions": "1. Introduces YearGuessr, the largest open benchmark dataset for building age estimation with multi-modal attributes (55,546 images, construction years, GPS, page-view counts). 2. Frames construction year prediction as an ordinal regression task and proposes popularity-aware interval accuracy metrics to quantify bias. 3. Benchmarks 30+ models, including YearCLIP, exposing significant popularity bias in VLMs where accuracy is up to 34% higher on famous vs. ordinary buildings.",
      "summary": "This paper exposes a popularity bias in vision-language models (VLMs), where models perform much better on famous buildings than ordinary ones, indicating reliance on memorization rather than generalizable understanding. To study this, the authors introduce YearGuessr, a large multi-modal benchmark dataset for building age prediction framed as ordinal regression, and propose metrics to measure bias. Their evaluation confirms VLMs excel on popular items but struggle with unrecognized subjects, revealing a critical flaw in reasoning.",
      "mindmap": "graph LR\n    A[Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models] --> B[核心问题/Problem: VLMs show popularity bias, relying on memorization over understanding]\n    A --> C[主要方法/Method: Introduce YearGuessr dataset and popularity-aware ordinal regression metrics]\n    A --> D[关键结果/Results: VLMs perform up to 34% better on famous buildings, exposing reasoning flaws]"
    },
    {
      "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
      "authors": "Haonan Qiu, Shikun Liu, Zijian Zhou, Zhaochong An, Weiming Ren, Zhiheng Liu, Jonas Schult, Sen He, Shoufa Chen, Yuren Cong, Tao Xiang, Ziwei Liu, Juan-Manuel Perez-Rua",
      "institution": "Meta AI, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.21338",
      "code": "http://haonanqiu.com/projects/HiStream.html",
      "tags": [
        "diffusion models",
        "video generation",
        "inference acceleration",
        "redundancy elimination",
        "autoregressive framework",
        "feature caching"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfd3105f76a9173c10ae0b43fda34a955ef34708de3f700fa0fc7c67d1f3ab3_w640_q70.webp",
      "contributions": "1. Introduces a three-axis redundancy elimination framework (spatial, temporal, timestep) for efficient high-resolution video generation. 2. Proposes a dual-resolution caching mechanism for spatial compression and a chunk-by-chunk strategy with a fixed-size anchor cache for temporal compression. 3. Demonstrates state-of-the-art visual quality with up to 107.5x faster denoising compared to baselines, making 1080p video generation practical.",
      "summary": "The paper addresses the computational bottleneck of high-resolution video generation by proposing HiStream, an autoregressive framework that eliminates redundancy across spatial, temporal, and timestep dimensions. The method achieves significant speedups (up to 107.5x) with negligible quality loss, making high-fidelity 1080p video generation scalable and practical.",
      "mindmap": "graph LR\n    A[HiStream: Efficient High-Resolution Video Generation] --> B[核心问题/Problem: Quadratic complexity makes high-res video generation infeasible]\n    A --> C[主要方法/Method: Redundancy elimination via Spatial, Temporal, Timestep Compression]\n    A --> D[关键结果/Results: Up to 107.5x faster denoising with SOTA quality]"
    },
    {
      "title": "Equivariant Multiscale Learned Invertible Reconstruction for Cone Beam CT: From Simulated to Real Data",
      "authors": "Nikita Moriakov, Efstratios Gavves, Jonathan H. Mason, Carmen Seller-Oria, Jonas Teuwen, Jan-Jakob Sonke",
      "institution": "Netherlands Cancer Institute, University of Amsterdam, Elekta Limited",
      "link": "https://arxiv.org/pdf/2512.21180",
      "code": null,
      "tags": [
        "medical image reconstruction",
        "rotational equivariance",
        "multiscale reconstruction",
        "learned invertible primal-dual",
        "quasi-Monte Carlo simulation",
        "cone beam CT"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80fc637f4006372b13c2de48295a289562b18e569dfcb797d3ad0d9157751aa8_w640_q70.webp",
      "contributions": "1. Proposed LIRE++, an end-to-end rotationally-equivariant multiscale learned invertible primal-dual scheme for fast and memory-efficient CBCT reconstruction. 2. Developed a fast quasi-Monte Carlo CBCT projection simulator to generate training data. 3. Demonstrated improved performance on both synthetic and real clinical data compared to deep learning baselines and a state-of-the-art proprietary hybrid method.",
      "summary": "This paper addresses the challenge of low image quality in Cone Beam CT (CBCT) by proposing LIRE++, a deep learning-based reconstruction method. LIRE++ incorporates rotational equivariance and multiscale processing for efficiency and was trained using a custom fast simulator. It shows improved reconstruction quality over existing methods on both simulated and real clinical data.",
      "mindmap": "graph LR\n    A[Equivariant Multiscale Learned Invertible Reconstruction for CBCT] --> B(核心问题/Problem: CBCT图像质量低, 缺乏真实数据, 内存限制/CBCT has lower image quality, lacks ground truth, has memory constraints)\n    A --> C(主要方法/Method: LIRE++: 旋转等变多尺度可逆对偶学习方案, 使用快速模拟器训练/LIRE++: rotationally-equivariant multiscale learned invertible scheme, trained with fast simulator)\n    A --> D(关键结果/Results: 合成数据PSNR提升1dB, 真实数据MAE降低10HU/1 dB PSNR gain on synthetic data, 10 HU MAE reduction on real data)"
    },
    {
      "title": "PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility",
      "authors": "Md Nahid Hasan Shuvo, Moinul Hossain",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19711",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d516fcc6c3de6b0e65c078e5e3f3dda23bfd77fbb9c5f4abfe2c509c2cb6dfe_w640_q70.webp",
      "contributions": "",
      "summary": "PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility",
      "mindmap": ""
    },
    {
      "title": "Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems",
      "authors": "Xiangzhong Luo, Weichen Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19731",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10d82314834148af959284b60a6d6f0fa9e36928106648626f8c43d4f07b79_w640_q70.webp",
      "contributions": "",
      "summary": "Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems",
      "mindmap": ""
    },
    {
      "title": "Learning to Refocus with Video Diffusion Models",
      "authors": "SaiKiran Tedla, Zhoutong Zhang, Xuaner Zhang, Shumian Xin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19823",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d67b80f0c15466284688038780928e07f307c7269e0df0ded0424d3e770fcd6_w640_q70.webp",
      "contributions": "",
      "summary": "Learning to Refocus with Video Diffusion Models",
      "mindmap": ""
    },
    {
      "title": "RANSAC Scoring Functions: Analysis and Reality Check",
      "authors": "A. Shekhovtsov",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19850",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd50e25656a023b3c2995d13c741fc721d03038e35f7503eb116497b3cb8637d_w640_q70.webp",
      "contributions": "",
      "summary": "RANSAC Scoring Functions: Analysis and Reality Check",
      "mindmap": ""
    },
    {
      "title": "Generating the Past, Present and Future from a Motion-Blurred Image",
      "authors": "SaiKiran Tedla, Kelly Zhu, Trevor Canham, Felix Taubner, Michael S. Brown, Kiriakos N. Kutulakos, David B. Lindell",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19817",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d34df9bd29ac26646aab435f8fb3761d7ba6f54b4c9919286b04b5436dba1d0_w640_q70.webp",
      "contributions": "",
      "summary": "Generating the Past, Present and Future from a Motion-Blurred Image",
      "mindmap": ""
    },
    {
      "title": "HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction",
      "authors": "Jong Wook Kim, Wonseok Roh, Ha Dam Baek, Pilhyeon Lee, Jonghyun Choi, Sangpil Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19871",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a4679f5f60c2af6c77a6712d0c27272bc4719706cb56235956b7caf4c1bac0f_w640_q70.webp",
      "contributions": "",
      "summary": "HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction",
      "mindmap": ""
    },
    {
      "title": "Unified Brain Surface and Volume Registration",
      "authors": "S. Mazdak Abulnaga, Andrew Hoopes, Malte Hoffmann, Robin Magnet, Maks Ovsjanikov, Lilla Zöllei, John Guttag, Bruce Fischl, Adrian Dalca",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19928",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52472b7c53844ab7af247ff699ee1c659d2b3ff27e2e21ee8f187677824a5d8d_w640_q70.webp",
      "contributions": "",
      "summary": "Unified Brain Surface and Volume Registration",
      "mindmap": ""
    },
    {
      "title": "Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs",
      "authors": "Houston H. Zhang, Tao Zhang, Baoze Lin, Yuanqi Xue, Yincheng Zhu, Huan Liu, Li Gu, Linfeng Ye, Ziqiang Wang, Xinxin Zuo, Yang Wang, Yuanhao Yu, Zhixiang Chi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19918",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afcd12646a75911cc37486f5af7f55491eaf6f54718182f47a36695579c55660_w640_q70.webp",
      "contributions": "",
      "summary": "Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs",
      "mindmap": ""
    },
    {
      "title": "Vehicle-centric Perception via Multimodal Structured Pre-training",
      "authors": "Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19934",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25393280cf5e09ecc25c375a88de26bef6b6904fd90a6775cf7591ed8a615fe1_w640_q70.webp",
      "contributions": "",
      "summary": "Vehicle-centric Perception via Multimodal Structured Pre-training",
      "mindmap": ""
    },
    {
      "title": "Block-Recurrent Dynamics in Vision Transformers",
      "authors": "Mozes Jacobs, Thomas Fel, Richard Hakim, Alessandra Brondetta, Demba Ba, T. Andy Keller",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19941",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e10f9b4ab6d210902b2c5092ebfe1fcc82dd7a3d2032bfc97fbfadd16867c2a_w640_q70.webp",
      "contributions": "",
      "summary": "Block-Recurrent Dynamics in Vision Transformers",
      "mindmap": ""
    },
    {
      "title": "SE360: Semantic Edit in 360$^$ Panoramas via Hierarchical Data Construction",
      "authors": "Haoyi Zhong, Fang-Lue Zhang, Andrew Chalmers, Taehyun Rhee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19943",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5acefee064893ec36480803c8a614ffcd70f5e22f389b691a22dff4da224abf7_w640_q70.webp",
      "contributions": "",
      "summary": "SE360: Semantic Edit in 360$^\\circ$ Panoramas via Hierarchical Data Construction",
      "mindmap": ""
    },
    {
      "title": "HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes",
      "authors": "Yuechen Yang, Junlin Guo, Yanfan Zhu, Jialin Yue, Junchao Zhu, Yu Wang, Shilin Zhao, Haichun Yang, Xingyi Guo, Jovan Tanevski, Laura Barisoni, Avi Z. Rosenberg, Yuankai Huo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19954",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebf1e5c1a2595dc81a69957123729d9aa0fa024a71bdfda164ce0ae6bf95d110_w640_q70.webp",
      "contributions": "",
      "summary": "HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes",
      "mindmap": ""
    },
    {
      "title": "How Much 3D Do Video Foundation Models Encode?",
      "authors": "Zixuan Huang, Xiang Li, Zhaoyang Lv, James M. Rehg",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19949",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b54fab192e555a908a0f7daf8d7992c85cd3241844d6a17c19d685c99c93dc5e_w640_q70.webp",
      "contributions": "",
      "summary": "How Much 3D Do Video Foundation Models Encode?",
      "mindmap": ""
    },
    {
      "title": "A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping",
      "authors": "Peng Gao, Ke Li, Di Wang, Yongshan Zhu, Yiming Zhang, Xuemei Luo, Yifeng Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19990",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1691a202602f3bcf2d0e97787c25b8551ee2510f815f2c351ebacb3b3a37953_w640_q70.webp",
      "contributions": "",
      "summary": "A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping",
      "mindmap": ""
    },
    {
      "title": "WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification",
      "authors": "Le Feng, Li Xiao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19982",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fecc5e0134d3112ae90b5c8ac61ae68bcd52b0697a6c32184a7fc10d72e0640_w640_q70.webp",
      "contributions": "",
      "summary": "WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification",
      "mindmap": ""
    },
    {
      "title": "A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection",
      "authors": "Tamim Ahasan Rijon, Yeasin Arafath",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19989",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7415dab1086d45b2bb6f7a49ab33be753b3da047de23310de552ead99dc2448_w640_q70.webp",
      "contributions": "",
      "summary": "A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection",
      "mindmap": ""
    },
    {
      "title": "Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models",
      "authors": "Zhenhao Li, Shaohan Yi, Zheng Liu, Leonartinus Gao, Minh Ngoc Le, Ambrose Ling, Zhuoran Wang, Md Amirul Islam, Zhixiang Chi, Yuanhao Yu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20000",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f2d07a108f93995c65cf13d740ca889c8e7dd494a7ef3c9222caba5e6ccf3c3_w640_q70.webp",
      "contributions": "",
      "summary": "Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models",
      "mindmap": ""
    },
    {
      "title": "PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification",
      "authors": "Blessing Agyei Kyem, Joshua Kofi Asamoah, Anthony Dontoh, Andrews Danyo, Eugene Denteh, Armstrong Aboah",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20011",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff4db8075e196efc52322813cedc23193239565c930612aec052f3acbb95eab5_w640_q70.webp",
      "contributions": "",
      "summary": "PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification",
      "mindmap": ""
    },
    {
      "title": "SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images",
      "authors": "Zepeng Xin, Kaiyu Li, Luodi Chen, Wanchen Li, Yuchen Xiao, Hui Qiao, Weizhan Zhang, Deyu Meng, Xiangyong Cao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20013",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24d12d8c96876ce1d14987d2507601d9688f72e9f86e06abfd01bc397241cbd6_w640_q70.webp",
      "contributions": "",
      "summary": "SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images",
      "mindmap": ""
    },
    {
      "title": "VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement",
      "authors": "Chang Sun, Dongliang Xie, Bo Qin, Hong Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20032",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/800e8f7c8c22b74ebd9c804082b6c7f5a5c4998365cdbe511167991351910a98_w640_q70.webp",
      "contributions": "",
      "summary": "VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement",
      "mindmap": ""
    },
    {
      "title": "A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments",
      "authors": "Anthony Dontoh, Stephanie Ivey, Armstrong Aboah",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20025",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8e8d7557e6f332097ab10b8c45895c9a3f2b19aa186bcd6c3071ac6e02ee8e7_w640_q70.webp",
      "contributions": "",
      "summary": "A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments",
      "mindmap": ""
    },
    {
      "title": "$H^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning",
      "authors": "Lin Li, Jiahui Li, Jiaming Lei, Jun Xiao, Feifei Shao, Long Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20029",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1aa2e8e4d0ae946b7e977e841084f034f6f83372776d08e5a2bf43a04e81003_w640_q70.webp",
      "contributions": "",
      "summary": "$\\text\\{H\\}^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning",
      "mindmap": ""
    },
    {
      "title": "MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis",
      "authors": "Ziwei Qin, Xuhui Song, Deqing Huang, Na Qin, Jun Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20026",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7dc5f8098baa15ab99d6ec49af6ea9b561b6b787a33ac1ec6e365ad910668048_w640_q70.webp",
      "contributions": "",
      "summary": "MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis",
      "mindmap": ""
    },
    {
      "title": "Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva",
      "authors": "Nguyen Lam Phu Quy, Pham Phu Hoa, Tran Chi Nguyen, Dao Sy Duy Minh, Nguyen Hoang Minh Ngoc, Huynh Trung Kiet",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20042",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fb136c3c44d734ccd03ee7608dfc92d3612b466bfffc82e1d2efdfd79e5f161_w640_q70.webp",
      "contributions": "",
      "summary": "Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva",
      "mindmap": ""
    },
    {
      "title": "FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs",
      "authors": "Andreas Zinonos, Michał Stypułkowski, Antoni Bigata, Stavros Petridis, Maja Pantic, Nikita Drobyshev",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20033",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24426a84ed65236012992da21d9b902f793ee45db54516610ca6203c50494625_w640_q70.webp",
      "contributions": "",
      "summary": "FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs",
      "mindmap": ""
    },
    {
      "title": "Progressive Learned Image Compression for Machine Perception",
      "authors": "Jungwoo Kim, Jun-Hyuk Kim, Jong-Seok Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20070",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24993bfda343219d771658c8c9767048aec44f5b6784dcfb07bb075ba4304dd1_w640_q70.webp",
      "contributions": "",
      "summary": "Progressive Learned Image Compression for Machine Perception",
      "mindmap": ""
    },
    {
      "title": "Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach",
      "authors": "Hao Li, Fabian Deuser, Wenping Yin, Steffen Knoblauch, Wufan Zhao, Filip Biljecki, Yong Xue, Wei Huang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20056",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c37110ad3319d1434c17c04ae94a4dcfa94a278faf51360e359e1a063ce32e7_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach",
      "mindmap": ""
    },
    {
      "title": "LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs",
      "authors": "Haiyun Wei, Fan Lu, Yunwei Zhu, Zehan Zheng, Weiyi Xue, Lin Shao, Xudong Zhang, Ya Wu, Rong Fu, Guang Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20105",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a2dee7bb51f34affab3884be56b1bfb46b76c6a66e40400ed09830c5aabd0b0_w640_q70.webp",
      "contributions": "",
      "summary": "LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs",
      "mindmap": ""
    },
    {
      "title": "Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models",
      "authors": "Subrata Kumer Paula, Dewan Nafiul Islam Noora, Rakhi Rani Paula, Md. Ekramul Hamidb, Fahmid Al Faridc, Hezerul Abdul Karimd, Md. Maruf Al Hossain Princee, Abu Saleh Musa Miahb",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20104",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8b67a33af7843f8ee6ec84eb87891b7d12b57e4af5fa1f77bce5e76a6b92b0d_w640_q70.webp",
      "contributions": "",
      "summary": "Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models",
      "mindmap": ""
    },
    {
      "title": "Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts",
      "authors": "Jinyoung Choi, Youngchae Kwon, Injung Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20088",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1946d7c3329c37db07556d963544aad7dbfeda194c515e4debdc6e3754d8920_w640_q70.webp",
      "contributions": "",
      "summary": "Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts",
      "mindmap": ""
    },
    {
      "title": "Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection",
      "authors": "Alireza Moayedikia, Sattar Dorafshan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20113",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00b50254f658c253763f9df9bd57707873bdb0982863020f26c263961aa323d8_w640_q70.webp",
      "contributions": "",
      "summary": "Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection",
      "mindmap": ""
    },
    {
      "title": "milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion",
      "authors": "Niraj Prakash Kini, Shiau-Rung Tsai, Guan-Hsun Lin, Wen-Hsiao Peng, Ching-Wen Ma, Jenq-Neng Hwang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20128",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b40cd83392a8ac698c9e282aef2bd70809bcd4957965aa0c36506eb68c77ec30_w640_q70.webp",
      "contributions": "",
      "summary": "milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion",
      "mindmap": ""
    },
    {
      "title": "HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer",
      "authors": "Mohammad Helal Uddin, Liam Seymour, Sabur Baidya",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20120",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f965376a2586790dd27548936a060ee2437b3c5085b58331952dfddc1265b29f_w640_q70.webp",
      "contributions": "",
      "summary": "HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer",
      "mindmap": ""
    },
    {
      "title": "Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs",
      "authors": "Cyrus Vachha, Yixiao Kang, Zach Dive, Ashwat Chidambaram, Anik Gupta, Eunice Jun, Bjoern Hartmann",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20129",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8400f1033a93635b0a5b706c568585f557f04cf8533a2b77ce1c55f08e14bef6_w640_q70.webp",
      "contributions": "",
      "summary": "Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs",
      "mindmap": ""
    },
    {
      "title": "DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation",
      "authors": "Jingqi Tian, Yiheng Du, Haoji Zhang, Yuji Wang, Isaac Ning Lee, Xulong Bai, Tianrui Zhu, Jingxuan Niu, Yansong Tang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20117",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a290a5343ed508cf106c79abdb63608b4b4b69f2e0d42fc218565b45b8eb64f_w640_q70.webp",
      "contributions": "",
      "summary": "DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation",
      "mindmap": ""
    },
    {
      "title": "UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis",
      "authors": "Thanh-Tung Le, Tuan Pham, Tung Nguyen, Deying Kong, Xiaohui Xie, Stephan Mandt",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20107",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/668ef8c168df89e0d5190bbd1ac0a2d6de0affc034e4a50d8e050242fd138dd8_w640_q70.webp",
      "contributions": "",
      "summary": "UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis",
      "mindmap": ""
    },
    {
      "title": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models",
      "authors": "Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20145",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp",
      "contributions": "",
      "summary": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models",
      "mindmap": ""
    },
    {
      "title": "Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)",
      "authors": "Robert van de Ven, Trim Bresilla, Bram Nelissen, Ard Nieuwenhuizen, Eldert J. van Henten, Gert Kootstra",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20148",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0476ec59995b5f3259a9b2ab1456d67ddbbb750902c5acd3d684600b97c47598_w640_q70.webp",
      "contributions": "",
      "summary": "Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)",
      "mindmap": ""
    },
    {
      "title": "CoDi -- an exemplar-conditioned diffusion model for low-shot counting",
      "authors": "Grega Šuštar, Jer Pelhan, Alan Lukežič, Matej Kristan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20153",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19ee3eb6d09129b7fef5f1398fe3a279f203f7ff1a53a416574b63094eeefdf5_w640_q70.webp",
      "contributions": "",
      "summary": "CoDi -- an exemplar-conditioned diffusion model for low-shot counting",
      "mindmap": ""
    },
    {
      "title": "AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model",
      "authors": "Sofian Chaybouti, Sanath Narayan, Yasser Dahou, Phúc H. Lê Khac, Ankit Singh, Ngoc Dung Huynh, Wamiq Reyaz Para, Hilde Kuehne, Hakim Hacid",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20157",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61612d4352aae9f7b2e5272f0843ecd4da94205a817c26dac3655aa0b24e73b2_w640_q70.webp",
      "contributions": "",
      "summary": "AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model",
      "mindmap": ""
    },
    {
      "title": "Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark",
      "authors": "Hao Guo, Xugong Qin, Jun Jie Ou Yang, Peng Zhang, Gangyan Zeng, Yubo Li, Hailun Lin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20174",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a74295652803d99c4b0631ef38e9684d12032ddd9797f217033f356497ff647_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark",
      "mindmap": ""
    },
    {
      "title": "Generative Latent Coding for Ultra-Low Bitrate Image Compression",
      "authors": "Zhaoyang Jia, Jiahao Li, Bin Li, Houqiang Li, Yan Lu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20194",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e95ca206803e3acc79be8a04fea7c616d1d6d5dc840a97528ce68341f589c143_w640_q70.webp",
      "contributions": "",
      "summary": "Generative Latent Coding for Ultra-Low Bitrate Image Compression",
      "mindmap": ""
    },
    {
      "title": "LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation",
      "authors": "Xiangxuan Ren, Zhongdao Wang, Pin Tang, Guoqing Wang, Jilai Zheng, Chao Ma",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20217",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8799680440a14b4dded747c223ca374b82c834b527822c9fff101d2247c1d65d_w640_q70.webp",
      "contributions": "",
      "summary": "LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation",
      "mindmap": ""
    },
    {
      "title": "JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement",
      "authors": "Tao Ye, Hongbin Ren, Chongbing Zhang, Haoran Chen, Xiaosong Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20213",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c290a68b3d97be1a375bfedfbc89e4ad87f03c4cfd59b51b99adeb151186d5ed_w640_q70.webp",
      "contributions": "",
      "summary": "JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement",
      "mindmap": ""
    },
    {
      "title": "How I Met Your Bias: Investigating Bias Amplification in Diffusion Models",
      "authors": "Nathan Roos, Ekaterina Iakovleva, Ani Gjergji, Vito Paolo Pastore, Enzo Tartaglione",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20233",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c43be0c18a78b4304c027f45372c35a663531d4f9536a15c2b4ca0dbb155b2_w640_q70.webp",
      "contributions": "",
      "summary": "How I Met Your Bias: Investigating Bias Amplification in Diffusion Models",
      "mindmap": ""
    },
    {
      "title": "Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion",
      "authors": "Xuanyu Hu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20249",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1c6a732f9a0082b695f01f79b2bcc47f909daa0f9d50a6af4d86a633213b328_w640_q70.webp",
      "contributions": "",
      "summary": "Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion",
      "mindmap": ""
    },
    {
      "title": "IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing",
      "authors": "Oikantik Nath, Sahithi Kukkala, Mitesh Khapra, Ravi Kiran Sarvadevabhatla",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20236",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/027b9426177f33cb782197baba41e405dda8dcde86fe03cd16387612d6f69294_w640_q70.webp",
      "contributions": "",
      "summary": "IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing",
      "mindmap": ""
    },
    {
      "title": "BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation",
      "authors": "Jinghao Shi, Jianing Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20255",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2b3920e5370f3c041a4e84894da06ffa2a4f371a30e7ead753e72b679159943_w640_q70.webp",
      "contributions": "",
      "summary": "BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation",
      "mindmap": ""
    },
    {
      "title": "LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation",
      "authors": "Daniele Cardullo, Simone Teglia, Irene Amerini",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20257",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77044e7e1eb0cbde388f554efc56df1d6faee9e6b4ec8be9ca9c2664befbf208_w640_q70.webp",
      "contributions": "",
      "summary": "LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation",
      "mindmap": ""
    },
    {
      "title": "Degradation-Aware Metric Prompting for Hyperspectral Image Restoration",
      "authors": "Binfeng Wang, Di Wang, Haonan Guo, Ying Fu, Jing Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20251",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4f8150acede0f77c8c88a8f6399592fa53d83830252a9609aa77972be050f28_w640_q70.webp",
      "contributions": "",
      "summary": "Degradation-Aware Metric Prompting for Hyperspectral Image Restoration",
      "mindmap": ""
    },
    {
      "title": "$\\{D\\}^\\{3\\}$\\{ETOR\\}: $\\{D\\}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive $\\{D\\}$ebiasing for Weakly-Supervised Camouflaged Object $\\{D\\}$etection with Scribble Annotations",
      "authors": "Jiawei Ge, Jiuxin Cao, Xinyi Li, Xuelin Zhu, Chang Liu, Bo Liu, Chen Feng, Ioannis Patras",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20260",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee0606d3fe6e6ae30bbf9417baa7948dab878cfcc6b70e36031424c107aafb81_w640_q70.webp",
      "contributions": "",
      "summary": "$\\{D\\}^\\{3\\}$\\{ETOR\\}: $\\{D\\}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive $\\{D\\}$ebiasing for Weakly-Supervised Camouflaged Object $\\{D\\}$etection with Scribble Annotations",
      "mindmap": ""
    },
    {
      "title": "UbiQVision: Quantifying Uncertainty in XAI for Image Recognition",
      "authors": "Akshat Dubey, Aleksandar Anžel, Bahar İlgen, Georges Hattab",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20288",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6106b87e48429449f0b11c2765bdedb47797d7822e4b38ad3a9fe7f94b92dacb_w640_q70.webp",
      "contributions": "",
      "summary": "UbiQVision: Quantifying Uncertainty in XAI for Image Recognition",
      "mindmap": ""
    },
    {
      "title": "TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation",
      "authors": "Ji-Hoon Kim, Junseok Ahn, Doyeop Kwak, Joon Son Chung, Shinji Watanabe",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20296",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/217e6d95bfc0e38be8f5a7356d3e869c3b3a5b9d4daf05b6435c3c756df247c6_w640_q70.webp",
      "contributions": "",
      "summary": "TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation",
      "mindmap": ""
    },
    {
      "title": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System",
      "authors": "Zhongyu Xia, Wenhao Chen, Yongtao Wang, Ming-Hsuan Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20299",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0961ae49fbc925ad5eacb6602aeec6cfdfabae07b252a044cd181bdb5a47746_w640_q70.webp",
      "contributions": "",
      "summary": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System",
      "mindmap": ""
    },
    {
      "title": "The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection",
      "authors": "Qingdong He, Xueqin Chen, Yanjie Pan, Peng Tang, Pengcheng Xu, Zhenye Gan, Chengjie Wang, Xiaobin Hu, Jiangning Zhang, Yabiao Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20340",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce852f1d570d91487912f5d0abf0a93c49ad00eeecfdd840d9dbb9025ff8fd3a_w640_q70.webp",
      "contributions": "",
      "summary": "The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection",
      "mindmap": ""
    },
    {
      "title": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation",
      "authors": "V. Kovalev, A. Kuvshinov, A. Buzovkin, D. Pokidov, D. Timonin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20362",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9180090a8d8d701e63ea635b11cc6a39fed9f252167568980aefb8786a66ef43_w640_q70.webp",
      "contributions": "",
      "summary": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation",
      "mindmap": ""
    },
    {
      "title": "Field-Space Attention for Structure-Preserving Earth System Transformers",
      "authors": "Maximilian Witte, Johannes Meuer, Étienne Plésiat, Christopher Kadow",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20350",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8623c14b77fd771294e1b936a58143cff6a715df1b4a5026fe2d59708383e6e2_w640_q70.webp",
      "contributions": "",
      "summary": "Field-Space Attention for Structure-Preserving Earth System Transformers",
      "mindmap": ""
    },
    {
      "title": "Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge",
      "authors": "Marta Moscati, Ahmed Abdullah, Muhammad Saad Saeed, Shah Nawaz, Rohan Kumar Das, Muhammad Zaigham Zaheer, Junaid Mir, Muhammad Haroon Yousaf, Khalid Mahmood Malik, Markus Schedl",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20376",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/483debde320d1a401b91478496a4b53ca3f74d52718353845af8598a16bf6167_w640_q70.webp",
      "contributions": "",
      "summary": "Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge",
      "mindmap": ""
    },
    {
      "title": "SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images",
      "authors": "Linfei Li, Lin Zhang, Zhong Wang, Ying Shen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20377",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06dc77546a31f168b83f87c8efbfe002590b29ab252385b482db477a74d615ac_w640_q70.webp",
      "contributions": "",
      "summary": "SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images",
      "mindmap": ""
    },
    {
      "title": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems",
      "authors": "YuChe Hsu, AnJui Wang, TsaiChing Ni, YuanFu Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20387",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d9aa5293bf4ca8e839b122277f1484ffb43b6211d54741d7eb7f58312f5509_w640_q70.webp",
      "contributions": "",
      "summary": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems",
      "mindmap": ""
    },
    {
      "title": "Chain-of-Anomaly Thoughts with Large Vision-Language Models",
      "authors": "Pedro Domingos, João Pereira, Vasco Lopes, João Neves, David Semedo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20417",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e26e3aba92227ff2a24c227033ffa15e8e191cf437a0a97c819748f4dfb7c94_w640_q70.webp",
      "contributions": "",
      "summary": "Chain-of-Anomaly Thoughts with Large Vision-Language Models",
      "mindmap": ""
    },
    {
      "title": "Simplifying Multi-Task Architectures Through Task-Specific Normalization",
      "authors": "Mihai Suteu, Ovidiu Serban",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20420",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d24e6af533166dd44855079b97d7233c65878aa61e02267e65f4e306467d04b_w640_q70.webp",
      "contributions": "",
      "summary": "Simplifying Multi-Task Architectures Through Task-Specific Normalization",
      "mindmap": ""
    },
    {
      "title": "DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning",
      "authors": "Junho Yoon, Jaemo Jung, Hyunju Kim, Dongman Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20409",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d4c944d1aa99e38d69c83c388503646f62271c8bb649aafcafb57ad0ba7c7d0_w640_q70.webp",
      "contributions": "",
      "summary": "DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning",
      "mindmap": ""
    },
    {
      "title": "Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks",
      "authors": "Abdullah Al Shafi, Abdul Muntakim, Pintu Chandra Shill, Rowzatul Zannat, Abdullah Al-Amin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20431",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/905e502b77110f2fdb7bcfd863c906242998a2915d794e39c3da2377a2743392_w640_q70.webp",
      "contributions": "",
      "summary": "Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks",
      "mindmap": ""
    },
    {
      "title": "High Dimensional Data Decomposition for Anomaly Detection of Textured Images",
      "authors": "Ji Song, Xing Wang, Jianguo Wu, Xiaowei Yue",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20432",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1df48b800de53f9fff1712283209aa73147125a9725200492a8bcc4cd35e7182_w640_q70.webp",
      "contributions": "",
      "summary": "High Dimensional Data Decomposition for Anomaly Detection of Textured Images",
      "mindmap": ""
    },
    {
      "title": "Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding",
      "authors": "Anh Dao, Manh Tran, Yufei Zhang, Xiaoming Liu, Zijun Cui",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20451",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab892d1698a54ca5bea9ec4d990ec556b33cc4b339ac925ef5d18dba7b5dbae_w640_q70.webp",
      "contributions": "",
      "summary": "Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding",
      "mindmap": ""
    },
    {
      "title": "Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems",
      "authors": "James E. Gallagher, Edward J. Oughton, Jana Kosecka",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20487",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09ac139242f7c2d01b6739f6aa8ae3ee3439e687ec6c7b420e17bccf33e9de40_w640_q70.webp",
      "contributions": "",
      "summary": "Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems",
      "mindmap": ""
    },
    {
      "title": "SirenPose: Dynamic Scene Reconstruction via Geometric Supervision",
      "authors": "Kaitong Cai, Jensen Zhang, Jing Yang, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20531",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbf03f3fbe032434f49cece08cea9e779ea7ea57fa26c8f00eeed2e7c9080766_w640_q70.webp",
      "contributions": "",
      "summary": "SirenPose: Dynamic Scene Reconstruction via Geometric Supervision",
      "mindmap": ""
    },
    {
      "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
      "authors": "Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20557",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e481c21d437a92edaec37fae6af73e02495508b9597f95d16d784b230f3165c_w640_q70.webp",
      "contributions": "",
      "summary": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
      "mindmap": ""
    },
    {
      "title": "Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition",
      "authors": "Gorjan Radevski",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20501",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e4fc93af34bdcb98669d1a3c900d03900cb8c1359b89368e444c1c70848cdd4_w640_q70.webp",
      "contributions": "",
      "summary": "Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition",
      "mindmap": ""
    },
    {
      "title": "UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images",
      "authors": "Yiming Zhao, Yuanpeng Gao, Yuxuan Luo, Jiwei Duan, Shisong Lin, Longfei Xiong, Zhouhui Lian",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20479",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4093d97a2621866450d2ab0e47bbdaa622a6fe81e981843cdd087e11e4301902_w640_q70.webp",
      "contributions": "",
      "summary": "UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images",
      "mindmap": ""
    },
    {
      "title": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving",
      "authors": "Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20563",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp",
      "contributions": "",
      "summary": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving",
      "mindmap": ""
    },
    {
      "title": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models",
      "authors": "Kaitong Cai, Jusheng Zhang, Jing Yang, Yijia Fan, Pengtao Xie, Jian Wang, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20561",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72707d221b008970aee7befd580ee702951e338a32b995af62d9c893fe16e7e1_w640_q70.webp",
      "contributions": "",
      "summary": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models",
      "mindmap": ""
    },
    {
      "title": "AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment",
      "authors": "Anna Šárová Mikeštíková, Médéric Fourmy, Martin Cífka, Josef Sivic, Vladimir Petrik",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20538",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f6450c33e31646a32cf9129dad4fbbcc84ead81c36e23e134b1b0f9930b9db8_w640_q70.webp",
      "contributions": "",
      "summary": "AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment",
      "mindmap": ""
    },
    {
      "title": "Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios",
      "authors": "Mingwei Tang, Jiahao Nie, Guang Yang, Ziqing Cui, Jie Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20556",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7eb3ac652519f5f42206d482e71edb24ef458336500aad066aa021c53b988cca_w640_q70.webp",
      "contributions": "",
      "summary": "Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios",
      "mindmap": ""
    },
    {
      "title": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs",
      "authors": "Dhruv Anand, Ehsan Shareghi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20595",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb95f031dbfb3f7bfc348a6d3e77d5c3ae7e2408f06dd57529b77764de0ce0b7_w640_q70.webp",
      "contributions": "",
      "summary": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs",
      "mindmap": ""
    },
    {
      "title": "FedPOD: the deployable units of training for federated learning",
      "authors": "Daewoon Kim, Si Young Yie, Jae Sung Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20610",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e1f095bc0447c82283e16e3fb09acd3ae3773107b9096b3a06a1659a978e0_w640_q70.webp",
      "contributions": "",
      "summary": "FedPOD: the deployable units of training for federated learning",
      "mindmap": ""
    },
    {
      "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
      "authors": "Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20618",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp",
      "contributions": "",
      "summary": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
      "mindmap": ""
    },
    {
      "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
      "authors": "Xuanhua He, Tianyu Yang, Ke Cao, Ruiqi Wu, Cheng Meng, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Qifeng Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20615",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/507ac9f976cb24dc0dc72fb35cca194ceb4a25829fd305c6f45493a8a9531c76_w640_q70.webp",
      "contributions": "",
      "summary": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
      "mindmap": ""
    },
    {
      "title": "Repurposing Video Diffusion Transformers for Robust Point Tracking",
      "authors": "Soowon Son, Honggyu An, Chaehyun Kim, Hyunah Ko, Jisu Nam, Dahyun Chung, Siyoon Jin, Jung Yi, Jaewon Min, Junhwa Hur, Seungryong Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20606",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415d9c5c4a502667ccd87f6d4f24a9276a6e9f5e7f729b850e01ea33b4b6681a_w640_q70.webp",
      "contributions": "",
      "summary": "Repurposing Video Diffusion Transformers for Robust Point Tracking",
      "mindmap": ""
    },
    {
      "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
      "authors": "Yuxi Xiao, Longfei Li, Shen Yan, Xinhang Liu, Sida Peng, Yunchao Wei, Xiaowei Zhou, Bingyi Kang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20617",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c18a98475f6303ed360b86b5c0ec7ec6b4ab088ad0e70f925606628c827b46d_w640_q70.webp",
      "contributions": "",
      "summary": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
      "mindmap": ""
    },
    {
      "title": "SemanticGen: Video Generation in Semantic Space",
      "authors": "Jianhong Bai, Xiaoshi Wu, Xintao Wang, Fu Xiao, Yuanxing Zhang, Qinghe Wang, Xiaoyu Shi, Menghan Xia, Zuozhu Liu, Haoji Hu, Pengfei Wan, Kun Gai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20619",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77e0d53ce289ecacefa061dc5d124f7b98c80f5b9417f83e06a4ccaafb10e8a8_w640_q70.webp",
      "contributions": "",
      "summary": "SemanticGen: Video Generation in Semantic Space",
      "mindmap": ""
    },
    {
      "title": "SAM Audio: Segment Anything in Audio",
      "authors": "Bowen Shi, Andros Tjandra, John Hoffman, Helin Wang, Yi-Chiao Wu, Luya Gao, Julius Richter, Matt Le, Apoorv Vyas, Sanyuan Chen, Christoph Feichtenhofer, Piotr Dollár, Wei-Ning Hsu, Ann Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18099",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4568ea657a5ec8dc7d7944a03205d941e2bc9d054f9586938d2caa09e0b272e6_w640_q70.webp",
      "contributions": "",
      "summary": "SAM Audio: Segment Anything in Audio",
      "mindmap": ""
    }
  ]
}