{
  "label": "cs.CV",
  "slug": "cscv",
  "week": "20260105-20260111",
  "items": [
    {
      "title": "From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers",
      "authors": "Abolhassan Pishahang, Maryam Badiei",
      "institution": "Florida Atlantic University, North Carolina State University",
      "link": "https://arxiv.org/pdf/2601.00029",
      "code": null,
      "tags": [
        "diffusion models",
        "generative AI",
        "diffusion models",
        "architectural intelligence",
        "computational reasoning",
        "vernacular architecture"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d658a502216dc69f4f977616d5c09ec4155b48e6d602df132e1eac107fa169f4_w640_q70.webp",
      "contributions": "1. Proposes a three-stage prompting methodology (referential, adaptive, speculative) to evaluate generative AI's interpretation of vernacular architecture. 2. Develops a five-criteria evaluation framework (typology, materiality, environment, realism, cultural specificity) to assess AI-generated architectural outputs. 3. Identifies a boundary between visual resemblance and architectural reasoning in AI, introducing the concept of \"computational vernacular reasoning\" as an analytical framework.",
      "summary": "This study investigates how generative AI interprets the architectural intelligence of vernacular forms, using Iranian pigeon towers as a case study. It tests three diffusion models (Midjourney, DALL-E 3, Stable Diffusion XL) across different prompt stages and evaluates outputs using a custom framework. The results show that AI reliably reproduces geometric patterns but fails to grasp underlying material and climatic reasoning, highlighting a gap between visual generation and true architectural understanding.",
      "mindmap": "graph TB\n    Root[”From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers”] --> Problem[”核心问题/Problem: How does generative AI interpret the architectural intelligence embedded in vernacular forms?”]\n    Root --> Method[”主要方法/Method: Test three diffusion models across three prompt stages (referential, adaptive, speculative) using a five-criteria evaluation framework.”]\n    Root --> Results[”关键结果/Results: AI reproduces geometry but misreads material/climatic reasoning; reference aids realism but limits creativity.”]"
    },
    {
      "title": "It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models",
      "authors": "Anne Harrington, A. Sophia Koepke, Shyamgopal Karthik, Trevor Darrell, Alexei A. Efros",
      "institution": "UC Berkeley, University of Tübingen (Tübingen AI Center), Technical University of Munich (MCML)",
      "link": "https://arxiv.org/pdf/2601.00090",
      "code": null,
      "tags": [
        "diffusion models",
        "mode collapse",
        "noise optimization",
        "frequency characteristics",
        "text-to-image generation",
        "inference-time scaling"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90471f45dfeeb47a677f1a1f9518845473c93d7756e36367a947d6ebb4031c24_w640_q70.webp",
      "contributions": "1. Proposes a simple noise optimization objective to mitigate mode collapse in trained diffusion models while preserving model fidelity. 2. Analyzes the frequency characteristics of noise and demonstrates that alternative noise initializations with different frequency profiles can improve optimization and search. 3. Empirically shows that the proposed noise optimization method yields superior results in generation quality and variety compared to existing approaches.",
      "summary": "The paper addresses the problem of mode collapse in text-to-image diffusion models, where repeated sampling with the same prompt yields nearly identical images. The proposed solution is to optimize the initial noise input to the model to encourage diverse outputs, while also analyzing and leveraging the frequency characteristics of the noise for better performance. The experiments demonstrate that this noise optimization approach effectively recovers diversity without compromising the quality of the generated images.",
      "mindmap": "graph TB\n    Root(”It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models”) --> Problem(”核心问题/Problem: Mode collapse in text-to-image models”)\n    Root --> Method(”主要方法/Method: Noise optimization with frequency analysis”)\n    Root --> Results(”关键结果/Results: Improved generation diversity and quality”)"
    },
    {
      "title": "TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model",
      "authors": "Yabo Chen, Yuanzhi Liang, Jiepeng Wang, Tingxi Chen, Junfei Cheng, Zixiao Gu, Yuyang Huang, Zicheng Jiang, Wei Li, Tian Li, Weichen Li, Zuoxin Li, Guangce Liu, Jialun Liu, Junqi Liu, Haoyuan Wang, Qizhen Weng, Xuan'er Wu, Xunzhi Xiang, Xiaoyan Yang, Xin Zhang, Shiwen Zhang, Junyu Zhou, Chengcheng Zhou, Haibin Huang, Chi Zhang, Xuelong Li",
      "institution": "TeleWorld Team (Institution inferred from corresponding author's email domain: ieee.org, but specific institution not explicitly stated in provided content. Likely an academic or research lab.)",
      "link": "https://arxiv.org/pdf/2601.00051",
      "code": null,
      "tags": [
        "video generation",
        "4D world model",
        "autoregressive diffusion",
        "Macro-from-Micro Planning (MMPL)",
        "Distribution Matching Distillation (DMD)",
        "generation-reconstruction-guidance"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd22588d6a4323504eff6af53ad388e6f896282e336d6c9dc1169ff7797cfd75_w640_q70.webp",
      "contributions": "1. Proposes TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term memory in a closed-loop system. 2. Introduces a novel generation-reconstruction-guidance paradigm where a reconstructed 4D spatio-temporal representation guides subsequent generation for consistency. 3. Employs an autoregressive diffusion video model enhanced with Macro-from-Micro Planning (MMPL) and Distribution Matching Distillation (DMD) for efficient, long-horizon, real-time synthesis.",
      "summary": "This paper presents TeleWorld, a framework for building practical world models by integrating video generation and dynamic scene reconstruction into a closed-loop 4D system. It uses a novel generation-reconstruction-guidance paradigm and efficient planning/distillation techniques to achieve long-term consistency and real-time performance. The results demonstrate strong performance in world understanding and generation, advancing towards interactive, memory-enabled AI systems.",
      "mindmap": "graph TB\n        A[TeleWorld: 4D World Model<br/>TeleWorld: 4D世界模型] --> B[核心问题/Problem<br/>Video models lack real-time interaction,<br/>long-horizon consistency, and memory<br/>视频模型缺乏实时交互、<br/>长时一致性与记忆]\n        A --> C[主要方法/Method<br/>Generation-Reconstruction-Guidance paradigm<br/>with MMPL & DMD for real-time 4D synthesis<br/>生成-重建-引导范式<br/>使用MMPL与DMD实现实时4D合成]\n        A --> D[关键结果/Results<br/>Strong performance in static/dynamic<br/>understanding, consistency, and efficiency<br/>在静态/动态理解、<br/>一致性与效率上表现优异]"
    },
    {
      "title": "Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark",
      "authors": "Pan Wang, Yang Liu, Guile Wu, Eduardo R. Corral-Soto, Chengjie Huang, Binbin Xu, Dongfeng Bai, Xu Yan, Yuan Ren, Xingxin Chen, Yizhe Wu, Tao Huang, Wenjun Wan, Xin Wu, Pei Zhou, Xuyang Dai, Kangbo Lv, Hongbo Zhang, Yosef Fried, Aixue Ye, Bailan Feng, Zhenyu Chen, Zhen Li, Yingcong Chen, Yiyi Liao, Bingbing Liu",
      "institution": "Huawei Technologies, CUHK-Shenzhen, HKUST-GZ, Zhejiang University, Tsinghua University",
      "link": "https://arxiv.org/pdf/2601.00092",
      "code": "https://spatial4d-bench.github.io/spatial4d/",
      "tags": [
        "multimodal reasoning",
        "spatial intelligence",
        "multimodal large language models",
        "benchmark",
        "spatiotemporal reasoning",
        "evaluation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0097cea07eeada4f8c5abb88f65e48c3017620922cc4628bd067545dd73a10f_w640_q70.webp",
      "contributions": "1. Introduces Spatial4D-Bench, a large-scale benchmark with ~40,000 QA pairs for evaluating 4D spatial intelligence in MLLMs. 2. Systematically organizes 18 tasks into six cognitive categories for structured and comprehensive assessment. 3. Benchmarks state-of-the-art MLLMs, revealing their substantial limitations in 4D spatial reasoning.",
      "summary": "This paper introduces Spatial4D-Bench, a large-scale benchmark designed to comprehensively evaluate the 4D spatial reasoning abilities of Multimodal Large Language Models (MLLMs). The benchmark covers 18 tasks across six cognitive categories. The evaluation reveals that current MLLMs have significant limitations in achieving human-level 4D spatial intelligence.",
      "mindmap": "graph TB\n        A[SPATIAL4D-BENCH: A VERSATILE 4D SPATIAL INTELLIGENCE BENCHMARK] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[评估MLLMs的4D空间智能水平/Assess MLLMs' 4D spatial intelligence]\n        C --> C1[构建大规模多任务基准/Build large-scale multi-task benchmark]\n        C --> C2[覆盖18个任务，6个认知类别/Cover 18 tasks, 6 cognitive categories]\n        D --> D1[MLLMs在4D空间推理上存在显著局限/MLLMs show substantial limitations in 4D spatial reasoning]"
    },
    {
      "title": "A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data",
      "authors": "Hyunho Lee, Wenwen Li",
      "institution": "Arizona State University",
      "link": "https://arxiv.org/pdf/2601.00123",
      "code": null,
      "tags": [
        "multimodal image segmentation",
        "Synthetic Aperture Radar (SAR)",
        "Multispectral Imaging (MSI)",
        "Spatially Masked Adaptive Gated Network (SMAGNet)",
        "feature fusion",
        "missing data robustness"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/301db27cf6a0b833c578daa334eb3b7d3ec33f0947279912344e4d78cc3853d5_w640_q70.webp",
      "contributions": "1. Proposes SMAGNet, a novel multimodal deep learning model for post-flood water extent mapping that uses SAR as the primary input and adaptively integrates MSI data. 2. Introduces a method for handling incomplete or partially available MSI data, enhancing model applicability in real-world scenarios. 3. Demonstrates superior performance and robustness compared to other multimodal models, maintaining performance even when MSI data is completely missing.",
      "summary": "This paper addresses the challenge of integrating incomplete multispectral data with SAR imagery for post-flood water mapping. It proposes SMAGNet, a multimodal deep learning model that adaptively fuses features from both data sources. The model shows improved accuracy and robustness to missing data, making it more practical for real-world disaster response.",
      "mindmap": "graph TB\n        Root[”A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data”] --> Problem[”核心问题/Problem: How to adaptively integrate partially available MSI data with SAR for water mapping?”]\n        Root --> Method[”主要方法/Method: Propose SMAGNet, a multimodal model using SAR as primary input with adaptive feature fusion.”]\n        Root --> Results[”关键结果/Results: Outperforms other models; robust to missing MSI data; enhances real-world applicability.”]"
    },
    {
      "title": "Explicit Abstention Knobs for Predictable Reliability in Video Question Answering",
      "authors": "Jorge Ortiz",
      "institution": "Rutgers University",
      "link": "https://arxiv.org/pdf/2601.00138",
      "code": null,
      "tags": [
        "selective prediction",
        "selective prediction",
        "confidence-based abstention",
        "risk-coverage tradeoff",
        "distribution shift",
        "video question answering"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c77afc466868547720556984ed3dee075c707916f0173e73e7904851983d320b_w640_q70.webp",
      "contributions": "1. Demonstrates that confidence thresholding provides smooth, mechanistic control over error rates in-distribution for video QA. 2. Shows that this confidence-based control is not epistemic and fails under distribution shift (evidence degradation), as confidence does not decrease with reduced visual information. 3. Proposes the need for warrant-based selective prediction, where confidence is explicitly bounded by the supporting evidence.",
      "summary": "This paper investigates the reliability of confidence-based abstention for controlling error rates in video question answering using VLMs. It finds that while confidence thresholding works well in-distribution, it fails under distribution shift because the model's confidence does not properly reflect reduced evidence quality. The results motivate moving towards warrant-based selective prediction for more predictable reliability.",
      "mindmap": "graph TB\n        A[Explicit Abstention Knobs for Predictable Reliability in Video Question Answering] --> B[核心问题/Problem: Can confidence-based abstention provide reliable error rate control in video QA, especially under distribution shift?]\n        A --> C[主要方法/Method: Use confidence thresholding on a VLM (Gemini 2.0 Flash) evaluated on the NExT-QA dataset, testing under evidence degradation.]\n        A --> D[关键结果/Results: In-distribution control works, but confidence fails to decrease under shift, motivating warrant-based prediction.]"
    },
    {
      "title": "Compressed Map Priors for 3D Perception",
      "authors": "Brady Zhou, Philipp Krähenbühl",
      "institution": "UT Austin",
      "link": "https://arxiv.org/pdf/2601.00139",
      "code": null,
      "tags": [
        "3D object detection",
        "spatial priors",
        "binarized hashmap",
        "map compression",
        "nuScenes dataset",
        "end-to-end training"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32160e88a58c9af3e298b04ccdb16e8b3661883dc08a9dacc052f6a0dca3ed36_w640_q70.webp",
      "contributions": "1. Proposes Compressed Map Priors (CMP), a framework to learn spatial priors from historic traversals for 3D perception. 2. Introduces a highly efficient binarized hashmap representation requiring only 32KB/km², achieving a 20x memory reduction. 3. Demonstrates seamless integration into existing 3D perception architectures with minimal computational overhead, leading to consistent performance improvements on nuScenes.",
      "summary": "The paper addresses the problem that autonomous vehicle perception systems ignore prior knowledge from repeated traversals of the same area. It proposes Compressed Map Priors (CMP), a framework that learns and stores spatial priors in a highly compressed binarized hashmap. The method integrates easily into existing systems with little computational cost and significantly improves 3D object detection performance on the nuScenes dataset.",
      "mindmap": "graph TB\n        A[Compressed Map Priors for 3D Perception] --> B[核心问题/Problem: 自动驾驶系统忽略历史遍历的先验知识/Autonomous systems ignore prior knowledge from past traversals]\n        A --> C[主要方法/Method: 提出压缩地图先验(CMP)框架，使用二值化哈希图存储空间先验/Propose CMP framework using binarized hashmap for spatial priors]\n        A --> D[关键结果/Results: 内存减少20倍，计算开销小，3D检测性能显著提升/20x memory reduction, low overhead, significant 3D detection improvement]"
    },
    {
      "title": "Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection",
      "authors": "Lawrence Han",
      "institution": "Independent researcher (inferred from personal email domain)",
      "link": "https://arxiv.org/pdf/2601.00141",
      "code": "GitHub",
      "tags": [
        "image forensics / ai-generated content detection",
        "GLASS",
        "stratified sampling",
        "global-local attention",
        "high-resolution image detection",
        "vision transformer"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c2089b3b64b3853b86afa92498a4cd117de1fac49f0b90abfb199ea7602ca9d_w640_q70.webp",
      "contributions": "1. Proposes GLASS, a novel architecture that combines a globally resized view with multiple original-resolution local crops for AI-generated image detection. 2. Introduces a spatially stratified sampling method to efficiently select diverse, non-overlapping local regions from high-resolution images. 3. Demonstrates the integration and effectiveness of GLASS with various backbone models (ViT, ResNet, ConvNeXt), showing superior performance over standard transfer learning.",
      "summary": "This paper addresses the problem of losing fine-grained details when detecting AI-generated images due to standard downsampling. It proposes the GLASS architecture, which processes both a global resized view and multiple original-resolution local crops, aggregated via attention. Experiments show GLASS improves detection performance across different model backbones within practical computational limits.",
      "mindmap": "graph TB\n        A[Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 高分辨率AI生成图像检测中，下采样导致细粒度细节丢失/Fine-grained detail loss in high-resolution AI-generated image detection due to downsampling]\n        C[主要方法/Method: GLASS架构，结合全局视图与分层采样的原始分辨率局部裁剪，使用注意力聚合/GLASS architecture combining global view & stratified-sampled original-resolution local crops with attention aggregation]\n        D[关键结果/Results: 在多种骨干网络上超越标准迁移学习性能/Outperforms standard transfer learning across various backbone models]"
    },
    {
      "title": "Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions",
      "authors": "Kaiwen Zheng, Junchen Fu, Songpei Xu, Yaoqing He, Joemon M.Jose, Han Hu, Xuri Ge",
      "institution": "University of Glasgow, Shandong University, Institute of Computing Technology, Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2601.00156",
      "code": null,
      "tags": [
        "facial attribute analysis",
        "multi-attribute description",
        "facial action units",
        "vision-language model",
        "fine-tuning",
        "region-focal analysis"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3349e43a2ecc55b86da3398a05bd86f3d3f6b4908e47c1d46e4b116fd1bd901_w640_q70.webp",
      "contributions": "1. Introduces the novel problem (FaceFocalDesc) of generating multi-attribute natural language descriptions for arbitrarily selected face regions. 2. Constructs a new dataset with region-level annotations and natural language descriptions for this task. 3. Proposes the Focal-RegionFace model, a fine-tuned vision-language model that incrementally refines focus on localized features for interpretable multi-attribute analysis.",
      "summary": "This paper introduces a new task of generating fine-grained, multi-attribute descriptions for specific face regions. To address it, the authors create a new dataset and propose Focal-RegionFace, a model fine-tuned from Qwen2.5-VL that progressively focuses on local features. Experiments show the model achieves state-of-the-art performance on the new benchmark, demonstrating its effectiveness for region-focal facial analysis.",
      "mindmap": "graph TB\n        A[Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>生成任意选定面部区域的细粒度多属性描述<br>Generating multi-attribute descriptions for arbitrarily selected face regions]\n        C[主要方法/Method<br>基于Qwen2.5-VL的多阶段渐进式微调模型<br>Multi-stage progressive fine-tuning of Qwen2.5-VL]\n        D[关键结果/Results<br>在新基准上取得最佳性能<br>Achieves best performance on the new benchmark]"
    },
    {
      "title": "FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications",
      "authors": "Yehui Yang, Dalu Yang, Wenshuo Zhou, Fangxin Shang, Yifan Liu, Jie Ren, Haojun Fei, Qing Yang, Tao Chen",
      "institution": "Qifu Technology, Fudan University",
      "link": "https://arxiv.org/pdf/2601.00150",
      "code": null,
      "tags": [
        "multimodal benchmark",
        "financial credit",
        "multimodal AI",
        "robustness evaluation",
        "vision-language models",
        "privacy-compliant dataset"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a51bae803814f634858ded96030e47cb55c5330c7e8901ac175d3536cf4b4a9_w640_q70.webp",
      "contributions": "1. Introduces FCMBench-V1.0, a large-scale, privacy-compliant multimodal benchmark specifically for financial credit applications, featuring 18 document types, 4,043 images, and 8,446 QA samples. 2. Proposes a novel three-dimensional evaluation framework (Perception, Reasoning, Robustness) with credit-specific reasoning tasks and real-world artifact stress testing. 3. Designs a closed synthesis-capture pipeline to construct realistic yet privacy-safe samples, mitigating data leakage and enabling effective performance discrimination across 23 state-of-the-art VLMs.",
      "summary": "This paper introduces FCMBench, a new multimodal benchmark for evaluating AI models on financial credit document review tasks. The benchmark is built using a privacy-compliant synthesis-capture pipeline and tests models on perception, reasoning, and robustness. Experiments show that even top models like Gemini 3 Pro struggle with robustness, while the authors' domain-specific model, Qfin-VL-Instruct, achieves the best overall performance.",
      "mindmap": "graph TB\n        A[FCMBench: A Comprehensive Financial Credit Multimodal Benchmark] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[缺乏金融信贷领域专用多模态基准/Lack of domain-specific multimodal benchmark for financial credit]\n        C --> C1[构建隐私合规的合成-采集管道/Build privacy-compliant synthesis-capture pipeline]\n        C --> C2[设计三维评估框架/Design three-dimensional evaluation framework (Perception, Reasoning, Robustness)]\n        D --> D1[评估23个VLM/Evaluate 23 VLMs]\n        D --> D2[Qfin-VL-Instruct性能最佳/Qfin-VL-Instruct achieves top score]\n        D --> D3[鲁棒性挑战/Robustness remains a challenge]"
    },
    {
      "title": "Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework",
      "authors": "Moirangthem Tiken Singh, Manibhushan Yaikhom",
      "institution": "Dibrugarh University Institute of Engineering and Technology (DUIET), Dibrugarh University; Regional Institute of Medical Sciences",
      "link": "https://arxiv.org/pdf/2601.00192",
      "code": null,
      "tags": [
        "on-device ai",
        "hybrid feature engineering",
        "wavelet decomposition",
        "graph-theoretic descriptors",
        "linear separability",
        "model compression"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a662793cb6f13133cb7159786a9b30cdc33ffe10a9ca51a20ba5d501aa94a04_w640_q70.webp",
      "contributions": "1. A resource-efficient, data-centric framework that makes high-dimensional ECG data linearly separable through hybrid feature engineering. 2. A novel feature space integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors like PageRank centrality. 3. An ultra-lightweight model achieving state-of-the-art efficiency (8.54 KB, 0.46 µs latency) for real-time arrhythmia detection on edge devices.",
      "summary": "This paper proposes a resource-efficient framework for arrhythmia detection on edge devices by using hybrid feature engineering (wavelet and graph descriptors) to make ECG data linearly separable, enabling the use of ultra-lightweight linear classifiers. The resulting model achieves high accuracy (98.44%) with a very small footprint (8.54 KB) and low latency, offering significant efficiency gains over compressed deep learning models for IoMT applications.",
      "mindmap": "graph TB\n        A[”Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection<br>优化的混合特征工程用于资源高效的心律失常检测”] --> B\n        A --> C\n        A --> D\n        B[”Problem: Deep learning models are too heavy for edge devices.<br>核心问题: 深度学习模型在边缘设备上计算开销过大”]\n        C[”Method: Hybrid feature engineering (wavelet + graph) for linear separability.<br>主要方法: 混合特征工程（小波+图论）实现线性可分性”]\n        D[”Results: 98.44% accuracy, 8.54 KB model, 0.46 µs latency.<br>关键结果: 98.44% 准确率, 8.54 KB 模型, 0.46 µs 延迟”]"
    },
    {
      "title": "DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery",
      "authors": "Salma Gonzalez-Sabbagh, Antonio Robles-Kelly, Shang Gao",
      "institution": "Deakin University, The University of Adelaide",
      "link": "https://arxiv.org/pdf/2601.00194",
      "code": null,
      "tags": [
        "underwater image restoration",
        "conditional generative adversarial network",
        "hyperspectral imagery",
        "underwater image formation equation",
        "satellite imagery",
        "PRISMA"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be68e0cd098733ddad1ebb98756053749a127b9805026bd451900ff62bd75701_w640_q70.webp",
      "contributions": "1. Proposes DichroGAN, a novel cGAN architecture for restoring in-air seafloor colours from satellite imagery. 2. Introduces a two-step simultaneous training process with four generators to model atmospheric radiance and underwater light transmission based on the image formation equation. 3. Demonstrates competitive performance on satellite and underwater datasets using a compact dataset derived from PRISMA imagery.",
      "summary": "This paper addresses the challenge of restoring the true colours of the seafloor from satellite images, which are degraded by light absorption and scattering in water. The authors propose DichroGAN, a conditional GAN that uses a multi-generator architecture to estimate and remove underwater effects based on hyperspectral data. Experiments show the method achieves competitive results compared to state-of-the-art underwater restoration techniques.",
      "mindmap": "graph TB\n        Root[DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Recovering in-air seafloor colours from degraded satellite imagery due to light attenuation in water.]\n        Method[主要方法/Method: A cGAN (DichroGAN) with a two-step, four-generator architecture to estimate atmospheric radiance and underwater transmission.]\n        Results[关键结果/Results: Achieves competitive performance on satellite and underwater datasets.]"
    },
    {
      "title": "CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting",
      "authors": "Md Ahmed Al Muzaddid, William J. Beksi",
      "institution": "The University of Texas at Arlington",
      "link": "https://arxiv.org/pdf/2601.00207",
      "code": null,
      "tags": [
        "3D instance segmentation",
        "Neural Radiance Field (NeRF)",
        "3D instance segmentation",
        "crop counting",
        "mask consistency",
        "view synthesis"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47e945ab54ccf5cb00069822c1c6264667ea9ffbd12ce279cdcb2f58a39c38ec_w640_q70.webp",
      "contributions": "1. A novel framework for exact crop enumeration via 3D instance segmentation using multi-view images and NeRF. 2. Introduction of crop visibility and mask consistency scores to effectively segment instances in 3D. 3. Demonstration of consistent performance across diverse crops (cotton, apples, pears) without crop-specific parameter tuning and release of a new cotton plant dataset.",
      "summary": "This paper introduces CropNeRF, a framework for accurate crop counting in agriculture. It uses multi-view 2D images and instance masks to train a Neural Radiance Field (NeRF), incorporating novel visibility and consistency scores to perform 3D instance segmentation and count crops. The method shows superior counting performance across different crop types and releases a new dataset to advance research.",
      "mindmap": "graph TB\n        A[CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[户外遮挡与聚类导致计数困难/Outdoor occlusions and clustering make counting hard]\n        C --> C1[使用多视角图像与NeRF进行3D实例分割/Use multi-view images & NeRF for 3D instance segmentation]\n        C --> C2[引入可见性与掩码一致性分数/Introduce visibility & mask consistency scores]\n        D --> D1[在多种作物上实现准确计数/Achieve accurate counting on multiple crops]\n        D --> D2[性能优于现有方法/Outperforms state-of-the-art]\n        D --> D3[贡献棉花数据集/Contribute a cotton dataset]"
    },
    {
      "title": "IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation",
      "authors": "Han Liu, Yubo Fan, Hao Li, Dewei Hu, Daniel Moyer, Zhoubing Xu, Benoit M. Dawant, Ipek Oguz",
      "institution": "Vanderbilt University, Siemens Healthineers, Johnson & Johnson Innovative Medicine",
      "link": "https://arxiv.org/pdf/2601.00212",
      "code": "https://github.com/han-liu/IntraStyler",
      "tags": [
        "domain adaptation",
        "style synthesis",
        "contrastive learning",
        "unpaired image translation",
        "disentanglement"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a0b3a9f7f09f7deb3ee5eff095c1a8b11767f7eec62f9f366a11c0a0e7557f1_w640_q70.webp",
      "contributions": "1. Proposes IntraStyler, an exemplar-based style synthesis method for cross-modality domain adaptation that can capture diverse intra-domain styles without requiring prior knowledge of the variations. 2. Introduces a style encoder based on contrastive learning to discriminatively extract style-only features for guiding the synthesis. 3. Demonstrates the method's efficacy in controllable style synthesis and the benefits of diverse synthetic data for improving downstream segmentation performance on the CrossMoDA 2023 dataset.",
      "summary": "This paper addresses the under-explored issue of intra-domain variability in unsupervised domain adaptation by proposing IntraStyler, an exemplar-based method for synthesizing diverse target domain styles without prior knowledge. It uses a contrastive learning-based style encoder to extract style features and guide synthesis to match an exemplar's style. The method shows effective controllable style synthesis and improves downstream segmentation on a cross-modality medical imaging dataset.",
      "mindmap": "graph TB\n        A[IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法忽略域内多样性 / Prior methods under-explore intra-domain variability]\n        C --> C1[提出基于范例的风格合成 / Propose exemplar-based style synthesis]\n        C --> C2[使用对比学习风格编码器 / Use contrastive learning style encoder]\n        D --> D1[可控风格合成有效 / Controllable style synthesis is effective]\n        D --> D2[多样数据提升分割性能 / Diverse data improves segmentation]"
    },
    {
      "title": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing",
      "authors": "Xiaokun Sun, Zeyu Cai, Hao Tang, Ying Tai, Jian Yang, Zhenyu Zhang",
      "institution": "Nanjing University, Peking University",
      "link": "https://arxiv.org/pdf/2601.00204",
      "code": null,
      "tags": [
        "3D generation and morphing",
        "Structured Latent (SLAT)",
        "Morphing Cross-Attention (MCA)",
        "Temporal-Fused Self-Attention (TFSA)",
        "training-free",
        "3D morphing"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3f44aeae5b55667e0a256e60be3bfcd2021c03662cb86eb300de39c194e2ad1_w640_q70.webp",
      "contributions": "1. A training-free framework (MorphAny3D) that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. 2. The introduction of Morphing Cross-Attention (MCA) for structural coherence and Temporal-Fused Self-Attention (TFSA) for temporal consistency. 3. An orientation correction strategy to mitigate pose ambiguity, enabling state-of-the-art morphing even for challenging cross-category cases.",
      "summary": "The paper addresses the challenge of generating semantically consistent and temporally smooth 3D morphing sequences, especially across categories. It proposes MorphAny3D, a training-free framework that intelligently blends Structured Latent (SLAT) features within a 3D generator's attention mechanisms using novel MCA and TFSA modules. The method achieves state-of-the-art results and supports advanced applications like decoupled morphing and 3D style transfer.",
      "mindmap": "graph TB\n        A[MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing] --> B(核心问题/Problem: 3D Morphing Challenges)\n        A --> C(主要方法/Method: MorphAny3D Framework)\n        A --> D(关键结果/Results: State-of-the-Art Morphing)\n        B --> B1(”挑战: 语义一致性与时序平滑性 / Challenge: Semantic Consistency & Temporal Smoothness”)\n        B --> B2(”尤其跨类别 / Especially Cross-Category”)\n        C --> C1(”核心: 结构化潜在表示 / Core: Structured Latent (SLAT)”)\n        C --> C2(”训练无关 / Training-Free”)\n        C --> C3(”关键模块: MCA & TFSA / Key Modules: MCA & TFSA”)\n        D --> D1(”高质量变形序列 / High-Quality Morphing Sequences”)\n        D --> D2(”支持高级应用 / Supports Advanced Applications”)\n        D --> D3(”泛化至其他模型 / Generalizable to Other Models”)"
    },
    {
      "title": "From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning",
      "authors": "Omar Sharif, Eftekhar Hossain, Patrick Ng",
      "institution": "Dartmouth College, University of Central Florida",
      "link": "https://arxiv.org/pdf/2601.00215",
      "code": null,
      "tags": [
        "reinforcement learning",
        "reinforcement learning",
        "multimodal large language models",
        "visual reasoning",
        "group relative policy optimization",
        "reward functions"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa78fbb9d1620ad3674739f2e8cf9cfb6929637fa0f209ae3ee4c439ab5205c8_w640_q70.webp",
      "contributions": "1. Identified visual perception as the primary bottleneck for MLLMs in visual puzzle tasks, empirically validated by showing significant performance gains when images are converted to text. 2. Proposed a reward-driven RL framework using GRPO to incentivize longer, structured visual reasoning in open-source MLLMs without costly supervision. 3. Designed and evaluated six novel reward functions targeting different reasoning aspects like image understanding, thinking steps, and answer accuracy.",
      "summary": "This paper addresses the problem that multimodal large language models (MLLMs) generate reasoning chains that lack integration of visual information, limiting their performance on visual puzzles. The authors propose using reinforcement learning with specifically designed reward functions and Group Relative Policy Optimization (GRPO) to incentivize longer, visually-grounded reasoning. Their method improves the performance of the Qwen-2.5-VL-7B model by 5.56%, demonstrating consistent gains across different settings.",
      "mindmap": "graph TB\n        Root[”From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning”] --> Problem[”核心问题/Problem: MLLMs lack visual grounding in reasoning”]\n        Root --> Method[”主要方法/Method: RL with reward functions & GRPO”]\n        Root --> Results[”关键结果/Results: 5.56% improvement on Qwen-2.5-VL-7B”]"
    },
    {
      "title": "LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization",
      "authors": "Jie Li, Kwan-Yee K. Wong, Kai Han",
      "institution": "The University of Hong Kong",
      "link": "https://arxiv.org/pdf/2601.00222",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "vector quantization",
        "compositional codebook",
        "extrapolation-by-interpolation",
        "codebook collapse",
        "low-dimensional codevectors"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ceb99c2a3db9f368cc5ad30d8a605f970e9d0b2b86eaa1b9baba790fc3ef514_w640_q70.webp",
      "contributions": "1. Introduces a parameter-efficient, low-dimensional compositional codebook that treats codevectors as compositional units, expanding the solution space and enabling a more compact representation. 2. Incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance feature smoothing and detail preservation during quantization. 3. Functions as a plug-and-play module for existing VQ-based methods, achieving state-of-the-art performance with full codebook usage and avoidance of collapse.",
      "summary": "This paper proposes LooC, a new vector quantization method that uses a low-dimensional, compositional codebook to achieve high capacity with a compact size. It introduces a novel way to combine codevectors and a feature smoothing mechanism, leading to better performance and full codebook utilization. Extensive evaluations show LooC outperforms existing VQ methods with a significantly smaller codebook.",
      "mindmap": "graph TB\n        Root(”LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization”) --> Problem(”核心问题/Problem: Need for high-capacity yet compact VQ methods”)\n        Root --> Method(”主要方法/Method: Low-dimensional compositional codebook & extrapolation-by-interpolation”)\n        Root --> Results(”关键结果/Results: SOTA performance with smaller codebook, plug-and-play”)"
    },
    {
      "title": "Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions",
      "authors": "Aobo Li, Jinjian Wu, Yongxu Liu, Leida Li, Weisheng Dong",
      "institution": "Xidian University",
      "link": "https://arxiv.org/pdf/2601.00225",
      "code": "https://github.com/Li-aobo/SynDR-IQA",
      "tags": [
        "image quality assessment",
        "synthetic data distribution",
        "generalization error",
        "distribution reshaping",
        "upsampling",
        "downsampling"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b5234a13da45ce4c7f0208fb30b09fd593e69954422ef5f2b30a452dda5177_w640_q70.webp",
      "contributions": "1. Identified a key issue where models trained on synthetic IQA data learn discrete, clustered feature representations that hinder regression performance. 2. Proposed a novel framework, SynDR-IQA, which reshapes synthetic data distribution based on theoretical analysis of sample diversity and redundancy's impact on generalization. 3. Introduced two core strategies: distribution-aware diverse content upsampling to enhance visual diversity, and density-aware redundant cluster downsampling to balance sample density.",
      "summary": "This paper addresses the limited generalization of Blind Image Quality Assessment (BIQA) models trained on synthetic data by identifying that the problem stems from the clustered distribution of synthetic data features. The authors propose the SynDR-IQA framework, which reshapes the data distribution through strategic upsampling and downsampling to improve sample diversity and balance. Extensive experiments across multiple cross-dataset settings demonstrate the effectiveness of their method in enhancing model generalization.",
      "mindmap": "graph TB\n        A[Towards Syn-to-Real IQA<br>重塑合成数据分布] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Limited generalization of BIQA models trained on synthetic data due to clustered feature distributions]\n        C[主要方法/Method<br>SynDR-IQA framework reshapes data distribution via diverse content upsampling and redundant cluster downsampling]\n        D[关键结果/Results<br>Method improves generalization across synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic settings]"
    },
    {
      "title": "Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection",
      "authors": "Chao Yang, Haoyuan Zheng, Yue Ma",
      "institution": "Xi’an Jiaotong Liverpool University",
      "link": "https://arxiv.org/pdf/2601.00237",
      "code": null,
      "tags": [
        "object detection",
        "CycleGAN",
        "YOLOv8",
        "infrared image generation",
        "data augmentation",
        "PCB defect detection"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6328838143840b03119bcacf495d6f90fd2cfbf75f09c866a7d8dbf7d351c32_w640_q70.webp",
      "contributions": "1. Proposes a cross-modal data augmentation framework using CycleGAN for unpaired translation from visible-light to infrared images to address IR data scarcity. 2. Introduces a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a YOLOv8 detector. 3. Demonstrates that the method significantly improves detection performance under low-data conditions, approaching fully supervised benchmarks.",
      "summary": "This paper tackles the problem of scarce infrared data for PCB defect detection by using CycleGAN to generate synthetic infrared images from abundant visible-light images and training a YOLOv8 detector on this augmented dataset. The proposed method enhances feature learning with limited real data, significantly outperforming models trained only on real data and nearly matching the performance of fully supervised training.",
      "mindmap": "graph TB\n        Root[”Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection”] --> Problem[”核心问题/Problem: 红外数据稀缺 / IR Data Scarcity”]\n        Root --> Method[”主要方法/Method: CycleGAN跨模态生成 + YOLOv8检测 / CycleGAN Cross-modal Generation + YOLOv8 Detection”]\n        Root --> Results[”关键结果/Results: 性能显著提升，接近全监督基准 / Performance Significantly Improved, Approaches Fully Supervised Benchmark”]"
    },
    {
      "title": "Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture",
      "authors": "Anirudha Ghosh, Ritam Sarkar, Debaditya Barman",
      "institution": "Visva-Bharati, Uttar Banga Krishi Viswavidyalaya",
      "link": "https://arxiv.org/pdf/2601.00243",
      "code": null,
      "tags": [
        "few-shot object detection",
        "lightweight CNN",
        "prototypical meta-learning",
        "decision support system",
        "precision agriculture",
        "pest recognition"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b158d9a6a0af4814e3e06857f269146ab38d6c48de45aa34ede928a60f203123_w640_q70.webp",
      "contributions": "1. A lightweight framework for pest detection and pesticide recommendation designed for low-resource devices like smartphones and drones. 2. A Pest Detection Module using a compact CNN with prototypical meta-learning for accurate identification with few training samples. 3. A Pesticide Recommendation Module that integrates environmental factors (crop type, growth stage) to suggest safe and eco-friendly pesticides.",
      "summary": "This paper proposes a lightweight framework for precision agriculture that combines a few-shot pest detection model with a context-aware pesticide recommendation system. The method uses a compact CNN with prototypical learning for detection and a rule-based system incorporating environmental factors for recommendations. The framework achieves high accuracy with low computational cost, demonstrating potential for real-time use on resource-constrained devices.",
      "mindmap": "graph TB\n        A[Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[传统害虫管理成本高、效率低、不环保/Traditional pest management is costly, inefficient, and harmful]\n        C --> C1[轻量级框架/Lightweight Framework]\n        C1 --> C1_1[害虫检测模块: 轻量CNN+原型网络/Pest Detection: Lightweight CNN + Prototypical Network]\n        C1 --> C1_2[农药推荐模块: 环境感知决策系统/Pesticide Recommendation: Context-Aware Decision System]\n        D --> D1[高精度，低计算成本/High accuracy, low computational cost]\n        D --> D2[适用于智能手机和无人机/Suitable for smartphones and drones]"
    },
    {
      "title": "Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective",
      "authors": "Aly Sabri Abdalla, Vuk Marojevic",
      "institution": "Mississippi State University",
      "link": "https://arxiv.org/pdf/2601.00257",
      "code": null,
      "tags": [
        "wireless networking",
        "O-RAN",
        "UAV",
        "trajectory optimization",
        "RIC",
        "low-altitude economy"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74cae00fb3b22c64bae2b8ae14c12c3a5262a87f4a6954a5d6578c0a24172848_w640_q70.webp",
      "contributions": "1. Proposes an O-RAN-enabled framework for intelligent orchestration of Low-Altitude Economy (LAE) operations, integrating disaggregated RAN architecture with AI-driven RAN Intelligent Controllers (RICs). 2. Presents a semantic-aware rApp and a reinforcement learning-enabled xApp for closed-loop, context-aware trajectory planning of UAV swarms. 3. Surveys available UAV testbeds for LAE research and identifies critical research challenges and standardization needs for future deployments.",
      "summary": "This paper addresses the challenge of orchestrating UAV missions in complex, signal-constrained environments for the Low-Altitude Economy (LAE). It proposes a novel framework that leverages the Open Radio Access Network (O-RAN) architecture, using AI-driven controllers (RICs) and specialized apps (rApp/xApp) for semantic-aware, real-time trajectory planning. The work concludes by evaluating the framework's feasibility and outlining future research and standardization directions for scalable LAE deployments.",
      "mindmap": "graph TB\n        Root[”Next Generation Intelligent Low-Altitude Economy Deployments: The O-RAN Perspective”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem: Lack of real-time, resilient orchestration for UAVs in complex environments”] --> P1[”子问题/Sub-Problem: Absence of AI-integrated, context-aware control for LAE”]\n        Method[”主要方法/Method: O-RAN-enabled LAE framework with AI-driven RICs”] --> M1[”组件/Component: Semantic-aware rApp (terrain interpreter)”]\n        Method --> M2[”组件/Component: RL-enabled xApp (trajectory planner)”]\n        Results[”关键结果/Results: Framework enables closed-loop, AI-optimized LAE operations”] --> R1[”评估/Evaluation: Feasibility and performance analysis presented”]\n        Results --> R2[”展望/Outlook: Research challenges and standardization needs surveyed”]"
    },
    {
      "title": "TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models",
      "authors": "Kohei Yamamoto, Tomohiro Kikuchi",
      "institution": "Jichi Medical University",
      "link": "https://arxiv.org/pdf/2601.00260",
      "code": null,
      "tags": [
        "medical image analysis",
        "3D-CT",
        "vision-language model",
        "organ separation",
        "contrastive learning",
        "zero-shot classification"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63f8af2a1f2287af4b6bfea2831333ab4c9aa61e972f8e6a82db66ceefc12ccc_w640_q70.webp",
      "contributions": "1. Proposes TotalFM, a 3D-CT vision foundation model based on an organ-separated learning framework to balance computational efficiency and representation capability. 2. Introduces an automated pipeline for creating organ volume and finding-sentence pairs using segmentation and LLM-based report processing. 3. Demonstrates superior zero-shot performance in organ-wise and finding-wise lesion classification compared to baselines like CT-CLIP and Merlin.",
      "summary": "This paper proposes TotalFM, a 3D-CT vision foundation model that uses an organ-separated framework and combines self-supervised pre-training with contrastive learning to efficiently align volumetric images with text. It shows strong zero-shot classification performance on clinical tasks, outperforming existing models, and demonstrates the framework's effectiveness for practical 3D-CT model implementation.",
      "mindmap": "graph TB\n        A[TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models] --> B(核心问题/Problem: 3D-CT基础模型训练的计算成本高/High computational cost for training 3D-CT foundation models)\n        A --> C(主要方法/Method: 器官分离框架，结合自监督预训练和对比学习/Organ-separated framework, combining self-supervised pre-training and contrastive learning)\n        A --> D(关键结果/Results: 在零样本器官和病灶分类任务中性能优越/Superior performance in zero-shot organ-wise and finding-wise lesion classification tasks)"
    },
    {
      "title": "S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding",
      "authors": "He Wang, Longteng Guo, Pengkang Huo, Xuanxu Lin, Yichen Yuan, Jie Jiang, Jing Liu",
      "institution": "Institute of Automation, Chinese Academy of Sciences; University of Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2601.00264",
      "code": "https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign",
      "tags": [
        "multimodal dataset",
        "multimodal learning",
        "image-text alignment",
        "semantic enhancement",
        "Qwen-VL",
        "CLIP score"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a274c2d0052fe8c7fd0017adcdc91d037137c89f27fc6fe6181e5428d3a20036_w640_q70.webp",
      "contributions": "1. Introduces S1-MMAlign, a large-scale, multi-disciplinary dataset with over 15.5 million scientific image-text pairs from 2.5 million papers. 2. Proposes an AI-ready semantic enhancement pipeline using the Qwen-VL model to recaption images by synthesizing context from abstracts and citations, improving alignment. 3. Demonstrates significant data quality improvement via technical validation, showing reduced semantic ambiguity and an 18.21% increase in CLIP scores for image-text alignment.",
      "summary": "This paper introduces S1-MMAlign, a large-scale multimodal dataset for scientific figure-text understanding, addressing the semantic gap between complex scientific imagery and sparse textual descriptions. It proposes a semantic enhancement pipeline using the Qwen-VL model to recaption images by integrating context from paper abstracts and citations, which significantly improves image-text alignment as validated by CLIP scores. The dataset serves as a foundational resource for advancing scientific reasoning and cross-modal understanding in AI for Science.",
      "mindmap": "graph TB\n        A[S1-MMAlign: 大规模跨学科数据集<br>S1-MMAlign: Large-Scale, Multi-Disciplinary Dataset] --> B(核心问题/Problem: 科学图像与文本语义鸿沟<br>Scientific Figure-Text Semantic Gap)\n        A --> C(主要方法/Method: 基于Qwen-VL的语义增强管道<br>Qwen-VL-based Semantic Enhancement Pipeline)\n        A --> D(关键结果/Results: 图像-文本对齐提升18.21%<br>18.21% Improvement in Image-Text Alignment)"
    },
    {
      "title": "ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching",
      "authors": "Yi Sun, Xinhao Zhong, Hongyan Li, Yimin Zhou, Junhao Li, Bin Chen, Xuan Wang",
      "institution": "Harbin Institute of Technology, Shenzhen; Tsinghua Shenzhen International Graduate School, Tsinghua University; Peng Cheng Laboratory",
      "link": "https://arxiv.org/pdf/2601.00267",
      "code": null,
      "tags": [
        "diffusion models",
        "concept erasure",
        "activation patching",
        "training-free",
        "text-to-image",
        "adversarial robustness"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d1b9f0e8dbea10b4d4a0767c89d80cc2225788088b0a6b57818b98fc389dd914_w640_q70.webp",
      "contributions": "1. Proposes a novel training-free paradigm for concept erasure in diffusion models, eliminating the need for data-intensive fine-tuning. 2. Introduces a method that identifies and patches activation differences via prompt-pair analysis to precisely remove target concepts. 3. Demonstrates state-of-the-art performance across multiple erasure tasks while preserving model capability and showing robustness against adversarial attacks.",
      "summary": "This paper proposes ActErase, a training-free method for erasing sensitive concepts from text-to-image diffusion models. It works by identifying and patching activation differences during inference, avoiding costly fine-tuning. The method achieves strong erasure performance, maintains general generation quality, and is robust to attacks.",
      "mindmap": "graph TB\n        A[ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[扩散模型生成不当内容 / Diffusion models generate inappropriate content]\n        B --> B2[现有擦除方法依赖昂贵微调 / Existing erasure relies on costly fine-tuning]\n        C --> C1[识别激活差异区域 / Identify activation difference regions]\n        C --> C2[提取并动态替换目标激活 / Extract and dynamically replace target activations]\n        D --> D1[SOTA擦除性能 / SOTA erasure performance]\n        D --> D2[保持生成能力 / Preserves generative capability]\n        D --> D3[对抗攻击鲁棒性 / Robust against adversarial attacks]"
    },
    {
      "title": "FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering",
      "authors": "Chaodong Tong, Qi Zhang, Chen Li, Lei Jiang, Yanbing Liu",
      "institution": "Institute of Information Engineering, Chinese Academy of Sciences (CAS); School of Cyber Security, University of CAS; China Industrial Control Systems Cyber Emergency Response Team; China Electronics Standardization Institute",
      "link": "https://arxiv.org/pdf/2601.00269",
      "code": null,
      "tags": [
        "multi-modal inference",
        "hallucination detection",
        "vision-language models",
        "uncertainty estimation",
        "model-driven learning",
        "LLM-as-a-Judge"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0bbbea5cd6fb55e2cc155e1b226cd59e138d25b8ec98a141d833613135b7cd_w640_q70.webp",
      "contributions": "1. Proposes FaithSCAN, a lightweight network for VQA hallucination detection that fuses rich internal signals from VLMs (token-level uncertainty, visual representations, cross-modal alignment) using branch-wise evidence encoding and uncertainty-aware attention. 2. Extends the LLM-as-a-Judge paradigm to VQA to automatically generate low-cost, model-dependent supervision signals for training, eliminating the need for expensive human annotation. 3. Provides an in-depth analysis showing hallucinations stem from systematic variations in internal states across visual perception, cross-modal reasoning, and language decoding, offering new insights into multimodal hallucination causes.",
      "summary": "This paper addresses the problem of detecting faithfulness hallucinations in Visual Question Answering (VQA), where models give fluent but visually ungrounded answers. It proposes FaithSCAN, a model-driven method that detects hallucinations in a single pass by exploiting and fusing internal signals from the vision-language model, and uses an automated strategy based on LLM-as-a-Judge for low-cost supervision. Experiments show FaithSCAN outperforms existing methods in both effectiveness and efficiency, and the analysis provides new insights into the internal causes of hallucinations.",
      "mindmap": "graph TB\n        A[FaithSCAN: Faithful VQA Hallucination Detection] --> B[核心问题/Problem: VQA模型产生流畅但视觉无根据的答案/Faithfulness Hallucinations in VQA]\n        A --> C[主要方法/Method: 利用VLM内部信号与不确定性感知融合/Exploit VLM Internal Signals & Uncertainty-Aware Fusion]\n        A --> D[关键结果/Results: 高效且优于现有方法/More Effective & Efficient than Prior Methods]"
    },
    {
      "title": "Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification",
      "authors": "Chi Ding, Junxiao Xue, Xinyi Yin, Shi Chen, Yunyun Shi, Yiduo Wang, Fengjian Xue, Xuecheng Wu",
      "institution": "Zhejiang Lab, Zhengzhou University, Xi'an Jiaotong University",
      "link": "https://arxiv.org/pdf/2601.00278",
      "code": null,
      "tags": [
        "long-tailed classification",
        "uncertainty estimation",
        "evidential deep learning",
        "remote sensing",
        "long-tailed distribution",
        "adaptive label smoothing"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88fd0f3a7e430d011bd0f213783194e9b67472a37625e97a47a9d515f2c442af_w640_q70.webp",
      "contributions": "1. Proposes a model-agnostic framework (DUAL) that disentangles prediction uncertainty into epistemic and aleatoric types to address long-tailed classification. 2. Introduces epistemic uncertainty to guide a reweighting strategy for hard-to-learn tail samples. 3. Leverages aleatoric uncertainty to quantify data ambiguity and employs an adaptive label smoothing mechanism to suppress noise.",
      "summary": "This paper addresses the challenge of distinguishing hard-to-learn samples from noisy ones in long-tailed remote sensing image classification. The authors propose DUAL, an uncertainty-aware framework that uses epistemic uncertainty to reweight tail samples and aleatoric uncertainty to apply adaptive label smoothing for noise suppression. Experiments show the framework outperforms strong baselines and generalizes across different datasets and backbones.",
      "mindmap": "graph TB\n        Root[”Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification”]\n        Root --> Problem[”核心问题/Problem: Long-tailed distributions in remote sensing; need to disentangle hard tail samples from noisy ones.”]\n        Root --> Method[”主要方法/Method: Propose DUAL framework using Evidential Deep Learning to separate epistemic (for reweighting) and aleatoric (for label smoothing) uncertainty.”]\n        Root --> Results[”关键结果/Results: Outperforms baselines (TGN, SADE); effective and generalizable across datasets and backbones.”]"
    },
    {
      "title": "SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting",
      "authors": "Jun-Jee Chao, Volkan Isler",
      "institution": "University of Minnesota, The University of Texas at Austin",
      "link": "https://arxiv.org/pdf/2601.00285",
      "code": null,
      "tags": [
        "4D reconstruction",
        "Gaussian Splatting",
        "Sparse View",
        "Skeleton-Driven",
        "Deformation Field",
        "Motion Interpolation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa4c8f4c46b1bfef8ae3be7aac4ec172ccaf47a8d9e0c9d1fd9369f8155ae0d8_w640_q70.webp",
      "contributions": "1. A framework (SV-GS) for 4D reconstruction from sparse observations in both time and viewpoint, using a skeleton-driven Gaussian Splatting approach. 2. A novel skeleton-driven deformation field with a time-dependent joint pose estimator and a separate fine-grained deformation module, enabling smooth motion interpolation. 3. Demonstrating that the method's initial static reconstruction input can be replaced by a diffusion-based generative prior, enhancing practical applicability.",
      "summary": "This paper tackles the challenging problem of 4D reconstruction from sparse observations in time and viewpoint. It proposes SV-GS, a method that uses a skeleton-driven deformation field with Gaussian Splatting to simultaneously estimate motion and geometry. The approach outperforms existing methods under sparse settings and shows practical potential by relaxing input requirements with generative priors.",
      "mindmap": "graph TB\n        Root(”SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting”) --> Problem(”核心问题/Problem: 4D reconstruction from sparse observations in time and viewpoint”)\n        Root --> Method(”主要方法/Method: Skeleton-driven deformation field with Gaussian Splatting”)\n        Root --> Results(”关键结果/Results: Outperforms baselines under sparse views; comparable to dense methods with fewer frames; works with diffusion priors”)"
    },
    {
      "title": "Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies",
      "authors": "Ali Anaissi, Ali Braytee, Weidong Huang, Junaid Akram, Alaa Farhat, Jie Hua",
      "institution": "University of Technology Sydney, University of Sydney, Shaoyang University",
      "link": "https://arxiv.org/pdf/2601.00286",
      "code": null,
      "tags": [
        "medical image classification",
        "Swin Transformer",
        "BatchFormer",
        "Focal Loss",
        "ReduceLROnPlateau",
        "ISIC2019"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b53f6f4ea6d1249b026a76d397de3fca0db67dcfee5584653cd543f2ac0bacf9_w640_q70.webp",
      "contributions": "1. Developed a deep learning model based on the Swin Transformer architecture for automated differential diagnosis of skin diseases. 2. Applied targeted data augmentation and imbalance-aware strategies (e.g., BatchFormer, Focal Loss) to handle class imbalance in medical image datasets. 3. Achieved a high prediction accuracy of 87.71% on the ISIC2019 dataset, demonstrating the model's potential as a clinical support tool.",
      "summary": "This paper addresses the limited access to dermatologists by developing a deep learning model for automated skin disease diagnosis. The method uses a Swin Transformer architecture pretrained on public datasets and employs imbalance-aware strategies like BatchFormer and Focal Loss to improve classification on the ISIC2019 dataset. The model achieved 87.71% accuracy, showing promise as a diagnostic aid for clinicians and patients.",
      "mindmap": "graph TB\n        A[Towards Automated Differential Diagnosis of Skin Diseases<br>皮肤疾病自动鉴别诊断] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Limited dermatologist access & need for diagnostic tools<br>皮肤科医生资源有限，需要诊断工具]\n        C[主要方法/Method<br>Deep learning (Swin Transformer) with imbalance-aware strategies<br>深度学习（Swin Transformer）与不平衡感知策略]\n        D[关键结果/Results<br>87.71% accuracy on ISIC2019 dataset<br>在ISIC2019数据集上达到87.71%准确率]"
    },
    {
      "title": "TimeColor: Flexible Reference Colorization via Temporal Concatenation",
      "authors": "Bryan Constantine Sadihin, Yihao Meng, Michael Hua Wang, Matteo Jiahao Chen, Hang Su",
      "institution": "Tsinghua University, HKUST",
      "link": "https://arxiv.org/pdf/2601.00296",
      "code": "https://bconstantine.github.io/TimeColor/",
      "tags": [
        "video colorization",
        "diffusion models",
        "temporal concatenation",
        "spatiotemporal correspondence-masked attention",
        "modality-disjoint RoPE indexing",
        "sketch-based colorization"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f086badbc2b843aa255965a7a32f6586803b7dd5e735b89850a7948f9825d28c_w640_q70.webp",
      "contributions": "1. Proposes a method to support heterogeneous, variable-count references for video colorization via temporal concatenation of reference latents, keeping model parameters fixed. 2. Introduces spatiotemporal correspondence-masked attention and modality-disjoint RoPE indexing to enforce subject-reference binding and prevent shortcutting and palette leakage. 3. Demonstrates improved color fidelity, identity consistency, and temporal stability over prior baselines on the SAKUGA-42M dataset under single- and multi-reference protocols.",
      "summary": "The paper addresses the limitation of existing video colorization models that rely on a single reference frame, which restricts the use of diverse references like character sheets. It proposes TimeColor, a sketch-based video colorization model that uses temporal concatenation to incorporate multiple references and novel attention mechanisms to bind subjects to references, improving color consistency and stability. Experiments show TimeColor outperforms prior methods in color fidelity and temporal coherence.",
      "mindmap": "graph TB\n        A[TimeColor: Flexible Reference Colorization via Temporal Concatenation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法仅支持单参考帧/Existing methods condition on single reference]\n        B --> B2[忽略其他参考源/Ignore other references (e.g., character sheets)]\n        C --> C1[时间拼接参考隐变量/Temporally concatenate reference latents]\n        C --> C2[时空对应掩码注意力/Spatiotemporal correspondence-masked attention]\n        C --> C3[模态分离RoPE索引/Modality-disjoint RoPE indexing]\n        D --> D1[提升颜色保真度/Improves color fidelity]\n        D --> D2[提升身份一致性/Improves identity consistency]\n        D --> D3[提升时间稳定性/Improves temporal stability]"
    },
    {
      "title": "VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning",
      "authors": "Anns Ijaz, Muhammad Azeem Javed",
      "institution": "University of Management and Technology",
      "link": "https://arxiv.org/pdf/2601.00307",
      "code": null,
      "tags": [
        "person re-identification",
        "feature fusion",
        "alpha-divergence loss",
        "dynamic multi-task learning",
        "semantic clustering",
        "computational efficiency"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a75221926fa101494a89575b6ea3a1c780209910ea62ecf484286140685d3de7_w640_q70.webp",
      "contributions": "1. A multi-scale feature fusion method with automatic attention that fuses ResNet50 stages without parallel paths. 2. A semantic clustering technique using rule-based pseudo-labeling for anatomical body partitioning. 3. A dynamic weight averaging technique and the use of the FIDI loss function for balanced multi-task learning and improved metric learning.",
      "summary": "This paper proposes VisNet, an efficient person re-identification model that combines multi-scale feature fusion, semantic clustering, and dynamic multi-task learning with an alpha-divergence loss to achieve a good balance between accuracy and computational cost. It achieves 87.05% Rank-1 and 77.65% mAP on Market-1501 with only 32.41M parameters and 4.601 GFLOPs. The work demonstrates a practical approach for real-time deployment in resource-constrained environments like surveillance and mobile applications.",
      "mindmap": "graph TB\n        A[VisNet: Efficient Person Re-Identification] --> B[核心问题/Problem: Accuracy vs. Computational Cost Trade-off]\n        A --> C[主要方法/Method: Feature Fusion, Semantic Clustering, Dynamic Multi-Task Learning, α-Divergence Loss]\n        A --> D[关键结果/Results: 87.05% Rank-1, 77.65% mAP, 4.601 GFLOPs]"
    },
    {
      "title": "ReMA: A Training-Free Plug-and-Play Mixing Augmentation for Video Behavior Recognition",
      "authors": "Feng-Qi Cui, Jinyang Huang, Sirui Zhao, Jinglong Guo, Qifan Cai, Xin Yan, Zhi Liu",
      "institution": "University of Science and Technology of China, Hefei University of Technology",
      "link": "https://arxiv.org/pdf/2601.00311",
      "code": null,
      "tags": [
        "video behavior recognition",
        "data augmentation",
        "representation-aware mixing",
        "spatiotemporal coherence",
        "plug-and-play",
        "motion-aware masking"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5aff87765b4798f78236528165f9ca3ca4ce0f7dc233c968a5c9d457b62260e0_w640_q70.webp",
      "contributions": "1. Proposes a novel plug-and-play, training-free augmentation strategy (ReMA) that formulates video mixing as a controlled replacement process to expand representations while preserving class-conditional stability. 2. Introduces a Representation Alignment Mechanism (RAM) to perform structured intra-class mixing under distributional constraints, suppressing irrelevant intra-class drift. 3. Introduces a Dynamic Selection Mechanism (DSM) to generate motion-aware spatiotemporal masks, localizing perturbations away from discrimination-sensitive regions to promote temporal coherence.",
      "summary": "The paper identifies that current perturbation-driven video data augmentation methods introduce uncontrolled variations that harm representation stability. To solve this, it proposes ReMA, a training-free plug-and-play method that uses representation alignment and dynamic spatiotemporal masking to control how and where mixing is applied. Experiments show ReMA consistently improves model generalization and robustness across various video behavior benchmarks.",
      "mindmap": "graph TB\n        A[ReMA: 视频行为识别的即插即用混合增强] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有视频增强引入非判别性变化，破坏类内分布和时序一致性]\n        C --> C1[表示对齐机制 / Representation Alignment Mechanism (RAM)]\n        C --> C2[动态选择机制 / Dynamic Selection Mechanism (DSM)]\n        D --> D1[提升泛化性和鲁棒性 / Improves generalization and robustness]"
    },
    {
      "title": "Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation",
      "authors": "Siyan Fang, Long Peng, Yuntao Wang, Ruonan Wei, Yuehuan Wang",
      "institution": "Huazhong University of Science and Technology, University of Science and Technology of China",
      "link": "https://arxiv.org/pdf/2601.00322",
      "code": "https://github.com/fashyon/DMDNet",
      "tags": [
        "image reflection separation",
        "Mamba",
        "state-space model",
        "depth-aware",
        "memory expert",
        "nighttime dataset"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de4a78d4e76dde376001e66b330ec905d0b5a81cc699f37a8614a613788ea9e0_w640_q70.webp",
      "contributions": "1. Proposed the Depth-Memory Decoupling Network (DMDNet) with Depth-Aware Scanning (DAScan) and a Depth-Synergized State-Space Model (DS-SSM) to guide Mamba for better layer disentanglement. 2. Introduced a Memory Expert Compensation Module (MECM) that leverages historical knowledge to provide layer-specific compensation. 3. Constructed a new Nighttime Image Reflection Separation (NightIRS) dataset to address the lack of data for nighttime scenes.",
      "summary": "This paper addresses the challenging problem of separating reflection and transmission layers from a single blended image, especially in low-contrast nighttime scenes. The authors propose DMDNet, a novel network that integrates depth-aware guidance into a Mamba-based state-space model and utilizes a memory expert module for compensation. The method, evaluated on a newly created nighttime dataset, is shown to outperform existing state-of-the-art approaches for both daytime and nighttime reflection separation.",
      "mindmap": "graph TB\n        A[Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[单图像信息有限，夜间分离困难/Single image info limited, severe at night]\n        C --> C1[深度记忆解耦网络 DMDNet/Depth-Memory Decoupling Network DMDNet]\n        C1 --> C2[深度感知扫描 DAScan/Depth-Aware Scanning DAScan]\n        C1 --> C3[深度协同状态空间模型 DS-SSM/Depth-Synergized State-Space Model DS-SSM]\n        C1 --> C4[记忆专家补偿模块 MECM/Memory Expert Compensation Module MECM]\n        C --> C5[构建夜间数据集 NightIRS/Construct NightIRS dataset]\n        D --> D1[在白天和夜间均超越SOTA/Outperforms SOTA in daytime & nighttime]"
    },
    {
      "title": "HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection",
      "authors": "Naiqi Zhang, Chuancheng Shi, Jingtong Dou, Wenhua Wu, Fei Shen, Jianhua Cao",
      "institution": "Tianjin University of Science and Technology, The University of Sydney, National University of Singapore",
      "link": "https://arxiv.org/pdf/2601.00327",
      "code": null,
      "tags": [
        "anomaly detection",
        "frequency-guided learning",
        "structural attention",
        "semantic consistency",
        "dual-branch framework",
        "CLIP"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e849537a51e67abdf003473c95f5885e48c7364ea926f9e5a9d19c230dd572ec_w640_q70.webp",
      "contributions": "1. Proposed HarmoniAD, a frequency-guided dual-branch framework that decouples features into high- and low-frequency paths to balance structural detail and semantic context. 2. Introduced two novel modules: a Fine-grained Structural Attention Module (FSAM) for enhancing textures/edges in the high-frequency branch, and a Global Structural Context Module (GSCM) for capturing long-range dependencies in the low-frequency branch. 3. Adopted a multi-class joint training strategy and demonstrated state-of-the-art performance on multiple benchmark datasets (MVTec-AD, VisA, BTAD).",
      "summary": "The paper addresses the trade-off between structural sensitivity and semantic consistency in anomaly detection. It proposes HarmoniAD, a framework that uses a CLIP encoder and frequency-domain decoupling into dual branches with specialized attention modules to model fine details and global context. Experiments show the method achieves state-of-the-art performance with improved sensitivity and robustness on industrial inspection datasets.",
      "mindmap": "graph TB\n        A[HarmoniAD: 异常检测/HarmoniAD: Anomaly Detection] --> B[核心问题/Problem: 结构-语义权衡/Structure-Semantics Trade-off]\n        A --> C[主要方法/Method: 频率引导双分支框架/Frequency-Guided Dual-Branch Framework]\n        A --> D[关键结果/Results: SOTA性能/SOTA Performance]\n        B --> B1[结构模型噪声敏感/Structure Models: Noise-Sensitive]\n        B --> B2[语义模型忽略细节/Semantic Models: Miss Details]\n        C --> C1[高频分支: FSAM模块/High-Freq Branch: FSAM]\n        C --> C2[低频分支: GSCM模块/Low-Freq Branch: GSCM]\n        D --> D1[数据集: MVTec-AD, VisA, BTAD/Datasets: MVTec-AD, VisA, BTAD]\n        D --> D2[结果: 高敏感性与鲁棒性/Results: High Sensitivity & Robustness]"
    },
    {
      "title": "Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion",
      "authors": "Yingzhi Tang, Qijian Zhang, Junhui Hou",
      "institution": "City University of Hong Kong, Tencent Games",
      "link": "https://arxiv.org/pdf/2601.00328",
      "code": "https://github.com/haiantyz/JGA-LBD",
      "tags": [
        "3D human reconstruction",
        "bridge diffusion",
        "latent representation",
        "3D Gaussian splatting",
        "variational autoencoder",
        "unified modeling"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/119bb1243f3a69d2681ae4bd998269231fc8133680dd32c7363d04c1c7d67b82_w640_q70.webp",
      "contributions": "1. Proposes a unified framework (JGA-LBD) that jointly models human geometry and appearance in a single latent space, addressing inconsistency issues in decoupled pipelines. 2. Introduces a method to unify heterogeneous input conditions (e.g., depth, SMPL) into 3D Gaussian representations and compresses them into a shared latent space via a sparse VAE. 3. Formulates the generation process using a specialized bridge diffusion model that infers missing components from a partially observed latent code, enabling high-fidelity reconstruction.",
      "summary": "This paper tackles the challenging problem of reconstructing consistent 3D digital humans from a single RGB image. It proposes JGA-LBD, a novel framework that unifies geometry and appearance modeling into a joint latent representation and uses bridge diffusion for generation. Experiments show the method outperforms state-of-the-art approaches in both geometry and appearance quality.",
      "mindmap": "graph TB\n        A[Joint Geometry–Appearance Human Reconstruction] --> B(核心问题/Problem: 单视图RGB图像重建高保真3D数字人/Single-view RGB to high-fidelity 3D human)\n        A --> C(主要方法/Method: JGA-LBD框架/JGA-LBD Framework)\n        C --> C1(统一潜在空间/Unified Latent Space)\n        C1 --> C1a(3D高斯表示/3D Gaussian Representation)\n        C1 --> C1b(稀疏变分自编码器/Sparse VAE)\n        C --> C2(桥接扩散/Bridge Diffusion)\n        C2 --> C2a(部分观测/Partial Observation)\n        C2 --> C2b(推断缺失组件/Infer Missing Components)\n        A --> D(关键结果/Results: 超越SOTA的几何与外观质量/Superior geometry & appearance quality vs. SOTA)"
    },
    {
      "title": "Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation",
      "authors": "Bruce Mugizi, Sudi Murindanyi, Olivia Nakacwa, Andrew Katumba",
      "institution": "Makerere University",
      "link": "https://arxiv.org/pdf/2601.00344",
      "code": null,
      "tags": [
        "object detection",
        "YOLOv8",
        "CNN",
        "Transformer",
        "License Plate Recognition",
        "Speed Estimation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e06adb8e70928bbdb99a1135f4b52eac80ef595baa23279552a5a3e3f811f32_w640_q70.webp",
      "contributions": "1. Developed a real-time intelligent traffic surveillance system specifically tailored for resource-constrained environments like Uganda, integrating vehicle detection, license plate recognition, and speed estimation. 2. Achieved high performance with a 97.9% mAP for license plate detection using YOLOv8 and a low 1.79% character error rate for recognition using a Transformer model, significantly improving over a CNN baseline. 3. Implemented a practical enforcement pipeline by creating a database to correlate vehicle data with user information and enabling automated ticket issuance via SMS using the Africa's Talking API.",
      "summary": "This paper proposes a real-time computer vision system to tackle speeding in developing countries by detecting vehicles, recognizing license plates, and estimating speed. The method uses YOLOv8 for detection and a Transformer for character recognition, achieving high accuracy and integrating with an SMS-based ticketing system. The work demonstrates a practical, automated solution for traffic enforcement in resource-limited settings.",
      "mindmap": "graph TB\n        Root(”Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”超速导致事故/Speeding causes accidents”)\n        Problem --> P2(”发展中国家缺乏基础设施/Developing countries lack infrastructure”)\n        Method --> M1(”车辆检测/Vehicle Detection”)\n        Method --> M2(”车牌识别/License Plate Recognition”)\n        Method --> M3(”速度估计/Speed Estimation”)\n        M1 --> M1_1(”使用YOLOv8/Using YOLOv8”)\n        M2 --> M2_1(”使用CNN和Transformer/Using CNN & Transformer”)\n        M3 --> M3_1(”使用ROI/Using ROI”)\n        Results --> R1(”高检测精度/High detection mAP: 97.9%”)\n        Results --> R2(”低字符错误率/Low CER: 1.79%”)\n        Results --> R3(”速度估计误差小/Speed error: ±10 km/h”)\n        Results --> R4(”自动罚单系统/Automated ticketing via SMS”)"
    },
    {
      "title": "OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning",
      "authors": "Liuxiang Qiu, Hui Da, Yuzhen Niu, Tiesong Zhao, Yang Cao, Zheng-Jun Zha",
      "institution": "Fuzhou University, University of Science and Technology of China",
      "link": "https://arxiv.org/pdf/2601.00352",
      "code": null,
      "tags": [
        "multimodal learning",
        "visual-tactile learning",
        "domain generalization",
        "fractional Fourier transform",
        "multimodal fusion",
        "hierarchical tree structure"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/953f6158cc5edbde2cdc2e32bb128da6cc19e6d671bc9af71f082f73051f598e_w640_q70.webp",
      "contributions": "1. Formulates a new task, Single Domain Generalization for Multimodal Visual-Tactile Learning (SDG-VTL), to address modality and domain gaps in VTL. 2. Proposes the OmniVaT framework, which introduces a Multimodal Fractional Fourier Adapter (MFFA) to map visual and tactile embeddings into a unified embedding-frequency space, mitigating the modality gap. 3. Incorporates a Discrete Tree Generation (DTG) module to obtain diverse and reliable multimodal fractional representations via a hierarchical tree structure, enhancing adaptability to unseen domain shifts.",
      "summary": "This paper addresses the challenges of modality discrepancies and domain gaps in visual-tactile learning by proposing a new task (SDG-VTL) and a framework called OmniVaT. OmniVaT uses a Multimodal Fractional Fourier Adapter to unify visual and tactile features and a Discrete Tree Generation module to enhance generalization to unseen domains. Experiments show that OmniVaT achieves superior cross-domain generalization performance on the SDG-VTL task.",
      "mindmap": "graph TB\n        A[OmniVaT: 单域泛化多模态视觉触觉学习<br>OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[模态差异与域差距<br>Modality & Domain Gaps]\n        B1 --> B2[新任务: SDG-VTL<br>New Task: SDG-VTL]\n        C --> C1[多模态分数傅里叶适配器 MFFA<br>Multimodal Fractional Fourier Adapter]\n        C --> C2[离散树生成模块 DTG<br>Discrete Tree Generation Module]\n        D --> D1[卓越的跨域泛化性能<br>Superior Cross-Domain Generalization]"
    },
    {
      "title": "Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers",
      "authors": "Söhnke Benedikt Fischedick, Daniel Seichter, Benedict Stephan, Robin Schmidt, Horst-Michael Gross",
      "institution": "Technische Universität Ilmenau",
      "link": "https://arxiv.org/pdf/2601.00359",
      "code": null,
      "tags": [
        "semantic segmentation",
        "dense visual embeddings",
        "knowledge distillation",
        "RGB-D transformer",
        "real-time inference",
        "Alpha-CLIP"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5eb4801e4f56b1c232cf2b5828f41733ec3576e1fe32d825febbfdde2e108d6c_w640_q70.webp",
      "contributions": "1. Proposes DVEFormer, an efficient RGB-D Transformer-based model for predicting dense, text-aligned visual embeddings via knowledge distillation from Alpha-CLIP. 2. Enables flexible, open-vocabulary scene understanding (e.g., text-based querying) beyond fixed-class semantic segmentation while maintaining the ability to perform classical segmentation. 3. Demonstrates real-time performance on embedded hardware (NVIDIA Jetson AGX Orin), making it suitable for mobile robotics applications like 3D mapping.",
      "summary": "The paper addresses the need for robots to have a detailed, open-vocabulary understanding of indoor environments. It proposes DVEFormer, an efficient model that uses an RGB-D Transformer and knowledge distillation from Alpha-CLIP to predict dense visual embeddings, enabling both classical segmentation and flexible text-based querying. The method achieves competitive performance and real-time inference speeds, making it a practical drop-in replacement for traditional segmentation in mobile robotics.",
      "mindmap": "graph TB\n        A[Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[机器人需要全面、开放词汇的场景理解/Robots need comprehensive, open-vocabulary scene understanding]\n        C --> C1[使用RGB-D Transformer和知识蒸馏/Use RGB-D Transformer and Knowledge Distillation]\n        C1 --> C2[从Alpha-CLIP教师模型学习密集视觉嵌入/Learn Dense Visual Embeddings from Alpha-CLIP teacher]\n        D --> D1[实现实时性能与有竞争力的结果/Achieves real-time performance & competitive results]\n        D --> D2[支持文本查询和3D映射/Enables text-based querying & 3D mapping]"
    },
    {
      "title": "Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting",
      "authors": "Aarya Sumuk",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2601.00368",
      "code": null,
      "tags": [
        "3D reconstruction and inpainting",
        "voxel diffusion",
        "mask-conditioned inpainting",
        "joint geometry-color completion",
        "3D U-Net",
        "cultural heritage"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e164a07c4c12a1e90fe17c8219fb61313f5381a2f2f4e0a77f870006403576f8_w640_q70.webp",
      "contributions": "1. A lightweight two-stage framework for joint geometry and color inpainting of 3D objects, separating damage localization from reconstruction. 2. A mask-conditioned volumetric diffusion model (3D U-Net) that directly inpaints voxel grids to reconstruct occupancy and color while preserving intact regions. 3. A composite training objective combining occupancy reconstruction, masked color reconstruction, and perceptual regularization for joint prediction.",
      "summary": "This paper proposes a two-stage method for digitally restoring damaged 3D cultural heritage artifacts. It first predicts a 3D damage mask from 2D RGB slices, then uses a mask-conditioned voxel diffusion model to jointly inpaint the missing geometry and color. The results show this approach produces more complete and coherent reconstructions than symmetry-based baselines.",
      "mindmap": "graph TB\n        Root[”Mask-Conditioned Voxel Diffusion<br>掩码条件体素扩散”] --> Problem[”Joint Geometry & Color Inpainting of Damaged 3D Objects<br>受损3D物体的几何与颜色联合修复”]\n        Root --> Method[”Two-Stage Framework: Mask Prediction + Mask-Conditioned Voxel Diffusion<br>两阶段框架：掩码预测 + 掩码条件体素扩散”]\n        Root --> Results[”More Complete & Coherent Reconstructions vs. Baselines<br>相比基线方法更完整、更连贯的重建结果”]"
    },
    {
      "title": "BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition",
      "authors": "Seungyeon Cho, Tae-kyun Kim",
      "institution": "Imperial College London (Inferred from author Tae-Kyun Kim's affiliation)",
      "link": "https://arxiv.org/pdf/2601.00369",
      "code": null,
      "tags": [
        "skeleton-based action recognition",
        "probabilistic fusion",
        "multi-modal integration",
        "reliability-aware learning",
        "noisy-or",
        "cross-modal ensemble"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceb23f64a9170fda547d5e2c6b35431a9988bf0c41f827b0bb8299b07dce413b_w640_q70.webp",
      "contributions": "1. A calibration-free preprocessing pipeline that learns directly from native skeleton coordinates, removing canonical-space transformations. 2. A probabilistic Noisy-OR fusion mechanism for reliability-aware dual-stream learning without explicit confidence supervision. 3. An intra- to cross-modal ensemble that couples four skeleton modalities with RGB representations in a unified framework.",
      "summary": "This paper addresses the limitation of body-centric skeleton action recognition by proposing BHaRNet, a probabilistic dual-stream framework that integrates body and hand modalities for fine-grained recognition. The method introduces reliability-aware learning and a unified cross-modal ensemble of skeleton and RGB data. It demonstrates improved performance and robustness across multiple benchmarks, including a new hand-centric dataset.",
      "mindmap": "graph TB\n        A[BHaRNet: 细粒度骨架动作识别 / Fine-grained Skeleton Action Recognition] --> B[核心问题/Problem: 现有方法忽略手部细微动作 / Existing methods neglect subtle hand articulations]\n        A --> C[主要方法/Method: 概率双流框架 / Probabilistic Dual-Stream Framework]\n        C --> C1[校准无关预处理 / Calibration-Free Preprocessing]\n        C --> C2[噪声或概率融合 / Probabilistic Noisy-OR Fusion]\n        C --> C3[跨模态集成 / Intra- to Cross-Modal Ensemble]\n        A --> D[关键结果/Results: 多基准测试性能提升 / Performance gains across multiple benchmarks & 噪声下鲁棒性 / Robustness under noise]"
    },
    {
      "title": "RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection",
      "authors": "Tao Wu, Qing Xu, Xiangjian He, Oakleigh Weekes, James Brown, Wenting Duan",
      "institution": "University of Nottingham Ningbo China, University of Lincoln",
      "link": "https://arxiv.org/pdf/2601.00398",
      "code": "https://github.com/xq141839/RoLID-11K",
      "tags": [
        "object detection",
        "small-object detection",
        "dashcam dataset",
        "roadside litter",
        "long-tail distribution",
        "transformer detectors"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07af957156a6b1fe042100c136a3a47ff653381303e2f5f88a5c4dfabde172c3_w640_q70.webp",
      "contributions": "1. Introduces RoLID-11K, the first large-scale dataset for roadside litter detection from dashcam footage, featuring over 11k annotated images with extreme small-object and long-tail characteristics. 2. Provides a comprehensive benchmark of modern object detectors, including transformer and YOLO models, on this challenging task. 3. Establishes a new benchmark for extreme small-object detection in dynamic driving scenes to support scalable, low-cost litter monitoring systems.",
      "summary": "This paper introduces RoLID-11K, a new dataset for detecting small roadside litter from dashcam videos, and benchmarks various modern object detectors on it. The results show that transformer-based models like CO-DETR achieve the best localization accuracy, while real-time models are limited by coarse feature hierarchies. The dataset aims to facilitate the development of scalable systems for automated roadside litter monitoring.",
      "mindmap": "graph TB\n        A[RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection] --> B1\n        A --> B2\n        A --> B3\n        B1[核心问题/Problem: 路边垃圾监测困难，现有数据集不适用于行车记录仪场景/Roadside litter monitoring is challenging, existing datasets are unsuitable for dashcam footage]\n        B2[主要方法/Method: 提出首个大规模行车记录仪路边垃圾检测数据集并提出基准测试/Propose the first large-scale dashcam roadside litter dataset and benchmark]\n        B3[关键结果/Results: CO-DETR等Transformer模型定位最准，实时模型受限/CO-DETR and related transformers achieve best accuracy, real-time models are constrained]"
    },
    {
      "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
      "authors": "Yuxue Yang, Lue Fan, Ziqi Shi, Junran Peng, Feng Wang, Zhaoxiang Zhang",
      "institution": "NLPR & MAIS, CASIA; CreateAI",
      "link": "https://arxiv.org/pdf/2601.00393",
      "code": "https://neoverse-4d.github.io",
      "tags": [
        "4D reconstruction and generation",
        "4D Gaussian Splatting",
        "monocular video",
        "feed-forward reconstruction",
        "novel view synthesis",
        "world model"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e49e322200509b4252a5a149b2bdd18f2f3e7138a6cc52255fbd8ace6f79697b_w640_q70.webp",
      "contributions": "1. A scalable 4D world model (NeoVerse) built from diverse in-the-wild monocular videos, eliminating the need for expensive multi-view data or cumbersome pre-processing. 2. A pose-free, feed-forward 4D reconstruction method using 4D Gaussian Splatting (4DGS). 3. An online monocular degradation pattern simulation technique to enable high-quality, coherent novel-trajectory video generation.",
      "summary": "This paper proposes NeoVerse, a scalable 4D world model that performs feed-forward 4D reconstruction from monocular videos and generates novel-viewpoint videos. It addresses scalability limitations of prior methods by not requiring multi-view data or complex pre-processing. The model achieves state-of-the-art performance on standard benchmarks for reconstruction and generation.",
      "mindmap": "graph TB\n        A[NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[当前4D世界模型的可扩展性受限/Scalability limitation in current 4D world models]\n        B1 --> B2[依赖昂贵多视角数据或复杂预处理/Reliance on expensive multi-view data or cumbersome pre-processing]\n        C --> C1[基于单目视频的可扩展流程/Scalable pipeline using monocular videos]\n        C1 --> C2[免姿态前馈4D重建/Pose-free feed-forward 4D reconstruction]\n        C1 --> C3[在线单目退化模式模拟/Online monocular degradation pattern simulation]\n        D --> D1[实现最先进的性能/Achieves state-of-the-art performance]\n        D1 --> D2[在标准重建与生成基准上/On standard reconstruction & generation benchmarks]"
    },
    {
      "title": "ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis",
      "authors": "Tyler Ward, Abdullah Imran",
      "institution": "University of Kentucky",
      "link": "https://arxiv.org/pdf/2601.00416",
      "code": "https://github.com/tbwa233/ABFR-KAN",
      "tags": [
        "medical image analysis",
        "Kolmogorov-Arnold Network (KAN)",
        "Functional Connectivity (FC)",
        "Transformer",
        "Autism Spectrum Disorder (ASD)",
        "ABIDE I"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51e3b468fa0a409dc2ba718a14a375622d6eec61468b3a97ab00e67542b75241_w640_q70.webp",
      "contributions": "1. Proposes ABFR-KAN, a novel transformer-based classification network that integrates advanced brain function representation components with Kolmogorov-Arnold Networks (KANs) to address limitations of atlas-based functional connectivity analysis. 2. Introduces a method designed to mitigate structural bias, improve anatomical conformity, and enhance the reliability of functional connectivity estimation for brain disorder diagnosis. 3. Demonstrates through extensive experiments, including cross-site evaluation and ablation studies on the ABIDE I dataset, that the proposed model consistently outperforms state-of-the-art baselines in autism spectrum disorder (ASD) classification.",
      "summary": "The paper addresses the issues of selection bias and lack of subject specificity in traditional atlas-based functional connectivity (FC) analysis for brain disorders. It proposes ABFR-KAN, a transformer-based classification network that incorporates novel brain function representation components and Kolmogorov-Arnold Networks (KANs) to improve FC estimation. Experiments on the ABIDE I dataset show that ABFR-KAN outperforms state-of-the-art methods in classifying autism spectrum disorder (ASD).",
      "mindmap": "graph TB\n        A[ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis] --> B[核心问题/Problem: Atlas-based parcellation leads to selection bias and lacks subject specificity in functional connectivity analysis.]\n        A --> C[主要方法/Method: Proposes a transformer-based network (ABFR-KAN) with advanced brain function representation and Kolmogorov-Arnold Networks (KANs).]\n        A --> D[关键结果/Results: Outperforms state-of-the-art baselines in ASD classification on the ABIDE I dataset.]"
    },
    {
      "title": "Robust Assembly Progress Estimation via Deep Metric Learning",
      "authors": "Kazuma Miura, Sarthak Pathak, Kazunori Umeda",
      "institution": "Chuo University",
      "link": "https://arxiv.org/pdf/2601.00422",
      "code": null,
      "tags": [
        "metric learning",
        "assembly progress estimation",
        "deep metric learning",
        "quadruplet loss",
        "anomaly detection",
        "small-scale dataset"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b36c1f1235c837f4744e7bc1a15ac0006eccde3fb6d1ab8e95980b3b73e5027_w640_q70.webp",
      "contributions": "1. Proposed a robust system for assembly progress estimation that handles occlusion and minimal visual changes using a small-scale dataset. 2. Introduced a Quadruplet Loss-based learning approach specifically designed for anomaly images. 3. Developed a custom data loader that strategically selects training samples to enhance estimation accuracy.",
      "summary": "This paper addresses the challenge of misclassification in assembly progress estimation when visual changes between tasks are subtle. It proposes Anomaly Quadruplet-Net, which uses a Quadruplet Loss and a custom data loader for robust learning. The method outperforms prior work, improving accuracy by 1.3% and reducing adjacent task misclassification by 1.9% on a desktop PC assembly dataset.",
      "mindmap": "graph TB\n        A[Robust Assembly Progress Estimation via Deep Metric Learning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 装配任务中视觉变化细微导致误分类/Misclassification due to subtle visual changes in assembly tasks]\n        C[主要方法/Method: 基于四元组损失和自定义数据加载器的异常检测网络/Anomaly Quadruplet-Net with Quadruplet Loss & custom data loader]\n        D[关键结果/Results: 在PC装配数据集上准确率提升1.3%，相邻任务误分类减少1.9%/1.3% accuracy improvement & 1.9% reduction in adjacent task misclassification on PC dataset]"
    },
    {
      "title": "Deep Delta Learning",
      "authors": "Yifan Zhang, Yifeng Liu, Mengdi Wang, Quanquan Gu",
      "institution": "Princeton University, University of California, Los Angeles",
      "link": "https://arxiv.org/pdf/2601.00417",
      "code": "https://github.com/yifanzhang-pro/deep-delta-learning",
      "tags": [
        "neural network architecture",
        "residual networks",
        "geometric transformation",
        "spectral analysis",
        "rank-1 perturbation",
        "dynamic gating"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75e9151cd8b5ec94dcb21276489c4319c73f4c1a7295a6715a6c2a36d36f9a9b_w640_q70.webp",
      "contributions": "1. Introduces Deep Delta Learning (DDL), a novel architecture that generalizes residual connections with a learnable, data-dependent geometric transformation called the Delta Operator. 2. Provides a spectral analysis of the Delta Operator, showing it can dynamically interpolate between identity mapping, orthogonal projection, and geometric reflection via a gating scalar. 3. Restructures the residual update as a synchronous rank-1 injection, unifying feature erasure and writing under a dynamic step size to enable complex, non-monotonic dynamics while preserving stable training.",
      "summary": "This paper identifies that the strictly additive inductive bias of standard residual networks limits their capacity to model complex state transitions. To address this, it proposes Deep Delta Learning (DDL), which modulates the identity shortcut with a learnable, data-dependent geometric transformation (the Delta Operator). This allows the network to explicitly control its layer-wise transition spectrum, enabling the modeling of complex dynamics like oscillations while maintaining stable training.",
      "mindmap": "graph TB\n        A[Deep Delta Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[残差网络限制/ResNet Limitation]\n        B1 --> B2[”刚性相加偏置/Rigid Additive Bias”]\n        B2 --> B3[”限制复杂状态转换/Limits Complex State Transitions”]\n        C --> C1[Delta 算子/Delta Operator]\n        C1 --> C2[”秩-1扰动/ Rank-1 Perturbation”]\n        C2 --> C3[”可学习几何变换/Learnable Geometric Transform”]\n        C3 --> C4[”动态门控/Dynamic Gating (β)”]\n        D --> D1[”谱分析/Spectral Analysis”]\n        D1 --> D2[”插值身份/投影/反射/Interpolates Identity/Projection/Reflection”]\n        D --> D3[”同步秩-1注入/Synchronous Rank-1 Injection”]\n        D3 --> D4[”控制转换谱/Controls Transition Spectrum”]\n        D4 --> D5[”保持稳定训练/Preserves Stable Training”]"
    },
    {
      "title": "E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models",
      "authors": "Shengjun Zhang, Zhang Zhang, Chensheng Dai, Yueqi Duan",
      "institution": "Tsinghua University",
      "link": "https://arxiv.org/pdf/2601.00423",
      "code": "https://github.com/shengjun-zhang/VisualGRPO",
      "tags": [
        "reinforcement learning for human feedback",
        "reinforcement learning from human feedback (RLHF)",
        "flow matching",
        "stochastic differential equations (SDE)",
        "group relative policy optimization (GRPO)",
        "entropy-aware sampling"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/517610c9d7d0b5f72ab6ea2e2c36dda14fb3879dc1ff5c4a8ae66c97dd3a6457_w640_q70.webp",
      "contributions": "1. Identified that high-entropy denoising steps are crucial for effective exploration in RL for flow models, while low-entropy steps lead to ambiguous rewards. 2. Proposed E-GRPO, an entropy-aware method that consolidates consecutive low-entropy steps into a single high-entropy step for SDE sampling and uses ODE sampling elsewhere. 3. Introduced a multi-step group normalized advantage calculation that computes advantages relative to samples sharing the same consolidated SDE step.",
      "summary": "The paper addresses the problem of sparse and ambiguous reward signals when applying reinforcement learning to flow models over multiple denoising steps. It proposes E-GRPO, an entropy-aware method that strategically uses SDE sampling on high-entropy steps and ODE sampling on others, along with a group-relative advantage calculation. Experiments show this approach is more effective for aligning flow models with human preferences.",
      "mindmap": "graph TB\n        A[E-GRPO: High Entropy Steps Drive Effective RL for Flow Models] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法在多个去噪步上优化，奖励信号稀疏模糊/Existing methods suffer from sparse & ambiguous rewards over multiple steps]\n        C --> C1[提出E-GRPO: 熵感知分组相对策略优化/Propose E-GRPO: Entropy-aware Group Relative Policy Optimization]\n        C1 --> C2[合并低熵步为高熵SDE采样步，其他步用ODE采样/Merge low-entropy steps for SDE, use ODE elsewhere]\n        C1 --> C3[引入多步分组归一化优势计算/Introduce multi-step group normalized advantage]\n        D --> D1[在不同奖励设置下验证了方法的有效性/Method effectiveness demonstrated across different reward settings]"
    },
    {
      "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
      "authors": "Ahmad Rezaei, Mohsen Gholami, Saeed Ranjbar Alvar, Kevin Cannons, Mohammad Asiful Hossain, Zhou Weimin, Shunbo Zhou, Yong Zhang, Mohammad Akbari",
      "institution": "Huawei Technologies Canada Co. Ltd., Huawei Cloud",
      "link": "https://arxiv.org/pdf/2601.00501",
      "code": null,
      "tags": [
        "reinforcement learning",
        "contrastive perception loss",
        "entropy shift",
        "vision-language models",
        "policy optimization",
        "multimodal reasoning"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa7f947cc56b98ca75000ef69e1ebe5ac183e118802862066844909b5763f7e9_w640_q70.webp",
      "contributions": "1. Introduces a method to detect perception tokens via entropy shifts under perturbed input images, avoiding reliance on extra LLMs or ground-truth data. 2. Proposes a Contrastive Perception Loss (CPL) that enforces output consistency for information-preserving perturbations and sensitivity for information-removing ones. 3. Demonstrates improved performance over prior perception-rewarding methods while enhancing training efficiency and scalability by eliminating the need for auxiliary models.",
      "summary": "CPPO is a reinforcement learning method for finetuning vision-language models that addresses the challenge of disentangling perception from reasoning tokens. It detects perception tokens using entropy shifts under image perturbations and applies a contrastive perception loss to optimize them. Experiments show CPPO outperforms previous methods without requiring extra models, making training more efficient.",
      "mindmap": "graph TB\n        A[CPPO: Contrastive Perception for Vision Language Policy Optimization] --> B[核心问题/Problem: Disentangling perception and reasoning tokens in VLMs is difficult with prior methods requiring extra LLMs or indiscriminate rewards]\n        A --> C[主要方法/Method: Detect perception tokens via entropy shifts under perturbed images; apply Contrastive Perception Loss (CPL) for consistency/sensitivity]\n        A --> D[关键结果/Results: CPPO surpasses previous perception-rewarding methods, avoids extra models, and improves training efficiency/scalability]"
    },
    {
      "title": "MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation",
      "authors": "Miaowei Wang, Jakub Zadrożny, Oisin Mac Aodha, Amir Vaxman",
      "institution": "The University of Edinburgh",
      "link": "https://arxiv.org/pdf/2601.00504",
      "code": "https://wangmiaowei.github.io/MotionPhysics.github.io/",
      "tags": [
        "physics-based simulation",
        "motion distillation",
        "differentiable simulation",
        "multimodal large language model",
        "material parameter estimation",
        "video diffusion models"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1f5f328bdf213bd8c50e27a819223c1b50a9adbc0e2f36cc8adcb40bdbdd5bb8_w640_q70.webp",
      "contributions": "1. An end-to-end differentiable framework that infers physical parameters from natural language prompts for 3D simulation, 2. A novel learnable motion distillation loss that extracts motion priors from video diffusion models while minimizing appearance/geometry bias, 3. A comprehensive evaluation across diverse scenarios (real-world, human-designed, AI-generated objects) and materials (solids, fluids) showing state-of-the-art performance.",
      "summary": "The paper introduces MotionPhysics, a framework that uses a multimodal LLM and a novel motion distillation loss from video diffusion models to automatically estimate plausible physical parameters from text prompts for 3D dynamic simulation. This approach eliminates the need for ground-truth trajectories or annotated videos. The method is shown to produce realistic simulations across a wide variety of materials and object types, outperforming prior work.",
      "mindmap": "graph TB\n        A[MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[物理参数调优耗时且需专业知识/Physical parameter tuning is time-consuming and requires expertise]\n        C --> C1[使用多模态大语言模型估计参数/Use multimodal LLM to estimate parameters]\n        C --> C2[提出可学习运动蒸馏损失/Propose learnable motion distillation loss]\n        C2 --> C2a[从视频扩散模型提取运动先验/Extract motion priors from video diffusion models]\n        D --> D1[在30+场景中评估/Evaluated across 30+ scenarios]\n        D --> D2[超越现有方法/Surpasses state-of-the-art]\n        D --> D3[自动确定合理参数/Automatically determines plausible parameters]"
    },
    {
      "title": "All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations",
      "authors": "Wenrui Li, Hongtao Chen, Yao Xiao, Wangmeng Zuo, Jiantao Zhou, Yonghong Tian, Xiaopeng Fan",
      "institution": "Harbin Institute of Technology, University of Macau, Peking University",
      "link": "https://arxiv.org/pdf/2601.00533",
      "code": "https://github.com/Friskknight/ORCANet-SEUD",
      "tags": [
        "video restoration",
        "all-in-one restoration",
        "smoothly evolving degradation",
        "prompt learning",
        "coarse intensity estimation",
        "flow prompt generation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/314d098b5e34dadb4e9353f07d267f249935ece4ae21fe942fc2f57313e50e49_w640_q70.webp",
      "contributions": "1. Introduces the Smoothly Evolving Unknown Degradations (SEUD) scenario for video restoration, where degradation types and intensities change continuously over time. 2. Proposes ORCANet, featuring a Coarse Intensity Estimation Dehazing (CIED) module for initialization and a Flow Prompt Generation (FPG) module that generates static and dynamic prompts to capture degradation types and intensity variations. 3. Designs a flexible synthesis pipeline to generate temporally coherent videos with single, compound, and evolving degradations for training and evaluation.",
      "summary": "This paper addresses the challenge of all-in-one video restoration under temporally continuous and evolving weather degradations. It proposes ORCANet, a network that uses coarse intensity estimation and a novel prompting mechanism to adapt to degradation changes over time. Experiments show the method achieves superior restoration quality and temporal consistency compared to existing baselines.",
      "mindmap": "graph TB\n        A[All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Extending all-in-one restoration to videos with temporally continuous, evolving degradations]\n        C[主要方法/Method: ORCANet with CIED for initialization and FPG for static/dynamic prompt generation]\n        D[关键结果/Results: Superior restoration quality, temporal consistency, and robustness]"
    },
    {
      "title": "FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection",
      "authors": "Ruiqiang Zhang, Hengyi Wang, Chang Liu, Guanjie Wang, Zehua Ma, Weiming Zhang",
      "institution": "University of Science and Technology of China",
      "link": "https://arxiv.org/pdf/2601.00535",
      "code": null,
      "tags": [
        "text-to-image generation",
        "diffusion transformers",
        "attention localization",
        "spectral glyph injection",
        "training-free",
        "text rendering"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/463d01d9f126e0686a5a88ff8da2bf9d31c4f68b3b838fbd41e6842bdf70c735_w640_q70.webp",
      "contributions": "1. Proposes a training-free framework (FreeText) that decomposes text rendering into \"where to write\" and \"what to write\" problems. 2. Introduces a method to localize writing regions using endogenous image-to-text attention with sink-like tokens and topology-aware refinement. 3. Presents Spectral-Modulated Glyph Injection (SGMI) to inject a noise-aligned glyph prior with frequency-domain modulation to enhance structure and suppress semantic leakage.",
      "summary": "The paper proposes FreeText, a training-free framework to improve text rendering in Diffusion Transformer models. It localizes writing regions via attention mechanisms and injects glyph structure using spectral modulation. Experiments show it improves text readability while preserving image quality with minimal overhead.",
      "mindmap": "graph TB\n        A[FreeText: Training-Free Text Rendering in Diffusion Transformers] --> B[核心问题/Problem: Diffusion models struggle with precise text rendering, especially for multi-line layouts and long-tailed scripts]\n        A --> C[主要方法/Method: Decomposes into ”where to write” (attention localization) and ”what to write” (spectral-modulated glyph injection)]\n        A --> D[关键结果/Results: Consistent gains in text readability, preserves semantic alignment and aesthetics, modest inference overhead]"
    },
    {
      "title": "DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction",
      "authors": "Jiacheng Sui, Yujie Zhou, Li Niu",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2601.00542",
      "code": null,
      "tags": [
        "image editing",
        "drag-style editing",
        "motion prediction",
        "predict-and-move framework",
        "motion supervision"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c465837de0f92a45bd31ff245e5f10563376186d973cd5f4a47cbf125a05967a_w640_q70.webp",
      "contributions": "1. Proposes a novel \"predict-and-move\" framework for drag-style image editing, which is the first of its kind. 2. Introduces an iterative method combining Motion Prediction and Motion Supervision to proactively guide handle points. 3. Proposes a dynamic adjustment mechanism for valid handle points to improve editing performance.",
      "summary": "The paper identifies issues like miss tracking and poor editability in existing drag-style image editing methods. It proposes DynaDrag, a new method under a \"predict-and-move\" framework that iteratively uses motion prediction and motion supervision to guide handle points. Experiments on face and human datasets show it outperforms previous works.",
      "mindmap": "graph TB\n        A[DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction] --> B[核心问题/Problem: 现有拖拽式图像编辑方法存在跟踪丢失、模糊跟踪、编辑性差等问题]\n        A --> C[主要方法/Method: 提出首个”预测-移动”框架，迭代执行运动预测和运动监督，动态调整有效控制点]\n        A --> D[关键结果/Results: 在人脸和人体数据集上的实验展示了优于先前方法的性能]"
    },
    {
      "title": "Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios",
      "authors": "Guangqian Guo, Pengfei Chen, Yong Guo, Huafeng Chen, Boqiang Zhang, Shan Gao",
      "institution": "Northwestern Polytechnical University, University of Chinese Academy of Sciences, Max Planck Institute for Informatics, University of Science and Technology of China",
      "link": "https://arxiv.org/pdf/2601.00537",
      "code": "https://guangqian-guo.github.io/VNS-SAM/",
      "tags": [
        "image segmentation",
        "Segment Anything Model",
        "zero-shot segmentation",
        "low-level features",
        "fine-tuning",
        "visually non-salient"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4bd924d5e254a323192d4b65b47fba860d34f9df6052e0b5cf36ea1076a78bef_w640_q70.webp",
      "contributions": "1. Proposed VNS-SAM, a method to enhance SAM's performance in visually non-salient (low-contrast) scenarios while preserving its zero-shot generalizability. 2. Introduced two key designs: a Mask-Edge Token Interactive decoder and a Non-Salient Feature Mining module to effectively exploit SAM's low-level features. 3. Created VNS-SEG, a large-scale unified dataset with over 35K images for training and benchmarking models on various visually non-salient segmentation tasks.",
      "summary": "This paper addresses the problem where the Segment Anything Model (SAM) struggles with visually non-salient scenarios where foreground and background have low contrast. The authors propose VNS-SAM, which enhances SAM's perception using a novel decoder and feature mining module, and introduce a new dataset called VNS-SEG. Experiments show VNS-SAM achieves superior performance, especially in zero-shot settings, demonstrating its practical potential.",
      "mindmap": "graph TB\n        Root[Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[SAM在视觉非显著场景表现不佳/SAM underperforms in visually non-salient scenarios]\n        Method[主要方法/Method] --> M1[提出VNS-SAM模型/Propose VNS-SAM model]\n        M1 --> M2[设计Mask-Edge Token Interactive解码器/Design Mask-Edge Token Interactive decoder]\n        M1 --> M3[设计Non-Salient Feature Mining模块/Design Non-Salient Feature Mining module]\n        M1 --> M4[构建VNS-SEG数据集/Build VNS-SEG dataset]\n        Results[关键结果/Results] --> R1[性能显著提升/Superior performance]\n        Results --> R2[保持零样本泛化能力/Preserves zero-shot generalizability]\n        Results --> R3[参数和计算成本低/Low parameter & computational cost]"
    },
    {
      "title": "A Comprehensive Dataset for Human vs. AI Generated Image Detection",
      "authors": "Rajarshi Roy, Nasrin Imanpour, Ashhar Aziz, Shashwat Bajpai, Gurpreet Singh, Shwetangshu Biswas, Kapil Wanaskar, Parth Patwa, Subhankar Ghosh, Shreyas Dixit, Nilesh Ranjan Pal, Vipula Rawte, Ritvik Garimella, Gaytri Jena, Vasu Sharma, Vinija Jain, Aman Chadha, Aishwarya Naresh Reganti, Amitava Das",
      "institution": "Kalyani Govt. Engg. College, AI Institute USC, IIIT Delhi, BITS Pilani, NIT Silchar, San José State Univ., UCLA, Washington State Univ., VIIT, GITA, Meta AI, Amazon AI",
      "link": "https://arxiv.org/pdf/2601.00553",
      "code": null,
      "tags": [
        "image forensics",
        "AI-Generated Images",
        "Detection Techniques",
        "Synthetic Media",
        "Generative AI",
        "Multimodal AI"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80669ac38c06acf6818488be6bd831b0e8cc20bc319a1b37c431681230968cd3_w640_q70.webp",
      "contributions": "1. Introduces MS COCOAI, a novel large-scale dataset of 96,000 real and AI-generated images for detection research., 2. Proposes two benchmark tasks: binary real-vs-AI classification and multi-class AI model attribution., 3. Provides a diverse dataset using five state-of-the-art generators (Stable Diffusion 3, SD 2.1, SDXL, DALL-E 3, MidJourney v6) built upon MS COCO.",
      "summary": "This paper addresses the challenge of detecting increasingly realistic AI-generated images by introducing MS COCOAI, a comprehensive dataset of 96,000 real and synthetic images created using five modern generators. The dataset enables two key tasks: distinguishing real from AI-generated images and identifying the specific AI model that created a synthetic image. The release of this dataset aims to advance research in AI-generated image detection and model attribution.",
      "mindmap": "graph TB\n        Root[”A Comprehensive Dataset for Human vs. AI Generated Image Detection”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”AI生成图像难以区分/AI-Generated Images Hard to Distinguish”]\n        Problem --> P2[”误导性内容传播/Spread of Misleading Content”]\n        Method --> M1[”构建MS COCOAI数据集/Build MS COCOAI Dataset”]\n        Method --> M2[”使用五种生成器/Use Five Generators”]\n        Results --> R1[”提供96000个数据点/Provide 96k Datapoints”]\n        Results --> R2[”定义两项检测任务/Define Two Detection Tasks”]"
    },
    {
      "title": "SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array",
      "authors": "Shuang Li, Yibing Wang, Jian Gao, Chulhong Kim, Seongwook Choi, Yu Zhang, Qian Chen, Yao Yao, Changhui Li",
      "institution": "Peking University, Nanjing University, Pohang University of Science and Technology",
      "link": "https://arxiv.org/pdf/2601.00551",
      "code": "https://github.com/ShuangLiPKU/SlingBAG-Pro",
      "tags": [
        "medical imaging reconstruction",
        "photoacoustic imaging",
        "point cloud",
        "iterative reconstruction",
        "irregular array",
        "hierarchical optimization"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4ac663649e0f38d28ffc4287bdae5acac62679da31ef28094152756b24cc0f8_w640_q70.webp",
      "contributions": "1. Proposed SlingBAG Pro, an advanced reconstruction algorithm extending point cloud iteration to arbitrary/irregular transducer array geometries. 2. Introduced a hierarchical optimization strategy combining zero-gradient filtering with progressively increased temporal sampling to accelerate convergence. 3. Demonstrated significant speed improvement (up to 2.2x) over the original method while maintaining quality, validated via simulation and in vivo experiments.",
      "summary": "This paper introduces SlingBAG Pro, an accelerated iterative reconstruction algorithm for 3D photoacoustic imaging that works with arbitrary, irregular transducer arrays. It uses a point cloud-based method with a hierarchical optimization strategy to remove redundant computations, speeding up convergence. The method achieves up to 2.2x faster reconstruction than its predecessor while maintaining quality, as shown in simulations and mouse experiments.",
      "mindmap": "graph TB\n        A[SingBAG Pro: 3D光声成像/SlingBAG Pro: 3D Photoacoustic Imaging] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[不规则阵列重建慢/Irregular Array Reconstruction is Slow]\n        C --> C1[点云迭代与分层优化/Point Cloud Iteration & Hierarchical Optimization]\n        D --> D1[速度提升2.2倍/Speedup of 2.2x]\n        D --> D2[仿真与活体验证/Simulation & In Vivo Validation]"
    },
    {
      "title": "A Cascaded Information Interaction Network for Precise Image Segmentation",
      "authors": "Hewen Xiao, Jie Mei, Guangfu Ma, Weiren Wu",
      "institution": "Harbin Institute of Technology, Shenzhen",
      "link": "https://arxiv.org/pdf/2601.00562",
      "code": null,
      "tags": [
        "image segmentation",
        "cascaded convolutional neural network",
        "Global Information Guidance Module",
        "multi-scale feature fusion"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b418eb7fd8e5a8768acb864c9583e6d602d0cbbea738237f0d183a02620d4ce_w640_q70.webp",
      "contributions": "1. Proposes a cascaded convolutional neural network architecture for robust image segmentation. 2. Introduces a novel Global Information Guidance Module to fuse low-level texture and high-level semantic features. 3. Demonstrates superior performance on benchmark datasets, particularly in cluttered or blurred environments.",
      "summary": "This paper addresses the challenge of robust image segmentation in complex scenarios by proposing a cascaded CNN with a novel Global Information Guidance Module. This module effectively fuses multi-scale features to overcome the limitations of single-scale extraction. Experimental results show the method outperforms state-of-the-art approaches, highlighting its potential for practical robotic applications.",
      "mindmap": "graph TB\n        A[A Cascaded Information Interaction Network for Precise Image Segmentation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[复杂场景下鲁棒分割是挑战/Robust segmentation in complex scenarios is a challenge]\n        C --> C1[级联卷积神经网络/Cascaded CNN]\n        C --> C2[全局信息引导模块/Global Information Guidance Module]\n        C2 --> C3[融合多尺度特征/Fuse multi-scale features]\n        D --> D1[在基准测试中表现优异/Superior performance on benchmarks]\n        D --> D2[超越现有方法/Outperforms SOTA methods]"
    },
    {
      "title": "AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models",
      "authors": "Jintao Lin, Bowen Dong, Weikang Shi, Chenyang Lei, Suiyun Zhang, Rui Liu, Xihui Liu",
      "institution": "University of Hong Kong, The Hong Kong Polytechnic University, The Chinese University of Hong Kong, Huawei Research",
      "link": "https://arxiv.org/pdf/2601.00561",
      "code": null,
      "tags": [
        "multimodal evaluation",
        "unified multimodal models",
        "world knowledge",
        "deterministic evaluation",
        "benchmark",
        "reasoning"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d213b94fc8cc3083a9cd6696a9015bb66bc67f497c2bbd6325bdba2cc4da71e_w640_q70.webp",
      "contributions": "1. Proposes AEGIS, a comprehensive multi-task benchmark for evaluating world knowledge capabilities of Unified Multimodal Models across visual understanding, generation, editing, and interleaved generation. 2. Introduces Deterministic Checklist-based Evaluation (DCE), a protocol using atomic \"Y/N\" judgments to replace ambiguous prompt-based scoring for more reliable evaluation. 3. Conducts extensive experiments revealing severe world knowledge deficits in current UMMs, performance degradation with complex reasoning, and the partial mitigation offered by plug-in reasoning modules.",
      "summary": "This paper identifies a critical gap in evaluating Unified Multimodal Models' (UMMs) ability to apply world knowledge. To address this, it proposes the AEGIS benchmark and a Deterministic Checklist-based Evaluation (DCE) protocol. The experiments show that current UMMs have significant world knowledge deficiencies, especially in complex reasoning tasks.",
      "mindmap": "graph TB\n        A[AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Multimodal Models] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有基准不足/Existing benchmarks are insufficient]\n        C --> C1[提出AEGIS基准/Propose AEGIS benchmark]\n        C --> C2[提出DCE评估协议/Propose DCE evaluation protocol]\n        D --> D1[模型存在知识缺陷/Models have knowledge deficits]\n        D --> D2[复杂推理性能下降/Performance degrades with complex reasoning]"
    },
    {
      "title": "GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval",
      "authors": "Mingyu Jeon, Sunjae Yoon, Jonghee Kim, Junyeoung Kim",
      "institution": "Chung-Ang University, Korea Advanced Institute of Science and Technology (KAIST), Electronics and Telecommunications Research Institute (ETRI)",
      "link": "https://arxiv.org/pdf/2601.00584",
      "code": null,
      "tags": [
        "video moment retrieval",
        "zero-shot",
        "granularity mismatch",
        "query rewriting",
        "caption generation",
        "vision-language models"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23be1befed58b8e1274e0b349e1e52f23148fa93c049fd218714eaf38b8f9a43_w640_q70.webp",
      "contributions": "1. Proposes a training-free framework (GranAlign) to address semantic granularity mismatch in zero-shot video moment retrieval. 2. Introduces granularity-based query rewriting to generate queries at varied semantic levels. 3. Introduces query-aware caption generation to embed query intent into video content descriptions.",
      "summary": "This paper addresses the granularity mismatch problem in zero-shot video moment retrieval by proposing GranAlign, a training-free framework that uses query rewriting and query-aware caption generation to align multi-level semantic representations. The method achieves state-of-the-art performance on three major benchmarks, including a 3.23% mAP@avg improvement on QVHighlights.",
      "mindmap": "graph TB\n        Root[GranAlign] --> Problem(核心问题: 粒度不匹配 / Problem: Granularity Mismatch)\n        Root --> Method(主要方法: 粒度感知对齐 / Method: Granularity-Aware Alignment)\n        Root --> Results(关键结果: SOTA性能 / Results: SOTA Performance)\n        Problem --> P1[查询与视频内容语义粒度不一致 / Query-Video Semantic Granularity Mismatch]\n        Method --> M1[粒度查询重写 / Granularity-based Query Rewriting]\n        Method --> M2[查询感知描述生成 / Query-aware Caption Generation]\n        Results --> R1[在三大基准测试中达到SOTA / SOTA on Three Major Benchmarks]\n        Results --> R2[在QVHighlights上显著提升 / Notable Improvement on QVHighlights]"
    },
    {
      "title": "Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception",
      "authors": "Xianhui Liu, Siqi Jiang, Yi Xie, Yuqing Lin, Siao Liu",
      "institution": "Tongji University, Soochow University, The University of Arizona",
      "link": "https://arxiv.org/pdf/2601.00598",
      "code": null,
      "tags": [
        "object detection",
        "RGB-Infrared fusion",
        "modality imbalance",
        "cross-modal learning",
        "optimization bias",
        "multimodal perception"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea14f519454d3351bfb99175ef4749ac8218341133d6b9799ea3b1f99453239f_w640_q70.webp",
      "contributions": "1. Proposes the Modality Dominance Index (MDI) to quantify optimization bias caused by asymmetric modality characteristics in RGB-IR fusion. 2. Develops the Modality Dominance-Aware Cross-modal Learning (MDACL) framework to regulate cross-modal optimization. 3. Introduces Hierarchical Cross-modal Guidance (HCG) and Adversarial Equilibrium Regularization (AER) within MDACL to enhance feature alignment and balance optimization dynamics.",
      "summary": "This paper addresses the optimization bias problem in RGB-Infrared multimodal object detection, where disparities in information density cause training to over-rely on a dominant modality. The authors propose a Modality Dominance Index (MDI) to measure this bias and a Modality Dominance-Aware Cross-modal Learning (MDACL) framework to mitigate it. Experiments show that MDACL effectively reduces optimization bias and achieves state-of-the-art performance on RGB-IR benchmarks.",
      "mindmap": "graph TB\n        A[Modality Dominance-Aware Optimization for Embodied RGB–Infrared Perception] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[模态优化偏差/Modality Optimization Bias]\n        C --> C1[模态主导指数/MDI]\n        C --> C2[模态主导感知学习框架/MDACL]\n        C2 --> C2_1[分层跨模态指导/HCG]\n        C2 --> C2_2[对抗均衡正则化/AER]\n        D --> D1[缓解优化偏差/Mitigates Bias]\n        D --> D2[实现SOTA性能/Achieves SOTA]"
    },
    {
      "title": "SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation",
      "authors": "Yiling Wang, Zeyu Zhang, Yiran Wang, Hao Tang",
      "institution": "The Australian National University, Peking University",
      "link": "https://arxiv.org/pdf/2601.00590",
      "code": "https://github.com/YilingWang98/SafeMo",
      "tags": [
        "text-to-motion generation",
        "machine unlearning",
        "diffusion models",
        "motion safety",
        "continuous kinematics",
        "safe dataset"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e31ba879f70fe1ccd3182c0dcf440f56ec134c723c4b1d1032dbe7d145432291_w640_q70.webp",
      "contributions": "1. Proposed SafeMo, a trustworthy motion generation framework integrating a two-stage Minimal Motion Unlearning (MMU) strategy for safe generation in continuous space. 2. Introduced the first safe text-to-motion dataset, SafeMoVAE-29K, with rewritten safe prompts and refined motions. 3. Demonstrated superior safety-utility trade-offs, achieving significantly higher forget-set FID scores than prior state-of-the-art methods.",
      "summary": "The paper addresses safety concerns in text-to-motion generation by proposing SafeMo, a framework that uses a two-stage machine unlearning strategy to remove unsafe behaviors while preserving motion quality in continuous space. It also introduces a new safe dataset for training. Experiments show SafeMo effectively forgets unsafe prompts while maintaining good performance on safe ones, outperforming previous methods.",
      "mindmap": "graph TB\n        A[SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法缺陷/Existing Method Flaws]\n        B1 --> B1a[离散代码本替换导致性能下降/Discrete codebook replacement degrades benign performance]\n        B1 --> B1b[量化导致伪影和不连贯/Quantization causes artifacts & jerky transitions]\n        B --> B2[数据集包含不安全内容/Datasets contain unsafe intents & motions]\n        C --> C1[提出SafeMo框架/Propose SafeMo Framework]\n        C1 --> C1a[最小化运动遗忘策略/Minimal Motion Unlearning (MMU)]\n        C1 --> C1b[连续空间生成/Generation in Continuous Space]\n        C --> C2[构建安全数据集/Build Safe Dataset SafeMoVAE-29K]\n        D --> D1[有效遗忘不安全提示/Effective Forgetting on Unsafe Prompts]\n        D1 --> D1a[遗忘集FID显著提升/Forget-set FID greatly improved]\n        D --> D2[保持良性性能/Benign Performance Preserved]"
    },
    {
      "title": "Noise-Robust Tiny Object Localization with Flows",
      "authors": "Huixin Sun, Linlin Yang, Ronyu Chen, Kerui Gu, Baochang Zhang, Angela Yao, Xianbin Cao",
      "institution": "Beihang University, Communication University of China, National University of Singapore",
      "link": "https://arxiv.org/pdf/2601.00617",
      "code": null,
      "tags": [
        "object detection",
        "Tiny Object Detection",
        "Noise Robustness",
        "Normalizing Flows",
        "Uncertainty-Guided Optimization",
        "Flow-Based Error Modeling"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b7f3a1410f8fcc28ad5c6cee9bc7ead457cf793040466a768ce5024835633cf_w640_q70.webp",
      "contributions": "1. Proposes Tiny Object Localization with Flows (TOLF), a noise-robust framework for tiny object detection. 2. Introduces flow-based error modeling to capture complex, non-Gaussian prediction distributions for robust learning under noisy supervision. 3. Designs an uncertainty-aware gradient modulation mechanism to suppress learning from high-uncertainty, noise-prone samples, mitigating overfitting.",
      "summary": "This paper addresses the problem of tiny object detection being highly sensitive to annotation noise, which leads to overfitting. The authors propose TOLF, a framework using normalizing flows for flexible error modeling and uncertainty-guided optimization to learn robustly from noisy labels. Experiments show TOLF effectively improves performance, boosting a DINO baseline by 1.2% AP on the AI-TOD dataset.",
      "mindmap": "graph TB\n        A[Noise-Robust Tiny Object Localization with Flows] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[微小物体检测对标注噪声敏感/Tiny object detection is sensitive to annotation noise]\n        C --> C1[基于归一化流的误差建模/Flow-based error modeling]\n        C --> C2[不确定性引导的优化/Uncertainty-guided optimization]\n        D --> D1[在多个数据集上验证有效/Validated on multiple datasets]\n        D --> D2[提升DINO基线1.2% AP/Boosts DINO baseline by 1.2% AP]"
    },
    {
      "title": "HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis",
      "authors": "Shuren Gabriel Yu, Sikang Ren, Yongji Tian",
      "institution": "Tsinghua University, Beijing Tiantan Hospital",
      "link": "https://arxiv.org/pdf/2601.00626",
      "code": null,
      "tags": [
        "medical image analysis",
        "Hypergraph Neural Network",
        "Learning Using Privileged Information",
        "Knowledge Distillation",
        "Severed Graph Strategy",
        "dual-stream distillation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b9ab4eaefd7ae386c2d1758797c14d52ec57c898fa468bb73264675d58297cb_w640_q70.webp",
      "contributions": "1. Proposes HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information framework for preoperative ependymoma prognosis. 2. Introduces a Severed Graph Strategy with a shared encoder to process both a Teacher graph (with post-surgery text) and a Student graph (with pre-op MRI only). 3. Employs dual-stream distillation to enable the Student model to hallucinate semantic community structures from visual features alone, transferring expert knowledge without requiring text at inference.",
      "summary": "The paper addresses the challenge of preoperative ependymoma prognosis where post-operative text reports are unavailable at inference. It proposes HyperPriv-EPN, a hypergraph learning framework that uses a severed graph strategy and dual-stream distillation to transfer knowledge from privileged text data to a model that only uses MRI. The method achieves state-of-the-art diagnostic accuracy and survival stratification on a multi-center cohort, enabling the use of historical post-operative data for new patient diagnosis.",
      "mindmap": "graph TB\n        A[HyperPriv-EPN] --> B[核心问题/Problem: Pre-op prognosis lacks semantic insights from post-op reports]\n        A --> C[主要方法/Method: Hypergraph LUPI with Severed Graph Strategy & dual-stream distillation]\n        A --> D[关键结果/Results: SOTA accuracy & survival stratification on 311 patients]"
    },
    {
      "title": "RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation",
      "authors": "Junxiao Xue, Pavel Smirnov, Ziao Li, Yunyun Shi, Shi Chen, Xinyi Yin, Xiaohan Yue, Lei Wang, Yiduo Wang, Feng Lin, Yijia Chen, Xiao Ma, Xiaoran Yan, Qing Zhang, Fengjian Xue, Xuecheng Wu",
      "institution": "Zhejiang Lab, Xi'an Jiaotong University",
      "link": "https://arxiv.org/pdf/2601.00625",
      "code": null,
      "tags": [
        "human pose estimation",
        "real-time 3D pose estimation",
        "biomechanical analysis",
        "SmoothNet",
        "multi-camera tracking",
        "Unity visualization"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b90bb8653fd0f2988586244cf96968ebee6859d08f1685dd8be9db9a00f20b34_w640_q70.webp",
      "contributions": "1. A unified, end-to-end pipeline for real-time 3D human pose estimation and motion analysis from multi-camera RGB video for rehabilitation. 2. A fast tracking method for multi-person scenarios in medical rehabilitation, achieving tracking in under 1ms per frame. 3. A modification of SmoothNet for real-time posture estimation to reduce errors and produce smoother, more accurate motion states.",
      "summary": "This paper proposes RePose, a real-time framework for 3D human pose estimation and biomechanical analysis to assist rehabilitation training. The method uses a multi-camera pipeline, a fast multi-person tracker, and a modified SmoothNet for smooth pose estimation, integrated with Unity for real-time feedback and muscle stress visualization. The system aims to provide immediate guidance to help patients perform exercises correctly.",
      "mindmap": "graph TB\n        A[RePose: 康复实时3D姿态估计与生物力学分析框架<br/>RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>Need for automatic, real-time monitoring and assessment in rehabilitation training] --> B1[远程康复监控需求<br/>Remote rehabilitation monitoring need]\n        C[主要方法/Method<br/>Real-time 3D pose estimation and analysis pipeline] --> C1[多摄像头RGB视频输入<br/>Multi-camera RGB video input]\n        C --> C2[快速多人跟踪 (<1ms/帧)<br/>Fast multi-person tracking (<1ms/frame)]\n        C --> C3[改进SmoothNet用于实时姿态估计<br/>Modified SmoothNet for real-time pose estimation]\n        C --> C4[Unity平台实时监控与肌肉应力显示<br/>Unity platform for real-time monitoring & muscle stress display]\n        D[关键结果/Results<br/>A unified framework for real-time motion correction and guidance] --> D1[实时反馈辅助正确康复训练<br/>Real-time feedback to assist correct rehabilitation training]"
    },
    {
      "title": "Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach",
      "authors": "Shrikant Kapse, Priyankkumar Dhrangdhariya, Priya Kedia, Manasi Patwardhan, Shankar Kausley, Soumyadipta Maiti, Beena Rai, Shirish Karande",
      "institution": "TCS Research",
      "link": "https://arxiv.org/pdf/2601.00645",
      "code": null,
      "tags": [
        "image classification",
        "transfer learning",
        "vision transformer",
        "DenseNet",
        "sprout detection",
        "shelf-life prediction"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a94f0adb825ef9b8afe22fb1ed85de1536913fd08af263977b5d34f95849c59_w640_q70.webp",
      "contributions": "1. Designed a high-precision binary classifier for potato sprout detection using transfer learning, achieving 98.03% accuracy with DenseNet. 2. Developed an advanced multi-class predictor for estimating weight loss and forecasting remaining shelf-life, demonstrating best performance with coarse class divisions. 3. Demonstrated the feasibility of integrating image-based deep learning models into automated sorting systems for improved inventory management and reduced food waste.",
      "summary": "This paper proposes using transfer learning with CNN and Vision Transformer models for non-invasive quality detection of stored potatoes. The method involves a binary classifier for sprout detection and a multi-class predictor for weight loss and shelf-life estimation. The study concludes that this approach is effective for automated sorting, with coarse class divisions yielding robust performance for practical applications.",
      "mindmap": "graph TB\n        A[Quality Detection of Stored Potatoes via Transfer Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[监测存储马铃薯质量/Monitoring stored potato quality]\n        B --> B2[检测发芽、估计重量损失/Predicting sprouting, weight loss & shelf-life]\n        C --> C1[使用预训练模型/Using pre-trained models (ResNet, VGG, DenseNet, ViT)]\n        C --> C2[设计二分类与多分类器/Designing binary & multi-class classifiers]\n        D --> D1[高精度发芽检测/High-accuracy sprout detection (98.03%)]\n        D --> D2[粗粒度保质期预测更优/Coarse class shelf-life prediction works best]\n        D --> D3[支持自动化分拣系统/Supports automated sorting systems]"
    },
    {
      "title": "CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models",
      "authors": "Neeraj Anand, Samyak Jha, Udbhav Bamba, Rahul Rahaman",
      "institution": "Indian Institute of Technology (ISM) Dhanbad, Transmute AI, National University of Singapore",
      "link": "https://arxiv.org/pdf/2601.00659",
      "code": "https://github.com/ubamba98/CRoPS-Mitigate-Hallucinations-in-Vision-Language-Models",
      "tags": [
        "multi-modal inference",
        "hallucination mitigation",
        "contrastive decoding",
        "vision-language models",
        "training-free"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13748e3293c59d4b319620d3651a674872996116396b3caafd1fd283bb3ab841_w640_q70.webp",
      "contributions": "1. Proposes a novel hallucinated model that captures hallucination effects by selectively removing key text tokens, addressing the limitation of visual information propagation. 2. Introduces Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. 3. Presents CRoPS, a training-free framework that significantly improves hallucination metrics (e.g., 20% CHAIR score gain) across multiple benchmarks and LVLM families.",
      "summary": "This paper addresses the problem of hallucinations in Large Vision-Language Models (LVLMs). It proposes CRoPS, a training-free framework that introduces a novel hallucinated model based on text token removal and a Generalized Contrastive Decoding method to mitigate diverse hallucination sources. The method achieves significant improvements in hallucination metrics across several benchmarks.",
      "mindmap": "graph TB\n        A[CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models] --> B[核心问题/Problem: LVLMs tend to generate hallucinated content, undermining reliability.]\n        A --> C[主要方法/Method: Novel hallucinated model via text token removal + Generalized Contrastive Decoding.]\n        A --> D[关键结果/Results: Improves CHAIR scores by 20%, gains across 6 benchmarks & 3 LVLM families.]"
    },
    {
      "title": "Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network",
      "authors": "Zhaiyu Chen, Yuanyuan Wang, Yilei Shi, Xiao Xiang Zhu",
      "institution": "Technical University of Munich (TUM)",
      "link": "https://arxiv.org/pdf/2601.00658",
      "code": "https://github.com/zhu-xlab/tomosar2height",
      "tags": [
        "3D reconstruction",
        "SAR tomography",
        "point cloud",
        "deep learning",
        "building height estimation",
        "dual-topology network"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/37a1a6d415c67af4db85e8077f3ab9a67e110f03583e90820fa3c4b93634ac87_w640_q70.webp",
      "contributions": "1. Proposes a novel learning-based framework for generating high-resolution building height maps directly from raw, noisy TomoSAR point clouds. 2. Introduces a dual-topology network architecture that alternates between a point branch and a grid branch to jointly model irregular scatterer features and enforce spatial consistency. 3. Demonstrates the first proof of concept for large-scale urban height mapping from TomoSAR and shows the framework's extensibility to incorporate optical imagery for enhanced quality.",
      "summary": "This paper addresses the challenge of accurate building height reconstruction from noisy and incomplete spaceborne TomoSAR point clouds. It proposes a dual-topology deep learning network that processes both point-based and grid-based representations to denoise and inpaint the data, producing continuous height maps. Experiments on urban datasets validate the method's effectiveness, and the framework is shown to be extensible by fusing with optical imagery.",
      "mindmap": "graph TB\n        A[Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[TomoSAR点云噪声多、分布不均、有数据空洞/TomoSAR point clouds are noisy, anisotropic, and have voids]\n        C --> C1[提出双拓扑网络/Propose a dual-topology network]\n        C1 --> C1_1[点分支处理不规则散射体/Point branch models irregular scatterers]\n        C1 --> C1_2[网格分支保证空间一致性/Grid branch enforces spatial consistency]\n        D --> D1[有效生成高分辨率高度图/Effectively produces high-resolution height maps]\n        D --> D2[首个大规模城市高度测绘概念验证/First proof of concept for large-scale urban height mapping]\n        D --> D3[可扩展融合光学影像/Extensible to incorporate optical imagery]"
    },
    {
      "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation",
      "authors": "Taekyung Ki, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Sung Ju Hwang",
      "institution": "KAIST, NTU Singapore, DeepAuto.ai",
      "link": "https://arxiv.org/pdf/2601.00664",
      "code": "https://taekyungki.github.io/AvatarForcing",
      "tags": [
        "talking head generation",
        "diffusion forcing",
        "direct preference optimization",
        "real-time interaction",
        "low latency",
        "multimodal inputs"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17340b71396da059acb45bce5718d0e4a786ccf313c43918ba84c25b9384bb11_w640_q70.webp",
      "contributions": "1. Proposes Avatar Forcing, a framework using diffusion forcing for real-time interactive head avatar generation that processes multimodal user inputs with low latency. 2. Introduces a label-free direct preference optimization method using synthetic losing samples to learn expressive interactions. 3. Demonstrates real-time performance (~500ms latency, 6.8x speedup) and generates avatars preferred over 80% against the baseline for expressiveness.",
      "summary": "The paper addresses the lack of truly interactive and emotionally engaging talking head avatars by proposing Avatar Forcing, a framework that uses diffusion forcing for real-time, low-latency generation and a direct preference optimization method for label-free learning of expressive reactions. The method achieves a significant speedup and produces avatar motions that are strongly preferred by users in evaluations.",
      "mindmap": "graph TB\n        Root[Avatar Forcing: Real-Time Interactive Head Avatar Generation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[缺乏真正互动/Lacks truly interactive communication]\n        Problem --> P2[单向反应缺乏情感/One-way responses lack emotional engagement]\n        Method[主要方法/Method] --> M1[扩散驱动框架/Diffusion forcing framework]\n        Method --> M2[无标签直接偏好优化/Label-free direct preference optimization]\n        Results[关键结果/Results] --> R1[低延迟实时交互/Low-latency real-time interaction (~500ms)]\n        Results --> R2[6.8倍加速/6.8x speedup]\n        Results --> R3[80%用户偏好/Over 80% user preference]"
    },
    {
      "title": "Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians",
      "authors": "Melonie de Almeida, Daniela Ivanova, Tong Shi, John H. Williamson, Paul Henderson",
      "institution": "University of Glasgow",
      "link": "https://arxiv.org/pdf/2601.00678",
      "code": "https://melonienimasha.github.io/Pixel-to-4D-Website/",
      "tags": [
        "video generation",
        "3D Gaussians",
        "camera-controlled generation",
        "temporal consistency",
        "single-image-conditioned"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05b260fc970977b430e0e91ebe61a5a7c0d11cd466b771b1dc287d76492ccd15_w640_q70.webp",
      "contributions": "1. Proposes a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion from a single image in a single forward pass., 2. Enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into rendered frames., 3. Achieves state-of-the-art video quality and inference efficiency on multiple datasets (KITTI, Waymo, RealEstate10K, DL3DV-10K).",
      "summary": "The paper addresses the problem of generating temporally consistent and geometrically sound videos from a single image with user-controlled camera paths. It proposes Pixel-to-4D, a method that builds a dynamic 3D Gaussian representation of the scene in one pass to enable fast, camera-guided video synthesis. The approach demonstrates superior video quality and efficiency compared to existing methods.",
      "mindmap": "graph TB\n        A[Pixel-to-4D: Camera-Controlled Image-to-Video Generation<br>Pixel-to-4D: 相机控制的图像到视频生成] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>缺乏用户可控性，现有方法在相机运动建模、时间一致性和几何完整性方面存在挑战<br>Lack of user controllability, challenges in camera motion modeling, temporal consistency, and geometric integrity]\n        C[主要方法/Method<br>构建动态3D高斯场景表示，单次前向传播采样物体运动<br>Construct dynamic 3D Gaussian scene representation, sample object motion in a single forward pass]\n        D[关键结果/Results<br>在多个数据集上实现SOTA视频质量和推理效率<br>Achieves SOTA video quality and inference efficiency on multiple datasets]"
    },
    {
      "title": "DefVINS: Visual-Inertial Odometry for Deformable Scenes",
      "authors": "Samuel Cerezo, Javier Civera",
      "institution": "Universidad de Zaragoza",
      "link": "https://arxiv.org/pdf/2601.00702",
      "code": null,
      "tags": [
        "visual-inertial odometry",
        "deformable scenes",
        "observability analysis",
        "embedded deformation graph",
        "IMU anchoring"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6b312fb0bd7d9381dfbae957a5aabad3939766f96c58738070e2c5334cb31268_w640_q70.webp",
      "contributions": "1. A VIO framework (DefVINS) that explicitly separates rigid motion (IMU-anchored) from non-rigid deformation (modeled by an embedded deformation graph). 2. An observability analysis characterizing how inertial measurements constrain rigid motion and identify modes in deformable scenes. 3. A conditioning-based activation strategy that progressively enables non-rigid degrees of freedom to prevent ill-posed updates.",
      "summary": "This paper introduces DefVINS, a visual-inertial odometry framework designed for deformable scenes. It separates rigid and non-rigid motion, uses an observability analysis to guide a progressive activation strategy for deformation, and shows improved robustness in non-rigid environments through ablation studies.",
      "mindmap": "graph TB\n        A[DefVINS: Visual-Inertial Odometry for Deformable Scenes] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[刚性假设失效 / Rigidity Assumption Violated]\n        B --> B2[VIO在非刚性场景中漂移 / VIO Drift in Non-Rigid Scenes]\n        C --> C1[分离刚性状态与非刚性形变 / Separate Rigid & Non-Rigid State]\n        C --> C2[可观测性分析与IMU锚定 / Observability Analysis & IMU Anchoring]\n        C --> C3[基于条件的渐进激活 / Conditioning-Based Progressive Activation]\n        D --> D1[提升非刚性环境鲁棒性 / Improved Robustness in Non-Rigid Environments]\n        D --> D2[消融实验验证有效性 / Ablation Studies Validate Benefits]"
    },
    {
      "title": "Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks",
      "authors": "Cory Fan, Wenchao Zhang",
      "institution": "Cornell University, OmniVision Technologies",
      "link": "https://arxiv.org/pdf/2601.00703",
      "code": null,
      "tags": [
        "image demosaicing",
        "isotropic networks",
        "spatial downsampling",
        "joint-demosaicing-and-denoising (JDD)",
        "DeepMAD",
        "JD3Net"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f98b6a6dc08bfce9cbda95473895cce4e107cbf797681c55dc3d3a75a0832b92_w640_q70.webp",
      "contributions": "1. Proposes that spatial downsampling can improve the efficiency and performance of isotropic networks for demosaicing, contrary to common design practices. 2. Designs and validates simple fully convolutional networks with downsampling using a mathematical architecture design technique adapted from DeepMAD. 3. Introduces JD3Net, a downsampled variant, which demonstrates strong empirical performance on various image demosaicing and JDD tasks.",
      "summary": "This paper addresses the computational inefficiency of isotropic networks for image demosaicing on mobile platforms. It proposes using spatial downsampling within these networks to improve efficiency and performance, validating the claim through network design and empirical testing of a model called JD3Net. The results show that the downsampled networks achieve strong performance on demosaicing and joint-demosaicing-and-denoising tasks.",
      "mindmap": "graph TB\n        Root(”Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”移动设备上深度学习去马赛克的计算成本高/High computational cost of deep learning demosaicing on mobile”)\n        Method --> M1(”在各项同性网络中使用空间下采样/Using spatial downsampling in isotropic networks”)\n        Method --> M2(”基于DeepMAD设计全卷积网络/Designing fully convolutional networks based on DeepMAD”)\n        Method --> M3(”提出JD3Net模型/Proposing the JD3Net model”)\n        Results --> R1(”下采样提高效率和性能/Downsampling improves efficiency and performance”)\n        Results --> R2(”JD3Net在多种任务上表现强劲/JD3Net shows strong performance on various tasks”)"
    },
    {
      "title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
      "authors": "Wei-Tse Cheng, Yen-Jen Chiou, Yuan-Fu Yang",
      "institution": "National Yang Ming Chiao Tung University",
      "link": "https://arxiv.org/pdf/2601.00705",
      "code": null,
      "tags": [
        "SLAM (Simultaneous Localization and Mapping)",
        "Gaussian Splatting",
        "Dense Initialization",
        "DINOv3",
        "Multi-view Triangulation",
        "Real-time Mapping"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4161c36bfb3fee2a260166b1caa94cf7f49f3bda268754e5be2f18620346aa62_w640_q70.webp",
      "contributions": "1. Proposes a one-shot, training-free dense Gaussian initialization via multi-view correspondence triangulation, replacing the iterative densification in GS-SLAM. 2. Introduces a robust correspondence generation method using DINOv3 descriptors refined by a confidence-aware inlier classifier. 3. Achieves faster convergence (~20% speedup), higher rendering fidelity, and maintains real-time performance (up to 925 FPS) while being compatible with existing GS-SLAM pipelines.",
      "summary": "The paper introduces RGS-SLAM, a robust SLAM framework that improves upon Gaussian Splatting SLAM by replacing its iterative densification with a one-shot, dense Gaussian initialization from triangulated multi-view correspondences. This method, using refined DINOv3 features, stabilizes mapping, accelerates convergence, and enhances rendering quality. Evaluations show it achieves competitive or superior accuracy and real-time performance compared to state-of-the-art systems.",
      "mindmap": "graph TB\n        A[RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[GS-SLAM的残差驱动致密化效率低/Inefficient residual-driven densification in GS-SLAM]\n        C --> C1[一次性密集初始化/One-shot dense initialization]\n        C1 --> C2[使用DINOv3特征与置信内点分类器/Using DINOv3 features & confidence-aware inlier classifier]\n        C2 --> C3[多视角三角化生成高斯先验/Multi-view triangulation for Gaussian prior]\n        D --> D1[收敛加速~20%/~20% faster convergence]\n        D --> D2[更高渲染保真度/Higher rendering fidelity]\n        D --> D3[实时性能达925 FPS/Real-time performance up to 925 FPS]"
    },
    {
      "title": "Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection",
      "authors": "Johannes C. Bauer, Paul Geng, Stephan Trattnig, Petr Dokládal, Rüdiger Daub",
      "institution": "Technical University of Munich, MINES Paris, Fraunhofer IGCV",
      "link": "https://arxiv.org/pdf/2601.00725",
      "code": null,
      "tags": [
        "continual learning",
        "multi-level feature fusion",
        "catastrophic forgetting",
        "visual quality inspection"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dc838232a328d451ea44fc50490b38b4e8ae077cf5f0ad87c37c75779e7a316_w640_q70.webp",
      "contributions": "1. Proposes a multi-level feature fusion (MLFF) approach for continual learning that utilizes representations from different depths of a pretrained network. 2. Demonstrates that MLFF matches end-to-end training performance for quality inspection tasks while using significantly fewer trainable parameters. 3. Shows that the approach reduces catastrophic forgetting and improves generalization robustness to new product types or defects.",
      "summary": "This paper addresses the challenge of continual learning for visual quality inspection in volatile manufacturing scenarios like remanufacturing. It proposes a Multi-Level Feature Fusion (MLFF) method that fuses features from different network depths to enable efficient model adaptation. The results show that MLFF achieves performance comparable to full retraining with fewer parameters, while also mitigating catastrophic forgetting and improving robustness to new defects.",
      "mindmap": "graph TB\n        A[Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection] --> B[核心问题/Problem: 模型在动态场景（如再制造）中需要持续适应，面临灾难性遗忘和计算效率挑战]\n        A --> C[主要方法/Method: 提出多层级特征融合方法，利用预训练网络不同深度的表征]\n        A --> D[关键结果/Results: 性能匹配端到端训练，参数更少，减少遗忘，提升泛化鲁棒性]"
    },
    {
      "title": "Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model",
      "authors": "Hao Guan, Li Zhou",
      "institution": "Brigham and Women's Hospital, Harvard Medical School",
      "link": "https://arxiv.org/pdf/2601.00716",
      "code": null,
      "tags": [
        "domain adaptation",
        "data shift detection",
        "performance degradation monitoring",
        "vision-language model",
        "confidence-based indicator",
        "digital pathology"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a1e297333bf6471523693e104d6b047f585e96fad854f63687658f33d5b22d7_w640_q70.webp",
      "contributions": "1. Developed DomainSAT, a lightweight toolbox with a graphical interface for systematic analysis and intuitive exploration of input data shift. 2. Introduced a label-free, confidence-based degradation indicator for output-based monitoring that directly captures changes in model prediction confidence. 3. Demonstrated that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift.",
      "summary": "This paper investigates how to detect performance degradation in a pathology Vision-Language Model (VLM) when the input data distribution shifts after deployment. The authors propose a two-part framework: analyzing input-level data shift using their developed toolbox, DomainSAT, and monitoring output-level prediction confidence with a new label-free indicator. Their experiments show that combining these input and output monitoring methods provides a more reliable and complementary approach for detecting model degradation under data shift in digital pathology.",
      "mindmap": "graph TB\n        A[Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[VLM性能在数据偏移后下降/VLM performance degrades after data shift]\n        C --> C1[开发输入数据偏移检测工具/DomainSAT toolbox for input shift]\n        C --> C2[提出基于置信度的输出监测指标/Confidence-based output indicator]\n        D --> D1[输入偏移检测有效但不总对应性能下降/Input shift detection effective but not always correlates with degradation]\n        D --> D2[置信度指标与性能下降密切相关/Confidence indicator closely related to degradation]\n        D --> D3[结合两者实现更可靠的监测/Combining both enables more reliable monitoring]"
    },
    {
      "title": "Grading Handwritten Engineering Exams with Multimodal Large Language Models",
      "authors": "Janez Perš, Jon Muhovič, Andrej Košir, Boštjan Murovec",
      "institution": "University of Ljubljana, Faculty of Electrical Engineering",
      "link": "https://arxiv.org/pdf/2601.00730",
      "code": null,
      "tags": [
        "multi-modal inference",
        "multimodal LLM",
        "handwritten exam grading",
        "reference grounding",
        "ensemble grading",
        "deterministic validation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb2f6c076cdb07cf31fcaea487e23c16eeb8f5f2479e5f44c8447c8ad253559_w640_q70.webp",
      "contributions": "1. An end-to-end workflow for grading scanned handwritten engineering exams using multimodal LLMs that preserves standard paper-based exam processes, requiring only a handwritten reference solution and grading rules from the lecturer. 2. A multi-stage reliability design featuring format/presence checks, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. 3. Empirical evaluation demonstrating the pipeline's effectiveness with state-of-the-art backends (GPT-5.2, Gemini-3 Pro), achieving ≈8-point mean absolute difference to lecturer grades and low bias, while ablations confirm the necessity of structured prompting and reference grounding.",
      "summary": "This paper presents an automated workflow for grading handwritten STEM exams using multimodal large language models. The method uses a lecturer's handwritten reference solution and grading rules, processed through a multi-stage pipeline with checks and ensemble grading for reliability. Evaluation shows the system achieves close agreement with human grades, confirming that structured prompting and reference grounding are essential for accuracy.",
      "mindmap": "graph TB\n        A[Grading Handwritten Engineering Exams with Multimodal LLMs] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[手动评分耗时且难以扩展/Manual grading is slow and hard to scale]\n        C --> C1[多模态LLM端到端工作流/Multimodal LLM end-to-end workflow]\n        C --> C2[参考解决方案与分级规则/Reference solution & grading rules]\n        C --> C3[多阶段可靠设计/Multi-stage reliability design]\n        D --> D1[≈8分平均绝对差/≈8-point mean absolute difference]\n        D --> D2[低偏差与触发率/Low bias & trigger rate]\n        D --> D3[结构化提示与参考至关重要/Structured prompting & reference essential]"
    },
    {
      "title": "Unified Primitive Proxies for Structured Shape Completion",
      "authors": "Zhaiyu Chen, Yuqing Wang, Xiao Xiang Zhu",
      "institution": "Technical University of Munich, Munich Center for Machine Learning",
      "link": "https://arxiv.org/pdf/2601.00759",
      "code": "https://unico-completion.github.io",
      "tags": [
        "3D shape completion",
        "primitive proxies",
        "structured shape completion",
        "primitive assembly",
        "online target updates",
        "Chamfer distance"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21550f37418321862a038f9a28188f8255cfde85add7d3392abec68b2b44b586_w640_q70.webp",
      "contributions": "1. Proposes UniCo, a model that predicts a complete set of primitives (geometry, semantics, inlier membership) in a single feed-forward pass for structured shape completion. 2. Introduces primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs. 3. Presents a training strategy that couples primitives and points with online target updates for consistent optimization.",
      "summary": "This paper addresses the problem of structured 3D shape completion from partial scans. It proposes UniCo, a model that uses primitive proxies to jointly predict a complete set of parametric primitives in a single pass, outperforming baselines by significantly lowering Chamfer distance and improving normal consistency.",
      "mindmap": "graph TB\n    A[Unified Primitive Proxies for Structured Shape Completion] --> B(核心问题/Problem: Incomplete 3D scans lack structural regularities for downstream tasks)\n    A --> C(主要方法/Method: UniCo model with primitive proxies and a dedicated decoding pathway)\n    A --> D(关键结果/Results: Lowers Chamfer distance by up to 50%, improves normal consistency by up to 7%)"
    },
    {
      "title": "Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection",
      "authors": "Akanksha Chuchra, Shukesh Reddy, Sudeepta Mishra, Abhijit Das, Abhinav Dhall",
      "institution": "Indian Institute of Technology Ropar, Birla Institute of Technology and Science Pilani Hyderabad Campus, Monash University",
      "link": "https://arxiv.org/pdf/2601.00777",
      "code": null,
      "tags": [
        "audio deepfake detection",
        "Multimodal Large Language Models",
        "audio deepfake detection",
        "zero-shot",
        "fine-tuning",
        "multi-prompt"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9e3660e734b6fff9841bbb8fb1f74d79456beeb841387a9fc33b145976d4701e_w640_q70.webp",
      "contributions": "1. Pioneering exploration of Multimodal Large Language Models for audio deepfake detection, a largely unexplored area. 2. Introduction of a text-aware, context-rich, question-answer based multi-prompt approach to facilitate multimodal understanding for the task. 3. Comprehensive evaluation of models (Qwen2-Audio-7B-Instruct, SALMONN) in zero-shot and fine-tuned modes, demonstrating their potential on in-domain data with minimal supervision.",
      "summary": "This paper investigates the use of Multimodal Large Language Models for detecting audio deepfakes by combining audio inputs with text prompts. The method employs a multi-prompt, question-answer approach and evaluates models in zero-shot and fine-tuned settings. The results show that while models struggle without training and on out-of-domain data, they achieve promising performance on in-domain data with minimal supervision, indicating a viable path forward.",
      "mindmap": "graph TB\n        Root[”Investigating MLLMs for Audio Deepfake Detection<br/>探究MLLMs用于音频深度伪造检测”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”MLLMs for audio deepfakes unexplored<br/>MLLMs用于音频深度伪造检测未被探索”]\n        Method[”Audio + Multi-prompt QA approach<br/>音频+多提示问答方法”]\n        Results[”Poor zero-shot, good fine-tuned on in-domain data<br/>零样本效果差，域内微调效果好”]"
    },
    {
      "title": "Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection",
      "authors": "Shukesh Reddy, Srijan Das, Abhijit Das",
      "institution": "Birla Institute of Technology and Science, Pilani; University of North Carolina at Charlotte",
      "link": "https://arxiv.org/pdf/2601.00789",
      "code": null,
      "tags": [
        "deepfake detection",
        "self-supervised learning",
        "auxiliary task",
        "feature fusion",
        "cross-dataset generalization",
        "local directional pattern"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9821725826aaddf6ca9d023a9670caa7eeef592d4bffd810a590e229d26cbb8_w640_q70.webp",
      "contributions": "1. Proposes a novel framework that uses self-supervised learning as an auxiliary task to optimize the primary task of generalized deepfake detection. 2. Introduces a feature fusion strategy to combine representations from the self-supervised and primary tasks, creating a more powerful and unique feature set. 3. Demonstrates superior cross-dataset generalization performance on multiple deepfake datasets compared to state-of-the-art methods.",
      "summary": "This paper addresses the problem of poor cross-dataset generalization in deepfake detection. The proposed method, Fusion-SSAT, leverages self-supervised learning as an auxiliary task and fuses its features with those from the primary detection task to create a more robust representation. The results show that this approach achieves better generalizability across multiple datasets than current state-of-the-art detectors.",
      "mindmap": "graph TB\n        A[Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection] --> B(核心问题/Problem: Poor cross-dataset generalization of deepfake detectors)\n        A --> C(主要方法/Method: Fuse features from self-supervised auxiliary task with primary detection task)\n        A --> D(关键结果/Results: Better generalizability on cross-dataset evaluation)"
    },
    {
      "title": "FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing",
      "authors": "Sunny Gupta, Amit Sethi",
      "institution": "Indian Institute of Technology Bombay",
      "link": "https://arxiv.org/pdf/2601.00785",
      "code": "github.com/sunnyinAI/FedHypeVAE",
      "tags": [
        "federated learning",
        "hypernetwork",
        "conditional VAE",
        "differential privacy",
        "MMD alignment",
        "client heterogeneity"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5726225d41733e7b16755efae5f3b9ec28771c58479913c7f5581e9843368fd0_w640_q70.webp",
      "contributions": "1. A bi-level framework using a shared hypernetwork to generate personalized, client-aware decoders and class-conditional priors for a conditional VAE, decoupling local data from shared parameters. 2. Incorporation of differential privacy during hypernetwork optimization with noise-perturbed, clipped gradients to provide formal privacy guarantees against gradient leakage. 3. Introduction of a local MMD alignment loss and Lipschitz regularization to enhance stability and distributional coherence under non-IID data conditions, along with a neutral meta-code for domain-agnostic synthesis.",
      "summary": "The paper proposes FedHypeVAE, a federated learning framework that uses a differentially private hypernetwork to generate personalized conditional VAEs for synthesizing embedding-level data across decentralized clients. It addresses challenges of non-IID data and privacy by decoupling local data from shared parameters and ensuring formal privacy guarantees. The method establishes a foundation for privacy-preserving data synthesis in federated settings by unifying personalization, privacy, and distribution alignment at the generator level.",
      "mindmap": "graph TB\n        A[FedHypeVAE] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[非IID数据与隐私挑战/non-IID & Privacy]\n        C --> C1[超网络生成条件VAE/Hypernetwork-Generated Conditional VAE]\n        C --> C2[差分隐私训练/Differentially Private Training]\n        C --> C3[MMD对齐与正则化/MMD Alignment & Regularization]\n        D --> D1[个性化与隐私统一/Unified Personalization & Privacy]\n        D --> D2[可控多域合成/Controllable Multi-Domain Synthesis]"
    },
    {
      "title": "Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI",
      "authors": "Wenhui Chu, Nikolaos V. Tsekos",
      "institution": "University of Houston",
      "link": "https://arxiv.org/pdf/2601.00794",
      "code": null,
      "tags": [
        "medical image segmentation",
        "U-Net",
        "layer normalization",
        "instance-batch normalization",
        "left ventricle segmentation",
        "cardiac MRI"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45a19599df20601908ea938c8bea28356e54685c5ce08dbe58a85a26dda95834_w640_q70.webp",
      "contributions": "1. Proposed LNU-Net, a novel segmentation architecture derived from U-Net that applies layer normalization in each convolutional block. 2. Proposed IBU-Net, another novel architecture that incorporates instance and batch normalization together in the first convolutional block. 3. Demonstrated that the proposed methods outperform state-of-the-art approaches on a dataset of 805 MRI images using metrics like dice coefficient and average perpendicular distance.",
      "summary": "This paper proposes two new deep learning models, LNU-Net and IBU-Net, for automated segmentation of the left ventricle in cardiac MRI images. The models are based on the U-Net architecture but incorporate different normalization strategies—layer normalization and a combined instance-batch normalization—to improve segmentation performance. Experimental results show that both proposed approaches outperform existing state-of-the-art methods on key evaluation metrics.",
      "mindmap": "graph TB\n        A[Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[左心室分割对临床诊断至关重要/Left ventricle segmentation is critical for clinical diagnosis]\n        C --> C1[提出LNU-Net和IBU-Net/Propose LNU-Net and IBU-Net]\n        C1 --> C2[基于U-Net，采用不同归一化策略/Based on U-Net with different normalization strategies]\n        D --> D1[在805张MRI图像上评估/Evaluated on 805 MRI images]\n        D1 --> D2[性能优于现有方法/Outperforms state-of-the-art approaches]"
    },
    {
      "title": "Automated electrostatic characterization of quantum dot devices in single- and bilayer heterostructures",
      "authors": "Merritt P. R. Losert, Dario Denora, Barnaby van Straaten, Michael Chan, Stefan D. Oosterhout, Lucas Stehouwer, Giordano Scappucci, Menno Veldhorst, Justyna P. Zwolak",
      "institution": "National Institute of Standards and Technology (NIST), Delft University of Technology (QuTech)",
      "link": "https://arxiv.org/pdf/2601.00067",
      "code": null,
      "tags": [
        "quantum computing",
        "quantum dot",
        "charge stability diagram",
        "automated characterization",
        "machine learning",
        "image processing"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c536889ababa23206b42f144321036dbbfd329505ea45c505aa53c2a04bf7815_w640_q70.webp",
      "contributions": "1. Developed an automated protocol combining ML, image processing, and object detection to extract capacitive properties from charge stability diagrams without manual labeling. 2. Demonstrated the method's effectiveness on complex bilayer germanium heterostructures, which feature interlayer tunneling and distinct loading lines. 3. Enabled statistical estimation of physically relevant device parameters, such as lever arms and capacitive couplings, facilitating rapid device characterization.",
      "summary": "This paper presents an automated method for characterizing quantum dot devices by analyzing charge stability diagrams. The protocol integrates machine learning and image processing to identify charge transitions and extract capacitive properties from experimental data. It enables rapid, scalable extraction of key device parameters, which is critical for advancing large-scale quantum dot arrays.",
      "mindmap": "graph TB\n        A[Automated electrostatic characterization of quantum dot devices<br>量子点器件自动静电表征] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Manual interpretation of CSDs is slow and error-prone<br>CSD手动解释慢且易错]\n        C --> C1[Integrates ML, image processing, object detection<br>集成ML、图像处理、目标检测]\n        D --> D1[Enables rapid extraction of device parameters (lever arms, couplings)<br>实现器件参数快速提取]"
    },
    {
      "title": "Deep Learning Approach for the Diagnosis of Pediatric Pneumonia Using Chest X-ray Imaging",
      "authors": "Fatemeh Hosseinabadi, Mohammad Mojtaba Rohani",
      "institution": "Zahedan University of Medical Sciences, Guilan University of Medical Sciences",
      "link": "https://arxiv.org/pdf/2601.00041",
      "code": null,
      "tags": [
        "medical image classification",
        "Convolutional Neural Network",
        "Transfer Learning",
        "Chest X-ray",
        "Pediatric Pneumonia",
        "RegNet"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbf67414c0b3072bcb906df1f0ca6e5942968a2374f2ac09a701ed20efb78f09_w640_q70.webp",
      "contributions": "1. Evaluated and compared the performance of state-of-the-art CNN architectures (ResNetRS, RegNet, EfficientNetV2) for automated pediatric pneumonia diagnosis. 2. Applied transfer learning with ImageNet-pretrained weights to a curated dataset of pediatric chest X-rays to address data and expertise limitations. 3. Demonstrated that the RegNet model achieved the highest accuracy and sensitivity for this specific binary classification task.",
      "summary": "This paper proposes using transfer learning with deep convolutional neural networks to automate the diagnosis of pediatric pneumonia from chest X-ray images. The authors fine-tuned and compared three CNN models (ResNetRS, RegNet, EfficientNetV2) on a curated dataset, finding that RegNet performed best with 92.4% accuracy. The study concludes that such deep learning approaches can provide reliable diagnostic support, especially in settings with limited radiological expertise.",
      "mindmap": "graph TB\n        A[Deep Learning Approach for Pediatric Pneumonia Diagnosis] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[儿童肺炎诊断困难 / Pediatric Pneumonia Diagnosis is Challenging]\n        C --> C1[使用预训练CNN进行迁移学习 / Use Pretrained CNNs with Transfer Learning]\n        C --> C2[评估ResNetRS, RegNet, EfficientNetV2 / Evaluate ResNetRS, RegNet, EfficientNetV2]\n        D --> D1[RegNet性能最佳 / RegNet Achieved Best Performance]\n        D --> D2[准确率92.4%, 敏感度90.1% / Accuracy 92.4%, Sensitivity 90.1%]"
    },
    {
      "title": "Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes",
      "authors": "Shahar Ain Kedem, Itamar Zimerman, Eliya Nachmani",
      "institution": "Ben Gurion University of the Negev, Tel Aviv University",
      "link": "https://arxiv.org/pdf/2601.00012",
      "code": "https://github.com/Shaharak88/neural-brain-fields",
      "tags": [
        "neural fields",
        "signal processing",
        "Neural Radiance Fields (NeRF)",
        "EEG",
        "brain-computer interfaces",
        "signal reconstruction",
        "continuous representation"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79dda8675c25fe2c8906136ddc20e55e51c835cb08bed2a4e02388b7dadc16da_w640_q70.webp",
      "contributions": "1. Proposes a novel NeRF-inspired method to learn a continuous representation of brain activity from discrete EEG electrode data. 2. Enables rendering of EEG signals at unseen time steps and spatial electrode positions, including simulating non-existent electrodes. 3. Demonstrates that the reconstructed signals can be used to improve the performance of standard EEG processing networks.",
      "summary": "This paper introduces Neural Brain Fields, a method inspired by Neural Radiance Fields (NeRF) to model EEG data. It trains a neural network on a single EEG sample to produce a fixed-size weight vector that encodes the continuous neural activity, allowing for signal reconstruction at any time or scalp location. The approach effectively generates data for non-existent electrodes, which can enhance downstream EEG analysis tasks.",
      "mindmap": "graph TB\n        Root[”Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>EEG data challenges: low SNR, variability, limited datasets”] --> P1[”具体挑战/Challenges<br>Varying length, low SNR, participant differences”]\n        Method[”主要方法/Method<br>NeRF-inspired neural network for EEG”] --> M1[”核心类比/Core Analogy<br>Viewpoints (NeRF) ↔ Electrodes (EEG)”]\n        Method --> M2[”技术实现/Technique<br>Train on single sample to get fixed weight vector”]\n        Results[”关键结果/Results<br>Enables continuous visualization & reconstruction”] --> R1[”功能一/Capability 1<br>Render signal at unseen times/positions”]\n        Results --> R2[”功能二/Capability 2<br>Simulate non-existent electrodes”]\n        Results --> R3[”实证结果/Empirical Result<br>Improves standard EEG network performance”]"
    },
    {
      "title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction",
      "authors": "Jiewen Chan, Zhenjun Zhao, Yu-Lun Liu",
      "institution": "National Yang Ming Chiao Tung University, University of Zaragoza",
      "link": "https://arxiv.org/pdf/2601.00796",
      "code": null,
      "tags": [
        "dynamic scene reconstruction",
        "Adaptive Gabor Representation",
        "Cubic Hermite Splines",
        "Temporal Curvature Regularization"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b1693b069cec7400ac55a2d212a74c9a9ef7f185c00490fc53dbec8796fa8f9_w640_q70.webp",
      "contributions": "1. Proposed Adaptive Gabor Representation, which extends Gaussian primitives with learnable frequency weights and adaptive energy compensation to capture high-frequency details while maintaining stability. 2. Introduced a temporal continuity modeling approach using Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution and reduce interpolation artifacts. 3. Designed an Adaptive Initialization mechanism that leverages depth estimation, point tracking, and foreground masks to establish a stable initial point cloud distribution for efficient training.",
      "summary": "This paper proposes AdaGaR, a novel framework for dynamic 3D scene reconstruction from monocular videos. It introduces an Adaptive Gabor Representation to capture high-frequency details and uses Cubic Hermite Splines with regularization to ensure temporal smoothness. Experiments show state-of-the-art performance on the DAVIS dataset, achieving superior rendering quality and strong generalization in tasks like frame interpolation and video editing.",
      "mindmap": "graph TB\n        A[AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法问题: Existing Method Issues]\n        B1 --> B1_1[高斯原语低通滤波: Gaussian Primitives Low-Pass Filtering]\n        B1 --> B1_2[标准Gabor能量不稳定: Standard Gabor Energy Instability]\n        B1 --> B1_3[缺乏时间连续性导致伪影: Lack of Temporal Continuity Causes Artifacts]\n        C --> C1[自适应Gabor表示: Adaptive Gabor Representation]\n        C1 --> C1_1[可学习频率权重: Learnable Frequency Weights]\n        C1 --> C1_2[自适应能量补偿: Adaptive Energy Compensation]\n        C --> C2[时间连续性建模: Temporal Continuity Modeling]\n        C2 --> C2_1[三次Hermite样条: Cubic Hermite Splines]\n        C2 --> C2_2[时间曲率正则化: Temporal Curvature Regularization]\n        C --> C3[自适应初始化: Adaptive Initialization]\n        C3 --> C3_1[深度估计: Depth Estimation]\n        C3 --> C3_2[点跟踪: Point Tracking]\n        C3 --> C3_3[前景掩码: Foreground Masks]\n        D --> D1[最先进性能: State-of-the-Art Performance]\n        D1 --> D1_1[PSNR: 35.49]\n        D1 --> D1_2[SSIM: 0.9433]\n        D1 --> D1_3[LPIPS: 0.0723]\n        D --> D2[强大泛化能力: Strong Generalization]\n        D2 --> D2_1[帧插值: Frame Interpolation]\n        D2 --> D2_2[深度一致性: Depth Consistency]\n        D2 --> D2_3[视频编辑: Video Editing]\n        D2 --> D2_4[立体视图合成: Stereo View Synthesis]"
    },
    {
      "title": "The Impact of Lesion Focus on the Performance of AI-Based Melanoma Classification",
      "authors": "Tanay Donde",
      "institution": "University of Illinois Urbana-Champaign",
      "link": "https://arxiv.org/pdf/2601.00355",
      "code": null,
      "tags": [
        "medical image analysis",
        "lesion attention",
        "explainable AI",
        "Grad-CAM",
        "sensitivity analysis",
        "transfer learning"
      ],
      "day": "2026-01-05",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6577c58857fa2854c7a747e548163eec65a456f081e789308195cccd972d8c9f_w640_q70.webp",
      "contributions": "1. Analyzed the relationship between model attention on lesion areas and diagnostic performance metrics (precision, recall, F1-score) in melanoma classification. 2. Employed a multi-faceted methodology involving masked images, bounding box detection, and transfer learning to investigate lesion focus. 3. Demonstrated that models with higher focus on lesion areas achieve better diagnostic performance, highlighting the value of interpretable AI for building trustworthy medical models.",
      "summary": "This study investigates how the focus (attention) of AI models on lesion areas affects their performance in classifying melanoma from skin images. The authors used methods like masked images and explainability techniques (e.g., Grad-CAM) to analyze this relationship. They found that models which pay more attention to the actual lesion regions perform better, suggesting that improving model interpretability can lead to more accurate and reliable diagnostic tools.",
      "mindmap": "graph TB\n        A[The Impact of Lesion Focus on AI-Based Melanoma Classification<br>病灶焦点对AI黑色素瘤分类性能的影响] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>CNN models lack reliability due to inconsistent focus on lesion areas.<br>CNN模型因对病灶区域关注不一致而缺乏可靠性。]\n        C[主要方法/Method<br>Analyze lesion attention using masked images, bounding box detection, transfer learning, and explainability methods.<br>使用掩码图像、边界框检测、迁移学习和可解释性方法分析病灶注意力。]\n        D[关键结果/Results<br>Higher model focus on lesion areas correlates with better diagnostic performance (precision, recall, F1-score).<br>模型对病灶区域的更高关注与更好的诊断性能（精确率、召回率、F1分数）相关。]"
    },
    {
      "title": "Break Out the Silverware -- Semantic Understanding of Stored Household Items",
      "authors": "Michaela Levi-Richter, Reuth Mirsky, Oren Glickman",
      "institution": "Bar Ilan University, Tufts University",
      "link": "https://arxiv.org/pdf/2512.23739",
      "code": null,
      "tags": [
        "commonsense reasoning",
        "benchmark dataset",
        "vision-language model",
        "hybrid agent pipeline",
        "storage location prediction",
        "semantic understanding"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/02333560037f17a9692645550b2d71e1239038dba5b036552b34388848b00f1f_w640_q70.webp",
      "contributions": "1. Introduces the Stored Household Item Challenge, a new benchmark for evaluating service robots' commonsense reasoning about predicting the storage location of non-visible household items. 2. Provides two associated datasets: a real-world evaluation set and a larger development set with annotated storage polygons. 3. Proposes NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with LLM inference to tackle the challenge, demonstrating improved accuracy approaching human performance.",
      "summary": "This paper addresses the challenge of enabling domestic robots to infer where non-visible household items are stored. It proposes a new benchmark task and datasets, and introduces NOAM, a hybrid vision-language agent that converts visual scenes into text for an LLM to predict storage locations. Evaluations show NOAM significantly outperforms baseline models and approaches human-level performance in this commonsense reasoning task.",
      "mindmap": "graph TB\n        Root[Break Out the Silverware: Semantic Understanding of Stored Household Items] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Robots lack commonsense reasoning to find stored, non-visible household items.]\n        Method[主要方法/Method: Proposes NOAM, a hybrid pipeline combining scene understanding and LLM inference.]\n        Results[关键结果/Results: NOAM approaches human-level accuracy on the new storage prediction benchmark.]"
    },
    {
      "title": "A Granular Grassmannian Clustering Framework via the Schubert Variety of Best Fit",
      "authors": "Karim Salta, Michael Kirby, Chris Peterson",
      "institution": "Colorado State University",
      "link": "https://arxiv.org/pdf/2512.23766",
      "code": null,
      "tags": [
        "subspace clustering",
        "Schubert Variety",
        "Grassmann Manifold",
        "Linde-Buzo-Grey (LBG)",
        "Subspace Clustering",
        "Geometric Learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a73e8fbe969f250567092a96ad55fca5bd7133b8ca34f30feedb918976b1faf5_w640_q70.webp",
      "contributions": "1. Introduces the concept of a trainable prototype called a Schubert Variety of Best Fit (SVBF) for representing clusters of subspaces. 2. Integrates the SVBF prototype into the Linde-Buzo-Grey (LBG) clustering pipeline to create the SVBF-LBG algorithm. 3. Demonstrates improved cluster purity on synthetic, image, spectral, and video action data compared to methods using subspace means.",
      "summary": "This paper proposes a new subspace clustering method that uses a geometric prototype called a Schubert Variety of Best Fit (SVBF) instead of a simple subspace mean. The SVBF is integrated into the Linde-Buzo-Grey algorithm, resulting in an SVBF-LBG framework that shows improved clustering performance on various data types while preserving mathematical structure for analysis.",
      "mindmap": "graph TB\n        Root[”A Granular Grassmannian Clustering Framework via the Schubert Variety of Best Fit”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Need for geometric representatives in subspace clustering”] --> Problem_Sub[”子问题/Sub-Problem<br>Subspace means on Grassmann manifold may not be optimal”]\n        Method[”主要方法/Method<br>Propose SVBF-LBG algorithm”] --> Method_Sub1[”方法组件/Component 1<br>Schubert Variety of Best Fit (SVBF) prototype”]\n        Method --> Method_Sub2[”方法组件/Component 2<br>Integration into Linde-Buzo-Grey (LBG) pipeline”]\n        Results[”关键结果/Results<br>Improved cluster purity”] --> Results_Sub1[”结果细节/Detail 1<br>Tested on synthetic, image, spectral, video data”]\n        Results --> Results_Sub2[”结果细节/Detail 2<br>Retains mathematical structure for downstream analysis”]"
    },
    {
      "title": "Leveraging Synthetic Priors for Monocular Depth Estimation in Specular Surgical Environments",
      "authors": "Ankan Aich, Yangming Lee",
      "institution": "Rochester Institute of Technology (RIT)",
      "link": "https://arxiv.org/pdf/2512.23786",
      "code": null,
      "tags": [
        "monocular depth estimation",
        "Depth Anything V2",
        "DV-LORA",
        "synthetic-to-real adaptation",
        "SCARED dataset"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dd5c633cc3115f9e880a9737b5b76f77a1dcd45eb6b882e43363394c4566463_w640_q70.webp",
      "contributions": "1. Leveraged the high-fidelity synthetic priors of the Depth Anything V2 model to capture precise geometric details of thin structures in surgical scenes. 2. Efficiently adapted these priors to the medical domain using Dynamic Vector Low-Rank Adaptation (DV-LORA) to minimize parameters and bridge the synthetic-to-real gap. 3. Introduced a physically-stratified evaluation protocol on the SCARED dataset to rigorously quantify performance in high-specularity regimes.",
      "summary": "This paper addresses the challenge of monocular depth estimation in specular, fluid-filled surgical environments by adapting the synthetic-prior-rich Depth Anything V2 model to the medical domain using DV-LORA. It also proposes a new stratified evaluation protocol for high-specularity scenes. The method achieves state-of-the-art results on the SCARED dataset, demonstrating superior robustness in adverse lighting conditions.",
      "mindmap": "graph TB\n        A[Leveraging Synthetic Priors for Monocular Depth Estimation<br>利用合成先验进行单目深度估计] --> B\n        A --> C\n        A --> D\n        B[Problem: MDE fragile in specular surgical scenes<br>问题：镜面手术场景中MDE脆弱]\n        C[Method: Adapt Depth Anything V2 with DV-LORA<br>方法：使用DV-LORA适配Depth Anything V2]\n        D[Results: SOTA on SCARED, 98.1% accuracy<br>结果：在SCARED上达到SOTA，98.1%准确率]"
    },
    {
      "title": "Video-Based Performance Evaluation for ECR Drills in Synthetic Training Environments",
      "authors": "Surya Rayala, Marcos Quinones-Grueiro, Naveeduddin Mohammed, Ashwin T S, Benjamin Goldberg, Randall Spain, Paige Lawton, Gautam Biswas",
      "institution": "Vanderbilt University, US Army DEVCOM Soldier Center",
      "link": "https://arxiv.org/pdf/2512.23819",
      "code": null,
      "tags": [
        "human pose estimation and action analysis",
        "video-based assessment",
        "2D skeleton extraction",
        "Cognitive Task Analysis (CTA)",
        "performance metrics",
        "synthetic training environments"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/33d390da3c0f68240f7cda6efac9e31e941e99ab611d61cd2837f0352453d12c_w640_q70.webp",
      "contributions": "1. A video-based assessment pipeline that extracts performance analytics (2D skeletons, gaze vectors, trajectories) from training videos without extra hardware. 2. Development of task-specific metrics for psychomotor fluency, situational awareness, and team coordination, integrated into an extended Cognitive Task Analysis hierarchy. 3. Demonstration of the approach via a case study on real-world Enter and Clear the Room drills and discussion of its integration into After Action Review systems like Gamemaster and GIFT.",
      "summary": "This paper addresses the challenge of automatically and objectively assessing soldier performance in synthetic training environments. It proposes a video-based pipeline using computer vision to extract movement and gaze data, from which it derives specific performance metrics for cognitive and teamwork skills. The method is demonstrated on real-world drills and shows potential for scalable, hardware-free evaluation to support training feedback.",
      "mindmap": "graph TB\n        Root(”Video-Based Performance Evaluation for ECR Drills”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”传统评估方法受限/Traditional assessment limited”)\n        P1 --> P1_1(”依赖昂贵传感器/Relies on costly sensors”)\n        P1 --> P1_2(”主观人为观察/Subjective human observation”)\n        Method --> M1(”视频分析管道/Video-based pipeline”)\n        M1 --> M1_1(”提取2D骨架、视线、轨迹/Extract 2D skeletons, gaze, trajectories”)\n        M1 --> M1_2(”开发任务特定指标/Develop task-specific metrics”)\n        M1 --> M1_3(”扩展认知任务分析/Extended Cognitive Task Analysis”)\n        Results --> R1(”案例研究验证/Case study validation”)\n        Results --> R2(”支持行动后评估/Supports After Action Reviews”)\n        Results --> R3(”未来: 3D分析/Future: 3D analysis”)"
    },
    {
      "title": "Pretraining Frame Preservation in Autoregressive Video Memory Compression",
      "authors": "Lvmin Zhang, Shengqu Cai, Muyang Li, Chong Zeng, Beijia Lu, Anyi Rao, Song Han, Gordon Wetzstein, Maneesh Agrawala",
      "institution": "Stanford University, MIT, Carnegie Mellon University, HKUST",
      "link": "https://arxiv.org/pdf/2512.23851",
      "code": null,
      "tags": [
        "video generation",
        "autoregressive video models",
        "memory compression",
        "frame preservation",
        "context length",
        "pretraining"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd63197bd634062ab10ea1d91296e2bdbae09b94e054d1cc001ce4fbedb7bfcd_w640_q70.webp",
      "contributions": "1. Proposes PFP, a novel neural network structure designed to compress long videos into short contexts while preserving high-frequency details of individual frames. 2. Introduces an explicit pretraining objective focused on preserving the perceptual appearance of frames at arbitrary temporal positions within the compressed context. 3. Demonstrates that pretrained PFP models can be effectively fine-tuned as memory encoders for autoregressive video models, enabling long-range history memory with low context cost and minimal fidelity loss.",
      "summary": "This paper addresses the challenge of managing long video history in autoregressive video generation models by proposing PFP, a pretraining method for compressing videos into short contexts while preserving frame details. The method allows a 20-second video to be compressed into a ~5k length context, enabling efficient long-term memory for video generation with low fidelity loss. The framework is evaluated through ablative studies, discussing trade-offs in neural architecture design.",
      "mindmap": "graph TB\n        Root(”Pretraining Frame Preservation in Autoregressive Video Memory Compression”) --> Problem(”核心问题/Problem: Long video history leads to high context cost in autoregressive models”)\n        Root --> Method(”主要方法/Method: PFP pretraining to compress video into short context while preserving frame details”)\n        Root --> Results(”关键结果/Results: 20s video compressed to ~5k context; enables long history memory with low fidelity loss”)"
    },
    {
      "title": "Lifelong Domain Adaptive 3D Human Pose Estimation",
      "authors": "Qucheng Peng, Hongfei Xue, Pu Wang, Chen Chen",
      "institution": "University of Central Florida, University of North Carolina at Charlotte",
      "link": "https://arxiv.org/pdf/2512.23860",
      "code": null,
      "tags": [
        "human pose estimation",
        "lifelong domain adaptation",
        "catastrophic forgetting",
        "generative adversarial network",
        "pose-aware knowledge",
        "temporal-aware knowledge"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbe905e18ac9835b4aae0dbb155169c447150fbd0e28ac454c7cc8a56bb7251e_w640_q70.webp",
      "contributions": "1. Proposes a novel lifelong domain adaptation task for 3D Human Pose Estimation, addressing the challenge of non-stationary target pose datasets. 2. Introduces an innovative GAN framework with 3D pose generators, a 2D pose discriminator, and a 3D pose estimator to mitigate domain shifts and align poses. 3. Constructs a novel 3D pose generator paradigm that integrates pose-aware, temporal-aware, and domain-aware knowledge to enhance adaptation and alleviate catastrophic forgetting.",
      "summary": "This paper proposes a lifelong domain adaptation framework for 3D human pose estimation to handle non-stationary target data distributions. The method uses a novel GAN-based framework with a knowledge-integrated 3D pose generator to adapt to new domains while preventing catastrophic forgetting of previous ones. Experiments show the approach achieves superior performance on diverse domain adaptive 3D HPE datasets.",
      "mindmap": "graph TB\n    A[Lifelong Domain Adaptive 3D Human Pose Estimation] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[3D HPE泛化挑战/3D HPE Generalization Challenge]\n    B --> B2[非平稳目标域/Non-stationary Target Domains]\n    B --> B3[灾难性遗忘/Catastrophic Forgetting]\n    C --> C1[终身域适应任务/Lifelong DA Task]\n    C --> C2[GAN框架/GAN Framework]\n    C --> C3[3D姿态生成器/3D Pose Generator]\n    C2 --> C2a[3D姿态生成器/3D Pose Generators]\n    C2 --> C2b[2D姿态判别器/2D Pose Discriminator]\n    C2 --> C2c[3D姿态估计器/3D Pose Estimator]\n    C3 --> C3a[姿态感知/Pose-aware]\n    C3 --> C3b[时序感知/Temporal-aware]\n    C3 --> C3c[域感知/Domain-aware]\n    D --> D1[缓解域偏移/Mitigates Domain Shifts]\n    D --> D2[对齐姿态/Aligns Poses]\n    D --> D3[卓越性能/Superior Performance]"
    },
    {
      "title": "Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation",
      "authors": "Guo Ye, Zexi Zhang, Xu Zhao, Shang Wu, Haoran Lu, Shihan Lu, Han Liu",
      "institution": "Northwestern University",
      "link": "https://arxiv.org/pdf/2512.23864",
      "code": null,
      "tags": [
        "robotic manipulation",
        "tactile sensing",
        "vision-language-action models",
        "hierarchical perception",
        "world model",
        "contact-rich manipulation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41a7e7908fa684e60efce68108a909c179516c57fb8ccd7e68353665652c969a_w640_q70.webp",
      "contributions": "1. Introduces DreamTacVLA, a framework that grounds VLA models in contact physics using a hierarchical perception scheme with high-resolution tactile images, wrist-camera vision, and third-person vision. 2. Proposes a Hierarchical Spatial Alignment (HSA) loss to align tactile tokens with their spatial counterparts in visual views, creating a unified representation. 3. Finetunes the system with a tactile world model that predicts future tactile signals, enabling the agent to condition actions on anticipated contact dynamics, and constructs a hybrid large-scale dataset from digital twin and real-world sources to overcome data scarcity.",
      "summary": "This paper addresses the limitation of Vision-Language-Action (VLA) models in contact-rich manipulation tasks by introducing DreamTacVLA. The method integrates high-resolution tactile sensing with vision using hierarchical spatial alignment and a tactile prediction world model, trained on a hybrid dataset. It demonstrates superior performance, achieving up to 95% success, highlighting the importance of touch-aware reasoning for robust robotic agents.",
      "mindmap": "graph TB\n        A[Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation] --> B[核心问题/Problem: VLA models are blind to physical contact, struggling with contact-rich tasks.]\n        A --> C[主要方法/Method: Hierarchical perception with tactile, wrist, and third-person vision, aligned via HSA loss and finetuned with a tactile world model.]\n        A --> D[关键结果/Results: Outperforms VLA baselines, achieving up to 95% success on contact-rich manipulation tasks.]"
    },
    {
      "title": "MRI-to-CT Synthesis With Cranial Suture Segmentations Using A Variational Autoencoder Framework",
      "authors": "Krithika Iyer, Austin Tapp, Athelia Paulli, Gabrielle Dickerson, Syed Muhammad Anwar, Natasha Lepore, Marius George Linguraru",
      "institution": "Sheikh Zayed Institute for Pediatric Surgical Innovation, Children’s National Hospital; CIBORG Lab, Children's Hospital Los Angeles; University of Southern California",
      "link": "https://arxiv.org/pdf/2512.23894",
      "code": null,
      "tags": [
        "medical image synthesis",
        "variational autoencoder",
        "synthetic CT",
        "cranial suture segmentation",
        "pediatric neuroimaging",
        "MRI-to-CT translation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1440b1c25db399b2255913b18d4389f7c541299e16cbe4a1083b532fb23ded2_w640_q70.webp",
      "contributions": "1. Proposed a deep learning pipeline to generate synthetic CTs (sCTs) from pediatric T1-weighted MRIs, enabling bone and suture visualization from radiation-free scans. 2. Introduced a method to predict detailed cranial bone segmentation and generate suture probability heatmaps, from which direct suture segmentations are derived. 3. Demonstrated, for the first time, a pediatric cranial CT synthesis framework capable of accurate suture segmentation on MRI-derived sCTs, validated with high structural similarity and Dice scores.",
      "summary": "This paper addresses the problem of visualizing cranial bones and sutures in pediatric patients without using ionizing radiation. The authors propose a pipeline based on variational autoencoders to synthesize CT scans from routine MRI, which also predicts bone and suture segmentations. The results show that the synthetic CTs are perceptually indistinguishable from real CTs and enable accurate, non-invasive cranial evaluation.",
      "mindmap": "graph TB\n        A[MRI-to-CT Synthesis With Cranial Suture Segmentations<br>MRI-to-CT合成与颅缝分割] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[CT有辐射, MRI无法显示骨骼/CT has radiation, MRI cannot show bone]\n        C --> C1[使用变分自编码器生成合成CT/Use VAE to generate synthetic CT]\n        C --> C2[预测骨骼分割和颅缝概率图/Predict bone segmentation & suture heatmaps]\n        D --> D1[合成CT与真实CT高度相似/sCT highly similar to real CT]\n        D --> D2[骨骼和颅缝分割准确/Bone & suture segmentation accurate]"
    },
    {
      "title": "Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale",
      "authors": "Charith Wickrema, Eliza Mace, Hunter Brown, Heidys Cabrera, Nick Krall, Matthew O'Neill, Shivangi Sarkar, Lowell Weissman, Eric Hughes, Guido Zarrella",
      "institution": "The MITRE Corporation",
      "link": "https://arxiv.org/pdf/2512.23903",
      "code": null,
      "tags": [
        "remote sensing",
        "vision transformer",
        "scaling laws",
        "data-limited regime",
        "electro-optical",
        "foundation models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/940589a24e7270959f66022b367ddfb910f57d2b706f38db894b2c98d2493dd3_w640_q70.webp",
      "contributions": "1. Conducted a large-scale empirical study on scaling vision transformer (ViT) models for remote sensing using over a quadrillion pixels of commercial satellite data, significantly exceeding prior scales. 2. Identified and reported success and failure modes observed during petascale training of remote sensing foundation models, providing practical insights for the community. 3. Demonstrated that performance in this domain, even at petascale, remains in a data-limited regime rather than a model parameter-limited one, challenging assumptions from natural-image domains.",
      "summary": "This paper investigates the scaling behavior of foundation models for remote sensing by training progressively larger vision transformer backbones on a massive dataset of over a quadrillion satellite pixels. The key finding is that, unlike in natural-image domains, remote sensing model performance at this scale is still limited by data availability rather than model capacity. These insights aim to guide future data collection, compute budgeting, and optimization for frontier-scale remote sensing AI.",
      "mindmap": "graph TB\n        Root(”Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”远程感知领域缺乏扩展规律/Lack of scaling laws in remote sensing”)\n        Problem --> P2(”需要领域专用编码器/Need for domain-specialized encoders”)\n        Method --> M1(”使用超万亿像素卫星数据/Train on quadrillion-pixel EO data”)\n        Method --> M2(”训练渐进式更大的ViT/Train progressively larger ViT backbones”)\n        Results --> R1(”观察到数据限制而非模型限制/Observed data-limited regime”)\n        Results --> R2(”报告成功与失败模式/Report success and failure modes”)"
    },
    {
      "title": "Learning to learn skill assessment for fetal ultrasound scanning",
      "authors": "Yipei Wang, Qianye Yang, Lior Drukker, Aris T. Papageorghiou, Yipeng Hu, J. Alison Noble",
      "institution": "University of Oxford, University College London",
      "link": "https://arxiv.org/pdf/2512.23920",
      "code": null,
      "tags": [
        "medical image analysis",
        "bi-level optimisation",
        "meta learning",
        "skill assessment",
        "fetal ultrasound",
        "task predictor"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18a809a751f481712110929a049f34570c336104353fc4936c1ea3360376670c_w640_q70.webp",
      "contributions": "1. Proposes a novel bi-level optimisation framework for skill assessment that does not rely on manually predefined skill ratings. 2. Introduces a method that quantifies skill by how well a clinical task is performed on the acquired images, using a joint clinical task predictor and skill predictor. 3. Validates the feasibility of the proposed framework on real-world clinical ultrasound videos for scanning the fetal head.",
      "summary": "This paper addresses the subjective and time-intensive nature of traditional ultrasound skill assessment by proposing a novel bi-level optimisation framework. The method assesses a sonographer's skill by how well a clinical task is performed on the acquired fetal ultrasound images, without needing predefined skill labels. The results demonstrate the framework's feasibility in predicting ultrasound skills by quantifying optimized task performance as a skill indicator.",
      "mindmap": "graph TB\n        A[Learning to learn skill assessment for fetal ultrasound scanning] --> B(核心问题/Problem: 传统超声技能评估主观且耗时/Traditional ultrasound skill assessment is subjective and time-intensive)\n        A --> C(主要方法/Method: 提出双层优化框架/Propose a bi-level optimisation framework)\n        A --> D(关键结果/Results: 验证了通过优化任务表现量化技能的可行性/Validated feasibility of quantifying skill via optimised task performance)"
    },
    {
      "title": "MGML: A Plug-and-Play Meta-Guided Multi-Modal Learning Framework for Incomplete Multimodal Brain Tumor Segmentation",
      "authors": "Yulong Zou, Bo Liu, Cun-Jing Zheng, Yuan-ming Geng, Siyue Li, Qiankun Zuo, Shuihua Wang, Yudong Zhang, Jin Hong",
      "institution": "Nanchang University",
      "link": "https://arxiv.org/pdf/2512.23936",
      "code": "https://github.com/worldlikerr/MGML",
      "tags": [
        "medical image segmentation",
        "incomplete multimodal learning",
        "meta-parameterized fusion",
        "consistency regularization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d47f46d185c2662482a0c33ab269c7e876415553d0649cca8e73ee3645e65e6_w640_q70.webp",
      "contributions": "1. Proposes a plug-and-play Meta-Guided Multi-Modal Learning (MGML) framework for incomplete multimodal brain tumor segmentation. 2. Introduces a meta-parameterized adaptive modality fusion (Meta-AMF) module to generate adaptive supervision for coherent multimodal fusion. 3. Incorporates a consistency regularization module to enhance segmentation performance and improve model robustness and generalization.",
      "summary": "The paper addresses the challenge of incomplete multimodal MRI data in brain tumor segmentation by proposing a plug-and-play MGML framework. The framework uses meta-parameterized adaptive fusion and consistency regularization to effectively utilize available modalities and improve segmentation performance. Experiments on BraTS datasets show the method outperforms state-of-the-art approaches.",
      "mindmap": "graph TB\n    A[MGML: A Plug-and-Play Meta-Guided Multi-Modal Learning Framework] --> B(核心问题/Problem: Incomplete Multimodal MRI Data for Brain Tumor Segmentation)\n    A --> C(主要方法/Method: Meta-parameterized Adaptive Modality Fusion & Consistency Regularization)\n    A --> D(关键结果/Results: Superior Performance on BraTS2020 & BraTS2023 Datasets)"
    },
    {
      "title": "Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation",
      "authors": "Hualin Ye, Bingxi Liu, Jixiang Du, Yu Qin, Ziyi Chen, Hong Zhang",
      "institution": "Huaqiao University, Southern University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.23938",
      "code": null,
      "tags": [
        "cross-view geo-localisation",
        "DINOv2",
        "Mixture-of-Experts (MoE)",
        "multi-scale channel reallocation",
        "convolution adapter",
        "parameter-efficient fine-tuning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/deb51d7736f1851644b47ea6627c5e39017ca1a64961ed88b6527ed2c0fb7b5c_w640_q70.webp",
      "contributions": "1. Leveraging the DINOv2 backbone with a convolution adapter for parameter-efficient fine-tuning to enhance adaptability to cross-view variations. 2. Proposing a multi-scale channel reallocation module to strengthen the diversity and stability of spatial representations. 3. Introducing an improved aggregation module that integrates Mixture-of-Experts (MoE) routing into a cross-attention framework for adaptive feature processing.",
      "summary": "This paper addresses the challenge of cross-view geo-localisation, where significant viewpoint discrepancies hinder image matching. The proposed method introduces a novel system featuring a DINOv2 backbone with a convolution adapter, a multi-scale channel reallocation module, and an MoE-enhanced aggregation module for adaptive feature processing. Experiments show the method achieves competitive performance with fewer trained parameters.",
      "mindmap": "graph TB\n        A[Learnable Query Aggregation with KV Routing for Cross-view Geo-localisation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[跨视角差异大/Cross-view discrepancies hinder feature alignment]\n        C --> C1[使用带卷积适配器的DINOv2/Use DINOv2 with convolution adapter]\n        C --> C2[多尺度通道重分配模块/Multi-scale channel reallocation module]\n        C --> C3[KV路由的MoE聚合模块/MoE aggregation with KV routing]\n        D --> D1[在University-1652和SUES-200上性能优异/Competitive performance on University-1652 & SUES-200]\n        D --> D2[参数更少/Fewer trained parameters]"
    },
    {
      "title": "Kinematic-Based Assessment of Surgical Actions in Microanastomosis",
      "authors": "Yan Meng, Daniel Donoho, Marcelle Altshuler, Omar Arnaout",
      "institution": "Children's National Hospital, Brigham and Women's Hospital, Harvard Medical School",
      "link": "https://arxiv.org/pdf/2512.23942",
      "code": null,
      "tags": [
        "surgical video analysis",
        "action segmentation",
        "self-similarity matrix",
        "YOLO-DeepSORT",
        "surgical skill assessment",
        "edge computing"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/454fd192bf8243137cb3f65f7d10780b527e77fd15d64c451fe6d42970107350_w640_q70.webp",
      "contributions": "1. Proposed a novel AI-driven framework for automated action segmentation and performance assessment in microanastomosis procedures, designed for edge computing platforms. 2. Introduced a system with three key modules: instrument tip tracking (YOLO & DeepSORT), action segmentation via self-similarity matrix and unsupervised clustering, and a supervised classification module for skill evaluation. 3. Demonstrated high effectiveness on a dataset of 58 expert-rated videos, achieving 92.4% frame-level action segmentation accuracy and 85.5% skill classification accuracy, validating its potential for objective, real-time surgical training feedback.",
      "summary": "This paper introduces an AI framework to automate the assessment of surgical skill in microanastomosis procedures. The method combines instrument tracking, action segmentation using self-similarity matrices, and supervised classification to evaluate performance. Experimental results show high accuracy in segmenting actions and classifying skill, demonstrating its potential for providing objective, real-time feedback in surgical training.",
      "mindmap": "graph TB\n        A[Kinematic-Based Assessment of Surgical Actions in Microanastomosis] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[主观评估耗时耗力/Subjective, time-consuming expert assessment]\n        C --> C1[目标追踪/YOLO-DeepSORT tracking]\n        C --> C2[动作分割/Self-similarity matrix segmentation]\n        C --> C3[技能分类/Supervised skill classification]\n        D --> D1[92.4% 分割准确率/92.4% segmentation accuracy]\n        D --> D2[85.5% 分类准确率/85.5% classification accuracy]"
    },
    {
      "title": "T2VAttack: Adversarial Attack on Text-to-Video Diffusion Models",
      "authors": "Changzhen Li, Yuecong Min, Jie Zhang, Zheng Yuan, Shiguang Shan, Xilin Chen",
      "institution": "Institute of Computing Technology (ICT), Chinese Academy of Sciences (CAS); University of Chinese Academy of Sciences (UCAS)",
      "link": "https://arxiv.org/pdf/2512.23953",
      "code": null,
      "tags": [
        "adversarial machine learning",
        "adversarial attack",
        "text-to-video",
        "diffusion models",
        "prompt perturbation",
        "semantic-temporal evaluation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8069e3c162b4bb54472f5b4383bd4f61d383e9d40b2077dc42f8a12561729b3_w640_q70.webp",
      "contributions": "1. Introduces T2VAttack, the first comprehensive study of adversarial attacks on Text-to-Video (T2V) diffusion models from both semantic and temporal perspectives. 2. Proposes two novel attack methods: T2VAttack-S (synonym replacement via greedy search) and T2VAttack-I (iterative word insertion). 3. Conducts a comprehensive evaluation revealing critical vulnerabilities in state-of-the-art T2V models, showing that minor prompt modifications can cause substantial degradation in output quality.",
      "summary": "This paper investigates the adversarial robustness of Text-to-Video (T2V) diffusion models. It proposes T2VAttack, a framework with two attack methods (synonym replacement and word insertion) targeting both semantic and temporal video quality. The experiments show that even small, subtle changes to the input prompt can significantly degrade the output of several leading T2V models, exposing a critical security vulnerability.",
      "mindmap": "graph TB\n        Root[”T2VAttack: Adversarial Attack on Text-to-Video Diffusion Models”] --> Problem[”核心问题/Problem: Vulnerability of T2V models to adversarial attacks”]\n        Root --> Method[”主要方法/Method: T2VAttack-S (synonym replacement) & T2VAttack-I (word insertion)”]\n        Root --> Results[”关键结果/Results: Minor prompt changes cause substantial degradation in SOTA models”]"
    },
    {
      "title": "U-Net-Like Spiking Neural Networks for Single Image Dehazing",
      "authors": "Huibin Li, Haoran Liu, Mingzhe Liu, Yulong Xiao, Peng Li, Guibin Zan",
      "institution": "Chengdu University of Technology, Wenzhou University of Technology, Politecnico di Milano, Sigray, Inc.",
      "link": "https://arxiv.org/pdf/2512.23950",
      "code": "https://github.com/HaoranLiu507/DehazeSNN",
      "tags": [
        "image dehazing",
        "Spiking Neural Networks",
        "U-Net",
        "Leaky-Integrate-and-Fire",
        "computational efficiency"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b4280687a6989b7dc46c7fdadaa4358338cd9373eaf17fdd4a397e16e7d6732_w640_q70.webp",
      "contributions": "1. Proposes DehazeSNN, a novel architecture integrating a U-Net-like design with Spiking Neural Networks (SNNs) for single image dehazing. 2. Introduces the Orthogonal Leaky-Integrate-and-Fire Block (OLIFBlock) to enhance cross-channel communication. 3. Demonstrates competitive performance with state-of-the-art methods while achieving a smaller model size and reduced computational operations (MACs).",
      "summary": "This paper addresses the challenge of single image dehazing by proposing DehazeSNN, a U-Net-like architecture built with Spiking Neural Networks. The method introduces a novel OLIFBlock to improve feature processing and aims to overcome the limitations of CNNs and Transformers regarding long-range dependencies and computational cost. Experiments show DehazeSNN delivers high-quality results with improved efficiency on benchmark datasets.",
      "mindmap": "graph TB\n        A[U-Net-Like Spiking Neural Networks for Single Image Dehazing] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>图像去雾/Image Dehazing] --> B1[CNN难以处理长程依赖/CNNs struggle with long-range dependencies]\n        B --> B2[Transformer计算成本高/Transformers demand high computational resources]\n        C[主要方法/Method<br>DehazeSNN] --> C1[U-Net-like SNN架构/U-Net-like SNN Architecture]\n        C --> C2[正交LIF块/Orthogonal LIF Block (OLIFBlock)]\n        D[关键结果/Results] --> D1[性能有竞争力/Competitive performance]\n        D --> D2[模型小，计算量少/Smaller model size, fewer MACs]"
    },
    {
      "title": "DriveExplorer: Images-Only Decoupled 4D Reconstruction with Progressive Restoration for Driving View Extrapolation",
      "authors": "Yuang Jia, Jinlong Wang, Jiayi Zhao, Chunlam Li, Shunzhou Wang, Wei Gao",
      "institution": "Peking University (Shenzhen Graduate School)",
      "link": "https://arxiv.org/pdf/2512.23983",
      "code": null,
      "tags": [
        "novel view synthesis",
        "4D Gaussian Splatting",
        "view extrapolation",
        "video diffusion model",
        "progressive restoration",
        "point cloud estimation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ed84d0d40dcaf8960a53c9af7a3ee129d60f54e15eeb76c74bc49c1cdd997a5_w640_q70.webp",
      "contributions": "1. Proposes an images-only, decoupled 4D reconstruction method that estimates and fuses static and dynamic point clouds without relying on LiDAR or manual annotations. 2. Introduces a progressive restoration pipeline that iteratively refines 4D Gaussian renderings using a video diffusion model to enhance quality for large viewpoint extrapolation. 3. Demonstrates a method for driving view extrapolation that outperforms baselines in generating high-quality images at novel, shifted viewpoints.",
      "summary": "This paper addresses the problem of generating high-quality images from novel viewpoints in autonomous driving scenarios without relying on expensive sensors or labels. The proposed method, DriveExplorer, first reconstructs a 4D scene using a deformable Gaussian framework from images only, then iteratively refines the renderings for extrapolated views using a video diffusion model in a progressive restoration loop. The results show that this approach produces higher-quality extrapolated views compared to existing baselines.",
      "mindmap": "graph TB\n        A[DriveExplorer: Images-Only Decoupled 4D Reconstruction] --> B[核心问题/Problem: 依赖昂贵先验的驾驶视图外推<br>Problem: View extrapolation relies on expensive priors (LiDAR, labels)]\n        A --> C[主要方法/Method: 渐进式恢复的4D高斯与扩散模型<br>Method: Progressive restoration with 4D Gaussian & diffusion model]\n        A --> D[关键结果/Results: 生成更高质量的外推视图图像<br>Results: Produces higher-quality extrapolated view images]"
    },
    {
      "title": "Anomaly detection in satellite imagery through temporal inpainting",
      "authors": "Bertrand Rouet-Leduc, Claudia Hulbert",
      "institution": "Kyoto University, Geolabe",
      "link": "https://arxiv.org/pdf/2512.23986",
      "code": null,
      "tags": [
        "anomaly detection",
        "temporal inpainting",
        "SATLAS foundation model",
        "Sentinel-2",
        "change detection",
        "deep learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50fd4012191b7a3ba947d1bc5a8c1c43be7c391abfdcc5dc5f22c18480ef540f_w640_q70.webp",
      "contributions": "1. Proposes a novel deep learning method for satellite anomaly detection using temporal inpainting to predict the expected state of the surface. 2. Leverages the SATLAS foundation model, fine-tuned on global Sentinel-2 time series data across diverse environments. 3. Demonstrates superior sensitivity and specificity in detecting surface ruptures from an earthquake, with detection thresholds approximately three times lower than traditional baseline methods.",
      "summary": "This paper tackles the challenge of detecting surface changes in satellite imagery by using a deep learning inpainting model to predict what a location should look like based on its past appearance. The discrepancy between this prediction and the actual observation highlights anomalies. The method, validated on earthquake ruptures, shows significantly higher sensitivity than traditional approaches, enabling more precise global monitoring.",
      "mindmap": "graph TB\n        Root[Anomaly detection in satellite imagery through temporal inpainting] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>Detecting surface changes is hard due to noise and seasonal variations.] --> P1[挑战/Challenges<br>Atmospheric noise, seasonal changes, sensor artifacts]\n        Method[主要方法/Method<br>Deep learning temporal inpainting model] --> M1[模型基础/Model Foundation<br>Built upon SATLAS foundation model]\n        Method --> M2[训练数据/Training Data<br>Global Sentinel-2 time series across diverse climates]\n        Method --> M3[检测原理/Detection Principle<br>Discrepancy between prediction and observation reveals anomalies]\n        Results[关键结果/Results<br>Validated on 2023 Turkey-Syria earthquake surface ruptures] --> R1[性能/Performance<br>Higher sensitivity & specificity than baselines (temporal median, Reed-Xiaoli)]\n        Results --> R2[灵敏度提升/Sensitivity Gain<br>Detection thresholds ~3x lower than baseline approaches]"
    },
    {
      "title": "GCA-ResUNet: Medical Image Segmentation Using Grouped Coordinate Attention",
      "authors": "Jun Ding, Shang Gao",
      "institution": "Not explicitly stated in provided content. Affiliation/email domain not found.",
      "link": "https://arxiv.org/pdf/2512.23990",
      "code": null,
      "tags": [
        "medical image segmentation",
        "Grouped Coordinate Attention",
        "GCA-ResUNet",
        "coordinate attention",
        "multi-organ segmentation",
        "CNN-Transformer hybrid"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dece05c6757ff0ac5c42368fc1f0f87e90e69b1db30883b770662cbd8f580481_w640_q70.webp",
      "contributions": "1. Identified a limitation of unified attention mechanisms in CNNs for handling channel-wise semantic heterogeneity in multi-organ and low-contrast medical image segmentation. 2. Proposed a lightweight, plug-and-play Grouped Coordinate Attention (GCA) module that decouples channel context modeling into groups and integrates direction-aware coordinate encoding. 3. Developed an effective integration strategy embedding GCA into a ResNet50 backbone, demonstrating consistent performance improvements over CNN and Transformer baselines on public benchmarks.",
      "summary": "This paper proposes GCA-ResUNet, a medical image segmentation framework that enhances a CNN backbone with a novel Grouped Coordinate Attention module to better capture long-range dependencies and channel heterogeneity. The method achieves state-of-the-art results on Synapse and ACDC datasets, offering a favorable trade-off between accuracy and computational efficiency for clinical deployment.",
      "mindmap": "graph TB\n        A[GCA-ResUNet: Medical Image Segmentation Using Grouped Coordinate Attention] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[CNN的局部性和统一注意力机制限制了长程上下文建模 / CNN locality & unified attention limit long-range context modeling]\n        B --> B2[Transformer计算成本高，需要大量数据 / Transformer is computationally expensive and data-hungry]\n        C --> C1[提出分组坐标注意力模块 / Propose Grouped Coordinate Attention (GCA) module]\n        C1 --> C1a[通道分组建模语义异质性 / Channel grouping for semantic heterogeneity]\n        C1 --> C1b[方向感知坐标编码捕获空间依赖 / Direction-aware coordinate encoding for spatial dependencies]\n        C --> C2[将GCA集成到ResNet50骨干网络中 / Integrate GCA into ResNet50 backbone]\n        D --> D1[在Synapse和ACDC数据集上达到SOTA / Achieves SOTA on Synapse & ACDC benchmarks]\n        D --> D2[在复杂边界和小结构分割上表现更好 / Better at delineating complex boundaries & small structures]\n        D --> D3[提供精度与效率的良好权衡 / Provides favorable accuracy-efficiency trade-off]"
    },
    {
      "title": "Improved 3D Gaussian Splatting of Unknown Spacecraft Structure Using Space Environment Illumination Knowledge",
      "authors": "Tae Ha Park, Simone D'Amico",
      "institution": "Nara Space Technology Inc., Stanford University",
      "link": "https://arxiv.org/pdf/2512.23998",
      "code": null,
      "tags": [
        "3D reconstruction",
        "3D Gaussian Splatting",
        "Rendezvous and Proximity Operations",
        "Photometric Optimization",
        "Shadow Splatting",
        "Spacecraft Pose Estimation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e32ab5204fb3cfe9ec9f948cf84dff85038750fa587faef643915e0d8fb067b_w640_q70.webp",
      "contributions": "1. Proposes a novel pipeline to recover the 3D structure of an unknown spacecraft using 3D Gaussian Splatting (3DGS) from monocular image sequences captured during RPO., 2. Incorporates prior knowledge of the Sun's position (estimated by the servicer spacecraft) into the 3DGS training pipeline to handle dynamic space illumination and improve photometric rendering quality., 3. Demonstrates that the enhanced 3DGS model can adapt to rapidly changing lighting, reflect global shadows and self-occlusion, and is crucial for improving downstream camera pose estimation tasks.",
      "summary": "This paper addresses the challenge of reconstructing the 3D structure of an unknown spacecraft from images taken during space rendezvous, where dynamic lighting violates the static scene assumption of standard 3D Gaussian Splatting. The proposed method integrates known Sun position information into the training process to significantly improve the photometric accuracy of the rendered model. Experiments show this approach enables the model to adapt to space illumination changes, producing better geometry and appearance for downstream pose estimation.",
      "mindmap": "graph TB\n        Root(”Improved 3D Gaussian Splatting of Unknown Spacecraft Structure Using Space Environment Illumination Knowledge”) --> Problem(”核心问题/Problem: 3DGS训练需要静态场景，与太空动态光照条件冲突”)\n        Root --> Method(”主要方法/Method: 将已知的太阳位置先验知识整合到3DGS训练流程中”)\n        Root --> Results(”关键结果/Results: 模型适应快速变化的光照，反映全局阴影和自遮挡”)"
    },
    {
      "title": "Bridging Structure and Appearance: Topological Features for Robust Self-Supervised Segmentation",
      "authors": "Haotang Li, Zhenyu Qi, Hao Qin, Huanrui Yang, Sen He, Kebin Peng",
      "institution": "The University of Arizona, East Carolina University",
      "link": "https://arxiv.org/pdf/2512.23997",
      "code": null,
      "tags": [
        "semantic segmentation",
        "self-supervised learning",
        "topological features",
        "cross-modal alignment",
        "adversarial augmentation",
        "differentiable box-counting"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6d58e5a3637c9b516e1fe8659ad85bc4de8415691d3af5e82026319ad6445e50_w640_q70.webp",
      "contributions": "1. Proposed GASeg, a novel self-supervised segmentation framework that bridges geometry and appearance using topological information. 2. Introduced the Differentiable Box-Counting (DBC) module to quantify multi-scale topological statistics from geometric and appearance feature streams. 3. Designed Topological Augmentation (TopoAug), an adversarial strategy using morphological operators, and GALoss, a multi-objective loss for cross-modal feature alignment.",
      "summary": "This paper addresses the problem of self-supervised semantic segmentation failing under appearance ambiguities. It proposes GASeg, a framework that uses topological features to bridge geometry and appearance, featuring a differentiable box-counting module, topological augmentation, and a cross-modal alignment loss. The method achieves state-of-the-art performance on benchmarks like COCO-Stuff and Cityscapes, demonstrating the effectiveness of integrating topological information for robust segmentation.",
      "mindmap": "graph TB\n        A[GASeg: Bridging Geometry and Appearance via Topology] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Self-supervised segmentation fails under appearance ambiguities 自监督分割在表观歧义下失败]\n        C --> C1[Proposed GASeg framework 提出GASeg框架]\n        C1 --> C2[Differentiable Box-Counting (DBC) module 可微盒计数模块]\n        C1 --> C3[Topological Augmentation (TopoAug) 拓扑增强]\n        C1 --> C4[Multi-objective GALoss 多目标GALoss]\n        D --> D1[Achieves SOTA on COCO-Stuff, Cityscapes, PASCAL 在多个基准上达到SOTA]"
    },
    {
      "title": "FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing",
      "authors": "Yunkai Dang, Donghao Wang, Jiacheng Yang, Yifan Jiang, Meiyi Zhu, Yuekun Yang, Cong Wang, Qi Fan, Wenbin Li, Yang Gao",
      "institution": "Nanjing University",
      "link": "https://arxiv.org/pdf/2512.24022",
      "code": "https://github.com/Yunkaidang/RSVLM",
      "tags": [
        "vision-language models",
        "multi-feature fusion",
        "recurrent visual injection",
        "remote sensing vision-language model"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51db66b7af59969350cb2c0f2ca84f598178ab4f9ea4040dfc08f46586ef7fe0_w640_q70.webp",
      "contributions": "1. Proposes a Multi-Feature Fusion Remote Sensing Vision-Language Model (MF-RSVLM) that extracts and fuses multi-scale visual features to better capture small and complex structures in remote sensing scenes., 2. Introduces a recurrent visual feature injection scheme to keep the language model grounded in visual evidence and mitigate visual forgetting during text generation., 3. Demonstrates state-of-the-art or highly competitive performance on diverse remote sensing benchmarks, including classification, image captioning, and VQA tasks.",
      "summary": "The paper addresses the challenge of applying general vision-language models to remote sensing data by proposing MF-RSVLM, a model that fuses multi-scale visual features and uses recurrent visual injection to reduce forgetting. It achieves strong results on remote sensing classification, captioning, and VQA tasks.",
      "mindmap": "graph TB\n        A[FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: VLMs struggle with fine-grained features and visual forgetting in remote sensing]\n        C[主要方法/Method: Multi-feature fusion and recurrent visual injection]\n        D[关键结果/Results: SOTA/competitive performance on RS benchmarks]"
    },
    {
      "title": "Bridging the Perception-Cognition Gap:Re-engineering SAM2 with Hilbert-Mamba for Robust VLM-based Medical Diagnosis",
      "authors": "Hao Wu, Hui Li, Yiyun Su",
      "institution": "Southern University of Science and Technology, Xiamen University, Rutgers University",
      "link": "https://arxiv.org/pdf/2512.24013",
      "code": null,
      "tags": [
        "medical image segmentation",
        "Hilbert Curve",
        "Mamba SSM",
        "Cross-Attention",
        "Multi-Modal Fusion",
        "Visual Language Model"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21f004815031e896e86d16784cfd665cc86890eb1d3ec08d8faf6d34527ab04f_w640_q70.webp",
      "contributions": "1. A systematic redesign of SAM2 incorporating Hilbert space-filling curves into the Mamba SSM scanning mechanism to preserve spatial locality in 3D medical data. 2. Introduction of a novel Hilbert-Mamba Cross-Attention (HMCA) mechanism and a scale-aware decoder to capture fine-grained details. 3. A two-stage \"segmentation-first, fusion-prompt\" framework that unifies segmentation masks and textual attributes into an enhanced prompt to guide VLM-based disease classification.",
      "summary": "The paper addresses the challenge of applying general-purpose VLMs to 3D multimodal medical image analysis by proposing Hilbert-VLM, a novel two-stage framework. It first uses a redesigned SAM2 model with Hilbert-Mamba for precise lesion segmentation and then fuses the visual masks with textual attributes to create enhanced prompts for a VLM classifier. The model demonstrates improved performance on the BraTS2021 benchmark, showing potential for more accurate and reliable medical diagnosis.",
      "mindmap": "graph TB\n        A[论文标题: Bridging the Perception-Cognition Gap<br>Paper Title: Bridging the Perception-Cognition Gap] --> B(核心问题: 3D多模态医学图像分析<br>Problem: 3D Multi-modal Medical Image Analysis)\n        A --> C(主要方法: Hilbert-VLM两阶段框架<br>Method: Hilbert-VLM Two-Stage Framework)\n        A --> D(关键结果: BraTS2021基准测试结果<br>Results: BraTS2021 Benchmark Results)\n        B --> B1[挑战: 信息融合与细节忽略<br>Challenges: Fusion & Detail Oversight]\n        C --> C1[阶段1: HilbertMed-SAM分割<br>Stage1: HilbertMed-SAM Segmentation]\n        C --> C2[阶段2: 增强提示引导VLM分类<br>Stage2: Enhanced Prompt for VLM Classification]\n        D --> D1[Dice分数: 82.35%<br>Dice Score: 82.35%]\n        D --> D2[分类准确率: 78.85%<br>Classification ACC: 78.85%]"
    },
    {
      "title": "RSAgent: Learning to Reason and Act for Text-Guided Segmentation via Multi-Turn Tool Invocations",
      "authors": "Xingqi He, Yujie Zhang, Shuyong Gao, Wenjie Li, Lingyi Hong, Mingxi Chen, Kaixun Jiang, Jiyuan Fu, Wenqiang Zhang",
      "institution": "Fudan University, Shanghai Jiao Tong University School of Medicine",
      "link": "https://arxiv.org/pdf/2512.24023",
      "code": null,
      "tags": [
        "text-guided segmentation",
        "agentic MLLM",
        "multi-turn tool invocation",
        "iterative mask refinement"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/492283683a8b1ec7673cb4aa97997d0e1ab70ed4a074c17cfa6a9a5e87c60998_w640_q70.webp",
      "contributions": "1. Proposes RSAgent, an agentic MLLM that interleaves reasoning and action for segmentation via multi-turn tool invocations, enabling iterative refinement. 2. Builds a data pipeline to synthesize multi-turn reasoning segmentation trajectories for training. 3. Introduces a two-stage training framework combining cold-start supervised fine-tuning with agentic reinforcement learning using fine-grained, task-specific rewards.",
      "summary": "The paper addresses the limitation of one-shot methods in text-guided segmentation, where initial errors cannot be corrected. It proposes RSAgent, an agentic multimodal LLM that iteratively uses a segmentation toolbox, observes feedback, and refines its spatial hypotheses over multiple turns. Experiments show RSAgent achieves state-of-the-art performance on benchmarks like ReasonSeg and RefCOCOg.",
      "mindmap": "graph TB\n        A[RSAgent: Learning to Reason and Act for Text-Guided Segmentation] --> B[核心问题/Problem: One-shot grounding methods lack verification and refinement capabilities]\n        A --> C[主要方法/Method: Agentic MLLM with multi-turn tool invocation for iterative reasoning and mask refinement]\n        A --> D[关键结果/Results: Achieves SOTA on benchmarks (66.5% gIoU on ReasonSeg, 81.5% cIoU on RefCOCOg)]"
    },
    {
      "title": "Structure-Guided Allocation of 2D Gaussians for Image Representation and Compression",
      "authors": "Huanxiong Liang, Yunuo Chen, Yicheng Pan, Sixian Wang, Jincheng Dai, Guo Lu, Wenjun Zhang",
      "institution": "Shanghai Jiao Tong University, Beijing University of Posts and Telecommunications",
      "link": "https://arxiv.org/pdf/2512.24018",
      "code": null,
      "tags": [
        "image representation and compression",
        "2D Gaussian Splatting",
        "rate-distortion optimization",
        "adaptive quantization",
        "structure-guided initialization",
        "geometry-consistent regularization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45f543cadfd5e91c5a71253cd986e9ef8c5270af87a338c12cf69f1ce15135a6_w640_q70.webp",
      "contributions": "1. A structure-guided initialization method that assigns 2D Gaussians based on spatial structural priors for a localized and semantically meaningful distribution. 2. An adaptive bitwidth quantization scheme for covariance parameters during fine-tuning, granting higher precision to complex regions for RD-aware optimization. 3. A geometry-consistent regularization that aligns Gaussian orientations with local gradient directions to better preserve structural details.",
      "summary": "This paper addresses the inefficiency of existing 2D Gaussian Splatting (2DGS) methods for image compression at low bitrates, which allocate representation capacity without considering image structure. The authors propose a structure-guided allocation principle that couples image structure with capacity and quantization precision through three techniques: structure-guided initialization, adaptive bitwidth quantization, and geometry-consistent regularization. The method significantly improves rate-distortion performance while maintaining over 1000 FPS decoding speed, achieving large BD-rate reductions compared to the baseline.",
      "mindmap": "graph TB\n        A[”Structure-Guided Allocation of 2D Gaussians<br>2D高斯结构引导分配”] --> B[”核心问题/Problem<br>Existing 2DGS allocates capacity oblivious to structure, limiting RD efficiency.”]\n        A --> C[”主要方法/Method<br>1. Structure-Guided Initialization<br>2. Adaptive Bitwidth Quantization<br>3. Geometry-Consistent Regularization”]\n        A --> D[”关键结果/Results<br>Improves RD performance, maintains 1000+ FPS, reduces BD-rate significantly.”]"
    },
    {
      "title": "FitControler: Toward Fit-Aware Virtual Try-On",
      "authors": "Lu Yang, Yicheng Liu, Yanan Li, Xiang Bai, Hao Lu",
      "institution": "Huazhong University of Science and Technology, Wuhan Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.24016",
      "code": null,
      "tags": [
        "virtual try-on",
        "garment fit",
        "layout generation",
        "diffusion models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/161fe70b2650b65573a6e86bb49034407bfe06a8eeb57afd8037d4fc5ba5658f_w640_q70.webp",
      "contributions": "1. Introducing the novel task of fit-aware virtual try-on and proposing FitControler, a learnable plug-in module for diffusion-based VTON models to enable customized fit control. 2. Constructing a new dataset, Fit4Men, containing 13,000 body-garment pairs with diverse fits, poses, and camera distances. 3. Proposing two new metrics to quantitatively assess the fit consistency of generated try-on images.",
      "summary": "This paper addresses the problem of controlling garment fit in virtual try-on, which is often neglected by prior work. The authors propose FitControler, a plug-in module that uses a fit-aware layout generator and a multi-scale fit injector to enable layout-driven, fit-controlled image generation. Experiments show the method works with various VTON models and achieves accurate fit control.",
      "mindmap": "graph TB\n        A[FitControler: Toward Fit-Aware Virtual Try-On] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有VTON忽略服装合身性/Existing VTON neglects garment fit]\n        C --> C1[可学习的插件模块/Learnable plug-in module]\n        C1 --> C2[合身感知布局生成器/Fit-aware layout generator]\n        C1 --> C3[多尺度合身性注入器/Multi-scale fit injector]\n        D --> D1[构建数据集Fit4Men/Built dataset Fit4Men]\n        D --> D2[提出合身性评估指标/Proposed fit consistency metrics]\n        D --> D3[实现精准的合身性控制/Achieved accurate fit control]"
    },
    {
      "title": "PipeFlow: Pipelined Processing and Motion-Aware Frame Selection for Long-Form Video Editing",
      "authors": "Mustafa Munir, Md Mostafijur Rahman, Kartikeya Bhardwaj, Paul Whatmough, Radu Marculescu",
      "institution": "The University of Texas at Austin, Qualcomm AI Research",
      "link": "https://arxiv.org/pdf/2512.24026",
      "code": null,
      "tags": [
        "diffusion models",
        "DDIM inversion",
        "motion analysis",
        "pipelined scheduling",
        "frame interpolation",
        "long-form video editing"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3787a383f3a5b9871e80fb8c9aaad731151ef891a01b7640701576d610e23860_w640_q70.webp",
      "contributions": "1. A motion-aware frame selection method using SSIM and Optical Flow to skip editing of low-motion frames. 2. A pipelined task scheduling algorithm that splits videos into segments for parallel DDIM inversion and joint editing based on GPU memory. 3. A neural network-based interpolation technique to smooth border frames and interpolate skipped frames.",
      "summary": "The paper addresses the high computational cost of long-form video editing with diffusion models. It proposes PipeFlow, a method that uses motion analysis to skip frames, parallel pipelined processing, and interpolation to achieve linear scaling with video length. The method achieves significant speedups (up to 31.7x) over prior work while maintaining quality.",
      "mindmap": "graph TB\n        A[PipeFlow: Pipelined Processing and Motion-Aware Frame Selection for Long-Form Video Editing] --> B[核心问题/Problem: Long-form video editing is computationally expensive due to DDIM inversion and joint editing.]\n        A --> C[主要方法/Method: 1. Motion-aware frame skipping. 2. Pipelined parallel processing. 3. Neural interpolation.]\n        A --> D[关键结果/Results: Achieves linear scaling, up to 31.7x speedup over baselines.]"
    },
    {
      "title": "On Exact Editing of Flow-Based Diffusion Models",
      "authors": "Zixiang Li, Yue Song, Jianing Peng, Ting Liu, Jun Huang, Xiaochao Qu, Luoqi Liu, Wei Wang, Yao Zhao, Yunchao Wei",
      "institution": "Beijing Jiaotong University, Meitu Inc, Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.24015",
      "code": null,
      "tags": [
        "image editing",
        "flow-based diffusion models",
        "velocity correction",
        "distribution transformation",
        "Tweedie correction",
        "Empirical Bayes Inference"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dfdb63e56ff58967475e42a4e69f692d60678dab6f6b842bd2ac85a1db6d86e_w640_q70.webp",
      "contributions": "1. Proposes Conditioned Velocity Correction (CVC), a principled framework that reformulates flow-based editing as a distribution transformation problem driven by a known source prior., 2. Introduces a dual-perspective velocity conversion mechanism that decomposes latent evolution into structure-preserving and semantically-guided branches for controlled editing., 3. Applies a posterior-consistent update derived from Empirical Bayes Inference and Tweedie correction to the conditional velocity field to compensate for error and ensure stable, interpretable latent dynamics.",
      "summary": "This paper addresses the problem of accumulated velocity errors and semantic inconsistency in flow-based diffusion model editing. It proposes Conditioned Velocity Correction (CVC), a framework that uses a dual-perspective velocity conversion and a posterior-consistent update based on Empirical Bayes to achieve stable and faithful image editing. The method demonstrates superior fidelity and semantic alignment across diverse editing tasks.",
      "mindmap": "graph TB\n        A[On Exact Editing of Flow-Based Diffusion Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[累积速度误差导致语义不一致/Accumulated velocity errors cause semantic inconsistency]\n        B --> B2[结构保真度损失/Loss of structural fidelity]\n        C --> C1[条件速度校正框架/Conditioned Velocity Correction (CVC) framework]\n        C1 --> C2[双视角速度转换机制/Dual-perspective velocity conversion]\n        C2 --> C3[结构保持分支/Structure-preserving branch]\n        C2 --> C4[语义引导分支/Semantically-guided branch]\n        C1 --> C5[后验一致更新/Posterior-consistent update]\n        C5 --> C6[经验贝叶斯与Tweedie校正/Empirical Bayes & Tweedie correction]\n        D --> D1[稳定的潜在动力学/Stable latent dynamics]\n        D --> D2[忠实重建与平滑语义转换/Faithful reconstruction & smooth semantic conversion]\n        D --> D3[卓越的保真度与语义对齐/Superior fidelity & semantic alignment]"
    },
    {
      "title": "Reinforced Diffusion: Learning to Push the Limits of Anisotropic Diffusion for Image Denoising",
      "authors": "Xinran Qin, Yuhui Quan, Ruotao Xu, Hui Ji",
      "institution": "South China University of Technology, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.24035",
      "code": null,
      "tags": [
        "image denoising",
        "anisotropic diffusion",
        "reinforcement learning",
        "deep Q-learning",
        "stochastic diffusion"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3be25ff96e1d5d489676fdfbae564029c066808ffe1f445710f3d58c023ed964_w640_q70.webp",
      "contributions": "1. A trainable anisotropic diffusion framework for image denoising based on reinforcement learning. 2. Modeling the denoising process as a series of diffusion actions with order learned by deep Q-learning, forming a stochastic anisotropic diffusion process. 3. Demonstrating that the proposed method outperforms existing diffusion-based methods and competes with deep CNN-based methods on multiple noise types.",
      "summary": "This paper proposes a novel image denoising method called Reinforced Diffusion, which uses deep reinforcement learning (deep Q-learning) to learn the optimal sequence of naive diffusion actions, forming an adaptive stochastic anisotropic diffusion process. This approach overcomes the limitations of traditional fixed diffusion operators. Experimental results show it outperforms other diffusion-based methods and is competitive with state-of-the-art deep CNN-based denoisers.",
      "mindmap": "graph TB\n        A[Reinforced Diffusion: Image Denoising] --> B(核心问题/Problem: Traditional anisotropic diffusion has limited performance due to non-adaptive operators.)\n        A --> C(主要方法/Method: Trainable diffusion framework using Deep Q-Learning to learn action sequences.)\n        A --> D(关键结果/Results: Outperforms diffusion-based methods, competes with deep CNN methods.)"
    },
    {
      "title": "Neighbor-aware Instance Refining with Noisy Labels for Cross-Modal Retrieval",
      "authors": "Yizhi Liu, Ruitao Pu, Shilin Xu, Yingke Chen, Quan-Hui Liu, Yuan Sun",
      "institution": "Sichuan University, State Key Laboratory of AI Safety, Northumbria University",
      "link": "https://arxiv.org/pdf/2512.24064",
      "code": "https://github.com/perquisite/NIRNL",
      "tags": [
        "cross-modal retrieval",
        "noisy labels",
        "instance refining",
        "margin preserving",
        "neighborhood consensus"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74b11b5fe9b39647355b1db0a9a978e8e0bd13a99b38678ec9490a28b9024434_w640_q70.webp",
      "contributions": "1. Proposes a novel robust cross-modal learning framework called Neighbor-aware Instance Refining with Noisy Labels (NIRNL). 2. Introduces Cross-modal Margin Preserving (CMP) to enhance discrimination between sample pairs by adjusting relative distances. 3. Designs Neighbor-aware Instance Refining (NIR) to identify and categorize instances into pure, hard, and noisy subsets for tailored optimization.",
      "summary": "This paper addresses the problem of noisy labels in cross-modal retrieval, which degrades model performance. The authors propose the NIRNL framework, which uses Cross-modal Margin Preserving and Neighbor-aware Instance Refining to better utilize all data and mitigate error propagation. Experiments show the method achieves state-of-the-art performance with strong robustness, especially under high noise rates.",
      "mindmap": "graph TB\n        Root[Neighbor-aware Instance Refining with Noisy Labels for Cross-Modal Retrieval] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Noisy labels degrade cross-modal retrieval performance] --> P1[标注噪声/Label Noise]\n        Method[主要方法/Method: NIRNL Framework] --> M1[Cross-modal Margin Preserving (CMP)]\n        Method --> M2[Neighbor-aware Instance Refining (NIR)]\n        M2 --> M2_1[识别子集/Identify Subsets: pure, hard, noisy]\n        Results[关键结果/Results: State-of-the-art performance] --> R1[高噪声下鲁棒/Robust under high noise]"
    },
    {
      "title": "Pathology Context Recalibration Network for Ocular Disease Recognition",
      "authors": "Zunjie Xiao, Xiaoqing Zhang, Risa Higashita, Jiang Liu",
      "institution": "Southern University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.24066",
      "code": null,
      "tags": [
        "medical image analysis",
        "Pathology Recalibration Module",
        "expert prior Guidance Adapter",
        "Integrated Loss"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2286b84bd9cbd7682cb726e99010af22979d096fd6391215bb5d0212bb1875c5_w640_q70.webp",
      "contributions": "1. Proposed a novel Pathology Recalibration Module (PRM) to leverage pathology context prior via pixel-wise context compression and pathology distribution concentration. 2. Introduced an expert prior Guidance Adapter (EPGA) to highlight significant pixel-wise regions by mining expert experience prior. 3. Designed an Integrated Loss (IL) to boost performance by considering sample-wise loss distributions and training label frequencies.",
      "summary": "This paper proposes PCRNet, a network for ocular disease recognition that incorporates a Pathology Recalibration Module and an expert prior Guidance Adapter to integrate clinical pathology context and expert experience priors into a DNN. An Integrated Loss is also introduced to handle sample and label imbalances. Experiments on three datasets show PCRNet's superiority over state-of-the-art methods, and visualizations explain its decision-making process.",
      "mindmap": "graph TB\n        Root[Pathology Context Recalibration Network for Ocular Disease Recognition] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: DNNs ignore pathology context & expert experience priors for ocular disease recognition] --> P1[问题1/Sub-Problem: Lack of pathology context utilization]\n        Problem --> P2[问题2/Sub-Problem: Lack of expert experience integration]\n        Method[主要方法/Method: PCRNet] --> M1[模块1/Module: Pathology Recalibration Module (PRM)]\n        Method --> M2[模块2/Module: expert prior Guidance Adapter (EPGA)]\n        Method --> M3[组件/Component: Integrated Loss (IL)]\n        Results[关键结果/Results] --> R1[结果1/Result: Superior performance on three datasets]\n        Results --> R2[结果2/Result: Better than SOTA attention networks & loss methods]\n        Results --> R3[结果3/Result: Visualization explains decision-making]"
    },
    {
      "title": "Balanced Hierarchical Contrastive Learning with Decoupled Queries for Fine-grained Object Detection in Remote Sensing Images",
      "authors": "Jingzhou Chen, Dexin Chen, Fengchao Xiong, Yuntao Qian, Liang Xiao",
      "institution": "Nanjing University of Science and Technology, Zhejiang University",
      "link": "https://arxiv.org/pdf/2512.24074",
      "code": null,
      "tags": [
        "object detection",
        "hierarchical contrastive learning",
        "decoupled queries",
        "DETR",
        "fine-grained detection",
        "remote sensing"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ead6df5aaaf078f6a443e544143fe4909df82deba2e44cdcb26ff94cd6a5a3d_w640_q70.webp",
      "contributions": "1. Proposes a balanced hierarchical contrastive loss that uses learnable class prototypes and equilibrates gradients to address data imbalance across hierarchical levels. 2. Introduces a decoupled learning strategy that separates DETR's object queries into classification and localization sets for task-specific optimization. 3. Demonstrates superior performance on three fine-grained remote sensing datasets with hierarchical annotations.",
      "summary": "This paper addresses challenges in fine-grained object detection for remote sensing images with hierarchical labels, where imbalanced data and conflicting learning objectives hinder performance. The authors propose a balanced hierarchical contrastive loss and a decoupled query strategy within the DETR framework to ensure balanced learning and separate feature optimization for classification and localization. Experiments show the method outperforms state-of-the-art approaches on three datasets.",
      "mindmap": "graph TB\n        Root[”Balanced Hierarchical Contrastive Learning with Decoupled Queries for Fine-grained Object Detection in Remote Sensing Images”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”数据分布不平衡/Imbalanced Data Distribution”]\n        Problem --> P2[”分类与定位任务冲突/Classification-Localization Conflict”]\n        Method --> M1[”平衡层次对比损失/Balanced Hierarchical Contrastive Loss”]\n        Method --> M2[”解耦查询学习策略/Decoupled Query Learning Strategy”]\n        M1 --> M1_Sub[”可学习原型与梯度均衡/Learnable Prototypes & Gradient Equalization”]\n        M2 --> M2_Sub[”分类查询与定位查询分离/Separate Classification & Localization Queries”]\n        Results --> R1[”在三个数据集上超越SOTA/Outperforms SOTA on Three Datasets”]"
    },
    {
      "title": "RainFusion2.0: Temporal-Spatial Awareness and Hardware-Efficient Block-wise Sparse Attention",
      "authors": "Aiyue Chen, Yaofu Liu, Junjian Huang, Guang Lian, Yiwu Yao, Wangli Lan, Jing Lin, Zhixin Ma, Tingting Zhou, Harry Yang",
      "institution": "Huawei Technologies Co., Ltd, The Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.24086",
      "code": null,
      "tags": [
        "diffusion models",
        "sparse attention",
        "hardware-efficient",
        "block-wise mean",
        "spatiotemporal-aware permutation",
        "first-frame sink"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/41a00bea8b8b0ba5148f027ed70530ca8414368fe966ff2da38fd4f95487de2b_w640_q70.webp",
      "contributions": "1. Proposes using block-wise mean values as representative tokens for low-overhead sparse mask prediction. 2. Implements spatiotemporal-aware token permutation to enhance the effectiveness of the sparse attention pattern. 3. Introduces a first-frame sink mechanism specifically optimized for video generation scenarios.",
      "summary": "This paper proposes RainFusion2.0, a hardware-efficient and adaptive sparse attention mechanism to reduce the high computational cost of Diffusion Transformers in video and image generation. The method uses block-wise mean tokens for mask prediction and introduces spatiotemporal-aware permutation and a first-frame sink mechanism. Experiments show it achieves 80% sparsity with 1.5-1.8x speedup without quality loss, and generalizes across models and hardware.",
      "mindmap": "graph TB\n        A[RainFusion2.0] --> B[核心问题/Problem: DiT模型注意力计算成本高，现有稀疏注意力方法开销大且硬件通用性差]\n        A --> C[主要方法/Method: 块均值代表令牌预测，时空感知令牌重排，首帧下沉机制]\n        A --> D[关键结果/Results: 80%稀疏度，1.5~1.8倍端到端加速，跨模型和硬件有效]"
    },
    {
      "title": "Factorized Learning for Temporally Grounded Video-Language Models",
      "authors": "Wenzheng Zeng, Difei Gao, Mike Zheng Shou, Hwee Tou Ng",
      "institution": "National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.24097",
      "code": "https://github.com/nusnlp/d2vlm",
      "tags": [
        "video-language models",
        "temporal grounding",
        "factorized learning",
        "preference optimization",
        "evidence tokens",
        "video understanding"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7daa6b2b83cd5b8b5e9ed9cbeed8ecfc3d04fe90ac708e60dda996b6def5b97_w640_q70.webp",
      "contributions": "1. Proposes D2VLM, a framework that decouples the learning of temporal grounding and textual response using a \"grounding then answering with evidence referencing\" paradigm and introduces evidence tokens for explicit event-level visual semantic capture. 2. Introduces Factorized Preference Optimization (FPO), a novel algorithm that explicitly incorporates probabilistic temporal grounding modeling into the preference optimization objective for both grounding and response. 3. Constructs a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding.",
      "summary": "This paper addresses the challenge of accurate temporal grounding in video-language models by proposing a factorized learning approach. It introduces the D2VLM framework, which decouples grounding and response generation, and a novel Factorized Preference Optimization (FPO) algorithm for joint optimization. Experiments show the approach achieves clear advantages over existing methods on various tasks.",
      "mindmap": "graph TB\n        A[Factorized Learning for Temporally Grounded Video-Language Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Existing models struggle with accurate temporal grounding for event-level perception. 现有模型在事件级感知的精确时间定位上存在困难。]\n        C[主要方法/Method: Propose D2VLM framework and Factorized Preference Optimization (FPO). 提出D2VLM框架和因子化偏好优化算法。]\n        D[关键结果/Results: Demonstrates clear advantage on various tasks. 在多种任务上展现出明显优势。]"
    },
    {
      "title": "Think Before You Move: Latent Motion Reasoning for Text-to-Motion Generation",
      "authors": "Yijie Qian, Juncheng Wang, Yuxiang Feng, Chao Xu, Wang Lu, Yang Liu, Baigui Sun, Yiqiang Chen, Yong Liu, Shujun Wang",
      "institution": "Zhejiang University, Hong Kong Polytechnic University, Institute of Computing Technology, Chinese Academy of Sciences, IROOTECH TECHNOLOGY, King's College London",
      "link": "https://arxiv.org/pdf/2512.24100",
      "code": "https://chenhaoqcdyq.github.io/LMR/",
      "tags": [
        "text-to-motion generation",
        "Latent Motion Reasoning",
        "Dual-Granularity Tokenizer",
        "Semantic-Kinematic Impedance Mismatch",
        "Hierarchical Motor Control",
        "Autoregressive Reasoning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce14b37e6dbbad8a248665b68bdf378f0c30148636495248943d2552d0ca994c_w640_q70.webp",
      "contributions": "1. Identifies the Semantic-Kinematic Impedance Mismatch as a fundamental bottleneck in direct text-to-motion translation. 2. Proposes Latent Motion Reasoning (LMR), a novel two-stage \"Think-then-Act\" generation framework inspired by hierarchical motor control. 3. Introduces a Dual-Granularity Tokenizer that disentangles motion into separate Reasoning and Execution latent spaces for planning and physical fidelity.",
      "summary": "The paper identifies a core problem in text-to-motion generation where directly mapping language to motion (a \"System 1\" approach) leads to a semantic-kinematic mismatch. To solve this, it proposes Latent Motion Reasoning (LMR), a two-stage method that first reasons in a high-level latent space before generating detailed motion, improving both semantic alignment and physical plausibility. The results validate that motion planning is more effective in a learned motion-aligned concept space than in natural language.",
      "mindmap": "graph TB\n        A[Think Before You Move: Latent Motion Reasoning for Text-to-Motion Generation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Semantic-Kinematic Impedance Mismatch]\n        C[主要方法/Method: Latent Motion Reasoning (LMR) with Dual-Granularity Tokenizer]\n        D[关键结果/Results: Improved semantic alignment and physical plausibility]"
    },
    {
      "title": "Guided Diffusion-based Generation of Adversarial Objects for Real-World Monocular Depth Estimation Attacks",
      "authors": "Yongtao Chen, Yanbo Wang, Wentao Zhao, Guole Shen, Tianchen Deng, Jingchuan Wang",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.24111",
      "code": null,
      "tags": [
        "adversarial attack",
        "diffusion models",
        "monocular depth estimation",
        "physical adversarial attack",
        "Jacobian Vector Product Guidance",
        "Salient Region Selection"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/866f9f80528d36a9ff26173e3b4cfe5a8d9aaa0c968fa621b3f8bc4fe8d6694d_w640_q70.webp",
      "contributions": "1. Proposes a training-free, diffusion-based generative framework for creating naturalistic adversarial objects to attack Monocular Depth Estimation models. 2. Introduces a Salient Region Selection module to identify MDE-critical areas and a Jacobian Vector Product Guidance mechanism to align adversarial gradients with the diffusion model's capabilities. 3. Demonstrates through extensive experiments that the generated adversarial objects are more effective, stealthy, and physically deployable than prior texture-based attacks.",
      "summary": "This paper addresses the vulnerability of Monocular Depth Estimation (MDE) in autonomous driving by proposing a novel adversarial attack framework. It uses a guided diffusion model to generate scene-consistent, physically plausible adversarial objects, which are shown to be more effective and stealthy than existing patch-based attacks in both digital and physical experiments.",
      "mindmap": "graph TB\n        Root(”Guided Diffusion-based Generation of Adversarial Objects for Real-World Monocular Depth Estimation Attacks”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”MDE易受攻击/MDE is vulnerable”)\n        Problem --> P2(”现有攻击不现实/Existing attacks lack realism”)\n        Method --> M1(”无训练生成框架/Training-free generative framework”)\n        Method --> M2(”显著区域选择/Salient Region Selection”)\n        Method --> M3(”JVP引导/Jacobian Vector Product Guidance”)\n        Results --> R1(”高攻击有效性/High attack effectiveness”)\n        Results --> R2(”高隐蔽性/High stealthiness”)\n        Results --> R3(”强物理可部署性/Strong physical deployability”)"
    },
    {
      "title": "Enhancing LLM-Based Neural Network Generation: Few-Shot Prompting and Efficient Validation for Automated Architecture Design",
      "authors": "Chandini Vysyaraju, Raghuvir Duvvuri, Avi Goyal, Dmitry Ignatov, Radu Timofte",
      "institution": "Computer Vision Lab, CAIDAS & IFI, University of Würzburg",
      "link": "https://arxiv.org/pdf/2512.24120",
      "code": null,
      "tags": [
        "llm training",
        "Neural Architecture Search",
        "Few-Shot Prompting",
        "Code Deduplication",
        "Automated Architecture Design",
        "Lightweight Validation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c601e530777a60f08e6c3607d74352f9046689f8debe4c26a2d893b33b09df97_w640_q70.webp",
      "contributions": "1. Few-Shot Architecture Prompting (FSAP), a systematic study determining n=3 examples as optimal for LLM-based architecture generation in vision tasks. 2. Whitespace-Normalized Hash Validation, a lightweight (&lt;1ms) deduplication method providing 100x speedup over AST parsing. 3. A dataset-balanced evaluation methodology for comparing architectures across heterogeneous vision benchmarks.",
      "summary": "This paper addresses challenges in using LLMs for automated neural network architecture design in computer vision. It introduces a systematic few-shot prompting strategy (FSAP) and a fast deduplication method to prevent redundant training. The main conclusion is that using three examples in prompts best balances diversity and focus, and the lightweight validation enables efficient large-scale generation of unique architectures.",
      "mindmap": "graph TB\n        Root(”Enhancing LLM-Based Neural Network Generation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”自动化架构设计的挑战 / Challenges in Automated Architecture Design”)\n        Problem --> P2(”LLM提示与验证策略未系统研究 / LLM Prompting & Validation Not Systematically Studied”)\n        Method --> M1(”少样本架构提示 / Few-Shot Architecture Prompting (FSAP)”)\n        Method --> M2(”空白标准化哈希验证 / Whitespace-Normalized Hash Validation”)\n        Results --> R1(”n=3示例为最优 / n=3 Examples is Optimal”)\n        Results --> R2(”验证速度提升100倍 / 100x Speedup in Validation”)\n        Results --> R3(”生成1900个独特架构 / Generated 1,900 Unique Architectures”)"
    },
    {
      "title": "GeoBench: Rethinking Multimodal Geometric Problem-Solving via Hierarchical Evaluation",
      "authors": "Yuan Feng, Yue Yang, Xiaohan He, Jiatong Zhao, Jianlong Chen, Zijun Chen, Daocheng Fu, Qi Liu, Renqiu Xia, Bo Zhang, Junchi Yan",
      "institution": "Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory, Fudan University, The Chinese University of Hong Kong, Shenzhen",
      "link": "https://arxiv.org/pdf/2512.24119",
      "code": "https://github.com/FrontierX-Lab/GeoBench",
      "tags": [
        "multimodal reasoning",
        "geometric problem solving",
        "hierarchical evaluation",
        "vision-language models",
        "reasoning benchmark",
        "theorem application"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1e39e6ff4463092b188cd464fc8a691dd432dcd024fb86c0073997019d94d82_w640_q70.webp",
      "contributions": "1. Proposes GeoBench, a novel hierarchical benchmark for geometric problem-solving with four reasoning levels (Visual Perception, Goal-Oriented Planning, Rigorous Theorem Application, Self-Reflective Backtracking). 2. Introduces six formally verified tasks generated via TrustGeoGen to systematically assess multimodal reasoning capabilities. 3. Provides key empirical findings, such as the critical influence of sub-goal decomposition and the unexpected degradation from Chain-of-Thought prompting in certain tasks.",
      "summary": "This paper introduces GeoBench, a hierarchical benchmark to address limitations in evaluating geometric reasoning in vision-language models, such as test data contamination and lack of diagnostic granularity. It systematically assesses models across four reasoning levels using six formally verified tasks. Key findings show that while reasoning models outperform general MLLMs, performance drops with complexity, and sub-goal decomposition is crucial, whereas Chain-of-Thought prompting can sometimes harm performance.",
      "mindmap": "graph TB\n        A[GeoBench: Rethinking Multimodal Geometric Problem-Solving via Hierarchical Evaluation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有评估的局限性/Limitations of Current Evaluations]\n        B1 --> B2[测试数据污染/Test Data Contamination]\n        B1 --> B3[重答案轻推理/Overemphasis on Final Answers]\n        B1 --> B4[诊断粒度不足/Insufficient Diagnostic Granularity]\n        C --> C1[分层基准/Hierarchical Benchmark]\n        C1 --> C2[四个推理层级/Four Reasoning Levels]\n        C2 --> C3[视觉感知/Visual Perception]\n        C2 --> C4[目标导向规划/Goal-Oriented Planning]\n        C2 --> C5[严谨定理应用/Rigorous Theorem Application]\n        C2 --> C6[自反思回溯/Self-Reflective Backtracking]\n        C --> C7[六个验证任务/Six Verified Tasks]\n        C7 --> C8[通过TrustGeoGen生成/Generated via TrustGeoGen]\n        D --> D1[推理模型表现更好/Reasoning Models Outperform General MLLMs]\n        D --> D2[性能随复杂度下降/Performance Declines with Complexity]\n        D --> D3[子目标分解关键/Sub-goal Decomposition Critical]\n        D --> D4[CoT提示可能有害/Chain-of-Thought Can Degrade Performance]"
    },
    {
      "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking",
      "authors": "Haoran He, Yuxiao Ye, Jie Liu, Jiajun Liang, Zhiyong Wang, Ziyang Yuan, Xintao Wang, Hangyu Mao, Pengfei Wan, Ling Pan",
      "institution": "Hong Kong University of Science and Technology, Kuaishou Technology, CUHK MMLab, The University of Edinburgh",
      "link": "https://arxiv.org/pdf/2512.24138",
      "code": "https://tinnerhrhe.github.io/gardo_project",
      "tags": [
        "reinforcement learning",
        "reward hacking",
        "diffusion models",
        "regularization",
        "mode collapse",
        "online RL"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df8973aa0f222e89b818973c0c7ef576738632b0095b29ac1f837f1a83f47f9b_w640_q70.webp",
      "contributions": "1. Proposed GARDO, a framework with gated regularization that selectively penalizes high-uncertainty samples to mitigate reward hacking efficiently., 2. Introduced an adaptive regularization mechanism that periodically updates the reference model to align with the online policy, enabling effective exploration., 3. Designed a diversity-aware reward amplification strategy to encourage mode coverage and prevent diversity collapse during RL fine-tuning.",
      "summary": "This paper addresses the problem of reward hacking in RL-fine-tuned diffusion models, where optimizing imperfect proxy rewards degrades real image quality and diversity. The authors propose GARDO, a framework featuring gated, adaptive regularization and diversity-aware optimization to prevent overfitting, maintain exploration, and enhance diversity. Experiments show GARDO effectively mitigates reward hacking and improves generation diversity without sacrificing sample efficiency.",
      "mindmap": "graph TB\n        A[GARDO: Reinforcing Diffusion Models without Reward Hacking] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Reward Hacking in RL for Diffusion Models/扩散模型RL中的奖励破解]\n        B --> B2[Proxy Reward Mismatch & Mode Collapse/代理奖励不匹配与模式崩溃]\n        C --> C1[Gated & Adaptive Regularization/门控自适应正则化]\n        C --> C2[Diversity-aware Reward Optimization/多样性感知奖励优化]\n        D --> D1[Mitigates Reward Hacking/缓解奖励破解]\n        D --> D2[Enhances Diversity & Maintains Efficiency/提升多样性并保持效率]"
    },
    {
      "title": "Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning",
      "authors": "Chubin Chen, Sujie Hu, Jiashu Zhu, Meiqi Wu, Jintao Chen, Yanxun Li, Nisha Huang, Chengyu Fang, Jiahong Wu, Xiangxiang Chu, Xiu Li",
      "institution": "Tsinghua University, AMAP, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.24146",
      "code": null,
      "tags": [
        "diffusion models",
        "Preference Mode Collapse",
        "Reinforcement Learning from Human Feedback",
        "reward hacking",
        "generative diversity",
        "directional correction"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f62594e4cc99762132258ccba4873b41aea26a85dbeb402b03da55458d0e32bf_w640_q70.webp",
      "contributions": "1. Introduces and quantifies the phenomenon of Preference Mode Collapse (PMC) in diffusion model alignment. 2. Proposes DivGenBench, a novel benchmark to measure the extent of PMC. 3. Proposes the Directional Decoupling Alignment (D²-Align) framework to mitigate PMC by directionally correcting the reward signal.",
      "summary": "This paper identifies Preference Mode Collapse (PMC), where diffusion models over-optimize for reward scores and lose generative diversity. To address this, the authors propose D²-Align, a framework that learns a directional correction to the reward signal to prevent collapse. The method achieves better alignment with human preference while preserving output diversity.",
      "mindmap": "graph TB\n        A[论文标题/Paper Title: Taming Preference Mode Collapse via Directional Decoupling Alignment in Diffusion Reinforcement Learning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Preference Mode Collapse (PMC)导致生成多样性下降/PMC degrades generative diversity]\n        C[主要方法/Method: 提出D²-Align框架，方向性校正奖励信号/Proposes D²-Align framework to directionally correct reward signal]\n        D[关键结果/Results: 在保持多样性的同时实现更好的人类偏好对齐/Achieves better human preference alignment while preserving diversity]"
    },
    {
      "title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
      "authors": "TsaiChing Ni, ZhenQi Chen, YuanFu Yang",
      "institution": "Institute of Intelligent Systems, National Yang Ming Chiao Tung University",
      "link": "https://arxiv.org/pdf/2512.24160",
      "code": null,
      "tags": [
        "multimodal learning",
        "industrial defect dataset",
        "vision-language foundation model",
        "diffusion model",
        "open-vocabulary understanding",
        "data-efficient adaptation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a4859b01599d7daa2377f92cee5fef8f3e4a9889e10c4f5a81f7747139b84b0_w640_q70.webp",
      "contributions": "1. Introduces IMDD-1M, the first large-scale multimodal industrial defect dataset with 1M aligned image-text pairs spanning 60+ materials and 400+ defect types. 2. Trains a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. 3. Demonstrates the model's data-efficient adaptation capability, achieving comparable performance with &lt;5% of task-specific data compared to dedicated expert models.",
      "summary": "This paper introduces IMDD-1M, a large-scale multimodal dataset for industrial defect analysis, and uses it to train a specialized diffusion-based vision-language foundation model. The model serves as a generalizable base that can be efficiently adapted to specific tasks with minimal data, showing strong performance for industrial inspection and generation tasks.",
      "mindmap": "graph TB\n        A[Towards Open-Vocabulary Industrial Defect Understanding<br>开放词汇工业缺陷理解] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[缺乏大规模工业多模态数据集<br>Lack of large-scale industrial multimodal dataset]\n        C --> C1[构建IMDD-1M数据集<br>Build IMDD-1M Dataset]\n        C --> C2[训练扩散式视觉语言基础模型<br>Train Diffusion-based VLM]\n        D --> D1[实现数据高效适应<br>Achieves Data-Efficient Adaptation]\n        D --> D2[性能媲美专家模型<br>Performance Comparable to Expert Models]"
    },
    {
      "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models",
      "authors": "Zefeng He, Xiaoye Qu, Yafu Li, Tong Zhu, Siyuan Huang, Yu Cheng",
      "institution": "Shanghai AI Laboratory, Nanjing University, The Chinese University of Hong Kong, Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.24165",
      "code": "https://diffthinker-project.github.io",
      "tags": [
        "multimodal reasoning",
        "diffusion models",
        "generative reasoning",
        "image-to-image generation",
        "vision-centric tasks",
        "multimodal large language models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4fc496e91826df74d41f1bf12c921b51dc427d1e0de947424de30a6dd0a5ca4_w640_q70.webp",
      "contributions": "1. Establishes a novel Generative Multimodal Reasoning paradigm that reformulates reasoning as a native image-to-image generation task. 2. Introduces DiffThinker, a diffusion-based reasoning framework designed for superior logical consistency and spatial precision in vision-centric tasks. 3. Provides the first systematic investigation into the properties of this paradigm, identifying core characteristics like efficiency, controllability, native parallelism, and collaboration.",
      "summary": "This paper identifies that current Multimodal Large Language Models (MLLMs) have text-centric reasoning processes that underperform on complex vision-centric tasks. To address this, it proposes DiffThinker, a new framework that uses diffusion models to perform multimodal reasoning by directly generating solution images. Extensive experiments show that DiffThinker significantly outperforms leading models like GPT-5 and Gemini-3-Flash, demonstrating the promise of generative, image-based reasoning.",
      "mindmap": "graph TB\n        Root[”DiffThinker: 生成式多模态推理 / Generative Multimodal Reasoning”] --> Problem[”MLLMs推理以文本为中心，视觉任务性能不佳 / MLLMs' reasoning is text-centric, suboptimal for vision tasks”]\n        Root --> Method[”提出扩散模型框架，将推理重构为图像生成任务 / Proposes diffusion framework, reformulates reasoning as image-to-image task”]\n        Root --> Results[”显著超越GPT-5等模型，展现新范式的潜力 / Significantly outperforms GPT-5 etc., shows promise of new paradigm”]"
    },
    {
      "title": "Bayesian Self-Distillation for Image Classification",
      "authors": "Anton Adelöw, Matteo Gamba, Atsuto Maki",
      "institution": "KTH Royal Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.24162",
      "code": "https://github.com/antonadelow/BSD",
      "tags": [
        "image classification",
        "self-distillation",
        "Bayesian inference",
        "calibration",
        "robustness",
        "label noise"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddb506b17a44678d283fc42cb840d215e54f2897aec39dd9d9f698c270430ac8_w640_q70.webp",
      "contributions": "1. Proposes Bayesian Self-Distillation (BSD), a novel method for constructing sample-specific target distributions via Bayesian inference using the model's own predictions, eliminating reliance on hard targets after initialization. 2. Demonstrates that BSD consistently improves test accuracy and significantly reduces Expected Calibration Error (ECE) compared to existing self-distillation methods across various architectures and datasets. 3. Shows that BSD enhances model robustness against data corruptions, perturbations, and label noise, achieving state-of-the-art robustness under label noise when combined with a contrastive loss.",
      "summary": "This paper addresses the limitations of using hard targets in training deep neural networks for image classification, which leads to overconfidence and poor calibration. The authors propose Bayesian Self-Distillation (BSD), a method that uses Bayesian inference on the model's own predictions to create sample-specific soft targets, avoiding hard targets after initialization. The method is shown to improve accuracy, calibration, and robustness across several benchmarks.",
      "mindmap": "graph TB\n        A[Bayesian Self-Distillation for Image Classification] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[硬标签导致过度自信与校准差/Hard targets cause overconfidence & poor calibration]\n        C --> C1[贝叶斯自蒸馏构建样本特定目标/Bayesian Self-Distillation constructs sample-specific targets]\n        C --> C2[不依赖初始化后的硬标签/No reliance on hard targets after initialization]\n        D --> D1[提升准确率与校准/Improves accuracy & calibration]\n        D --> D2[增强对噪声与扰动的鲁棒性/Enhances robustness to noise & corruptions]"
    },
    {
      "title": "Deep Global Clustering for Hyperspectral Image Segmentation: Concepts, Applications, and Open Challenges",
      "authors": "Yu-Tang Chang, Pin-Wei Chen, Shih-Fang Chen",
      "institution": "National Taiwan University",
      "link": "https://arxiv.org/pdf/2512.24172",
      "code": "https://github.com/b05611038/HSI_global_clustering",
      "tags": [
        "hyperspectral image segmentation",
        "deep global clustering",
        "memory-efficient segmentation",
        "unsupervised disease detection",
        "multi-objective loss balancing"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c5ca8fa4968e4f11184d8faa275b2f2edacdd6e6374072e519e722e16016a06_w640_q70.webp",
      "contributions": "1. Proposed Deep Global Clustering (DGC), a conceptual framework for memory-efficient hyperspectral image segmentation that learns global clustering from local patches without pre-training., 2. Demonstrated the framework's ability to achieve background-tissue separation and unsupervised disease detection on a leaf disease dataset with high efficiency (training &lt;30 min on consumer hardware)., 3. Identified and analyzed the key challenge of optimization instability due to multi-objective loss balancing, positioning the work as intellectual scaffolding for future principled solutions.",
      "summary": "This paper addresses the computational bottleneck in hyperspectral image (HSI) analysis by proposing Deep Global Clustering (DGC), a memory-efficient framework that learns global segmentation from local patches without pre-training. It successfully demonstrates background-tissue separation and unsupervised disease detection on agricultural data. However, the main conclusion is that while the design philosophy is promising, the framework suffers from optimization instability due to loss balancing, requiring more principled solutions for stable implementation.",
      "mindmap": "graph TB\n        A[Deep Global Clustering for Hyperspectral Image Segmentation<br>高光谱图像分割的深度全局聚类] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Computational bottlenecks in HSI analysis<br>Foundation models fail in domain-specific transfer]\n        C[主要方法/Method<br>Deep Global Clustering (DGC)<br>Memory-efficient, learns from local patches]\n        D[关键结果/Results<br>Achieves background-tissue separation<br>Shows unsupervised disease detection<br>Suffers from optimization instability]"
    },
    {
      "title": "Guiding a Diffusion Transformer with the Internal Dynamics of Itself",
      "authors": "Xingyu Zhou, Qifan Li, Xiaobin Hu, Hai Chen, Shuhang Gu",
      "institution": "University of Electronic Science and Technology of China, National University of Singapore, Sun Yat-sen University, North China Institute of Computer Systems Engineering",
      "link": "https://arxiv.org/pdf/2512.24176",
      "code": null,
      "tags": [
        "diffusion models",
        "Internal Guidance",
        "Diffusion Transformer",
        "Classifier-Free Guidance",
        "Sampling Guidance",
        "Denoising Diffusion"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/605c83917fcbd68353ebfb53e0da152ab446f909b59cdfd81ee3bd6c6023a44f_w640_q70.webp",
      "contributions": "1. Proposes Internal Guidance (IG), a novel sampling guidance strategy that uses intermediate-layer outputs within a Diffusion Transformer to improve generation quality. 2. Introduces an auxiliary supervisory signal at an intermediate layer during training and extrapolates outputs during sampling, requiring no extra training, degradation strategies, or additional sampling steps. 3. Demonstrates state-of-the-art performance on ImageNet 256x256, achieving an FID of 1.19 when combined with CFG, and shows significant improvements in training efficiency and generation quality across various baselines.",
      "summary": "This paper addresses the issue of standard classifier-free guidance (CFG) causing over-simplified or distorted samples in diffusion models. It proposes Internal Guidance (IG), a simple method that adds auxiliary supervision to an intermediate layer during training and extrapolates outputs during sampling. The method significantly improves generation quality and efficiency, achieving state-of-the-art FID scores on ImageNet.",
      "mindmap": "graph TB\n    A[Guiding a Diffusion Transformer with the Internal Dynamics of Itself] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[标准CFG导致样本过简或失真/Standard CFG leads to over-simplified or distorted samples]\n    B --> B2[现有替代方法需要额外训练或步骤/Existing alternatives require extra training or steps]\n    C --> C1[提出内部引导/Propose Internal Guidance (IG)]\n    C --> C2[训练时中间层辅助监督/Auxiliary supervision on intermediate layer during training]\n    C --> C3[采样时输出外推/Extrapolate outputs during sampling]\n    D --> D1[显著提升训练效率和生成质量/Significant improvements in training efficiency and generation quality]\n    D --> D2[在ImageNet上达到SOTA FID=1.19/Achieves SOTA FID=1.19 on ImageNet]"
    },
    {
      "title": "PointRAFT: 3D deep learning for high-throughput prediction of potato tuber weight from partial point clouds",
      "authors": "Pieter M. Blok, Haozhou Wang, Hyun Kwon Suh, Peicheng Wang, James Burridge, Wei Guo",
      "institution": "The University of Tokyo, Sejong University",
      "link": "https://arxiv.org/pdf/2512.24193",
      "code": "https://github.com/pieterblok/pointraft.git",
      "tags": [
        "3D point cloud regression",
        "PointRAFT",
        "partial point clouds",
        "object height embedding",
        "PointNet++",
        "RGB-D"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c71915dfc1ce9e8203a646c9242d84788697d59124a23181fa869b682fd4cdf_w640_q70.webp",
      "contributions": "1. Proposed PointRAFT, a high-throughput point cloud regression network for directly predicting continuous 3D shape properties from partial point clouds. 2. Introduced a novel object height embedding as an architectural component to incorporate tuber height, improving regression performance under occlusion. 3. Demonstrated superior performance and real-time capability on a large-scale agricultural dataset, achieving high accuracy for potato tuber weight prediction.",
      "summary": "The paper addresses the problem of systematically underestimating potato tuber weight from incomplete 3D point clouds captured on harvesters. It proposes PointRAFT, a deep learning network that directly regresses weight from partial point clouds using a novel object height embedding. The method significantly outperforms baselines and achieves real-time processing speeds suitable for commercial harvesters.",
      "mindmap": "graph TB\n        Root[”PointRAFT: 3D deep learning for high-throughput prediction of potato tuber weight from partial point clouds”] --> Problem[”核心问题/Problem: Incomplete point clouds from RGB-D lead to weight underestimation”]\n        Root --> Method[”主要方法/Method: PointRAFT network with object height embedding for direct regression”]\n        Root --> Results[”关键结果/Results: Low error (MAE 12.0g), high speed (150 tubers/sec)”]"
    },
    {
      "title": "CorGi: Contribution-Guided Block-Wise Interval Caching for Training-Free Acceleration of Diffusion Transformers",
      "authors": "Yonglak Son, Suhyeok Kim, Seungryong Kim, Young Geun Kim",
      "institution": "Korea University, KAIST AI",
      "link": "https://arxiv.org/pdf/2512.24195",
      "code": "https://casl-ku.github.io/CorGi",
      "tags": [
        "diffusion models",
        "diffusion transformer",
        "inference acceleration",
        "interval caching",
        "cross-attention",
        "training-free"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92c567dd9feea285991c6dbbdd5d85a85129420ce1925c585625dca252b806c4_w640_q70.webp",
      "contributions": "1. Proposes CorGi, a training-free framework that accelerates DiT inference by selectively caching and reusing outputs of low-contribution transformer blocks across denoising steps. 2. Introduces CorGi+, an extension for text-to-image tasks that uses cross-attention maps to identify salient tokens and applies partial attention updates to protect important details. 3. Demonstrates significant speedup (up to 2.0x on average) on state-of-the-art DiT models while preserving high generation quality.",
      "summary": "This paper addresses the high inference cost of Diffusion Transformers (DiT) by proposing CorGi, a training-free acceleration framework that reduces redundant computation through contribution-guided, block-wise interval caching. For text-to-image tasks, CorGi+ further refines the approach using cross-attention maps for partial updates. Evaluations show the methods achieve up to 2.0x speedup while maintaining image quality.",
      "mindmap": "graph TB\n        A[CorGi: Contribution-Guided Block-Wise Interval Caching<br>CorGi: 贡献引导的块级间隔缓存] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[DiT推理成本高<br>High DiT Inference Cost]\n        B1 --> B2[去噪步骤间存在冗余计算<br>Redundant Computation Across Steps]\n        C --> C1[CorGi: 缓存低贡献块<br>Cache Low-Contribution Blocks]\n        C1 --> C2[CorGi+: 使用交叉注意力图<br>Use Cross-Attention Maps]\n        C2 --> C3[部分注意力更新<br>Partial Attention Updates]\n        D --> D1[加速高达2.0倍<br>Up to 2.0x Speedup]\n        D --> D2[保持生成质量<br>Preserve Generation Quality]"
    },
    {
      "title": "RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation",
      "authors": "Ming-Ming Yu, Yi Chen, Börje F. Karlsson, Wenjun Wu",
      "institution": "Beihang University, Beijing Academy of Artificial Intelligence (BAAI), Institute of Automation, Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.24212",
      "code": null,
      "tags": [
        "semantic navigation",
        "zero-shot navigation",
        "monocular camera",
        "in-context learning",
        "3D foundation models",
        "open-vocabulary"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec071dfb8e64cc84b9968fc8e7e295d190c6ad5e4a190eaac70aeb44778d22a5_w640_q70.webp",
      "contributions": "1. Proposes RANGER, a zero-shot semantic navigation framework that operates using only a monocular camera, eliminating the dependency on depth and pose sensors. 2. Introduces strong in-context learning capability, allowing the system to quickly adapt to new environments by observing a short video without architectural changes or fine-tuning. 3. Integrates key components like 3D reconstruction, semantic point cloud generation, and VLM-driven exploration into a cohesive framework, validated on benchmarks and real-world tests.",
      "summary": "The paper proposes RANGER, a monocular zero-shot semantic navigation framework that uses 3D foundation models to operate without depth or pose data and can quickly adapt to new environments via in-context learning from short videos. Experiments show it achieves competitive navigation performance and superior adaptability without prior 3D mapping.",
      "mindmap": "graph TB\n        A[RANGER: A Monocular Zero-Shot Semantic Navigation Framework] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[依赖深度与位姿/Depth & Pose Dependency]\n        B --> B2[缺乏情境学习能力/Lack of ICL Capability]\n        C --> C1[仅单目相机/Monocular Camera Only]\n        C --> C2[3D基础模型/3D Foundation Models]\n        C --> C3[情境学习适应/In-Context Learning Adaptation]\n        D --> D1[竞争性导航性能/Competitive Navigation Performance]\n        D --> D2[优越的情境学习适应性/Superior ICL Adaptability]"
    },
    {
      "title": "Medical Image Classification on Imbalanced Data Using ProGAN and SMA-Optimized ResNet: Application to COVID-19",
      "authors": "Sina Jahromi, Farshid Hajati, Alireza Rezaee, Javaher Nourian",
      "institution": "University of Tehran, University of New England",
      "link": "https://arxiv.org/pdf/2512.24214",
      "code": null,
      "tags": [
        "medical image classification",
        "imbalanced data",
        "progressive generative adversarial network (ProGAN)",
        "slime mould algorithm (SMA)",
        "ResNet",
        "synthetic data generation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95bbc93b1f6719a8c5ffdfe98638f4844e331a5a71715b6698a65f669e17f927_w640_q70.webp",
      "contributions": "1. Proposes a Progressive GAN (ProGAN) to generate synthetic medical images to address data imbalance. 2. Introduces a weighted approach for combining synthetic and real data before classification. 3. Employs the Slime Mould Algorithm (SMA), a multi-objective meta-heuristic, to optimize the hyper-parameters of the ResNet classifier.",
      "summary": "This paper tackles the problem of imbalanced medical image data, particularly for COVID-19 detection from chest X-rays. The proposed method uses a Progressive GAN to generate synthetic data and a weighted combination of real and synthetic data, with a ResNet classifier optimized by the Slime Mould Algorithm. The model achieved high accuracy (95.5% for 4-class, 98.5% for 2-class) on an imbalanced dataset, demonstrating its effectiveness for pandemic-related medical image classification.",
      "mindmap": "graph TB\n    A[Medical Image Classification on Imbalanced Data Using ProGAN and SMA-Optimized ResNet: Application to COVID-19] --> B(核心问题/Problem: 医学图像数据不平衡/Imbalanced Medical Image Data)\n    A --> C(主要方法/Method: 使用ProGAN生成数据并用SMA优化ResNet/Use ProGAN for Data Generation and SMA to Optimize ResNet)\n    A --> D(关键结果/Results: 在COVID-19胸部X光数据集上取得高准确率/High Accuracy on COVID-19 Chest X-ray Dataset)\n    B --> B1(挑战: 疫情中数据集更不平衡/Challenge: Increased Imbalance During Pandemics)\n    C --> C1(步骤1: 用ProGAN生成合成数据/Step 1: Generate Synthetic Data with ProGAN)\n    C --> C2(步骤2: 加权结合真实与合成数据/Step 2: Weighted Combination of Real and Synthetic Data)\n    C --> C3(步骤3: 用SMA优化ResNet超参数/Step 3: Optimize ResNet Hyper-parameters with SMA)\n    D --> D1(4分类准确率: 95.5%/4-Class Accuracy: 95.5%)\n    D --> D2(2分类准确率: 98.5%/2-Class Accuracy: 98.5%)"
    },
    {
      "title": "Mirage: One-Step Video Diffusion for Photorealistic and Coherent Asset Editing in Driving Scenes",
      "authors": "Shuyun Wang, Haiyang Sun, Bing Wang, Hangjun Ye, Xin Yu",
      "institution": "The University of Queensland, Xiaomi EV",
      "link": "https://arxiv.org/pdf/2512.24227",
      "code": "https://github.com/wm-research/mirage",
      "tags": [
        "video editing",
        "video diffusion",
        "temporal coherence",
        "3D causal VAE",
        "asset editing",
        "two-stage alignment"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd3ac6637a44c226026b70a284fe5a6fad57f59339114a7deea74b0da4d2aebb_w640_q70.webp",
      "contributions": "1. Proposes Mirage, a one-step video diffusion model for photorealistic and temporally coherent asset editing in driving scenes. 2. Introduces a method to inject temporally agnostic latents from a pretrained 2D encoder into a 3D decoder to restore spatial detail while preserving temporal causality. 3. Presents a two-stage data alignment strategy combining coarse 3D and fine 2D refinement to mitigate pose misalignment between scene objects and inserted assets.",
      "summary": "This paper proposes Mirage, a one-step video diffusion model for editing objects in driving scene videos. It addresses challenges in maintaining visual fidelity and temporal coherence by injecting 2D encoder features into a 3D decoder and using a two-stage alignment strategy. Experiments show the method achieves high realism and consistency, and it can generalize to other video-to-video tasks.",
      "mindmap": "graph TB\n        A[Mirage: 驾驶场景中的逼真连贯资产编辑 / Mirage: Photorealistic Coherent Asset Editing in Driving Scenes] --> B\n        A --> C\n        A --> D\n        B[核心问题 / Problem] --> B1[现有方法难以同时保证高视觉保真度和时间连贯性 / Existing methods struggle with high fidelity and temporal coherence]\n        C[主要方法 / Method] --> C1[基于文本到视频扩散先验 / Builds on text-to-video diffusion prior]\n        C --> C2[注入2D编码器潜在特征到3D解码器 / Injects 2D encoder latents into 3D decoder]\n        C --> C3[两阶段数据对齐策略 / Two-stage data alignment strategy]\n        D[关键结果 / Results] --> D1[实现高真实感和时间一致性 / Achieves high realism and temporal consistency]\n        D --> D2[可泛化到其他视频到视频任务 / Generalizes to other video-to-video tasks]"
    },
    {
      "title": "MotivNet: Evolving Meta-Sapiens into an Emotionally Intelligent Foundation Model",
      "authors": "Rahul Medicharla, Alper Yilmaz",
      "institution": "The Ohio State University",
      "link": "https://arxiv.org/pdf/2512.24231",
      "code": "https://github.com/OSUPCVLab/EmotionFromFaceImages",
      "tags": [
        "facial expression recognition",
        "facial emotion recognition",
        "generalization",
        "foundation model",
        "masked autoencoder",
        "downstream task"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81d3ca85dac4626b7644912941beba01e188b0d41eefb51c00fdb16cd74177b8_w640_q70.webp",
      "contributions": "1. Proposes MotivNet, a novel FER model built on the Meta-Sapiens foundation model to achieve strong generalization without cross-domain training. 2. Defines and applies three criteria (benchmark performance, model similarity, data similarity) to validate a new downstream task for the Sapiens foundation model. 3. Demonstrates that the proposed approach achieves competitive performance across diverse datasets, making FER more viable for real-world, in-the-wild applications.",
      "summary": "This paper introduces MotivNet, a facial emotion recognition model that uses the Meta-Sapiens foundation model as a backbone to achieve strong generalization across datasets without requiring cross-domain training. The authors validate MotivNet as a suitable downstream task for Sapiens using specific criteria and show it achieves competitive performance. The work aims to make FER more robust and applicable in real-world scenarios.",
      "mindmap": "graph TB\n        Root[MotivNet: Evolving Meta-Sapiens into an Emotionally Intelligent Foundation Model] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[FER模型泛化能力弱 / Poor generalization of FER models]\n        P1 --> P2[跨域训练不切实际 / Cross-domain training is impractical]\n        Method[主要方法/Method] --> M1[使用Sapiens作为主干 / Use Sapiens as backbone]\n        M1 --> M2[定义下游任务评估标准 / Define downstream task evaluation criteria]\n        Results[关键结果/Results] --> R1[跨数据集具有竞争力 / Competitive across datasets]\n        R1 --> R2[验证为有效的下游任务 / Validated as a viable downstream task]"
    },
    {
      "title": "ARM: A Learnable, Plug-and-Play Module for CLIP-based Open-vocabulary Semantic Segmentation",
      "authors": "Ziquan Liu, Zhewei Zhu, Xuyang Shi",
      "institution": "Southwest University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.24224",
      "code": null,
      "tags": [
        "semantic segmentation",
        "open-vocabulary segmentation",
        "attention refinement",
        "training-free",
        "CLIP",
        "plug-and-play"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1f320a432567db109650fb122082758ebd1181c40fae33b7e17d4556ae80c600_w640_q70.webp",
      "contributions": "1. Proposes a lightweight, learnable Attention Refinement Module (ARM) that adaptively fuses hierarchical CLIP features to refine coarse spatial details. 2. Introduces a \"train once, use anywhere\" paradigm where ARM, trained once on a general dataset, acts as a universal plug-and-play post-processor for diverse training-free frameworks. 3. Demonstrates consistent performance improvements across multiple benchmarks with negligible inference overhead, establishing an efficient paradigm for training-free OVSS.",
      "summary": "The paper addresses the problem of coarse, image-level CLIP representations for open-vocabulary semantic segmentation. It proposes the Attention Refinement Module (ARM), a learnable module that adaptively fuses hierarchical CLIP features to refine pixel-level details. Experiments show ARM consistently boosts baseline performance with minimal overhead, offering an efficient plug-and-play solution.",
      "mindmap": "graph TB\n        Root[ARM: CLIP-based Open-vocabulary Semantic Segmentation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: CLIP特征粗糙，缺乏像素级细节] --> P1[现有方法/Existing Methods]\n        P1 --> P1_1[依赖外部模型，计算昂贵/Relies on external models, expensive]\n        P1 --> P1_2[静态启发式方法，效果次优/Static heuristics, sub-optimal]\n        Method[主要方法/Method: 注意力精炼模块 (ARM)] --> M1[轻量可学习模块/Lightweight Learnable Module]\n        M1 --> M2[语义引导的交叉注意力/Semantically-guided Cross-Attention]\n        M2 --> M3[自适应融合层次特征/Adaptively fuses hierarchical features]\n        Method --> M4[训练一次，随处使用/Train once, use anywhere]\n        Results[关键结果/Results] --> R1[一致提升基线性能/Consistently boosts baseline performance]\n        Results --> R2[推理开销可忽略/Negligible inference overhead]\n        Results --> R3[高效有效的训练免费范式/Efficient & effective training-free paradigm]"
    },
    {
      "title": "MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation",
      "authors": "Fuqiang Gu, Yuanke Li, Xianlei Long, Kangping Ji, Chao Chen, Qingyi Gu, Zhenliang Ni",
      "institution": "Chongqing University, Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.24243",
      "code": "https://github.com/CQU-UISC/MambaSeg",
      "tags": [
        "semantic segmentation",
        "Mamba",
        "multimodal fusion",
        "event camera",
        "dual-branch",
        "temporal modeling"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69734d37c2ca74823c81a268f32960cc1bdcd9f8ca90e9ad27d6167cfe5d04fe_w640_q70.webp",
      "contributions": "1. Proposes MambaSeg, a novel dual-branch semantic segmentation framework using parallel Mamba encoders for efficient RGB and event stream modeling. 2. Introduces the Dual-Dimensional Interaction Module (DDIM) with Cross-Spatial (CSIM) and Cross-Temporal (CTIM) modules for fine-grained multimodal fusion along spatial and temporal dimensions. 3. Achieves state-of-the-art segmentation performance on DDD17 and DSEC datasets while significantly reducing computational cost.",
      "summary": "This paper addresses the limitations of RGB-based semantic segmentation in challenging conditions by proposing MambaSeg, a framework that fuses RGB images and event camera data using Mamba encoders and a novel dual-dimensional fusion module. The method effectively leverages the complementary strengths of both modalities, achieving superior accuracy and efficiency compared to existing approaches.",
      "mindmap": "graph TB\n        Root[”MambaSeg: 图像-事件语义分割 / Image-Event Semantic Segmentation”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”RGB方法在动态/弱光下失效 / RGB methods fail under fast motion/low-light”]\n        Problem --> P2[”事件流缺乏颜色纹理 / Event streams lack color & texture”]\n        Problem --> P3[”现有融合方法计算昂贵且忽略时序 / Existing fusion is costly & neglects temporal dynamics”]\n        Method --> M1[”双分支Mamba编码器 / Dual-branch Mamba encoders”]\n        Method --> M2[”双维度交互模块 (DDIM) / Dual-Dimensional Interaction Module (DDIM)”]\n        M2 --> M2a[”跨空间交互模块 (CSIM) / Cross-Spatial Interaction Module (CSIM)”]\n        M2 --> M2b[”跨时间交互模块 (CTIM) / Cross-Temporal Interaction Module (CTIM)”]\n        Results --> R1[”在DDD17/DSEC上SOTA性能 / SOTA performance on DDD17 & DSEC”]\n        Results --> R2[”显著降低计算成本 / Significantly reduces computational cost”]"
    },
    {
      "title": "Physically-Grounded Manifold Projection with Foundation Priors for Metal Artifact Reduction in Dental CBCT",
      "authors": "Zhi Li, Yaqi Wang, Bingtao Ma, Yifan Zhang, Huiyu Zhou, Shuai Wang",
      "institution": "Hangzhou Dianzi University, University of Leicester, Hangzhou Dental Hospital Group",
      "link": "https://arxiv.org/pdf/2512.24260",
      "code": "https://github.com/",
      "tags": [
        "medical image reconstruction",
        "metal artifact reduction",
        "denoising diffusion models",
        "manifold projection",
        "physics simulation",
        "foundation models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bd16a1a110bf902fb100678cfdd04e4141b492d81205b2bcb44cce6e47345d9_w640_q70.webp",
      "contributions": "1. An Anatomically-Adaptive Physics Simulation (AAPS) pipeline for synthesizing high-fidelity training data, bridging the synthetic-to-real gap. 2. A DMP-Former model that reformulates restoration as a deterministic manifold projection, enabling artifact removal in a single forward pass and eliminating the need for slow iterative sampling. 3. A Semantic-Structural Alignment (SSA) module that leverages priors from medical foundation models (MedDINOv3) to ensure the clinical plausibility of the reconstructed images.",
      "summary": "This paper proposes the Physically-Grounded Manifold Projection (PGMP) framework to reduce metal artifacts in Dental CBCT scans. The method uses a physics-based data synthesis pipeline, a deterministic transformer for fast reconstruction, and a module that aligns outputs with anatomical priors from a foundation model. Experiments show it outperforms existing methods in efficiency and diagnostic reliability on both synthetic and clinical datasets.",
      "mindmap": "graph TB\n        A[Physically-Grounded Manifold Projection with Foundation Priors for Metal Artifact Reduction in Dental CBCT] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Metal artifacts in CBCT hinder diagnosis] --> B1[监督方法有频谱模糊/Supervised: spectral blurring]\n        B --> B2[无监督方法有结构幻觉/Unsupervised: structural hallucinations]\n        B --> B3[扩散模型采样慢/Diffusion models: slow sampling]\n        C[主要方法/Method<br>PGMP Framework] --> C1[AAPS: 物理模拟合成数据/AAPS: Physics-based data synthesis]\n        C --> C2[DMP-Former: 确定性流形投影/DMP-Former: Deterministic manifold projection]\n        C --> C3[SSA: 基础模型先验对齐/SSA: Foundation model prior alignment]\n        D[关键结果/Results<br>Outperforms SOTA] --> D1[效率高/High efficiency (single pass)]\n        D --> D2[诊断可靠性高/High diagnostic reliability]\n        D --> D3[在合成与临床数据上验证/Validated on synthetic & clinical data]"
    },
    {
      "title": "One-shot synthesis of rare gastrointestinal lesions improves diagnostic accuracy and clinical training",
      "authors": "Jia Yu, Yan Zhu, Peiyao Fu, Tianyi Chen, Zhihua Wang, Fei Wu, Quanlin Li, Pinghong Zhou, Shuo Wang, Xian Yang",
      "institution": "Zhejiang University, Fudan University, Imperial College London, The University of Manchester",
      "link": "https://arxiv.org/pdf/2512.24278",
      "code": null,
      "tags": [
        "medical image synthesis",
        "one-shot synthesis",
        "language-guided concept disentanglement",
        "data augmentation",
        "rare disease",
        "generative framework"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf643227c51e8cd7c3969dfe80d336673878c555397635a5c9831a5997ed5bd_w640_q70.webp",
      "contributions": "1. Proposed EndoRare, a one-shot, retraining-free generative framework for synthesizing diverse, high-fidelity images of rare gastrointestinal lesions from a single reference image., 2. Introduced a language-guided concept disentanglement method to separate pathognomonic lesion features from non-diagnostic attributes, ensuring diversity while preserving diagnostic fidelity., 3. Demonstrated that synthetic images improve both AI classifier performance (true positive rate) and novice clinician diagnostic accuracy (recall and precision) for rare pathologies.",
      "summary": "This paper addresses the data scarcity problem for rare gastrointestinal lesions in AI development and clinical training. It proposes EndoRare, a one-shot generative framework that uses language-guided concept disentanglement to synthesize diverse and clinically plausible lesion images from a single example. The results show that these synthetic images significantly enhance the performance of AI classifiers and improve the diagnostic accuracy of novice endoscopists.",
      "mindmap": "graph TB\n        A[One-shot synthesis of rare gastrointestinal lesions] --> B[核心问题/Problem: Rare lesions are infrequent, limiting AI model data and clinician training.]\n        A --> C[主要方法/Method: EndoRare framework uses one-shot, language-guided concept disentanglement to generate diverse, high-fidelity synthetic images.]\n        A --> D[关键结果/Results: Improves AI classifier true positive rate and novice clinician recall & precision.]"
    },
    {
      "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation",
      "authors": "Zhe Huang, Hao Wen, Aiming Hao, Bingze Song, Meiqi Wu, Jiahong Wu, Xiangxiang Chu, Sheng Lu, Haoqian Wang",
      "institution": "Tsinghua University, Beihang University, AMAP (Alibaba Group)",
      "link": "https://arxiv.org/pdf/2512.24271",
      "code": "https://amap-ml.github.io/Taming-Hallucinations/",
      "tags": [
        "multi-modal training",
        "counterfactual video generation",
        "visual hallucination",
        "diffusion-based video editing",
        "advantage normalization",
        "contrastive training"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa10c3c7d7f412fdbab92df4afa93ab6b3653346b89fc3443bf25d8615391b81_w640_q70.webp",
      "contributions": "1. Introduces DualityForge, a framework for automatically synthesizing counterfactual video QA data using controllable diffusion-based video editing. 2. Presents DualityVidQA, a large-scale video dataset built using DualityForge to mitigate MLLM hallucinations. 3. Proposes DNA-Train, a two-stage SFT-RL training regime with pair-wise advantage normalization for stable and efficient policy optimization on contrastive data.",
      "summary": "This paper addresses the problem of visual hallucinations in Multimodal Large Language Models (MLLMs) when processing counterfactual videos. The proposed solution, DualityForge, synthesizes counterfactual video QA data for training, and a novel training method, DNA-Train, leverages this data to improve grounding. Experiments show the method significantly reduces hallucinations and improves performance on both hallucination and general benchmarks.",
      "mindmap": "graph TB\n        A[”Taming Hallucinations: Boosting MLLMs' Video Understanding<br>驯服幻觉：通过反事实视频生成提升MLLM视频理解”] --> B[”核心问题/Problem<br>MLLMs over-rely on language priors, causing visual hallucinations on counterfactual videos.”]\n        A --> C[”主要方法/Method<br>DualityForge: Counterfactual video & QA synthesis.<br>DNA-Train: Contrastive SFT-RL training.”]\n        A --> D[”关键结果/Results<br>24.0% hallucination reduction.<br>Strong generalization on benchmarks.”]"
    },
    {
      "title": "LiftProj: Space Lifting and Projection-Based Panorama Stitching",
      "authors": "Yuan Jia, Ruimin Wu, Rui Song, Jiaojiao Li, Bin Song",
      "institution": "The affiliations indicate the authors are members of IEEE. The specific institution is not explicitly stated on the first page, but the acknowledgment mentions support from the \"National Nature Science Foundation of China,\" suggesting a Chinese research institution.",
      "link": "https://arxiv.org/pdf/2512.24276",
      "code": null,
      "tags": [
        "image stitching",
        "3D lifting",
        "panoramic stitching",
        "point cloud fusion",
        "cylindrical projection",
        "hole filling"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa42233e64d5c4ec1cce0b3482d738a3664cd44807fdde7db2fc77e9f5b1a58b_w640_q70.webp",
      "contributions": "1. Proposes a novel panoramic stitching framework that shifts from 2D warping to a 3D consistency paradigm by first lifting images into a dense 3D point representation. 2. Introduces a global cross-view fusion process in a unified 3D coordinate system, augmented by confidence metrics, followed by a unified projection to create a geometrically consistent 360° layout. 3. Designs the framework to be modular, allowing flexible integration of various 3D lifting and completion modules to address unknown regions via hole filling.",
      "summary": "This paper addresses the problem of ghosting and distortion in traditional 2D image stitching for complex 3D scenes with parallax. It proposes LiftProj, a method that lifts images to 3D point clouds for fusion and then projects them onto a panoramic manifold. The approach significantly reduces geometric artifacts and produces more natural panoramas in challenging scenarios.",
      "mindmap": "graph TB\n        A[LiftProj: Space Lifting and Projection-Based Panorama Stitching] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 2D传统拼接在复杂3D场景中产生重影和扭曲/Ghosting & distortion in 3D scenes with parallax using 2D methods]\n        C[主要方法/Method: 3D空间提升与融合后投影/3D lifting, fusion, and cylindrical projection]\n        D[关键结果/Results: 减少几何失真，生成更自然的全景图/Reduces geometric artifacts, yields more natural panoramas]"
    },
    {
      "title": "UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots",
      "authors": "Nan Jiang, Zimo He, Wanhe Yu, Lexi Pang, Yunhao Li, Hongjie Li, Jieming Cui, Yuhan Li, Yizhou Wang, Yixin Zhu, Siyuan Huang",
      "institution": "Peking University, Beijing Institute for General Artificial Intelligence (BIGAI), Huazhong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.24321",
      "code": "https://jnnan.github.io/uniact/",
      "tags": [
        "embodied ai / robot learning",
        "multimodal motion generation",
        "action streaming",
        "discrete codebook",
        "FSQ (Finite Scalar Quantization)",
        "zero-shot tracking"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d71f14808bc6cc1375b432715e3356f7f970e4a056f6960261c0aed51ae956aa_w640_q70.webp",
      "contributions": "1. A unified two-stage framework (UniAct) that integrates a fine-tuned MLLM with a causal streaming pipeline for low-latency (&lt;500ms) multimodal instruction execution. 2. The use of a shared discrete codebook via FSQ to unify heterogeneous inputs (language, music, trajectory, motion) and constrain motions to a physically grounded manifold. 3. Introduction of the UniMoCap benchmark and demonstration of robust generalization, including a 19% improvement in zero-shot tracking success rate.",
      "summary": "This paper proposes UniAct, a unified framework for generating and streaming motions to humanoid robots from diverse multimodal instructions like language and music. It uses a fine-tuned MLLM and a shared discrete codebook to translate instructions into actions with low latency. The method shows improved zero-shot motion tracking and robust generalization on a new benchmark.",
      "mindmap": "graph TB\n        A[UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Bridging high-level multimodal perception with whole-body execution for humanoid robots]\n        C[主要方法/Method: Two-stage framework with fine-tuned MLLM and causal streaming pipeline, using shared discrete codebook (FSQ)]\n        D[关键结果/Results: Sub-500 ms latency, 19% improvement in zero-shot tracking, robust generalization on UniMoCap benchmark]"
    },
    {
      "title": "Virtual-Eyes: Quantitative Validation of a Lung CT Quality-Control Pipeline for Foundation-Model Cancer Risk Prediction",
      "authors": "Md. Enamul Hoq, Linda Larson-Prior, Fred Prior",
      "institution": "University of Arkansas for Medical Sciences",
      "link": "https://arxiv.org/pdf/2512.24294",
      "code": null,
      "tags": [
        "medical image analysis",
        "CT preprocessing",
        "quality control",
        "foundation models",
        "lung cancer screening",
        "validation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6f230cd5094d46243a5c6fe260cf946e5540833e1a82b86423bbe80a292b76c_w640_q70.webp",
      "contributions": "1. Development and validation of Virtual-Eyes, a novel, anatomically targeted 16-bit CT quality-control pipeline for lung cancer screening. 2. Quantitative demonstration that such preprocessing significantly improves the performance and calibration of generalist foundation models (e.g., RAD-DINO) for cancer risk prediction. 3. Discovery that specialist models (e.g., Sybil) can degrade with the same preprocessing, revealing their potential reliance on contextual shortcuts in raw clinical data.",
      "summary": "This paper develops Virtual-Eyes, a quality-control pipeline for lung CT scans that standardizes resolution and extracts lung regions. The study finds that this preprocessing significantly boosts the cancer risk prediction performance of generalist foundation models but can harm specialist models that have adapted to raw, unprocessed clinical data.",
      "mindmap": "graph TB\n        Root[Virtual-Eyes: 肺癌CT质量控制流程验证] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>LDCT预处理影响未量化] --> P1[缺乏量化/Lack of Quantification]\n        Problem --> P2[通用vs专用模型差异/Generalist vs. Specialist]\n        Method[主要方法/Method<br>Virtual-Eyes Pipeline] --> M1[质量控制/Quality Control<br>512x512, 过滤系列]\n        Method --> M2[肺块提取/Lung Block Extraction<br>HU过滤, 覆盖评分]\n        Method --> M3[模型评估/Model Evaluation<br>RAD-DINO, Sybil等]\n        Results[关键结果/Results<br>预处理效果不同] --> R1[提升通用模型/Improves Generalist FMs<br>RAD-DINO AUC↑]\n        Results --> R2[损害专用模型/Harms Specialist Models<br>Sybil AUC↓]\n        Results --> R3[揭示捷径学习/Reveals Shortcut Learning<br>上下文依赖]"
    },
    {
      "title": "Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention",
      "authors": "Haijing Liu, Zhiyuan Song, Hefeng Wu, Tao Pu, Keze Wang, Liang Lin",
      "institution": "Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.24323",
      "code": null,
      "tags": [
        "video object segmentation",
        "causal intervention",
        "backdoor adjustment",
        "front-door adjustment",
        "egocentric vision",
        "referring segmentation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/183bcfa950e9779617d770bdebe6c5435404af301a891a12dde386054365b0da_w640_q70.webp",
      "contributions": "1. Proposes CERES, a plug-in causal framework to adapt pre-trained RVOS models to the egocentric domain. 2. Introduces dual-modal causal intervention, using backdoor adjustment to mitigate language bias and front-door adjustment to address visual confounding factors. 3. Achieves state-of-the-art performance on Ego-RVOS benchmarks, demonstrating the effectiveness of causal reasoning for robust egocentric video understanding.",
      "summary": "This paper addresses the challenge of robust Egocentric Referring Video Object Segmentation (Ego-RVOS), which suffers from dataset biases and visual confounders like motion and occlusion. The authors propose CERES, a causal framework that applies backdoor and front-door adjustments to language and visual features, respectively, to build more robust representations. Experiments show that CERES achieves state-of-the-art results, highlighting the value of causal reasoning for reliable egocentric video models.",
      "mindmap": "graph TB\n        A[Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Ambiguities in egocentric videos and training data biases lead to spurious correlations.]\n        C[主要方法/Method: CERES framework with dual-modal causal intervention (backdoor & front-door adjustment).]\n        D[关键结果/Results: Achieves state-of-the-art performance on Ego-RVOS benchmarks.]"
    },
    {
      "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning",
      "authors": "Yong Xien Chng, Tao Hu, Wenwen Tong, Xueheng Li, Jiandong Chen, Haojia Yu, Jiefan Lu, Hewei Guo, Hanming Deng, Chengjun Xie, Gao Huang, Dahua Lin, Lewei Lu",
      "institution": "SenseTime Research, Tsinghua University, University of Science and Technology of China",
      "link": "https://arxiv.org/pdf/2512.24330",
      "code": "https://github.com/OpenSenseNova/SenseNova-MARS",
      "tags": [
        "agent system",
        "multimodal agent",
        "reinforcement learning",
        "tool-use",
        "policy optimization",
        "benchmark"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a82c0f2ba917fb46c8e884b39cb5c9d4a0a7e1d4671707b44bf5a18d77fa8e73_w640_q70.webp",
      "contributions": "1. Introduces SenseNova-MARS, a novel framework that empowers Vision-Language Models (VLMs) with interleaved visual reasoning and tool-use capabilities via Reinforcement Learning (RL). 2. Proposes the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve RL training stability and enhance the model's ability to invoke tools and reason effectively. 3. Introduces the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions, for evaluating agentic VLMs on complex visual tasks.",
      "summary": "This paper introduces SenseNova-MARS, a framework that uses reinforcement learning to enable Vision-Language Models to dynamically interleave reasoning with external tools like search and image cropping. The proposed BN-GSPO algorithm improves training stability and tool-use capability. Experiments show the model achieves state-of-the-art performance on search and fine-grained image understanding benchmarks, even surpassing some proprietary models.",
      "mindmap": "graph TB\n        A[SenseNova-MARS] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[VLMs缺乏动态工具调用与推理交织的能力/VLMs lack dynamic interleaving of tool-use and reasoning]\n        C --> C1[提出BN-GSPO强化学习算法/Propose BN-GSPO RL algorithm]\n        C --> C2[集成图像搜索、文本搜索、图像裁剪工具/Integrate image search, text search, image crop tools]\n        D --> D1[在MMSearch和HR-MMSearch上SOTA/Achieves SOTA on MMSearch and HR-MMSearch]\n        D --> D2[超越Gemini-3-Flash和GPT-5/Surpasses proprietary models like Gemini-3-Flash and GPT-5]"
    },
    {
      "title": "Spatial-aware Vision Language Model for Autonomous Driving",
      "authors": "Weijie Wei, Zhipeng Luo, Ling Feng, Venice Erin Liong",
      "institution": "Motional, University of Amsterdam",
      "link": "https://arxiv.org/pdf/2512.24331",
      "code": null,
      "tags": [
        "autonomous driving",
        "Vision-Language Model",
        "LiDAR",
        "3D spatial reasoning",
        "Gradual Fusion Q-Former",
        "spatial-aware QA"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e07c9f7613087f7d69052670bb8235b51f29f857774c6631936664f17a18b77_w640_q70.webp",
      "contributions": "1. Proposes LVLDrive, a novel framework that upgrades VLMs for autonomous driving by incorporating LiDAR point clouds for robust 3D metric spatial understanding., 2. Introduces a Gradual Fusion Q-Former to incrementally inject LiDAR features, mitigating catastrophic disturbance to pre-trained VLMs and preserving their knowledge., 3. Develops a spatial-aware question-answering (SA-QA) dataset to explicitly teach the model advanced 3D perception and reasoning capabilities.",
      "summary": "The paper identifies that current Vision-Language Models (VLMs) for autonomous driving lack accurate 3D spatial understanding due to their reliance on 2D images. To solve this, it proposes LVLDrive, a framework that integrates LiDAR point clouds using a Gradual Fusion Q-Former and trains the model with a spatial-aware QA dataset. The results show that LVLDrive outperforms vision-only methods in scene understanding, spatial perception, and driving decision-making, demonstrating the necessity of explicit 3D data for trustworthy autonomous systems.",
      "mindmap": "graph TB\n        A[LVLDrive<br>空间感知视觉语言模型] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[2D图像VLM缺乏3D度量空间理解<br>2D VLMs lack 3D metric spatial understanding]\n        C --> C1[融合LiDAR点云与视觉<br>Fuse LiDAR point cloud with vision]\n        C --> C2[渐进融合Q-Former<br>Gradual Fusion Q-Former]\n        C --> C3[空间感知QA数据集<br>Spatial-aware QA dataset]\n        D --> D1[性能优于纯视觉方法<br>Outperforms vision-only counterparts]\n        D --> D2[证明3D数据对可信系统的必要性<br>Demonstrates necessity of 3D data for trustworthy systems]"
    },
    {
      "title": "The Mechanics of CNN Filtering with Rectification",
      "authors": "Liam Frija-Altrac, Matthew Toews",
      "institution": "École de Technologie Supérieure",
      "link": "https://arxiv.org/pdf/2512.24338",
      "code": null,
      "tags": [
        "cnn theory",
        "convolutional filtering",
        "rectification",
        "even-odd decomposition",
        "discrete cosine transform",
        "energy-momentum analogy"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e07a90b40ec2be4040a8a82b963be5a6a8e15890031f07ced02c5665f6381c87_w640_q70.webp",
      "contributions": "1. Proposes \"elementary information mechanics\" as a new theoretical model for understanding CNN operations, drawing analogies to physical concepts like energy and momentum. 2. Demonstrates that small CNN filters are dominated by low-frequency DCT components (DC and gradients), which correspond to fundamental information propagation modes (diffusion, vibration, translation). 3. Shows a linear relationship between the speed of information displacement and the ratio of odd to total kernel energy, linking CNN information processing to the relativistic energy-momentum relation.",
      "summary": "This paper introduces a new theoretical framework called \"elementary information mechanics\" to model convolutional filtering with ReLU, drawing analogies from physics. It decomposes kernels into even and odd components, linking them to diffusion and directional displacement of information, and shows that low-frequency DCT components dominate filter behavior. The main conclusion is that fundamental physical principles, like the energy-momentum relation, can explain core information propagation mechanisms in generic CNNs.",
      "mindmap": "graph TB\n        A[”The Mechanics of CNN Filtering with Rectification<br/>CNN滤波与整流机制”] --> B[”核心问题/Problem<br/>What fundamental processes govern information propagation in CNNs?”]\n        A --> C[”主要方法/Method<br/>Propose elementary information mechanics model; Analyze kernel even-odd decomposition & DCT spectrum”]\n        A --> D[”关键结果/Results<br/>Low-frequency DCT components dominate; Speed of info displacement linked to odd/total energy ratio; Connection to energy-momentum relation”]"
    },
    {
      "title": "DermaVQA-DAS: Dermatology Assessment Schema (DAS) & Datasets for Closed-Ended Question Answering & Segmentation in Patient-Generated Dermatology Images",
      "authors": "Wen-wai Yim, Yujuan Fu, Asma Ben Abacha, Meliha Yetisgen, Noel Codella, Roberto Andres Novoa, Josep Malvehy",
      "institution": "Microsoft, University of Washington, Stanford University, Hospital Clinic of Barcelona",
      "link": "https://arxiv.org/pdf/2512.24340",
      "code": "https://osf.io/72rp3",
      "tags": [
        "medical image analysis",
        "visual question answering",
        "lesion segmentation",
        "multimodal models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dabc426dbf988c067106f76b5dca274abd76ad3a46bcddbd05c76b76893e508a_w640_q70.webp",
      "contributions": "1. Introduction of the Dermatology Assessment Schema (DAS), a novel expert-developed framework for structured dermatological feature assessment. 2. Release of DermaVQA-DAS, an extended dataset supporting closed-ended question answering and lesion segmentation on patient-generated images. 3. Comprehensive benchmarking of state-of-the-art multimodal models on the new tasks, analyzing the impact of prompt design on segmentation performance.",
      "summary": "This paper addresses the lack of patient-centered benchmarks in dermatology by introducing DermaVQA-DAS, a dataset extension built upon a novel expert-developed assessment schema (DAS) for structured feature annotation. It supports two tasks—closed-ended visual question answering and lesion segmentation—on patient-generated images and queries. The study benchmarks modern multimodal models, finding strong QA performance and demonstrating that prompt design significantly impacts segmentation results.",
      "mindmap": "graph TB\n        Root[DermaVQA-DAS] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[核心问题/Problem] --> P1[现有数据集缺乏患者视角/Existing datasets lack patient perspective]\n        P1 --> P2[限制以患者为中心的护理应用/Limits patient-centered care applications]\n    \n        Method[主要方法/Method] --> M1[提出皮肤病评估框架(DAS)/Propose Dermatology Assessment Schema (DAS)]\n        M1 --> M2[扩展DermaVQA数据集/Extend DermaVQA dataset]\n        M2 --> M3[支持两项任务:封闭式问答与分割/Support two tasks: closed QA & segmentation]\n    \n        Results[关键结果/Results] --> R1[提示设计影响分割性能/Prompt design impacts segmentation performance]\n        R1 --> R2[模型在QA上表现强劲/Models perform strongly on QA]\n        R2 --> R3[公开数据集与评估协议/Publicly release dataset & evaluation protocols]"
    },
    {
      "title": "Geometric Multi-Session Map Merging with Learned Local Descriptors",
      "authors": "Yanlong Ma, Nakul S. Joshi, Christa S. Robison, Philip R. Osteen, Brett T. Lopez",
      "institution": "University of California, Los Angeles (UCLA), DEVCOM Army Research Laboratory (ARL)",
      "link": "https://arxiv.org/pdf/2512.24384",
      "code": null,
      "tags": [
        "SLAM (Simultaneous Localization and Mapping)",
        "map merging",
        "learned local descriptors",
        "geometric transformer",
        "factor-graph optimization",
        "loop closure detection"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c330f606af1c67ed97c26c40d98ecfd5f18fd1d10de890504cf057e102199d9_w640_q70.webp",
      "contributions": "1. Proposes GMLD, a learning-based local descriptor framework for multi-session point cloud map merging. 2. Introduces a keypoint-aware encoder and a plane-based geometric transformer to extract discriminative features for robust loop closure detection and pose estimation. 3. Incorporates inter-session scan matching cost factors into factor-graph optimization to enhance global map consistency.",
      "summary": "This paper addresses the challenge of merging point cloud maps from multiple sessions or agents in large-scale environments. The proposed GMLD framework uses learned local descriptors and a geometric transformer for feature extraction, combined with factor-graph optimization for global consistency. Experimental results on public and self-collected datasets demonstrate accurate and robust map merging performance.",
      "mindmap": "graph TB\n        A[Geometric Multi-Session Map Merging with Learned Local Descriptors] --> B(核心问题/Problem: Multi-session map merging in large-scale environments)\n        A --> C(主要方法/Method: GMLD framework with learned descriptors & geometric transformer)\n        A --> D(关键结果/Results: Accurate and robust map merging with low error)"
    },
    {
      "title": "Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems",
      "authors": "Song Wang, Lingdong Kong, Xiaolu Liu, Hao Shi, Wentong Li, Jianke Zhu, Steven C. H. Hoi",
      "institution": "Zhejiang University, National University of Singapore, Nanjing University of Aeronautics and Astronautics, Alibaba Group, Singapore Management University",
      "link": "https://arxiv.org/pdf/2512.24385",
      "code": "https://github.com/worldbench/awesome-spatial-intelligence",
      "tags": [
        "multi-modal perception",
        "spatial intelligence",
        "multi-modal pre-training",
        "3D object detection",
        "semantic occupancy prediction",
        "foundation models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d3c33bdbff1adb3e8892d63c2303634ba223ae7e14138e8573b929718fecafc_w640_q70.webp",
      "contributions": "1. Formulates a unified taxonomy for multi-modal pre-training paradigms, from single-modality to unified frameworks. 2. Investigates the integration of textual inputs and occupancy representations for open-world perception and planning. 3. Identifies critical bottlenecks (e.g., computational efficiency) and proposes a roadmap towards general-purpose multi-modal foundation models for robust Spatial Intelligence.",
      "summary": "This paper addresses the challenge of integrating diverse sensor data (e.g., cameras, LiDAR) to achieve Spatial Intelligence for autonomous systems. It proposes a comprehensive framework and taxonomy for multi-modal pre-training, analyzing techniques for unified representation learning and identifying future research directions. The main conclusion is a roadmap towards building general-purpose multi-modal foundation models capable of robust real-world perception and planning.",
      "mindmap": "graph TB\n        Root[”Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems<br>打造空间智能：自主系统多模态数据预训练路线图”] --> Problem[”核心问题/Problem<br>Integrating multi-modal sensor data for unified Spatial Intelligence<br>融合多模态传感器数据以实现统一的空间智能”]\n        Root --> Method[”主要方法/Method<br>Comprehensive framework & unified taxonomy for multi-modal pre-training<br>全面的多模态预训练框架与统一分类法”]\n        Root --> Results[”关键结果/Results<br>Roadmap for general-purpose multi-modal foundation models<br>通用多模态基础模型的路线图”]"
    },
    {
      "title": "RedunCut: Measurement-Driven Sampling and Accuracy Performance Modeling for Low-Cost Live Video Analytics",
      "authors": "Gur-Eyal Sela, Kumar Krishna Agrawal, Bharathan Balaji, Joseph Gonzalez, Ion Stoica",
      "institution": "UC Berkeley, Amazon",
      "link": "https://arxiv.org/pdf/2512.24386",
      "code": null,
      "tags": [
        "others",
        "dynamic model size selection",
        "live video analytics",
        "measurement-driven sampling",
        "accuracy performance modeling",
        "cost-accuracy tradeoff"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b99d771bb4464484bc20a905a41cf7ba47990b34219a4a1e3c0b5dc5c0c242bb_w640_q70.webp",
      "contributions": "1. A measurement-driven planner that estimates the cost-benefit tradeoff of sampling to avoid inefficient sampling. 2. A lightweight, data-driven performance model to improve per-segment accuracy prediction. 3. A new DMSS system (RedunCut) that reduces compute cost by 14-62% at fixed accuracy across diverse video workloads and remains robust to limited data and drift.",
      "summary": "The paper addresses the high inference cost in live video analytics by proposing RedunCut, a dynamic model size selection system. It introduces a measurement-driven planner to optimize sampling and a data-driven model to predict accuracy, reducing compute costs by 14-62% while maintaining target accuracy across diverse video types. The system demonstrates robustness to limited historical data and concept drift.",
      "mindmap": "graph TB\n        Root[RedunCut: Low-Cost Live Video Analytics] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[高推理成本/High Inference Cost]\n        Problem --> P2[现有方法泛化性差/Prior Methods Fail to Generalize]\n        P2 --> P2_1[采样效率低/Inefficient Sampling]\n        P2 --> P2_2[精度预测不准/Inaccurate Accuracy Prediction]\n        Method[主要方法/Method] --> M1[测量驱动的规划器/Measurement-Driven Planner]\n        Method --> M2[轻量级性能模型/Lightweight Performance Model]\n        Results[关键结果/Results] --> R1[成本降低 14-62%/Cost Reduction 14-62%]\n        Results --> R2[保持目标精度/Maintains Target Accuracy]\n        Results --> R3[对数据漂移鲁棒/Robust to Drift]"
    },
    {
      "title": "Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning",
      "authors": "Soham Pahari, M. Srinivas",
      "institution": "UPES (School of Computer Science), NIT Warangal (Department of CS&E)",
      "link": "https://arxiv.org/pdf/2512.24404",
      "code": null,
      "tags": [
        "cross-view geo-localization",
        "visual reasoning",
        "reinforcement learning",
        "contrastive learning",
        "cross-view alignment",
        "visual planning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d4164fa0c19f567cf79df4a59a8129e5b520a00f0d6456c7d4f5649d4a580da_w640_q70.webp",
      "contributions": "1. Proposes a novel visual reasoning paradigm called Geo-Consistent Visual Planning and a framework named ViReLoc for planning and localization using only visual representations. 2. Introduces a method that learns spatial and geometric dependencies through step-by-step visual inference optimized with reinforcement learning objectives. 3. Integrates contrastive learning and adaptive feature interaction to align ground and aerial perspectives and reduce viewpoint differences.",
      "summary": "This paper addresses the limitation of text-based reasoning in spatial tasks by proposing ViReLoc, a visual reasoning framework for ground-to-aerial localization and route planning. The method uses reinforcement learning and contrastive learning to perform inference directly in the visual domain without relying on GPS. Experiments show improved spatial reasoning and cross-view retrieval, establishing visual reasoning as a secure complementary approach for navigation.",
      "mindmap": "graph TB\n        A[Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[文本推理在空间任务中的局限性<br/>Limitations of Text-Based Reasoning in Spatial Tasks]\n        C --> C1[视觉推理框架ViReLoc<br/>Visual Reasoning Framework ViReLoc]\n        C1 --> C2[Geo-Consistent Visual Planning<br/>Geo-Consistent Visual Planning]\n        C1 --> C3[强化学习与对比学习<br/>Reinforcement & Contrastive Learning]\n        D --> D1[空间推理与跨视图检索性能提升<br/>Improved Spatial Reasoning & Cross-View Retrieval]\n        D --> D2[无需GPS的安全导航方案<br/>Secure Navigation Without GPS]"
    },
    {
      "title": "DyStream: Streaming Dyadic Talking Heads Generation via Flow Matching-based Autoregressive Model",
      "authors": "Bohong Chen, Haiyang Liu",
      "institution": "Zhejiang University, The University of Tokyo",
      "link": "https://arxiv.org/pdf/2512.24408",
      "code": "https://robinwitch.github.io/DyStream-Page",
      "tags": [
        "talking head generation",
        "flow matching",
        "autoregressive model",
        "low latency",
        "lip synchronization",
        "causal encoder"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60897d46da28b4d10e44efe7e6ee02b27fe594926b04b3ae834d8f1d5a78f416_w640_q70.webp",
      "contributions": "1. Proposed DyStream, a flow matching-based autoregressive model for real-time, streaming dyadic talking head video generation. 2. Introduced a causal encoder enhanced with a short lookahead module to incorporate minimal future context for quality improvement while maintaining ultra-low latency. 3. Demonstrated state-of-the-art lip-sync quality and a per-frame generation time of 34 ms, keeping total system latency under 100 ms.",
      "summary": "The paper addresses the high latency problem in generating realistic listener responses for dyadic talking head videos. It proposes DyStream, a flow matching-based autoregressive model with a causal lookahead encoder, which enables real-time, frame-by-frame generation from streaming dual-track audio. The method achieves ultra-low latency (under 100 ms) and state-of-the-art lip-sync quality.",
      "mindmap": "graph TB\n        A[DyStream Paper] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法延迟高/High Latency in Existing Methods]\n        B --> B2[影响倾听者实时反馈/Affects Real-time Listener Feedback]\n        C --> C1[流式自回归框架/Stream-friendly Autoregressive Framework]\n        C --> C2[基于流匹配的头部/Flow-Matching Heads]\n        C --> C3[带前瞻模块的因果编码器/Causal Encoder with Lookahead]\n        D --> D1[每帧34ms生成/34 ms per Frame Generation]\n        D --> D2[总延迟<100ms/Total Latency < 100 ms]\n        D --> D3[领先的唇同步质量/State-of-the-art LipSync Quality]"
    },
    {
      "title": "AI-Driven Evaluation of Surgical Skill via Action Recognition",
      "authors": "Yan Meng, Daniel A. Donoho, Marcelle Altshuler, Omar Arnaout",
      "institution": "Children's National Hospital, Brigham and Women's Hospital, Harvard Medical School",
      "link": "https://arxiv.org/pdf/2512.24411",
      "code": null,
      "tags": [
        "action recognition",
        "video transformer",
        "hierarchical temporal attention",
        "YOLO-based tracking",
        "microanastomosis",
        "surgical skill assessment"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b060680d765ea8d59b727d7a45b5d8ddec7501ea571a866eb276f7cbbe079811_w640_q70.webp",
      "contributions": "1. A novel AI-driven framework for automated assessment of microanastomosis surgical skill. 2. Integration of an improved video transformer (TimeSformer with hierarchical temporal and weighted spatial attention) for action recognition. 3. Extraction and analysis of fine-grained motion features using YOLO-based object detection and tracking for detailed instrument kinematics.",
      "summary": "This paper proposes an AI system to automate the evaluation of surgical skill in microanastomosis procedures. The method uses an enhanced video transformer for action recognition and YOLO-based tracking for motion analysis, achieving high accuracy in segmenting actions and classifying skill levels. The results demonstrate the system's potential to provide objective and scalable feedback for surgical training.",
      "mindmap": "graph TB\n        Root[AI-Driven Evaluation of Surgical Skill via Action Recognition] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[主观且耗时的传统评估方法/Subjective & time-consuming traditional assessment]\n        Problem --> P2[评估方法难以规模化/Assessment methods are hard to scale]\n        Method[主要方法/Method] --> M1[基于改进TimeSformer的视频动作识别/Video action recognition with improved TimeSformer]\n        Method --> M2[基于YOLO的器械运动特征提取/YOLO-based instrument motion feature extraction]\n        Results[关键结果/Results] --> R1[动作分割准确率93.62%/Action segmentation accuracy 93.62%]\n        Results --> R2[技能分类平均准确率76%/Average skill classification accuracy 76%]"
    },
    {
      "title": "Exploring Compositionality in Vision Transformers using Wavelet Representations",
      "authors": "Akshad Shyam Purushottamdas, Pranav K Nayak, Divya Mehul Rajparia, Deekshith Patel, Yashmitha Gogineni, Konda Reddy Mopuri, Sumohana S. Channappayya",
      "institution": "IIT Hyderabad",
      "link": "https://arxiv.org/pdf/2512.24438",
      "code": null,
      "tags": [
        "vision transformer interpretability",
        "Vision Transformer (ViT)",
        "compositionality",
        "Discrete Wavelet Transform (DWT)",
        "representation learning",
        "encoder analysis"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9ed3d61f79d7e2fec6558aa9fe2df11bfd9e9c1d70312c79be40b99f44455a4_w640_q70.webp",
      "contributions": "1. A framework for testing compositionality in ViT encoder representations, analogous to prior work in representation learning. 2. The novel use of the Discrete Wavelet Transform (DWT) to generate input-specific primitives (basis sets) for images to analyze ViTs. 3. Empirical results demonstrating that ViT encoder representations exhibit approximate compositionality when using primitives from a one-level DWT decomposition.",
      "summary": "This paper investigates whether Vision Transformer (ViT) encoders learn compositional representations. The authors propose a framework that uses the Discrete Wavelet Transform (DWT) to decompose images into primitives and then tests if the ViT's representation of the whole image can be composed from the representations of these primitives. Their findings show that ViT encoder representations do exhibit approximate compositionality, offering a new perspective on how ViTs structure visual information.",
      "mindmap": "graph TB\n        Root(”Exploring Compositionality in Vision Transformers using Wavelet Representations”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”ViT表征是否具有组合性?/Do ViT representations exhibit compositionality?”)\n        Method --> M1(”使用DWT获取图像基元/Use DWT to get image primitives”)\n        Method --> M2(”测试组合表征的还原能力/Test recomposition ability of representations”)\n        Results --> R1(”DWT基元产生近似可组合的表征/DWT primitives produce approximately compositional representations”)"
    },
    {
      "title": "Spectral and Spatial Graph Learning for Multispectral Solar Image Compression",
      "authors": "Prasiddha Siwakoti, Atefeh Khoshkhahtinat, Piyush M. Mehta, Barbara J. Thompson, Michael S. F. Kirk, Daniel da Silva",
      "institution": "West Virginia University, NASA Goddard Space Flight Center",
      "link": "https://arxiv.org/pdf/2512.24463",
      "code": "https://github.com/agyat4/sgraph",
      "tags": [
        "image compression",
        "graph neural network",
        "multispectral image compression",
        "spectral graph embedding",
        "spatial graph attention",
        "learned image compression"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22141561baf730c3986a8299115f064f9a3f82996264d04ebeefffad89a18be1_w640_q70.webp",
      "contributions": "1. Proposed an Inter-Spectral Windowed Graph Embedding (iSWGE) module to model inter-band relationships by representing spectral channels as graph nodes with learned edges. 2. Introduced a Windowed Spatial Graph Attention and Convolutional Block Attention (WSGA-C) module to reduce spatial redundancy and emphasize fine-scale structures. 3. Developed a learned image compression framework tailored for multispectral solar imagery, achieving improved spectral fidelity and reconstruction quality on the SDOML dataset.",
      "summary": "This paper addresses the challenge of compressing high-volume multispectral solar imagery for space missions. It proposes a learned compression framework that uses two novel graph-based modules to model spectral and spatial dependencies. The method demonstrates improved performance in preserving spectral information and reconstruction quality compared to strong baselines.",
      "mindmap": "graph TB\n        A[论文标题: Spectral and Spatial Graph Learning for Multispectral Solar Image Compression] --> B(核心问题/Problem: High-fidelity compression of multispectral solar imagery with limited bandwidth)\n        A --> C(主要方法/Method: Two complementary graph learning modules: iSWGE for spectral, WSGA-C for spatial dependencies)\n        A --> D(关键结果/Results: 20.15% MSID reduction, up to 1.09% PSNR gain, 1.62% MS-SSIM gain)"
    },
    {
      "title": "F2IDiff: Real-world Image Super-resolution using Feature to Image Diffusion Foundation Model",
      "authors": "Devendra K. Jangid, Ripon K. Saha, Dilshan Godaliyadda, Jing Li, Seok-Jun Lee, Hamid R. Sheikh",
      "institution": "MPI Lab, Samsung Research America",
      "link": "https://arxiv.org/pdf/2512.24473",
      "code": null,
      "tags": [
        "image super-resolution",
        "diffusion models",
        "DINOv2",
        "feature conditioning",
        "hallucination control",
        "real-world images"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccded9f21c9c6de980a50bfd5c308052e1cce34fc21b16a1a1b0cbd3a8c8c57d_w640_q70.webp",
      "contributions": "1. Proposes F2IDiff, a new Feature-to-Image Diffusion Foundation Model for SISR that uses lower-level DINOv2 features for conditioning instead of text. 2. Demonstrates that this approach provides stricter and richer conditioning for small patches, enabling controlled generation and higher fidelity, especially for high-fidelity smartphone LR images. 3. Shows that the model can be trained effectively with a much smaller dataset (38K images) and a smaller U-Net than large text-to-image models like SD2.1, while achieving superior performance.",
      "summary": "This paper addresses the problem of undesirable hallucinations in generative super-resolution for high-fidelity smartphone images. It proposes F2IDiff, a diffusion foundation model conditioned on DINOv2 features instead of text, which allows for stricter control and richer description of image patches. The method achieves better fidelity and performance than text-conditioned models while requiring significantly less data and a smaller network.",
      "mindmap": "graph TB\n        A[F2IDiff: Real-world Image Super-resolution] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[文本特征对细节描述不足<br/>Text features lack detail]\n        B --> B2[智能手机高分辨率LR图像需要无幻觉生成<br/>Smartphone LR needs hallucination-free generation]\n        C --> C1[使用DINOv2特征进行条件控制<br/>Use DINOv2 features for conditioning]\n        C --> C2[构建特征到图像扩散基础模型(F2IDiff)<br/>Build Feature-to-Image Diffusion FM (F2IDiff)]\n        D --> D1[比基于文本的模型保真度更高<br/>Higher fidelity than text-based models]\n        D --> D2[使用更小的数据集和网络实现<br/>Achieved with smaller dataset & network]"
    },
    {
      "title": "Training-Free Color-Aware Adversarial Diffusion Sanitization for Diffusion Stegomalware Defense at Security Gateways",
      "authors": "Vladimir Frants, Sos Agaian",
      "institution": "University of Texas at San Antonio (inferred from author \"Sos Agaian\" affiliation)",
      "link": "https://arxiv.org/pdf/2512.24499",
      "code": null,
      "tags": [
        "steganography defense",
        "adversarial sanitization",
        "diffusion models",
        "pretrained denoiser",
        "quaternion update",
        "stegomalware"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3c855da6cedcd1ea0c4dd38e35e187e7080351881b3a1515b6079801e94495a6_w640_q70.webp",
      "contributions": "1. Proposes Adversarial Diffusion Sanitization (ADS), a training-free defense that neutralizes hidden payloads in diffusion-based steganography instead of detecting them. 2. Introduces a color-aware, quaternion-coupled update rule to minimize perceptual artifacts under strict distortion constraints. 3. Demonstrates effectiveness by driving decoder success rates of the state-of-the-art method Pulsar to near zero with minimal impact on image utility.",
      "summary": "This paper addresses the threat of diffusion model-based steganography used for covert malware delivery. It proposes Adversarial Diffusion Sanitization (ADS), a training-free method that uses a pretrained denoiser and a color-aware update rule to subtly perturb images and neutralize hidden payloads. The results show ADS effectively disrupts steganographic decoding with minimal visual distortion, offering a practical defense for security gateways.",
      "mindmap": "graph TB\n    A[Training-Free Color-Aware Adversarial Diffusion Sanitization<br>免训练颜色感知对抗扩散净化] --> B\n    A --> C\n    A --> D\n    B[核心问题/Problem<br>Diffusion steganography enables covert stegomalware delivery<br>扩散隐写术实现隐蔽的隐写恶意软件传递]\n    C[主要方法/Method<br>ADS: Training-free sanitization using pretrained denoiser & color-aware quaternion update<br>ADS: 使用预训练去噪器和颜色感知四元数更新的免训练净化]\n    D[关键结果/Results<br>Neutralizes Pulsar payloads with near-zero success rate & minimal distortion<br>以接近零的成功率和最小失真中和Pulsar有效载荷]"
    },
    {
      "title": "Using Large Language Models To Translate Machine Results To Human Results",
      "authors": "Trishna Niraula, Jonathan Stubblefield",
      "institution": "Arkansas State University",
      "link": "https://arxiv.org/pdf/2512.24518",
      "code": null,
      "tags": [
        "object detection",
        "YOLOv5",
        "YOLOv8",
        "GPT-4",
        "automated report generation",
        "cosine similarity"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/843beb25a0a5b1b5d3f0617e07b26982372c49b0980f8c9a7193a05521b3d256_w640_q70.webp",
      "contributions": "1. Introduced a novel pipeline integrating YOLO-based object detection models with a large language model (GPT-4) to generate natural-language radiology reports from chest X-ray images. 2. Conducted a comparative evaluation of YOLOv5 and YOLOv8 in terms of detection accuracy, inference latency, and the quality of the generated text reports. 3. Performed human evaluation to assess the clarity and natural writing flow of the AI-generated reports, revealing a trade-off between clinical accuracy and stylistic authenticity.",
      "summary": "This study addresses the gap between structured AI predictions in medical imaging and the narrative reports used by clinicians. It proposes a pipeline that uses YOLO models for anomaly detection in chest X-rays and GPT-4 to translate these detections into descriptive radiology reports. The results show the system achieves strong semantic accuracy but the generated text lacks the natural flow of human-authored reports.",
      "mindmap": "graph TB\n        A[Using Large Language Models To Translate Machine Results To Human Results] --> B(核心问题/Problem: Structured AI outputs vs. narrative clinical reports)\n        A --> C(主要方法/Method: YOLO detection + LLM (GPT-4) report generation)\n        A --> D(关键结果/Results: High semantic similarity (0.88), good clarity but poor natural flow)"
    },
    {
      "title": "Hierarchical Vector-Quantized Latents for Perceptual Low-Resolution Video Compression",
      "authors": "Manikanta Kotthapalli, Banafsheh Rekabdar",
      "institution": "Portland State University",
      "link": "https://arxiv.org/pdf/2512.24547",
      "code": null,
      "tags": [
        "video compression",
        "VQ-VAE",
        "hierarchical latents",
        "perceptual loss",
        "vector quantization",
        "3D convolutions"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d388ce684f252e97356733df6d2d163284bb7c1b55eb7a05e5ff13f6b1a929b_w640_q70.webp",
      "contributions": "1. Proposes a Multi-Scale Vector Quantized VAE (MS-VQ-VAE) for low-resolution video compression, extending VQ-VAE-2 to a spatiotemporal domain with a two-level hierarchical latent structure. 2. Introduces a lightweight architecture (~18.5M parameters) using 3D residual convolutions, optimized for deployment on resource-constrained edge devices. 3. Incorporates a perceptual loss from a pre-trained VGG16 network to enhance the perceptual quality of the reconstructed video.",
      "summary": "This paper addresses the need for video compression methods that generate machine-learning-friendly latent representations for bandwidth-sensitive applications. It proposes a lightweight, hierarchical VQ-VAE model for low-resolution video that uses 3D convolutions and perceptual loss. The model shows improved performance over a baseline and is suitable for edge deployment in scenarios like streaming and mobile analytics.",
      "mindmap": "graph TB\n        A[Hierarchical Vector-Quantized Latents for Perceptual Low-Resolution Video Compression] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: High bandwidth/storage demands for video; Traditional codecs lack ML-friendly latents]\n        C[主要方法/Method: Multi-Scale VQ-VAE (MS-VQ-VAE) with hierarchical latents, 3D convolutions, perceptual loss]\n        D[关键结果/Results: 25.96 dB PSNR, 0.8375 SSIM; Improves over baseline; Suitable for edge devices]"
    },
    {
      "title": "OCP-LS: An Efficient Algorithm for Visual Localization",
      "authors": "Jindi Zhong, Hongxia Wang, Huanshui Zhang",
      "institution": "Shandong University of Science and Technology, Shandong University",
      "link": "https://arxiv.org/pdf/2512.24552",
      "code": null,
      "tags": [
        "others",
        "second-order optimization",
        "Hessian approximation",
        "visual localization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6ce7efe81a3ae02d86a4b55e643a250e273037d6af6755d5dfc5185a4581838_w640_q70.webp",
      "contributions": "1. Proposes a novel second-order optimization algorithm (OCP-LS) that incorporates the OCP method and approximates the diagonal elements of the Hessian matrix. 2. Applies the algorithm to large-scale optimization problems in deep learning for visual localization, demonstrating its independence from specific network architectures. 3. Achieves superior performance on standard benchmarks, including faster convergence, enhanced training stability, and improved robustness to noise compared to conventional optimizers.",
      "summary": "This paper proposes OCP-LS, a novel second-order optimization algorithm for deep learning that incorporates the OCP method and approximates the Hessian's diagonal. It is applied to visual localization tasks using a MapNet architecture. Experiments show the method achieves competitive accuracy with faster convergence, better stability, and improved noise robustness compared to conventional optimizers.",
      "mindmap": "graph TB\n        A[OCP-LS: An Efficient Algorithm for Visual Localization] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[大规模深度学习优化/Large-scale Deep Learning Optimization]\n        C --> C1[二阶优化算法/Second-order Optimization Algorithm]\n        C1 --> C2[结合OCP方法/Incorporates OCP Method]\n        C1 --> C3[近似Hessian对角元素/Approximates Hessian Diagonal]\n        D --> D1[更快的收敛/Faster Convergence]\n        D --> D2[增强的训练稳定性/Enhanced Training Stability]\n        D --> D3[改进的噪声鲁棒性/Improved Robustness to Noise]"
    },
    {
      "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation",
      "authors": "Yuanhao Cai, Kunpeng Li, Menglin Jia, Jialiang Wang, Junzhe Sun, Feng Liang, Weifeng Chen, Felix Juefei-Xu, Chu Wang, Ali Thabet, Xiaoliang Dai, Xuan Ju, Alan Yuille, Ji Hou",
      "institution": "Meta Superintelligence Labs, Johns Hopkins University, Meta BizAI, CUHK",
      "link": "https://arxiv.org/pdf/2512.24551",
      "code": "https://caiyuanhao1998.github.io/project/PhyGDPO",
      "tags": [
        "video generation",
        "direct preference optimization",
        "physics-aware learning",
        "vision-language model",
        "Plackett-Luce model",
        "LoRA"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65a1d8df7eb5b611a50baf19a4ea822c74b9bb43565ee7d4123aa4ab4a91efbc_w640_q70.webp",
      "contributions": "1. Introduces PhyAugPipe, a pipeline using a vision-language model with chain-of-thought reasoning to construct a large-scale physics-augmented video dataset (PhyVidGen-135K). 2. Proposes PhyGDPO, a physics-aware groupwise direct preference optimization framework based on the Plackett-Luce model for holistic preference learning. 3. Designs a Physics-Guided Rewarding (PGR) scheme and a LoRA-Switch Reference (LoRA-SR) scheme to embed physics rewards and enable efficient training.",
      "summary": "This paper addresses the challenge of generating physically consistent videos from text. It proposes a new method, PhyGDPO, which uses a physics-aware groupwise preference optimization framework and a novel data construction pipeline to improve physical plausibility. The method significantly outperforms state-of-the-art open-source models on physics-focused benchmarks.",
      "mindmap": "graph TB\n        A[PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[文本生成视频的物理一致性差/Physically inconsistent T2V generation]\n        B --> B2[缺乏物理交互训练数据/Lack of physics-rich training data]\n        C --> C1[数据构建管道 PhyAugPipe/Data Pipeline PhyAugPipe]\n        C --> C2[组偏好优化框架 PhyGDPO/Groupwise DPO Framework PhyGDPO]\n        C2 --> C21[物理引导奖励 PGR/Physics-Guided Rewarding PGR]\n        C2 --> C22[高效训练方案 LoRA-SR/LoRA-Switch Reference LoRA-SR]\n        D --> D1[构建数据集 PhyVidGen-135K/Built dataset PhyVidGen-135K]\n        D --> D2[在PhyGenBench和VideoPhy2上表现优异/Outperforms SOTA on PhyGenBench & VideoPhy2]"
    },
    {
      "title": "RGBT-Ground Benchmark: Visual Grounding Beyond RGB in Complex Real-World Scenarios",
      "authors": "Tianyi Zhao, Jiawen Xi, Linhui Xiao, Junnan Li, Xue Yang, Maoxun Yuan, Xingxing Wei",
      "institution": "Beihang University, Pengcheng Laboratory, Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.24561",
      "code": null,
      "tags": [
        "visual grounding",
        "RGB-TIR",
        "multi-modal fusion",
        "benchmark",
        "robust perception",
        "thermal infrared"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0755cf2abb7623aaedd6b399439e49a428afb715c921daca7861337c872a0cb9_w640_q70.webp",
      "contributions": "1. Introduces RGBT-Ground, the first large-scale visual grounding benchmark with aligned RGB and thermal infrared image pairs for complex real-world scenarios. 2. Proposes a unified visual grounding framework supporting uni-modal (RGB/TIR) and multi-modal (RGB-TIR) inputs. 3. Presents RGBT-VGNet, a simple yet effective baseline model that outperforms adapted methods, especially in challenging conditions like nighttime and long-distance.",
      "summary": "This paper introduces RGBT-Ground, a new benchmark for visual grounding that uses aligned RGB and thermal infrared images to address robustness in complex real-world conditions like poor lighting and weather. The authors also propose RGBT-VGNet, a multi-modal fusion model, which shows superior performance, particularly in nighttime and long-distance scenarios, compared to adapted existing methods.",
      "mindmap": "graph TB\n        Root[RGBT-Ground Benchmark] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有基准缺乏现实复杂性/Existing benchmarks lack real-world complexity]\n        Method[主要方法/Method] --> M1[提出RGB-TIR基准/RGBT-Ground Benchmark]\n        Method --> M2[统一多模态框架/Unified Multi-modal Framework]\n        Method --> M3[基线模型RGBT-VGNet/Baseline Model RGBT-VGNet]\n        Results[关键结果/Results] --> R1[模型在夜间和远距离场景表现优异/Superior performance in nighttime & long-distance]\n        Results --> R2[促进鲁棒视觉定位研究/Promotes robust visual grounding research]"
    },
    {
      "title": "Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning",
      "authors": "Fuyu Dong, Ke Li, Di Wang, Nan Luo, Yiming Zhang, Kaiyu Li, Jianfei Yang, Quan Wang",
      "institution": "Xidian University, University of California, San Diego, Xi'an Jiaotong University",
      "link": "https://arxiv.org/pdf/2512.24591",
      "code": null,
      "tags": [
        "vision-language reasoning",
        "decision-ambiguous samples",
        "reinforcement fine-tuning",
        "group-relative policy optimization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0e5d66094df5593ae9dd42856f1adba2aea6d5f079e4b7c888e839a61516cc5_w640_q70.webp",
      "contributions": "1. Formally defines Decision-Ambiguous Samples (DAS) as a key challenge in CDVQA, where model confidence is similar between the correct answer and strong distractors. 2. Proposes DARFT, a novel fine-tuning framework that first mines DAS using an SFT-trained policy and then applies group-relative policy optimization on this subset. 3. Demonstrates that DARFT effectively suppresses distractors and sharpens decision boundaries, leading to consistent performance gains over SFT baselines, especially in few-shot settings.",
      "summary": "The paper addresses the problem of decision ambiguity in Change Detection Visual Question Answering (CDVQA), where models struggle to choose between the correct answer and strong distractors. It proposes DARFT, a reinforcement fine-tuning framework that identifies and optimizes these ambiguous samples using group-relative policy optimization. The method shows improved performance over standard supervised fine-tuning, particularly with limited data.",
      "mindmap": "graph TB\n        A[Improving Few-Shot CDVQA via DARFT] --> B[核心问题/Problem: Decision Ambiguity in CDVQA]\n        A --> C[主要方法/Method: DARFT Framework]\n        A --> D[关键结果/Results: Gains over SFT, Few-shot Robustness]\n        B --> B1[模型对正确答案与强干扰项置信度相近/Model assigns similar confidence to correct answer and strong distractors]\n        C --> C1[挖掘决策模糊样本/Mine Decision-Ambiguous Samples (DAS)]\n        C --> C2[应用组内相对优势优化/Apply Group-Relative Policy Optimization]\n        D --> D1[减少模糊性，锐化决策边界/Reduced ambiguity, sharper decision boundaries]\n        D --> D2[在少样本设置下性能提升显著/Performance gains under few-shot settings]"
    },
    {
      "title": "SliceLens: Fine-Grained and Grounded Error Slice Discovery for Multi-Instance Vision Tasks",
      "authors": "Wei Zhang, Chaoqun Wang, Zixuan Guan, Sam Kao, Pengfei Zhao, Peng Wu, Sifeng He",
      "institution": "Apple",
      "link": "https://arxiv.org/pdf/2512.24592",
      "code": null,
      "tags": [
        "model evaluation and analysis",
        "error slice discovery",
        "multi-instance vision tasks",
        "fine-grained reasoning",
        "vision-language models",
        "benchmark"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/728a923247ac54e5968776493b08bc65f1a41354b66a829473a0dd25289e7248_w640_q70.webp",
      "contributions": "1. Proposed SliceLens, a hypothesis-driven framework leveraging LLMs and VLMs for fine-grained and grounded error slice discovery in multi-instance vision tasks. 2. Introduced FeSD, the first benchmark for evaluating fine-grained error slice discovery across instance-level vision tasks with expert-annotated ground truth. 3. Demonstrated state-of-the-art performance on benchmarks and showed identified slices facilitate actionable model improvements via repair experiments.",
      "summary": "The paper addresses the challenge of discovering systematic failure patterns (error slices) in multi-instance vision tasks like detection and segmentation, where existing methods are limited. It proposes SliceLens, a framework that uses LLMs and VLMs to generate and verify failure hypotheses through visual reasoning. The method achieves superior performance on a new benchmark and provides interpretable slices that help improve model robustness.",
      "mindmap": "graph TB\n        Root[SliceLens: Fine-Grained and Grounded Error Slice Discovery] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有方法限制/Existing methods limited to image classification]\n        Problem --> P2[缺乏细粒度推理/Lack fine-grained reasoning for multi-instance tasks]\n        Problem --> P3[基准不完善/Benchmarks biased or artificial]\n        Method[主要方法/Method] --> M1[假设驱动框架/Hypothesis-driven framework]\n        M1 --> M1_1[利用LLM和VLM/Leverages LLMs & VLMs]\n        M1 --> M1_2[生成验证假设/Generates & verifies failure hypotheses]\n        M1 --> M1_3[基于视觉的推理/Grounded visual reasoning]\n        Results[关键结果/Results] --> R1[提出新基准FeSD/Proposes new benchmark FeSD]\n        Results --> R2[性能SOTA/State-of-the-art performance]\n        Results --> R3[可解释切片助力模型修复/Interpretable slices enable model repair]"
    },
    {
      "title": "3D Semantic Segmentation for Post-Disaster Assessment",
      "authors": "Nhut Le, Maryam Rahnemoonfar",
      "institution": "Lehigh University",
      "link": "https://arxiv.org/pdf/2512.24593",
      "code": null,
      "tags": [
        "3D semantic segmentation",
        "3D point clouds",
        "Structure-from-Motion (SfM)",
        "Multi-View Stereo (MVS)",
        "Fast Point Transformer (FPT)",
        "Point Transformer v3 (PTv3)"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7f40d3c2e21cef6917feeb765394fae515f763b5c2f2f5b374f921e3a8ac1ba_w640_q70.webp",
      "contributions": "1. Constructed a specialized 3D dataset for post-disaster assessment using UAV footage and 3D reconstruction techniques (SfM/MVS). 2. Evaluated state-of-the-art 3D semantic segmentation models (FPT, PTv3, OA-CNNs) on this new dataset. 3. Identified significant limitations of existing models in disaster-stricken environments, highlighting the need for new techniques and benchmarks.",
      "summary": "This paper addresses the lack of specialized datasets for 3D semantic segmentation in post-disaster scenarios by constructing a new 3D point cloud dataset from UAV footage of Hurricane Ian. The authors evaluated several state-of-the-art models on this dataset and found their performance to be significantly limited, demonstrating an urgent need for improved methods and benchmarks tailored to disaster environments.",
      "mindmap": "graph TB\n        A[3D Semantic Segmentation for Post-Disaster Assessment] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[缺乏灾后3D数据集 / Lack of post-disaster 3D datasets]\n        C --> C1[使用无人机与SfM/MVS构建3D数据集 / Construct 3D dataset using UAV & SfM/MVS]\n        C --> C2[评估SOTA 3D分割模型 / Evaluate SOTA 3D segmentation models]\n        D --> D1[现有模型存在显著局限 / Existing models have significant limitations]\n        D --> D2[需要新技术与基准 / Need for new techniques & benchmarks]"
    },
    {
      "title": "Collaborative Low-Rank Adaptation for Pre-Trained Vision Transformers",
      "authors": "Zheng Liu, Jinchao Zhu, Gao Huang",
      "institution": "University of Science and Technology Beijing, Nankai University, Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.24603",
      "code": null,
      "tags": [
        "parameter-efficient fine-tuning",
        "low-rank adaptation",
        "vision transformers",
        "diversity enhancement",
        "parameter-efficient fine-tuning",
        "collaborative learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7b118cdde5f39c672a7a43fbf0a798c2d097cc700ad5018dfd61a3a755467de_w640_q70.webp",
      "contributions": "1. Proposes a novel Collaborative Low-Rank Adaptation (CLoRA) method that balances learning performance and parameter efficiency for fine-tuning Vision Transformers. 2. Introduces a base-space sharing mechanism that allows all low-rank modules to share projection spaces, expanding learning capacity while maintaining parameter efficiency. 3. Designs a Sample-Agnostic Diversity Enhancement (SADE) component to regularize similarities among low-rank matrices and encourage diverse representations during training.",
      "summary": "The paper addresses the trade-off between performance and parameter efficiency in fine-tuning pre-trained Vision Transformers. It proposes CLoRA, a method featuring base-space sharing and a diversity enhancement component to collaboratively construct low-rank modules. Experiments show CLoRA achieves a better balance between learning performance and parameter efficiency compared to state-of-the-art methods.",
      "mindmap": "graph TB\n        A[Collaborative Low-Rank Adaptation for Pre-Trained Vision Transformers] --> B(核心问题/Problem: 现有方法在性能与参数效率间难以平衡/Existing methods fail to balance performance and parameter efficiency)\n        A --> C(主要方法/Method: 提出CLoRA，包含基础空间共享与SADE/Propose CLoRA with base-space sharing and SADE)\n        A --> D(关键结果/Results: 在性能与效率间取得更好平衡，点云分析所需GFLOPs最少/Achieves better balance and requires fewest GFLOPs for point cloud analysis)"
    },
    {
      "title": "MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding",
      "authors": "Panquan Yang, Junfei Huang, Zongzhangbao Yin, Yingsong Hu, Anni Xu, Xinyi Luo, Xueqi Sun, Hai Wu, Sheng Ao, Zhaoxing Zhu, Chenglu Wen, Cheng Wang",
      "institution": "Xiamen University, Pengcheng Laboratory",
      "link": "https://arxiv.org/pdf/2512.24605",
      "code": null,
      "tags": [
        "3D visual grounding",
        "multi-modal dataset",
        "roadside infrastructure",
        "point cloud",
        "natural language understanding",
        "3D object localization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c61ac917fbc36f6e9ad317d668ea3e64b3a82f0dbd27c014c4abf4fd960de2e_w640_q70.webp",
      "contributions": "1. Introduces a novel task of 3D Visual Grounding for Outdoor Monitoring Scenarios from a roadside infrastructure perspective. 2. Constructs MoniRefer, the first real-world, large-scale multi-modal dataset for this task, featuring over 136k objects and 411k language expressions from complex traffic intersections. 3. Proposes a new end-to-end method, Moni3DVG, which leverages multi-modal features from images and point clouds for 3D object localization.",
      "summary": "This paper addresses the lack of data for 3D visual grounding in roadside monitoring scenarios by introducing a new task and constructing the large-scale MoniRefer dataset. It also proposes the Moni3DVG method, which fuses image and point cloud features for 3D object localization. Experiments show the proposed method is effective and superior on the new benchmark.",
      "mindmap": "graph TB\n        A[MoniRefer: A Real-world Large-scale Multi-modal Dataset based on Roadside Infrastructure for 3D Visual Grounding] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有3D视觉定位数据集和方法主要关注室内和驾驶场景，缺乏路边监控场景的配对数据/Existing 3D visual grounding focuses on indoor/driving scenes, lacking data for roadside monitoring.]\n        C --> C1[提出新任务：用于户外监控场景的3D视觉定位/Propose new task: 3D Visual Grounding for Outdoor Monitoring Scenarios.]\n        C --> C2[构建首个大规模真实世界多模态数据集MoniRefer/Build first large-scale real-world multi-modal dataset MoniRefer.]\n        C --> C3[提出端到端方法Moni3DVG，融合图像和点云特征/Propose end-to-end method Moni3DVG, fusing image and point cloud features.]\n        D --> D1[数据集包含约13.6万个物体和41.1万条语言描述/Dataset contains ~136k objects and 411k expressions.]\n        D --> D2[实验证明了所提方法的优越性和有效性/Experiments demonstrate the method's superiority and effectiveness.]"
    },
    {
      "title": "LLHA-Net: A Hierarchical Attention Network for Two-View Correspondence Learning",
      "authors": "Shuyuan Lin, Yu Guo, Xiao Chen, Yanjie Liang, Guobao Xiao, Feiran Huang",
      "institution": "Jinan University, Guangzhou City University of Technology, Peng Cheng Laboratory, Tongji University",
      "link": "https://arxiv.org/pdf/2512.24620",
      "code": "http://www.linshuyuan.com",
      "tags": [
        "feature matching",
        "hierarchical attention",
        "outlier removal",
        "camera pose estimation",
        "two-view correspondence",
        "stage fusion"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/815e060fd0161c2bcb2bed3957a4053a624617e006ffadacd94f49bdc4d97dfe_w640_q70.webp",
      "contributions": "1. A layer-by-layer channel fusion module that preserves semantic information from each network stage to enhance feature representation. 2. A hierarchical attention module that adaptively captures and fuses global and structural semantic information. 3. Two network architectures designed to extract and integrate features, improving the model's adaptability.",
      "summary": "This paper proposes LLHA-Net, a hierarchical attention network for two-view correspondence learning. It addresses the challenge of outliers in feature matching by using stage fusion and hierarchical attention to better capture semantic information. Experiments on YFCC100M and SUN3D datasets show it outperforms state-of-the-art methods in outlier removal and camera pose estimation.",
      "mindmap": "graph TB\n        A[LLHA-Net: A Hierarchical Attention Network for Two-View Correspondence Learning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[大量异常值影响匹配精度/Outliers degrade matching accuracy]\n        C --> C1[分层注意力网络/Hierarchical Attention Network]\n        C1 --> C2[逐层通道融合模块/Layer-by-Layer Channel Fusion]\n        C1 --> C3[分层注意力模块/Hierarchical Attention Module]\n        D --> D1[在YFCC100M和SUN3D上超越SOTA/Outperforms SOTA on YFCC100M & SUN3D]\n        D1 --> D2[异常值去除/Outlier Removal]\n        D1 --> D3[相机姿态估计/Camera Pose Estimation]"
    },
    {
      "title": "FireRescue: A UAV-Based Dataset and Enhanced YOLO Model for Object Detection in Fire Rescue Scenes",
      "authors": "Qingyu Xu, Runtong Zhang, Zihuan Qiu, Fanman Meng",
      "institution": "University of Electronic Science and Technology of China",
      "link": "https://arxiv.org/pdf/2512.24622",
      "code": null,
      "tags": [
        "object detection",
        "UAV",
        "attention module",
        "dynamic feature sampler",
        "YOLO",
        "dataset"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf6e55c31951249ccd5b18b3a05c08e7f08682d074fbcab089ea2b819535a76d_w640_q70.webp",
      "contributions": "1. Constructed the \"FireRescue\" dataset for UAV-based fire rescue scenes, covering urban, mountainous, forest, and water areas with 8 key categories. 2. Proposed the FRS-YOLO model, introducing a plug-and-play multidimensional collaborative enhancement attention module to reduce inter-class confusion. 3. Integrated a dynamic feature sampler into the model to strengthen foreground features and mitigate issues like smoke occlusion and small target miss-detection.",
      "summary": "This paper addresses the challenges of object detection in complex fire rescue scenes by first creating a comprehensive UAV-based dataset named FireRescue. It then proposes an enhanced YOLO model, FRS-YOLO, which incorporates a novel attention module and a dynamic feature sampler to improve detection accuracy, especially for confused categories and small targets. Experiments show the method effectively boosts the performance of YOLO models in this challenging domain.",
      "mindmap": "graph TB\n        Root(”FireRescue: UAV-Based Dataset and Enhanced YOLO Model”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”现有研究不足/Existing Limitations”)\n        P1 --> P1_1(”场景偏差: 缺乏复杂城市救援场景/Scenario Bias: Lack of complex urban scenes”)\n        P1 --> P1_2(”类别不匹配: 缺少关键指挥目标/Category Mismatch: Missing key command targets”)\n        Problem --> P2(”检测挑战: 类间混淆、小目标漏检/Detection Challenges: Inter-class confusion, small target miss”)\n        Method --> M1(”构建新数据集 'FireRescue'/Construct new dataset 'FireRescue'”)\n        M1 --> M1_1(”多场景: 城市、山地、森林、水域/Multi-scenario: Urban, mountainous, forest, water”)\n        M1 --> M1_2(”8个关键类别，15,980张图像/8 key categories, 15,980 images”)\n        Method --> M2(”提出改进模型 FRS-YOLO/Propose improved model FRS-YOLO”)\n        M2 --> M2_1(”引入多维协同增强注意力模块/Introduce multidimensional collaborative enhancement attention module”)\n        M2 --> M2_2(”集成动态特征采样器/Integrate dynamic feature sampler”)\n        Results --> R1(”实验证明火灾救援场景检测极具挑战性/Experiments show fire rescue detection is highly challenging”)\n        Results --> R2(”所提方法有效提升YOLO系列模型性能/The proposed method effectively improves YOLO model performance”)"
    },
    {
      "title": "From Sequential to Spatial: Reordering Autoregression for Efficient Visual Generation",
      "authors": "Siyang Wang, Hanting Li, Wei Li, Jie Hu, Xinghao Chen, Feng Zhao",
      "institution": "University of Science and Technology of China, Huawei Noah's Ark Lab",
      "link": "https://arxiv.org/pdf/2512.24639",
      "code": null,
      "tags": [
        "multi-modal inference",
        "autoregressive visual generation",
        "radial parallel prediction",
        "nested attention mechanism"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bffd5ae0463dd26d6d9beddadbbf4c0de1249205c2c3865172fb5172c4b22d2_w640_q70.webp",
      "contributions": "1. Proposed a radial parallel prediction framework (RadAR) that reorders the autoregressive generation process from sequential to spatial, grouping tokens into concentric rings for parallel prediction. 2. Introduced a nested attention mechanism to dynamically refine inconsistent predictions during the forward pass, mitigating error accumulation from parallel generation. 3. Designed a method that preserves the structural locality and spatial coherence of visual scenes while significantly improving inference efficiency and parallelizability.",
      "summary": "This paper addresses the low inference efficiency of traditional autoregressive models in visual generation due to their sequential token-by-token decoding. It proposes RadAR, a framework that organizes generation around a radial topology, enabling parallel prediction of tokens within the same spatial ring and using a nested attention mechanism to correct errors. The method significantly accelerates generation while maintaining representational capacity.",
      "mindmap": "graph TB\n        A[From Sequential to Spatial: Reordering Autoregression for Efficient Visual Generation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统自回归模型顺序解码导致推理效率低/Traditional autoregressive sequential decoding leads to low inference efficiency]\n        C --> C1[提出RadAR框架：径向并行预测/Propose RadAR framework: Radial Parallel Prediction]\n        C1 --> C2[将token按空间距离分组为同心环/Group tokens into concentric rings by spatial distance]\n        C1 --> C3[引入嵌套注意力机制进行动态修正/Introduce nested attention for dynamic correction]\n        D --> D1[保持视觉结构局部性和空间连贯性/Preserves structural locality and spatial coherence]\n        D --> D2[显著提高并行化和生成效率/Significantly improves parallelization and generation efficiency]"
    },
    {
      "title": "Renormalization Group Guided Tensor Network Structure Search",
      "authors": "Maolin Wang, Bowen Yu, Sheng Zhang, Linjie Mi, Wanyu Wang, Yiqi Wang, Pengyue Jia, Xuetao Wei, Zenglin Xu, Ruocheng Guo, Xiangyu Zhao",
      "institution": "City University of Hong Kong, National University of Defense Technology, Southern University of Science and Technology, Fudan University, Intuit AI",
      "link": "https://arxiv.org/pdf/2512.24663",
      "code": "https://github.com/Applied-Machine-Learning-Lab/RGTN",
      "tags": [
        "model compression (quantization/pruning)",
        "tensor network structure search",
        "renormalization group",
        "multi-scale optimization",
        "edge gates",
        "node tension"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9fdccc30b2aad8da00c1fdf132004ba3347f8992425dfb71406c3fa11d073c_w640_q70.webp",
      "contributions": "1. Proposes RGTN, a physics-inspired framework that uses multi-scale renormalization group flows for continuous tensor network structure evolution, overcoming the limitations of fixed-scale, discrete search methods. 2. Introduces learnable edge gates for dynamic topology modification and intelligent proposals based on physical quantities like node tension and edge information flow to guide the search. 3. Demonstrates state-of-the-art performance, achieving superior compression ratios and running 4-600 times faster than existing methods on tasks like light field data and video completion.",
      "summary": "This paper addresses the limitations of existing Tensor Network Structure Search (TN-SS) methods, which struggle with computational tractability and structure adaptivity. The authors propose RGTN, a novel framework guided by renormalization group theory, which enables multi-scale, continuous structure optimization. Experiments show RGTN achieves better compression and is significantly faster than prior methods.",
      "mindmap": "graph TB\n        Root[Renormalization Group Guided Tensor Network Structure Search] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有TN-SS方法局限/Limitations of existing TN-SS]\n        P1 --> P1_1[单尺度优化/Single-scale optimization]\n        P1 --> P1_2[离散搜索空间/Discrete search space]\n        P1 --> P1_3[分离的结构-参数优化/Separated structure-parameter optimization]\n        Method[主要方法/Method] --> M1[RGTN框架/RGTN Framework]\n        M1 --> M1_1[多尺度重整化群流/Multi-scale RG flows]\n        M1 --> M1_2[可学习边门/Learnable edge gates]\n        M1 --> M1_3[智能提案(节点张力, 边信息流)/Intelligent proposals (node tension, edge info flow)]\n        Results[关键结果/Results] --> R1[SOTA压缩比/State-of-the-art compression ratio]\n        Results --> R2[4-600倍加速/4-600x speedup]"
    },
    {
      "title": "Evolving, Not Training: Zero-Shot Reasoning Segmentation via Evolutionary Prompting",
      "authors": "Kai Ye, Xiaotong You, Jianghang Lin, Jiayi Ji, Pingyang Dai, Liujuan Cao",
      "institution": "Xiamen University, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.24702",
      "code": "https://github.com/AHideoKuzeA/Evol-SAM3",
      "tags": [
        "reasoning segmentation",
        "evolutionary prompting",
        "zero-shot learning",
        "visual arena",
        "semantic mutation",
        "heterogeneous arena"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b60e24e2f5e5668a6d7d977acfb32177365388575df74e72ccb5a8b3d4e7f7e_w640_q70.webp",
      "contributions": "1. Proposes EVOL-SAM3, a novel zero-shot framework that reformulates reasoning segmentation as an inference-time evolutionary search process. 2. Introduces a \"Generate-Evaluate-Evolve\" loop with a Visual Arena for reference-free fitness assessment and a Semantic Mutation operator for diversity and error correction. 3. Designs a Heterogeneous Arena module that integrates geometric priors with semantic reasoning for robust final selection.",
      "summary": "This paper addresses the limitations of static, training-free methods for reasoning segmentation by proposing EVOL-SAM3, a zero-shot framework that uses an evolutionary prompting strategy to iteratively refine prompt hypotheses at inference time. The method outperforms both static baselines and fully supervised state-of-the-art methods on the ReasonSeg benchmark without any training.",
      "mindmap": "graph TB\n        A[EVOL-SAM3: 零样本推理分割的进化提示 / EVOL-SAM3: Zero-Shot Reasoning Segmentation via Evolutionary Prompting] --> B\n        A --> C\n        A --> D\n        B[核心问题 / Problem] --> B1[静态推理范式 / Static Inference Paradigm]\n        B1 --> B2[推理深度不足 / Insufficient Reasoning Depth]\n        B1 --> B3[无法自我纠正 / Lack of Self-Correction]\n        C[主要方法 / Method] --> C1[进化搜索 / Evolutionary Search]\n        C1 --> C2[生成-评估-进化循环 / Generate-Evaluate-Evolve Loop]\n        C2 --> C3[视觉竞技场 / Visual Arena]\n        C2 --> C4[语义突变 / Semantic Mutation]\n        C2 --> C5[异构竞技场 / Heterogeneous Arena]\n        D[关键结果 / Results] --> D1[超越静态基线 / Outperforms Static Baselines]\n        D --> D2[超越全监督SOTA / Surpasses Fully Supervised SOTA]\n        D --> D3[零样本设置 / Zero-Shot Setting]"
    },
    {
      "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation",
      "authors": "Jibin Song, Mingi Kwon, Jaeseok Jeong, Youngjung Uh",
      "institution": "Yonsei University",
      "link": "https://arxiv.org/pdf/2512.24724",
      "code": "https://jibin86.github.io/flowblending_project_page",
      "tags": [
        "diffusion models",
        "video generation",
        "diffusion sampling",
        "model capacity",
        "velocity divergence",
        "inference acceleration"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ac4128c7bf14ee06860b55c2869f6a243b40a94d93faf3a63e6549b8c5d06f8_w640_q70.webp",
      "contributions": "1. The observation that model capacity impact varies across denoising timesteps, being crucial in early/late stages but negligible in intermediate stages., 2. The proposal of FlowBlending, a stage-aware multi-model sampling strategy that dynamically allocates large and small models to different stages., 3. The introduction of simple criteria and a velocity-divergence analysis to identify capacity-sensitive regions and choose stage boundaries.",
      "summary": "This paper addresses the high computational cost of video diffusion models by proposing FlowBlending, a stage-aware sampling strategy that uses a large model for critical early/late denoising stages and a small model for the intermediate stage. This method achieves up to 1.65x faster inference with significantly fewer FLOPs while preserving the output quality of the large model, and is compatible with other acceleration techniques.",
      "mindmap": "graph TB\n        Root(”FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation”) --> Problem(”核心问题/Problem: High computational cost of video diffusion models”)\n        Root --> Method(”主要方法/Method: Stage-aware multi-model sampling (large model for early/late stages, small model for intermediate stage)”)\n        Root --> Results(”关键结果/Results: Faster inference, fewer FLOPs, maintained visual fidelity”)"
    },
    {
      "title": "EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation",
      "authors": "Bingxuan Li, Yiming Cui, Yicheng He, Yiwei Wang, Shu Zhang, Longyin Wen, Yulei Niu",
      "institution": "ByteDance, University of Illinois Urbana-Champaign, University of California, Merced, University of California, Los Angeles",
      "link": "https://arxiv.org/pdf/2512.24731",
      "code": "https://echofoley.github.io/",
      "tags": [
        "multi-modal inference",
        "video-to-audio generation",
        "event-centric control",
        "hierarchical semantic control",
        "symbolic representation",
        "agentic generation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60e5e566b45c8f830a2ee00d47e98f66961d9c829c11fbf1ba96469f6be93ddc_w640_q70.webp",
      "contributions": "1. Introduces EchoFoley, a new task for video-grounded sound generation with fine-grained event-level and hierarchical semantic control. 2. Constructs EchoFoley-6k, a large-scale, expert-curated benchmark with detailed sounding event annotations. 3. Proposes EchoVidia, a sounding-event-centric agentic generation framework with a slow-fast thinking strategy.",
      "summary": "This paper addresses limitations in video-text-to-audio generation by proposing a new task, EchoFoley, for fine-grained, controllable sound effect synthesis. It introduces a symbolic representation for sounding events and a corresponding benchmark, and develops the EchoVidia framework, which demonstrates superior controllability and perceptual quality compared to existing models.",
      "mindmap": "graph TB\n        Root(”EchoFoley: Event-Centric Hierarchical Control for Video Grounded Creative Sound Generation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”视觉主导/Visual Dominance”)\n        Problem --> P2(”缺乏细粒度控制定义/Lack of Fine-grained Control Definition”)\n        Problem --> P3(”指令理解弱/Weak Instruction Following”)\n        Method --> M1(”新任务: EchoFoley/New Task: EchoFoley”)\n        Method --> M2(”符号化声音事件表示/Symbolic Sounding Event Representation”)\n        Method --> M3(”基准数据集: EchoFoley-6k/Benchmark: EchoFoley-6k”)\n        Method --> M4(”生成框架: EchoVidia/Generation Framework: EchoVidia”)\n        Results --> R1(”可控性提升 40.7%/Controllability +40.7%”)\n        Results --> R2(”感知质量提升 12.5%/Perceptual Quality +12.5%”)"
    },
    {
      "title": "Splatwizard: A Benchmark Toolkit for 3D Gaussian Splatting Compression",
      "authors": "Xiang Liu, Yimin Zhou, Jinxiang Wang, Yujun Huang, Shuzhao Xie, Shiyu Qin, Mingyao Hong, Jiawei Li, Yaowei Wang, Zhi Wang, Shu-Tao Xia, Bin Chen",
      "institution": "Tsinghua University, Harbin Institute of Technology, Shenzhen, Pengcheng Laboratory, Huawei Technologies Ltd.",
      "link": "https://arxiv.org/pdf/2512.24742",
      "code": "https://github.com/",
      "tags": [
        "model compression (quantization/pruning)",
        "3D Gaussian Splatting",
        "Benchmark",
        "Compression",
        "Rendering Speed",
        "Geometric Accuracy"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7da61f07627db84d20834d2e3342abb34432aaec56ccf5e79f5b1b3e3b00775_w640_q70.webp",
      "contributions": "1. Introduces Splatwizard, a unified benchmark toolkit specifically designed for evaluating 3D Gaussian Splatting (3DGS) compression models. 2. Provides an easy-to-use framework for implementing new compression models and utilizing state-of-the-art techniques. 3. Includes an integrated pipeline that automates the calculation of key performance indicators, including image quality, geometric accuracy (chamfer distance), rendering speed, and resource consumption.",
      "summary": "This paper introduces Splatwizard, a benchmark toolkit designed to address the lack of standardized evaluation tools for 3D Gaussian Splatting compression methods. It provides a unified framework to implement and automatically evaluate compression models on key metrics like rendering speed, quality, and geometric accuracy. The main conclusion is that Splatwizard fills a critical gap for consistent and comprehensive assessment in this rapidly growing field.",
      "mindmap": "graph TB\n        A[Splatwizard: A Benchmark Toolkit for 3D Gaussian Splatting Compression] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>缺乏标准化评估工具<br>Lack of Standardized Evaluation Tools]\n        C[主要方法/Method<br>统一基准测试工具包<br>Unified Benchmark Toolkit]\n        D[关键结果/Results<br>自动化评估流程<br>Automated Evaluation Pipeline]"
    },
    {
      "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
      "authors": "Karthik Dharmarajan, Wenlong Huang, Jiajun Wu, Li Fei-Fei, Ruohan Zhang",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.24766",
      "code": null,
      "tags": [
        "robotic manipulation",
        "3D object flow",
        "video generation",
        "zero-shot manipulation",
        "trajectory optimization",
        "reinforcement learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c22839be4198942eb08182abe0606d486a3126572afb757ce865cb1b3a787721_w640_q70.webp",
      "contributions": "1. Proposes Dream2Flow, a framework that bridges video generation and robotic control using 3D object flow as an intermediate representation. 2. Demonstrates the ability to reconstruct 3D object motions from generated videos and formulate manipulation as object trajectory tracking, overcoming the embodiment gap. 3. Shows that the method enables zero-shot guidance from pre-trained video models to manipulate diverse object categories (rigid, articulated, deformable, granular) without task-specific demonstrations.",
      "summary": "The paper introduces Dream2Flow, a framework that uses 3D object flow extracted from videos generated by off-the-shelf models as an interface for robotic manipulation. It translates these generated motions into executable robot actions via trajectory optimization or reinforcement learning, enabling zero-shot manipulation of diverse objects in open-world settings. The results demonstrate 3D object flow as a general and scalable bridge between video generation models and robotic control.",
      "mindmap": "graph TB\n        A[Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow] --> B(核心问题/Problem: Translating human-like motions from video models into low-level robot actions)\n        A --> C(主要方法/Method: Use 3D object flow as intermediate representation, reconstruct motions from videos, track trajectories)\n        A --> D(关键结果/Results: Enables zero-shot manipulation of diverse objects, bridges video generation to robot control)"
    },
    {
      "title": "UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning",
      "authors": "Ankit Dhiman, Srinath R, Jaswanth Reddy, Lokesh R Boregowda, Venkatesh Babu Radhakrishnan",
      "institution": "Indian Institute of Science, Bangalore; Samsung R&D Institute India - Bangalore",
      "link": "https://arxiv.org/pdf/2512.24763",
      "code": "https://github.com/val-iisc/UniC-Lift",
      "tags": [
        "3D instance segmentation",
        "3D Gaussian Splatting",
        "contrastive learning",
        "feature embedding",
        "embedding-to-label",
        "hard-mining"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/29fe9b6395a1fd545bcd589e312ef8c6e3ad1d451a08ccc87467409b090443b2_w640_q70.webp",
      "contributions": "1. A unified framework that merges feature learning and label decoding for 3D instance segmentation, reducing training time compared to two-stage approaches. 2. A novel \"Embedding-to-Label\" process to efficiently decode learnable feature embeddings into instance labels. 3. A stabilized hard-mining strategy using a linear layer on rasterized embeddings to improve performance at object boundaries.",
      "summary": "This paper addresses the challenge of inconsistent 2D instance labels across views for 3D instance segmentation. It proposes UniC-Lift, a unified framework that uses contrastive learning on learnable 3D Gaussian feature embeddings and a novel decoding process, enhanced by a stabilized hard-mining technique for object boundaries. The method outperforms baselines on multiple datasets.",
      "mindmap": "graph TB\n        Root[UniC-Lift: Unified 3D Instance Segmentation via Contrastive Learning] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Inconsistent 2D instance labels across views] --> P1[导致/Leads to poor 3D predictions]\n        Method[主要方法/Method: Unified framework] --> M1[引入/Introduces learnable feature embedding]\n        M1 --> M2[解码/Decodes via Embedding-to-Label]\n        Method --> M3[优化/Optimization: Stabilized hard-mining at boundaries]\n        Results[关键结果/Results] --> R1[性能提升/Outperforms baselines]\n        Results --> R2[数据集/Datasets: ScanNet, Replica3D, Messy-Rooms]"
    },
    {
      "title": "Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation",
      "authors": "Takeru Kusakabe, Yudai Hirose, Mashiho Mukaida, Satoshi Ono",
      "institution": "Kagoshima University",
      "link": "https://arxiv.org/pdf/2512.24792",
      "code": null,
      "tags": [
        "monocular depth estimation",
        "adversarial attack",
        "physics-in-the-loop optimization",
        "sep-CMA-ES"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5085ff109d1d570d16fe1fa964d0eb5cc890f0ebaed8dcfe0c3cdaa01d4f00bd_w640_q70.webp",
      "contributions": "1. Proposes a projection-based adversarial attack method for monocular depth estimation models, using projected light as the perturbation. 2. Employs physics-in-the-loop (PITL) optimization to design perturbations in real-world environments, accounting for device specifications and disturbances. 3. Utilizes a distributed covariance matrix adaptation evolution strategy (sep-CMA-ES) for effective black-box optimization to generate adversarial examples.",
      "summary": "This paper proposes a physical adversarial attack method for monocular depth estimation models. The method projects perturbation light onto a target object and uses physics-in-the-loop optimization with a distributed evolution strategy to create adversarial examples. Experiments confirmed the attack's success, causing depth misestimations that made parts of objects disappear from the scene.",
      "mindmap": "graph TB\n        Root[Projection-based Adversarial Attack using Physics-in-the-Loop Optimization for Monocular Depth Estimation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: DNN-based monocular depth estimation models are vulnerable to adversarial attacks, threatening reliability in applications like autonomous systems.]\n        Method[主要方法/Method: Proposes a projection-based attack using physics-in-the-loop optimization and sep-CMA-ES to generate adversarial light perturbations.]\n        Results[关键结果/Results: Successfully created adversarial examples causing depth misestimation, making parts of objects disappear from the scene.]"
    },
    {
      "title": "Nonlinear Noise2Noise for Efficient Monte Carlo Denoiser Training",
      "authors": "Andrew Tinits, Stephen Mann",
      "institution": "University of Waterloo",
      "link": "https://arxiv.org/pdf/2512.24794",
      "code": null,
      "tags": [
        "image denoising",
        "Noise2Noise",
        "Monte Carlo denoising",
        "high dynamic range",
        "tone mapping",
        "Jensen gap"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a7ef23248abb9edf960ef0ea8dac0c52ee12d9db03ab8dd602dfd8c83c62645_w640_q70.webp",
      "contributions": "1. Identified that certain nonlinear functions can be applied to noisy targets in Noise2Noise training without introducing significant bias. 2. Developed a theoretical framework to analyze the effects of nonlinearities and described a class of functions with minimal bias. 3. Demonstrated the method's effectiveness for training Monte Carlo denoisers on HDR images using only noisy data, achieving results comparable to models trained with clean references.",
      "summary": "This paper addresses the limitation of Noise2Noise training where applying nonlinear functions to noisy targets introduces bias. The authors propose a theoretical framework to identify low-bias nonlinearities and apply this to denoise high dynamic range Monte Carlo renderings using tone mapping. Their method, trained only on noisy data, achieves performance close to models trained with clean reference images.",
      "mindmap": "graph TB\n        Root(”Nonlinear Noise2Noise for Efficient Monte Carlo Denoiser Training”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”Noise2Noise训练中非线性函数导致偏差/Bias from nonlinearities in Noise2Noise”)\n        Problem --> P2(”HDR图像训练被异常值干扰/HDR training overwhelmed by outliers”)\n        Method --> M1(”理论分析非线性影响/Theoretical analysis of nonlinear effects”)\n        Method --> M2(”识别低偏差非线性函数类/Identify low-bias nonlinear function class”)\n        Method --> M3(”特定损失与色调映射组合/Specific loss & tone mapping combination”)\n        Results --> R1(”仅用噪声数据训练/Train with only noisy data”)\n        Results --> R2(”性能接近干净数据训练模型/Performance approaches clean-data-trained model”)"
    },
    {
      "title": "Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control",
      "authors": "Jason Armitage, Rico Sennnrich",
      "institution": "University of Zurich",
      "link": "https://arxiv.org/pdf/2512.24826",
      "code": null,
      "tags": [
        "multi-modal inference",
        "derivative-free optimisation",
        "regret minimisation",
        "multivariate mutual information",
        "in-scene camera control",
        "vision-language models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66c8b5c775488157feb95c4650ebd70cf6bfc3b2373d13cfde2db9f88f119e8d_w640_q70.webp",
      "contributions": "1. A new method that improves multivariate mutual information estimates using regret minimisation with derivative-free optimisation. 2. An algorithm enabling off-the-shelf 2D-trained cross-modal systems to adapt online to object occlusions and differentiate features in 3D scenes. 3. A pipeline that controls an in-scene camera to learn directly from noisy VLM outputs, improving performance on 3D multi-object scenes without pretraining or finetuning.",
      "summary": "This paper addresses the dimensional shift when 2D-trained vision-language models process 3D scenes. It proposes a method using derivative-free optimisation and regret minimisation to improve mutual information estimates and control an in-scene camera, allowing the system to adapt online and improve performance on cross-modal tasks for 3D multi-object scenes without additional training.",
      "mindmap": "graph TB\n        Root(”Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”2D系统处理3D场景的维度偏移/Dimensional shift for 2D systems on 3D scenes”)\n        Problem --> P2(”需要学习相机控制模块/Need to learn an in-scene camera control module”)\n        Method --> M1(”基于遗憾最小化的多元互信息估计/Multivariate mutual information estimates via regret minimisation”)\n        Method --> M2(”使用无导数优化/Using derivative-free optimisation”)\n        Results --> R1(”使现成的2D系统能在线适应3D场景/Enables off-the-shelf 2D systems to adapt online to 3D scenes”)\n        Results --> R2(”无需预训练或微调即可提升性能/Improves performance without pretraining or finetuning”)"
    },
    {
      "title": "CropTrack: A Tracking with Re-Identification Framework for Precision Agriculture",
      "authors": "Md Ahmed Al Muzaddid, Jordan A. James, William J. Beksi",
      "institution": "The University of Texas at Arlington",
      "link": "https://arxiv.org/pdf/2512.24838",
      "code": null,
      "tags": [
        "multiple-object tracking",
        "appearance-motion association",
        "re-identification",
        "exponential moving average"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9408f95352ab182fdf244f790c589fbdfab17f28b86d4a84534385d1d25eb2fd_w640_q70.webp",
      "contributions": "1. A reranking-enhanced appearance association module to improve feature matching. 2. A one-to-many association strategy with an appearance-based conflict resolution mechanism. 3. An exponential moving average prototype feature bank for robust appearance modeling.",
      "summary": "This paper proposes CropTrack, a novel multiple-object tracking framework for agricultural environments that combines appearance and motion information to address challenges like occlusions and similar object appearances. It introduces techniques like reranking-enhanced association and a prototype feature bank to improve identity preservation. The method outperforms traditional motion-based trackers and achieves state-of-the-art association accuracy on agricultural MOT datasets.",
      "mindmap": "graph TB\n        A[CropTrack: A Tracking with Re-Identification Framework for Precision Agriculture] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[农业MOT挑战: 遮挡, 外观相似/MOT Challenges: Occlusions, Similar Appearance]\n        C --> C1[结合外观与运动信息/Combine Appearance & Motion]\n        C --> C2[重排序增强关联/Reranking-enhanced Association]\n        C --> C3[指数移动平均特征库/Exponential Moving Average Feature Bank]\n        D --> D1[更优的身份保持/Better Identity Preservation]\n        D --> D2[更高的关联准确率/Higher Association Accuracy]"
    },
    {
      "title": "VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents",
      "authors": "Xunyi Zhao, Gengze Zhou, Qi Wu",
      "institution": "Adelaide University, Australian Institute of Machine Learning",
      "link": "https://arxiv.org/pdf/2512.24851",
      "code": null,
      "tags": [
        "embodied ai",
        "Vision-and-Language Navigation",
        "Multimodal Large Language Models",
        "evaluation framework",
        "spatial reasoning",
        "sequential decision-making"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15acf4c91a5dc52a1564da96c7602f92c4733b8ba50a6e941b2f51f2a00f6501_w640_q70.webp",
      "contributions": "1. Introduced VLN-MME, a unified and extensible evaluation framework for probing MLLMs as zero-shot agents in Vision-and-Language Navigation. 2. Provided a highly modular and accessible design that enables structured comparisons and component-level ablations across diverse MLLM architectures and agent designs. 3. Discovered that enhancing agents with Chain-of-Thought reasoning and self-reflection leads to performance degradation, revealing MLLMs' poor context awareness and low 3D spatial reasoning fidelity in embodied navigation tasks.",
      "summary": "This paper investigates the performance of Multimodal Large Language Models (MLLMs) as embodied agents for Vision-and-Language Navigation (VLN). It proposes a unified evaluation framework called VLN-MME to benchmark MLLMs in a zero-shot setting. The key finding is that, contrary to expectations, advanced reasoning techniques like Chain-of-Thought degrade navigation performance, indicating MLLMs have significant limitations in 3D spatial reasoning and sequential decision-making for embodied tasks.",
      "mindmap": "graph TB\n        A[VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[评估MLLMs作为具身智能体的性能/Evaluating MLLMs as Embodied Agents]\n        C --> C1[提出统一评估框架VLN-MME/Propose Unified Evaluation Framework VLN-MME]\n        D --> D1[CoT与反思导致性能下降/CoT & Self-Reflection Degrade Performance]\n        D --> D2[MLLMs空间推理能力弱/MLLMs Have Poor Spatial Reasoning]"
    },
    {
      "title": "OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation",
      "authors": "Meng Lan, Lefei Zhang, Xiaomeng Li",
      "institution": "The Hong Kong University of Science and Technology, Wuhan University",
      "link": "https://arxiv.org/pdf/2512.24861",
      "code": "https://github.com/xmed-lab/OFL-SAM2",
      "tags": [
        "medical image segmentation",
        "Segment Anything Model 2 (SAM2)",
        "few-shot learning",
        "prompt-free segmentation",
        "online learning",
        "adaptive fusion"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2aff9624bd2c875a87e76830cf04f7468fc6ad3ed0290f0a8189f5a76f92143d_w640_q70.webp",
      "contributions": "1. Proposes a prompt-free SAM2 framework (OFL-SAM2) that eliminates the need for manual prompts in medical image segmentation. 2. Introduces an online few-shot learner that trains a lightweight mapping network to generate target features from limited data and supports online parameter updates during inference. 3. Designs an adaptive fusion module that dynamically integrates the learned target features with the frozen SAM2's memory-attention features for robust segmentation.",
      "summary": "This paper addresses the challenge of adapting the Segment Anything Model 2 (SAM2) to medical image segmentation without requiring extensive annotated data or manual prompts. It proposes OFL-SAM2, a framework that uses an online few-shot learner to train a mapping network for generating target features and an adaptive module to fuse them with SAM2's features. Experiments on three datasets show that OFL-SAM2 achieves state-of-the-art performance with limited training data.",
      "mindmap": "graph TB\n        A[OFL-SAM2: Prompt SAM2 with Online Few-shot Learner] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Adapting SAM2 to medical images needs大量标注和手动提示/Requires extensive annotation and manual prompts]\n        C --> C1[在线小样本学习器训练轻量映射网络/Online few-shot learner trains lightweight mapping network]\n        C --> C2[自适应融合模块整合特征/Adaptive fusion module integrates features]\n        D --> D1[在三个数据集上实现SOTA/State-of-the-art on three datasets]\n        D --> D2[仅需有限训练数据/Requires only limited training data]"
    },
    {
      "title": "FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation",
      "authors": "Zichen Tang, Haihong E, Rongjin Li, Jiacheng Liu, Linwei Jia, Zhuodi Hao, Zhongjun Yang, Yuanze Li, Haolin Tian, Xinyi Hu, Peizhi Zhao, Yuan Liu, Zhengyu Wang, Xianghe Wang, Yiling Huang, Xueyuan Lin, Ruofei Bai, Zijian Xie, Qian Huang, Ruining Cao, Haocheng Gao",
      "institution": "Beijing University of Posts and Telecommunications, Hithink RoyalFlush Information Network Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.24903",
      "code": "https://bupt-reasoning-lab.github.io/FinMMDocR",
      "tags": [
        "multimodal reasoning",
        "multimodal large language models",
        "financial numerical reasoning",
        "retrieval-augmented generation",
        "benchmark",
        "scenario awareness"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a504876f4f69142bef32716f3ea6e7e385402763fd546c94f689c9b33389ebe8_w640_q70.webp",
      "contributions": "1. Introduces scenario awareness with 12 types of implicit financial scenarios integrated into 57.9% of problems. 2. Provides extensive document understanding with 837 bilingual documents averaging 50.8 pages across 9 types. 3. Requires complex multi-step computation averaging 11 reasoning steps, with 65% of problems needing cross-page evidence.",
      "summary": "This paper introduces FinMMDocR, a bilingual multimodal benchmark designed to evaluate MLLMs on complex, real-world financial numerical reasoning. It challenges models with scenario-aware problems, long documents, and multi-step computations. The best-performing model achieved only 58.0% accuracy, highlighting the difficulty of the benchmark and its potential to drive improvements in MLLMs and reasoning methods.",
      "mindmap": "graph TB\n        A[FinMMDocR] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[评估MLLMs在真实世界金融数值推理上的表现/Evaluating MLLMs on Real-World Financial Numerical Reasoning]\n        C --> C1[构建包含场景感知、文档理解、多步计算的基准/Building a Benchmark with Scenario Awareness, Document Understanding, Multi-Step Computation]\n        D --> D1[最佳模型准确率仅为58.0%/Best Model Accuracy is Only 58.0%]\n        D --> D2[RAG方法表现差异显著/Significant Performance Variations in RAG Methods]"
    },
    {
      "title": "Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection",
      "authors": "Bartłomiej Olber, Jakub Winter, Paweł Wawrzyński, Andrii Gamalii, Daniel Górniak, Marcin Łojek, Robert Nowak, Krystian Radlak",
      "institution": "Warsaw University of Technology, IDEAS NCBR",
      "link": "https://arxiv.org/pdf/2512.24922",
      "code": null,
      "tags": [
        "3D object detection",
        "domain adaptation",
        "LiDAR",
        "neuron activation patterns",
        "continual learning",
        "annotation budget"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9018c4d77628a42d8b5e9e448c8a20879fa60110552085be08d3517a806dbe44_w640_q70.webp",
      "contributions": "1. A novel LiDAR domain adaptation method based on neuron activation patterns for selecting a small, diverse subset of target domain samples for annotation., 2. A combined approach that uses a minimal annotation budget and incorporates post-training techniques inspired by continual learning to prevent model weight drift., 3. Empirical demonstration that the proposed method outperforms linear probing and state-of-the-art domain adaptation techniques.",
      "summary": "This paper addresses the poor cross-domain generalization of 3D object detectors for autonomous vehicles by proposing a novel LiDAR domain adaptation method. The method selects a small, diverse, and representative subset of target domain samples for annotation based on neuron activation patterns and combines this with continual learning-inspired techniques to prevent weight drift. The approach achieves state-of-the-art performance with a very low annotation cost.",
      "mindmap": "graph TB\n        A[Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[3D目标检测器跨域泛化能力差/3D object detectors generalize poorly across domains]\n        C --> C1[基于神经元激活模式选择样本/Select samples via neuron activation patterns]\n        C --> C2[标注少量多样化的目标域样本/Annotate small, diverse target subset]\n        C --> C3[结合持续学习防止权重漂移/Combine with continual learning to prevent weight drift]\n        D --> D1[以极低标注成本实现SOTA性能/Achieves SOTA with very low annotation cost]\n        D --> D2[优于线性探测和现有域适应方法/Outperforms linear probing & SOTA DA methods]"
    },
    {
      "title": "CPJ: Explainable Agricultural Pest Diagnosis via Caption-Prompt-Judge with LLM-Judged Refinement",
      "authors": "Wentao Zhang, Tao Fang, Lina Lu, Lifei Wang, Weihe Zhong",
      "institution": "Shandong University of Technology, Macau Millennium College",
      "link": "https://arxiv.org/pdf/2512.24947",
      "code": "https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis",
      "tags": [
        "vision-language models",
        "LLM-as-a-Judge",
        "training-free",
        "few-shot",
        "explainable AI",
        "agricultural VQA"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/677e58cca4c831f09b054974c399f7092a40a64b068915b259e624f2728ef082_w640_q70.webp",
      "contributions": "1. Proposes a novel Caption-Prompt-Judge (CPJ) framework for training-free, few-shot agricultural pest diagnosis. 2. Introduces an LLM-as-Judge module for iterative refinement of structured, multi-angle image captions to enhance interpretability. 3. Demonstrates significant performance improvements on a benchmark dataset using lightweight models, advancing robust and explainable diagnosis without fine-tuning.",
      "summary": "This paper proposes CPJ, a training-free framework that uses large vision-language models to generate and refine image captions, which are then used to improve agricultural visual question answering. The method significantly boosts disease classification and QA performance on the CDDMBench dataset, providing transparent, evidence-based reasoning without requiring model fine-tuning.",
      "mindmap": "graph TB\n        A[CPJ: Explainable Agricultural Pest Diagnosis] --> B[核心问题/Problem: Existing methods rely on costly fine-tuning and lack explainability under domain shifts.]\n        A --> C[主要方法/Method: CPJ framework uses LVLMs to generate captions, refines them via LLM-as-Judge, and performs caption-informed VQA.]\n        A --> D[关键结果/Results: Significant performance gains (+22.7 pp in classification, +19.5 in QA score) on CDDMBench.]"
    },
    {
      "title": "ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT",
      "authors": "Xinran Gong, Gorkem Durak, Halil Ertugrul Aktas, Vedat Cicek, Jinkui Hao, Ulas Bagci, Nilay S. Shah, Bo Zhou",
      "institution": "Northwestern University, Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.24948",
      "code": null,
      "tags": [
        "medical image analysis",
        "diffusion models",
        "motion correction",
        "synthetic data",
        "coronary artery calcium",
        "non-gated CT"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd53987de700ad7024fad5ea5d53da57ecb375707cda1bff594c7264ec4ca118_w640_q70.webp",
      "contributions": "1. A CAC motion simulation data engine that synthesizes realistic non-gated CT acquisitions from gated CTs for supervised training without paired data. 2. A property-aware learning strategy that incorporates calcium-specific priors via a differentiable consistency loss to preserve lesion integrity. 3. A progressive correction scheme that gradually reduces motion artifacts across diffusion steps to enhance stability and calcium fidelity.",
      "summary": "The paper proposes ProDM, a diffusion model framework to correct motion artifacts in coronary calcium lesions from non-gated chest CT scans. The method uses a synthetic data engine for training, incorporates calcium-specific priors, and applies progressive correction. Experiments show it improves scoring accuracy, lesion fidelity, and risk stratification compared to baselines.",
      "mindmap": "graph TB\n        A[ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[非门控CT中冠脉钙化运动伪影影响评分/Non-gated CT CAC scoring affected by motion artifacts]\n        C --> C1[合成数据引擎/Synthetic Data Engine]\n        C --> C2[属性感知学习/Property-aware Learning]\n        C --> C3[渐进式校正/Progressive Correction]\n        D --> D1[提高评分准确性和病灶保真度/Improved scoring accuracy & lesion fidelity]\n        D --> D2[提升风险分层性能/Enhanced risk stratification]\n        D --> D3[抑制伪影，提升临床可用性/Suppresses artifacts & improves clinical usability]"
    },
    {
      "title": "VIPER: Process-aware Evaluation for Generative Video Reasoning",
      "authors": "Yifan Li, Yukai Gu, Yingqian Min, Zikang Liu, Yifan Du, Kun Zhou, Min Yang, Wayne Xin Zhao, Minghui Qiu",
      "institution": "Renmin University of China, ByteDance",
      "link": "https://arxiv.org/pdf/2512.24952",
      "code": null,
      "tags": [
        "video generation and reasoning",
        "Chain-of-Frames reasoning",
        "Process-outcome Consistency",
        "VLM-as-Judge"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5284fdefc82fba6a147f2ca20a0d1efbfeec49032fc77145fce8a9fc02faf645_w640_q70.webp",
      "contributions": "1. Proposes a process-aware evaluation paradigm for Generative Video Reasoning (GVR) to address outcome-hacking. 2. Introduces the VIPER benchmark spanning 16 tasks across six reasoning domains. 3. Proposes the Process-outcome Consistency (POC@r) metric, which uses a VLM-as-Judge with a hierarchical rubric to evaluate both intermediate steps and final results.",
      "summary": "This paper identifies a flaw in evaluating generative video reasoning, where models can get the right answer via a wrong process (outcome-hacking). To address this, the authors propose VIPER, a new benchmark with a process-aware evaluation metric called POC@r. Their experiments show state-of-the-art video models perform poorly (≈20% POC@1.0), revealing a significant gap between current video generation and true visual reasoning.",
      "mindmap": "graph TB\n        A[VIPER: Process-aware Evaluation for Generative Video Reasoning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有评估导致结果作弊/Existing evaluation leads to outcome-hacking]\n        C --> C1[提出过程感知评估范式/Propose process-aware evaluation paradigm]\n        C --> C2[引入VIPER基准/Introduce VIPER benchmark]\n        C --> C3[提出POC@r指标/Propose POC@r metric]\n        D --> D1[SOTA模型POC@1.0仅约20%/SOTA models achieve only ~20% POC@1.0]\n        D --> D2[存在显著结果作弊/Exhibit significant outcome-hacking]"
    },
    {
      "title": "HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films",
      "authors": "Rongji Xun, Junjie Yuan, Zhongjie Wang",
      "institution": "Tongji University, Shanghai Film Restoration Laboratory",
      "link": "https://arxiv.org/pdf/2512.24946",
      "code": "https://anonymous.4open.science/r/HaineiFRDM",
      "tags": [
        "video restoration",
        "diffusion model",
        "film restoration",
        "high-resolution video",
        "patch-wise training",
        "global-local frequency module"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49ddd01a0523d32c1f20822a4a1d270659f5454212011a90e04a39fe796d1ab_w640_q70.webp",
      "contributions": "1. Proposed HaineiFRDM, a film restoration framework leveraging diffusion models for content understanding to restore indistinguishable film defects. 2. Introduced a patch-wise training/testing strategy with a position-aware Global Prompt and Frame Fusion Module and a global-local frequency module to enable high-resolution restoration on a single 24GB GPU and ensure texture consistency. 3. Constructed a new film restoration dataset containing restored real-degraded films and realistic synthetic data.",
      "summary": "This paper proposes HaineiFRDM, a diffusion model-based framework for restoring high-resolution, real-world films. It addresses limitations of existing open-source methods by using a patch-wise strategy and novel modules to handle high-resolution videos efficiently and introduces a new dataset. Experiments show the model outperforms existing open-source methods in defect restoration.",
      "mindmap": "graph TB\n        A[HaineiFRDM: Explore Diffusion to Restore High-resolution Real-World Films] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[开源方法性能有限/Open-source methods have limited performance]\n        B --> B2[高分辨率电影未探索/High-resolution films unexplored]\n        C --> C1[基于扩散模型的修复框架/Diffusion-based restoration framework]\n        C --> C2[分块训练与测试策略/Patch-wise training & testing]\n        C --> C3[全局-局部频率模块/Global-local frequency module]\n        C --> C4[构建新数据集/Construct new dataset]\n        D --> D1[缺陷修复能力优越/Superior defect restoration ability]\n        D --> D2[代码与数据集将开源/Code & dataset to be released]"
    },
    {
      "title": "ShowUI-$π$: Flow-based Generative Models as GUI Dexterous Hands",
      "authors": "Siyuan Hu, Kevin Qinghong Lin, Mike Zheng Shou",
      "institution": "Show Lab, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.24965",
      "code": "https://github.com/showlab/showui-pi",
      "tags": [
        "human-computer interaction",
        "flow-based generative model",
        "GUI automation",
        "continuous trajectory prediction",
        "unified discrete-continuous actions",
        "ScreenDrag benchmark"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd6391f609bd9b67bc717f5e3756501bf8f4dedd5a207352ff5b4f02bc902207_w640_q70.webp",
      "contributions": "1. Proposed ShowUI-π, the first flow-based generative model for GUI dexterous manipulation, unifying discrete clicks and continuous drags in a shared model. 2. Introduced a flow-based action generation method for drag modeling, predicting incremental cursor adjustments from continuous visual observations. 3. Created ScreenDrag, a benchmark with 20K drag trajectories across five domains and comprehensive evaluation protocols to assess GUI agents' drag capabilities.",
      "summary": "This paper addresses the limitation of existing GUI agents that only perform discrete clicks, lacking the ability for continuous, closed-loop drag interactions. The authors propose ShowUI-π, a flow-based generative model that unifies discrete and continuous actions and generates smooth drag trajectories from visual observations. Experiments show ShowUI-π outperforms proprietary GUI agents on the new ScreenDrag benchmark, demonstrating effective dexterous control for GUI automation.",
      "mindmap": "graph TB\n    A[ShowUI-π: Flow-based Generative Models as GUI Dexterous Hands] --> B[核心问题/Problem: Existing GUI agents only support discrete clicks, lacking continuous drag capability for closed-loop trajectories]\n    A --> C[主要方法/Method: Flow-based generative model with unified discrete-continuous actions and incremental trajectory prediction]\n    A --> D[关键结果/Results: Outperforms proprietary agents on ScreenDrag benchmark (score 26.98), demonstrating effective dexterous control]"
    },
    {
      "title": "Evaluating the Impact of Compression Techniques on the Robustness of CNNs under Natural Corruptions",
      "authors": "Itallo Patrick Castro Alves Da Silva, Emanuel Adler Medeiros Pereira, Erick de Andrade Barboza, Baldoino Fonseca dos Santos Neto, Marcio de Medeiros Ribeiro",
      "institution": "Federal University of Alagoas, Federal University of Rio Grande do Norte",
      "link": "https://arxiv.org/pdf/2512.24971",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "quantization",
        "pruning",
        "weight clustering",
        "robustness",
        "multiobjective assessment"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8640f8930863d5573a7df0bb7287b8800d92c67a60523b6b0dab2eb044c58bb_w640_q70.webp",
      "contributions": "1. Conducted a comprehensive evaluation of individual and combined compression techniques (quantization, pruning, weight clustering) on CNNs for robustness under natural corruptions. 2. Demonstrated that certain compression strategies can preserve or even improve model robustness, especially on complex architectures. 3. Utilized multiobjective assessment to identify optimal compression configurations that balance robustness, accuracy, and compression ratio for real-world deployment.",
      "summary": "This paper evaluates how compression techniques like quantization, pruning, and weight clustering affect the robustness of CNNs (ResNet-50, VGG-19, MobileNetV2) against natural image corruptions on CIFAR-10-C/100-C. It finds that specific compression strategies, particularly when combined, can maintain or enhance robustness, with multiobjective analysis revealing the best trade-offs for efficient and robust model deployment.",
      "mindmap": "graph TB\n        A[Evaluating the Impact of Compression Techniques on CNNs under Natural Corruptions] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[压缩模型在自然损坏下的鲁棒性/Robustness of compressed models under natural corruptions]\n        C --> C1[评估量化、剪枝、权重聚类技术/Evaluate quantization, pruning, weight clustering]\n        C --> C2[使用CIFAR-10/100-C数据集/Use CIFAR-10/100-C datasets]\n        C --> C3[多目标评估/Multiobjective assessment]\n        D --> D1[特定策略保持或提升鲁棒性/Certain strategies preserve or improve robustness]\n        D --> D2[定制组合产生有益结果/Customized combinations yield beneficial results]"
    },
    {
      "title": "DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments",
      "authors": "Yohan Park, Hyunwoo Ha, Wonjun Jo, Tae-Hyun Oh",
      "institution": "Korea Advanced Institute of Science and Technology (KAIST), Pohang University of Science and Technology (POSTECH)",
      "link": "https://arxiv.org/pdf/2512.24985",
      "code": null,
      "tags": [
        "embodied vision-language reasoning",
        "low-light vision",
        "embodied question answering",
        "vision-language models",
        "image enhancement",
        "benchmark"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f144c0ff2baabb069f605975462919cef76b3f54919a8c9db67dab0432973003_w640_q70.webp",
      "contributions": "1. Introduces DarkEQA, the first benchmark for evaluating Embodied Question Answering (EQA) under multi-level, physics-based low-light conditions. 2. Features a physically faithful degradation pipeline that models illumination drop and sensor noise in linear RAW space, followed by an ISP-inspired renderer. 3. Systematically evaluates and reveals the limitations of state-of-the-art VLMs and the effectiveness of Low-Light Image Enhancement (LLIE) models as pre-processors in this challenging scenario.",
      "summary": "This paper identifies a gap in evaluating Vision-Language Models (VLMs) for embodied agents under low-light conditions and proposes DarkEQA, a new benchmark that simulates realistic dark environments. The benchmark uses a physics-based image degradation model to test VLM robustness and the utility of image enhancement techniques. The evaluation reveals significant performance drops in VLMs under low-light, highlighting a critical area for improvement in robust embodied AI.",
      "mindmap": "graph TB\n        A[DarkEQA: Benchmarking VLMs for EQA in Low-Light] --> B[核心问题/Problem: Existing EQA benchmarks overlook low-light conditions, a necessity for 24/7 robot operation.]\n        A --> C[主要方法/Method: Proposes DarkEQA benchmark with physics-based low-light simulation in RAW space and ISP pipeline.]\n        A --> D[关键结果/Results: Evaluates VLMs & LLIE models, systematically revealing VLM limitations under low-light.]"
    },
    {
      "title": "Bi-C2R: Bidirectional Continual Compatible Representation for Re-indexing Free Lifelong Person Re-identification",
      "authors": "Zhenyu Cui, Jiahuan Zhou, Yuxin Peng",
      "institution": "Peking University",
      "link": "https://arxiv.org/pdf/2512.25000",
      "code": "https://github.com/PKU-ICST-MIPL/Bi-C2R-TPAMI2026",
      "tags": [
        "person re-identification",
        "lifelong learning",
        "compatible representation",
        "re-indexing free",
        "continual learning",
        "feature distillation"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4b768cca9f9f0fa320a67a2646aa080f3367ff2175466f98a339c2aee634a4bd_w640_q70.webp",
      "contributions": "1. Introduces a new, more challenging task called Re-indexing Free Lifelong Person Re-identification (RFL-ReID) that eliminates the need for costly and privacy-sensitive re-indexing of historical gallery images. 2. Proposes the Bi-C2R framework, which includes a bidirectional compatible transfer network to update old gallery features to the new feature space and distillation modules to balance compatibility and prevent forgetting. 3. Designs a feature-level exponential moving average strategy to adaptively handle knowledge gaps across different data domains.",
      "summary": "This paper tackles the problem of lifelong person re-identification without re-indexing historical gallery images, a task termed RFL-ReID. To solve it, the authors propose the Bi-C2R framework, which updates old gallery features for compatibility with new models and uses distillation techniques to balance new and old knowledge. Experiments show the method achieves leading performance on both the new RFL-ReID task and the traditional lifelong Re-ID task.",
      "mindmap": "graph TB\n        A[Bi-C2R: Bidirectional Continual Compatible Representation] --> B[核心问题/Problem: 终身Re-ID中重索引成本高、有隐私问题 / Re-indexing in L-ReID is costly and has privacy issues]\n        A --> C[主要方法/Method: 提出Bi-C2R框架，双向兼容特征更新与蒸馏 / Propose Bi-C2R framework for bidirectional compatible feature update and distillation]\n        A --> D[关键结果/Results: 在新RFL-ReID和传统L-ReID任务上取得领先性能 / Achieves leading performance on both new RFL-ReID and traditional L-ReID tasks]"
    },
    {
      "title": "PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes",
      "authors": "Luca Collorone, Mert Kiray, Indro Spinelli, Fabio Galasso, Benjamin Busam",
      "institution": "Sapienza University of Rome, Technical University of Munich, Munich Center for Machine Learning (MCML)",
      "link": "https://arxiv.org/pdf/2512.24986",
      "code": null,
      "tags": [
        "3D scene understanding and animation",
        "3D Gaussian Splatting",
        "physics simulation",
        "large language model",
        "real-time animation",
        "open-vocabulary"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1113cf9009e210565f13d799d298867f98470a28066f31b961bce3b6577044fe_w640_q70.webp",
      "contributions": "1. Introduces the first framework to directly couple 3D Gaussian Splatting with a physics simulator, bypassing slow mesh extraction. 2. Enables open-vocabulary, real-time, and interactive 4D animation through an LLM that generates executable code to modify scene parameters. 3. Presents a train-free and computationally lightweight pipeline, shifting animation workflows from offline rendering to interactive dialogue.",
      "summary": "The paper presents PhysTalk, a system that generates real-time, physics-based 4D animations from text prompts and 3D Gaussian Splatting scenes. It uses a large language model to produce code that manipulates scene proxies for physics simulation, enabling interactive and realistic motion without requiring model training. This approach makes physically realistic animation more accessible and interactive.",
      "mindmap": "graph TB\n        A[PhysTalk: Language-driven Real-time Physics in 3D Gaussian Scenes] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法缺乏物理真实性和实时交互性/Current methods lack physical realism and real-time interaction]\n        C --> C1[使用LLM将文本提示转化为可执行代码/Uses LLM to translate prompts into executable code]\n        C --> C2[通过轻量级代理直接修改3DGS参数/Modifies 3DGS parameters via lightweight proxies]\n        C --> C3[集成物理模拟器进行碰撞感知动画/Integrates physics simulator for collision-aware animation]\n        D --> D1[实现实时、开集词汇的4D动画/Achieves real-time, open-vocabulary 4D animation]\n        D --> D2[无需训练，计算轻量/Train-free and computationally lightweight]\n        D --> D3[将工作流转向交互式对话/Shifts workflow to interactive dialogue]"
    },
    {
      "title": "FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM",
      "authors": "Yuchen Wu, Jiahe Li, Fabio Tosi, Matteo Poggi, Jin Zheng, Xiao Bai",
      "institution": "Beihang University, University of Bologna",
      "link": "https://arxiv.org/pdf/2512.25008",
      "code": null,
      "tags": [
        "visual SLAM",
        "monocular dense SLAM",
        "depth foundation models",
        "bundle adjustment",
        "optical flow",
        "geometric consistency"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f726314fa4bdd1cd04071665eebafc2932edc767a54b92772f39f3f9d16942af_w640_q70.webp",
      "contributions": "1. A Hybrid Flow Network that integrates geometric guidance from depth foundation models to produce geometry-aware correspondences. 2. A Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints for global consistency. 3. A Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions.",
      "summary": "The paper proposes FoundationSLAM, a learning-based monocular dense SLAM system that addresses the lack of geometric consistency in flow-based approaches by leveraging depth foundation models to guide correspondence estimation and multi-view optimization. The method introduces a hybrid flow network, a bi-consistent bundle adjustment layer, and a reliability-aware refinement mechanism. Experiments show it achieves superior trajectory accuracy and reconstruction quality in real-time, demonstrating strong generalization.",
      "mindmap": "graph TB\n        A[FoundationSLAM] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[缺乏几何一致性<br>Lack of Geometric Consistency]\n        C --> C1[混合光流网络<br>Hybrid Flow Network]\n        C --> C2[双向一致BA层<br>Bi-Consistent Bundle Adjustment]\n        C --> C3[可靠性感知细化<br>Reliability-Aware Refinement]\n        D --> D1[高精度轨迹与重建<br>High Trajectory & Reconstruction Accuracy]\n        D --> D2[实时性能(18 FPS)<br>Real-time Performance (18 FPS)]\n        D --> D3[强泛化能力<br>Strong Generalization]"
    },
    {
      "title": "Generative Classifiers Avoid Shortcut Solutions",
      "authors": "Alexander C. Li, Ananya Kumar, Deepak Pathak",
      "institution": "Carnegie Mellon University, Stanford University",
      "link": "https://arxiv.org/pdf/2512.25034",
      "code": "https://github.com/alexlioralexli/generative-classifiers",
      "tags": [
        "generative models",
        "generative classifiers",
        "spurious correlations",
        "distribution shift",
        "diffusion models",
        "autoregressive models"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f744eff83ad768a9dc5e431ef5b2d98baefe24c12d01fc980aa2fa92c3c21c65_w640_q70.webp",
      "contributions": "1. Demonstrates that generative classifiers (using class-conditional generative models) inherently avoid shortcut learning by modeling all features, not just spurious ones. 2. Shows that generative classifiers achieve state-of-the-art performance on multiple image and text distribution shift benchmarks without specialized techniques. 3. Provides a theoretical analysis in a Gaussian toy setting to explain the inductive biases and data conditions favoring generative classifiers.",
      "summary": "The paper addresses the problem of discriminative classifiers learning spurious shortcuts that fail under distribution shift. It proposes using generative classifiers, which model p(x|y), and finds they avoid shortcuts and achieve state-of-the-art robustness on standard benchmarks without needing specialized training tricks. The main conclusion is that generative classifiers offer a simple and effective alternative for building more robust models.",
      "mindmap": "graph TB\n        A[Generative Classifiers Avoid Shortcut Solutions] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Discriminative models learn spurious shortcuts<br>判别模型学习虚假捷径]\n        C --> C1[Use class-conditional generative models<br>使用类条件生成模型]\n        C --> C2[Model p(x|y) instead of p(y|x)<br>建模 p(x|y) 而非 p(y|x)]\n        D --> D1[Avoid shortcuts & SOTA on distribution shift<br>避免捷径并在分布偏移上达到SOTA]\n        D --> D2[Simple training, no specialized techniques<br>训练简单，无需专门技术]"
    },
    {
      "title": "FineTec: Fine-Grained Action Recognition Under Temporal Corruption via Skeleton Decomposition and Sequence Completion",
      "authors": "Dian Shao, Mingfei Shi, Like Liu",
      "institution": "- **link:** https://arxiv.org/pdf/2512.25067",
      "link": "https://arxiv.org/pdf/2512.25067",
      "code": null,
      "tags": [],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5283ff5d47b7344e785800154886fe627c149576029db00dc6b0fd29ebf4b9fb_w640_q70.webp",
      "contributions": "",
      "summary": "",
      "mindmap": ""
    },
    {
      "title": "From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing",
      "authors": "Xu He, Haoxian Zhang, Hejia Chen, Changyuan Zheng, Liyang Chen, Songlin Tang, Jiehui Huang, Xiaoqiang Liu, Pengfei Wan, Zhiyong Wu",
      "institution": "Tsinghua University, Kuaishou Technology (Kling Team), Beihang University, Hong Kong University of Science and Technology, The Chinese University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.25066",
      "code": "https://hjrphoebus.github.io/X-Dub/",
      "tags": [
        "audio-visual synthesis",
        "visual dubbing",
        "diffusion transformer",
        "self-bootstrapping",
        "video-to-video editing",
        "lip synchronization"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d6232f4b29adb66dc490f9b2d29652266a1c5fe1f49e7b7502c3833d4ad4fbb_w640_q70.webp",
      "contributions": "1. A novel self-bootstrapping framework that reframes visual dubbing from a mask-inpainting task into a well-conditioned video-to-video editing problem. 2. A timestep-adaptive multi-phase learning strategy to disentangle conflicting editing objectives across diffusion timesteps for stable training. 3. The introduction of ContextDubBench, a comprehensive benchmark dataset for robust evaluation in diverse and challenging scenarios.",
      "summary": "This paper addresses the challenge of audio-driven visual dubbing, where ideal paired training data is lacking. It proposes a self-bootstrapping framework that first uses a Diffusion Transformer to generate ideal, lip-altered companion videos, and then trains an audio-driven editor on these aligned pairs. This approach enables precise lip synchronization, faithful identity preservation, and robustness in complex, real-world scenarios.",
      "mindmap": "graph TB\n        Root[”From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing”]\n        Root --> Problem[”核心问题/Problem: Lack of ideal training data (aligned video pairs) for visual dubbing”]\n        Root --> Method[”主要方法/Method: Self-bootstrapping framework using DiT as data generator and audio-driven editor”]\n        Root --> Results[”关键结果/Results: Accurate lip sync, identity preservation, and robustness in challenging scenarios”]"
    },
    {
      "title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction",
      "authors": "Yi-Chuan Huang, Hao-Jen Chien, Chin-Yang Lin, Ying-Huan Chen, Yu-Lun Liu",
      "institution": "National Yang Ming Chiao Tung University",
      "link": "https://arxiv.org/pdf/2512.25073",
      "code": null,
      "tags": [
        "3D reconstruction",
        "sparse-view reconstruction",
        "multi-view outpainting",
        "3D Gaussian Splatting",
        "diffusion models",
        "geometry-aware denoising"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cb876042c4f6f417db4c360c60e273183cfc5b6039f35f552b59209074fc463e_w640_q70.webp",
      "contributions": "1. Proposes a novel sparse-view 3D reconstruction framework (GaMO) that reformulates the problem as multi-view outpainting to expand the field of view from existing poses, preserving geometric consistency. 2. Introduces a zero-shot, geometry-aware denoising strategy within a multi-view diffusion model, eliminating the need for training. 3. Achieves state-of-the-art reconstruction quality and a significant speedup (25x faster) compared to prior diffusion-based methods.",
      "summary": "The paper addresses the challenge of low-quality 3D reconstruction from sparse input views. It proposes GaMO, a method that uses multi-view diffusion outpainting to widen the field of view of existing images, which are then used to refine a 3D Gaussian Splatting model. This approach improves geometric consistency and visual quality while being significantly faster than previous state-of-the-art methods.",
      "mindmap": "graph TB\n        Root(”GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction”) --> Problem(”核心问题/Problem: Sparse-view 3D reconstruction struggles with limited coverage, geometric inconsistency, and high computation”)\n        Root --> Method(”主要方法/Method: Multi-view outpainting to expand FOV from known poses, using zero-shot geometry-aware diffusion”)\n        Root --> Results(”关键结果/Results: SOTA quality on benchmarks, 25x speedup, improved geometric consistency”)"
    },
    {
      "title": "Edit3r: Instant 3D Scene Editing from Sparse Unposed Images",
      "authors": "Jiageng Liu, Weijie Lyu, Xueting Li, Yejie Guo, Ming-Hsuan Yang",
      "institution": "University of Massachusetts Amherst, University of California, Merced, NVIDIA, Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.25071",
      "code": "https://edit3r.github.io/edit3r/",
      "tags": [
        "3D scene editing",
        "feed-forward 3D editing",
        "SAM2-based recoloring",
        "asymmetric input strategy",
        "DL3DV-Edit-Bench"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e25767650653d1a03ee1a7fcb676ec0786928604bf424428075decbe81db98f_w640_q70.webp",
      "contributions": "1. Proposes Edit3r, a feed-forward framework for instant 3D scene editing from sparse, unposed, and view-inconsistent 2D edited images, eliminating the need for per-scene optimization or pose estimation. 2. Introduces a novel training strategy using a SAM2-based recoloring method to generate cross-view-consistent supervision and an asymmetric input strategy to fuse disparate observations, addressing the lack of multi-view consistent training data. 3. Establishes DL3DV-Edit-Bench, a new benchmark for large-scale quantitative evaluation of 3D scene editing methods.",
      "summary": "This paper introduces Edit3r, a fast, feed-forward model that directly reconstructs and edits 3D scenes from sparse, unposed, and view-inconsistent 2D edited images. To overcome the lack of multi-view consistent training data, it uses a SAM2-based recoloring strategy for supervision and an asymmetric input strategy. The method achieves superior semantic alignment and 3D consistency compared to baselines at high inference speed (~0.5s), enabling real-time applications.",
      "mindmap": "graph TB\n        A[Edit3r: Instant 3D Scene Editing] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[从无位姿、视图不一致的2D编辑图像进行3D编辑/3D editing from unposed, view-inconsistent 2D edits]\n        B --> B2[缺乏多视图一致的训练数据/Lack of multi-view consistent training data]\n        C --> C1[前馈框架/Feed-forward framework]\n        C --> C2[SAM2重着色监督/SAM2-based recoloring supervision]\n        C --> C3[非对称输入策略/Asymmetric input strategy]\n        D --> D1[语义对齐与3D一致性更好/Better semantic alignment & 3D consistency]\n        D --> D2[推理速度快(~0.5秒)/Fast inference (~0.5s)]\n        D --> D3[新评测基准: DL3DV-Edit-Bench/New benchmark: DL3DV-Edit-Bench]"
    },
    {
      "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
      "authors": "Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang, Joan Lasenby, Chun-Hao Huang",
      "institution": "University of Cambridge, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.25075",
      "code": "https://github.com/zheninghuang/Space-Time-Pilot",
      "tags": [
        "video generation",
        "video diffusion model",
        "space-time disentanglement",
        "temporal-warping training",
        "camera-conditioning",
        "generative rendering"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82552564f2c6cc5799df28c30493304ecd3600d22b5bcd81578cb3aaf6f15150_w640_q70.webp",
      "contributions": "1. Introduced an animation time-embedding mechanism for explicit motion sequence control in video diffusion. 2. Proposed a temporal-warping training scheme to repurpose multi-view datasets for learning temporal variations. 3. Created the CamxTime synthetic dataset and an improved camera-conditioning mechanism for precise dual space-time control.",
      "summary": "This paper introduces SpaceTimePilot, a video diffusion model that independently controls camera viewpoint and motion sequence to re-render dynamic scenes from a monocular video. The method uses a novel time-embedding mechanism and a temporal-warping training strategy to achieve robust space-time disentanglement. Experiments show the model enables continuous exploration across space and time, outperforming prior work.",
      "mindmap": "graph TB\n        A[SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time] --> B[核心问题/Problem: 如何从单目视频中解耦空间和时间以进行可控生成渲染/How to disentangle space and time from a monocular video for controllable generative rendering]\n        A --> C[主要方法/Method: 引入动画时间嵌入机制和时域扭曲训练方案/Introduce animation time-embedding and temporal-warping training scheme]\n        A --> D[关键结果/Results: 实现鲁棒的时空解耦与连续可控渲染/Achieve robust space-time disentanglement and continuous controllable rendering]"
    },
    {
      "title": "q3-MuPa: Quick, Quiet, Quantitative Multi-Parametric MRI using Physics-Informed Diffusion Models",
      "authors": "Shishuai Wang, Florian Wiesinger, Noemi Sgambelluri, Carolin Pirkl, Stefan Klein, Juan A. Hernandez-Tamames, Dirk H.J. Poot",
      "institution": "Erasmus MC (Erasmus University Medical Center)",
      "link": "https://arxiv.org/pdf/2512.23726",
      "code": null,
      "tags": [
        "medical imaging",
        "diffusion models",
        "quantitative MRI",
        "data consistency",
        "physics-informed",
        "multi-parametric mapping"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e081389f251cbbacaf7c704cd1a34c3a032b2173e63192833db976abd6541d5e_w640_q70.webp",
      "contributions": "1. Proposes a diffusion model-based method (q3-MuPa) for quantitative MRI mapping that combines a deep generative model with a physics-based data consistency constraint., 2. Enables high-quality mapping from a fourfold-accelerated, nearly silent MRI scan (MuPa-ZTE), reducing acquisition time to ~1 minute., 3. Demonstrates successful training on synthetic data from digital phantoms alone, with strong generalization to real patient and phantom scans.",
      "summary": "This paper proposes q3-MuPa, a method that uses a physics-informed diffusion model to generate high-quality quantitative MRI maps (T1, T2, proton density) from accelerated, silent scans. The method integrates a denoising diffusion model with the MRI signal physics as a constraint during inference. It achieves accurate mapping from 1-minute scans and generalizes well to real data despite being trained only on synthetic phantoms.",
      "mindmap": "graph TB\n        A[q3-MuPa: Quick, Quiet, Quantitative Multi-Parametric MRI] --> B(核心问题/Problem: Need for fast, quiet, and accurate quantitative MRI mapping)\n        A --> C(主要方法/Method: Physics-informed diffusion model with data consistency)\n        A --> D(关键结果/Results: High-accuracy maps from 1-min scans, trained on synthetic data)"
    },
    {
      "title": "Leveraging Machine Learning for Early Detection of Lung Diseases",
      "authors": "Bahareh Rahmani, Harsha Reddy Bindela, Rama Kanth Reddy Gosula, Krishna Yedubati, Mohammad Amir Salari, Leslie Hinyard, Payam Norouzzadeh, Eli Snir, Martin Schoen",
      "institution": "Saint Louis University, Washington University at Saint Louis",
      "link": "https://arxiv.org/pdf/2512.23757",
      "code": null,
      "tags": [
        "medical image analysis",
        "deep learning",
        "convolutional neural networks",
        "chest x-ray",
        "disease classification",
        "VGG16"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d5eaabf315a0348dff119282fff830baf854bc7edaf57b6601b94745f8aa312_w640_q70.webp",
      "contributions": "1. Proposes a diagnostic framework combining traditional image processing with advanced neural networks for lung disease detection. 2. Trains and validates multiple deep learning models (CNNs, VGG16, InceptionV3, EfficientNetB0) on chest x-rays for COVID-19, lung cancer, and pneumonia. 3. Demonstrates high accuracy, precision, recall, and F1 scores, highlighting the potential for real-world, non-invasive diagnostic applications in resource-limited settings.",
      "summary": "This paper proposes using deep learning models, including CNNs, VGG16, InceptionV3, and EfficientNetB0, to diagnose lung diseases like COVID-19, lung cancer, and pneumonia from chest x-rays. The method combines traditional image processing with neural networks to create a rapid, non-invasive diagnostic tool. The study concludes that these models achieve high performance metrics, showing reliability and potential for real-world healthcare applications, especially where radiologists are scarce.",
      "mindmap": "graph TB\n        A[Leveraging Machine Learning for Early Detection of Lung Diseases<br>利用机器学习进行肺部疾病早期检测] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Limited access to radiologists & resources<br>放射科医生和资源有限]\n        B --> B2[Need for rapid, non-invasive diagnosis<br>需要快速、无创诊断]\n        C --> C1[Combine image processing & neural networks<br>结合图像处理和神经网络]\n        C --> C2[Train models (CNN, VGG16, etc.) on chest X-rays<br>在胸部X光片上训练模型]\n        D --> D1[High accuracy, precision, recall, F1 scores<br>高准确率、精确率、召回率、F1分数]\n        D --> D2[Potential for real-world diagnostic applications<br>具有实际诊断应用潜力]"
    },
    {
      "title": "A multimodal Transformer for InSAR-based ground deformation forecasting with cross-site generalization across Europe",
      "authors": "Wendong Yao, Binhua Huang, Soumyabrata Dev",
      "institution": "ADAPT SFI Research Centre, University College Dublin",
      "link": "https://arxiv.org/pdf/2512.23906",
      "code": null,
      "tags": [
        "remote sensing image analysis",
        "InSAR",
        "Transformer",
        "ground deformation forecasting",
        "cross-site generalization",
        "multimodal learning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/36024dc5598b92b146557714c9e66eeb494b793bdcfaacb98b73cf6725cc8bfa_w640_q70.webp",
      "contributions": "1. Proposed a novel multimodal patch-based Transformer architecture for InSAR-based ground deformation nowcasting, integrating displacement snapshots with static kinematic indicators and temporal encodings. 2. Demonstrated superior performance of the proposed model over baseline models (CNN-LSTM, STGCN) on a test tile in eastern Ireland, achieving high accuracy (RMSE=0.90mm, R²=0.97). 3. Showcased strong cross-site generalization by training on one tile and applying the model without fine-tuning to five unseen European tiles, maintaining high performance (R²≥0.93) across diverse deformation patterns.",
      "summary": "This paper addresses the challenge of forecasting ground deformation from InSAR time series data. It proposes a multimodal Transformer model that combines recent displacement maps with kinematic indicators and temporal features to predict the next displacement epoch. The model achieves high accuracy and demonstrates strong generalization across different geographic sites in Europe without requiring retraining.",
      "mindmap": "graph TB\n        Root[论文标题: A multimodal Transformer for InSAR-based ground deformation forecasting] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: 如何利用历史InSAR数据预测未来的地表形变?] --> P1[挑战/Challenges: 长期趋势、季节周期、突变事件的叠加]\n        Method[主要方法/Method: 多模态Transformer] --> M1[输入/Inputs: 近期形变图、静态运动学指标、时间编码]\n        Method --> M2[任务/Task: 单步、固定间隔的下一时期临近预报]\n        Results[关键结果/Results] --> R1[性能/Performance: RMSE=0.90mm, R²=0.97 (爱尔兰测试集)]\n        Results --> R2[泛化/Generalization: 跨欧洲5个未见区域，R²≥0.93]"
    },
    {
      "title": "One-Shot Structured Pruning of Quantum Neural Networks via $q$-Group Engineering and Quantum Geometric Metrics",
      "authors": "Haijian Shao, Wei Liu, Xing Deng, Yingtao Jiang",
      "institution": "Jiangsu University of Science and Technology, University of Nevada, Las Vegas",
      "link": "https://arxiv.org/pdf/2512.24019",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "quantum neural networks",
        "structured pruning",
        "q-deformed groups",
        "quantum geometry",
        "noise calibration"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cacc21f5549082cf6a65797fca25540bc063dd1ba9c51a7d5be4a807cd4d1b35_w640_q70.webp",
      "contributions": "1. A one-shot structured pruning framework (q-iPrune) for QNNs based on q-deformed group algebra and task-conditioned quantum geometry., 2. Rigorous theoretical guarantees including completeness of pruning, bounded functional equivalence, and computational feasibility., 3. Introduction of a noise-calibrated deformation parameter to adapt pruning decisions to hardware imperfections.",
      "summary": "This paper proposes q-iPrune, a one-shot structured pruning method for Quantum Neural Networks (QNNs) that uses q-deformed group theory and quantum geometric metrics to identify and remove redundant gates while provably bounding task performance degradation. The method provides theoretical guarantees and adapts to hardware noise. Experiments show it achieves significant gate reduction with controlled performance loss.",
      "mindmap": "graph TB\n        Root(”One-Shot Structured Pruning of QNNs via q-Group Engineering and Quantum Geometric Metrics”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”QNNs suffer from gate-level redundancy / QNN存在门级冗余”)\n        Problem --> P2(”Hinders deployment on NISQ devices / 阻碍在NISQ设备上的部署”)\n        Method --> M1(”Proposes q-iPrune framework / 提出q-iPrune框架”)\n        Method --> M2(”Uses q-deformed groups & task-conditioned quantum geometry / 使用q变形群和任务条件量子几何”)\n        Method --> M3(”Defines q-overlap distance for gate comparison / 定义q重叠距离进行门比较”)\n        Results --> R1(”Theoretical guarantees: completeness, bounded error, feasibility / 理论保证: 完备性、有界误差、可行性”)\n        Results --> R2(”Substantial gate reduction with bounded performance degradation / 显著减少门数量且性能下降有界”)\n        Results --> R3(”Noise adaptation via parameter λ / 通过参数λ适应噪声”)"
    },
    {
      "title": "Targeted Semantic Segmentation of Himalayan Glacial Lakes Using Time-Series SAR: Towards Automated GLOF Early Warning",
      "authors": "Pawan Adhikari, Satish Raj Regmi, Hari Ram Shrestha",
      "institution": "Space Research Center, Nepal Academy of Science and Technology (NAST)",
      "link": "https://arxiv.org/pdf/2512.24117",
      "code": null,
      "tags": [
        "semantic segmentation",
        "U-Net",
        "EfficientNet-B3",
        "temporal-first training",
        "Sentinel-1 SAR",
        "Dockerized pipeline"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6518e1c9ef6798da83807e183229f6799ac7cd2d2484a373f3a792675390241f_w640_q70.webp",
      "contributions": "1. A \"temporal-first\" training strategy for targeted semantic segmentation of glacial lakes using time-series SAR data. 2. An end-to-end automated deep learning pipeline achieving high IoU (0.9130) on a curated dataset of four high-risk Himalayan lakes. 3. A proposed operational engineering architecture featuring a Dockerized pipeline with automated data ingestion and a RESTful API, enabling dynamic early warning.",
      "summary": "This paper addresses the challenge of monitoring high-risk Himalayan glacial lakes for GLOF early warning by proposing an automated deep learning pipeline using time-series Sentinel-1 SAR imagery. The method employs a U-Net with an EfficientNet-B3 backbone trained with a novel \"temporal-first\" strategy on a curated dataset of four lakes. The model achieves high segmentation accuracy (IoU 0.9130), and the proposed Dockerized system architecture provides a scalable foundation for transitioning from static mapping to dynamic, automated early warning.",
      "mindmap": "graph TB\n        A[Targeted Semantic Segmentation of Himalayan Glacial Lakes Using Time-Series SAR] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>GLOF灾害监测/GLOF Hazard Monitoring<br>云层遮挡/Cloud Occlusion<br>通用模型局限/General Model Limitations]\n        C[主要方法/Method<br>时间优先训练/Temporal-First Training<br>U-Net + EfficientNet-B3<br>哨兵1号SAR/ Sentinel-1 SAR<br>Docker化管道/Dockerized Pipeline]\n        D[关键结果/Results<br>高IoU得分/High IoU (0.9130)<br>自动化预警系统/Automated Early Warning System<br>可扩展架构/Scalable Architecture]"
    },
    {
      "title": "Automated Classification of First-Trimester Fetal Heart Views Using Ultrasound-Specific Self-Supervised Learning",
      "authors": "Youssef Megahed, Aylin Erman, Robin Ducharme, Mark C. Walker, Steven Hawken, Adrian D. C. Chan",
      "institution": "Carleton University, University of Ottawa, Ottawa Hospital Research Institute",
      "link": "https://arxiv.org/pdf/2512.24492",
      "code": null,
      "tags": [
        "medical image classification",
        "self-supervised learning",
        "masked autoencoder",
        "vision transformer",
        "ultrasound foundation model",
        "fetal echocardiography"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f73d9d90b381ff2a6f244e15b64b3e24d9b28e8c2fbe78fa50c79ef8f58d1c66_w640_q70.webp",
      "contributions": "1. Evaluated a self-supervised ultrasound foundation model (USF-MAE) for the challenging task of first-trimester fetal heart view classification. 2. Demonstrated that ultrasound-specific pretraining on a large, unlabeled dataset yields more transferable representations than models pretrained on natural images (ImageNet) or standard supervised CNNs. 3. Showed robust performance without the need for aggressive image preprocessing or region-of-interest cropping, and improved discrimination of non-diagnostic frames.",
      "summary": "This paper addresses the challenge of automated first-trimester fetal heart view classification in ultrasound by fine-tuning a self-supervised ultrasound foundation model (USF-MAE). The model, pretrained on over 370,000 unlabeled ultrasound images, outperformed supervised CNN baselines and a natural image-pretrained Vision Transformer, achieving over 90% accuracy. The results indicate that ultrasound-specific self-supervised pretraining enables more generalizable representations for early fetal cardiac imaging.",
      "mindmap": "graph TB\n        A[Automated Classification of First-Trimester Fetal Heart Views<br>早孕期胎儿心脏视图自动分类] --> B\n        A --> C\n        A --> D\n        B[Problem: Early detection of congenital heart disease is challenging<br>核心问题: 先天性心脏病早期检测困难]\n        C[Method: Fine-tune USF-MAE, an ultrasound-specific self-supervised model<br>主要方法: 微调超声专用自监督模型USF-MAE]\n        D[Results: Achieved SOTA 90.57% accuracy, outperforming baselines<br>关键结果: 达到90.57%准确率，超越基线模型]"
    },
    {
      "title": "Towards autonomous time-calibration of large quantum-dot devices: Detection, real-time feedback, and noise spectroscopy",
      "authors": "Anantha S. Rao, Barnaby van Straaten, Valentin John, Cécile X. Yu, Stefan D. Oosterhout, Lucas Stehouwer, Giordano Scappucci, M. D. Stewart Jr., Menno Veldhorst, Francesco Borsoi, Justyna P. Zwolak",
      "institution": "University of Maryland, Delft University of Technology, National Institute of Standards and Technology",
      "link": "https://arxiv.org/pdf/2512.24894",
      "code": null,
      "tags": [
        "quantum computing",
        "quantum-dot",
        "charge stability diagrams",
        "noise spectroscopy",
        "autonomous calibration",
        "electrostatic drift"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd18e88d286e5eeb4d0c678ec6f483639d082a566d746717bc187c721ae3b41a_w640_q70.webp",
      "contributions": "1. Introduces a method using repeated charge stability diagrams as a multidimensional probe to autonomously detect and compensate for electrostatic drift in quantum-dot devices. 2. Demonstrates robust stabilization and real-time diagnostic access to dot-specific noise processes on a 10-quantum-dot device. 3. Enables time-domain noise spectroscopy to extract noise power spectral densities, identify two-level fluctuators, and analyze spatial noise correlations across the array.",
      "summary": "This paper addresses the problem of electrostatic drift in large quantum-dot arrays by proposing an autonomous calibration method that tracks charge transitions in stability diagrams to provide real-time feedback and compensation. The approach also enables detailed noise spectroscopy, revealing the noise characteristics and spatial correlations within the device. This forms a scalable foundation for maintaining stable, high-fidelity operations in quantum processors.",
      "mindmap": "graph TB\n        A[Towards autonomous time-calibration of large quantum-dot devices<br>大型量子点器件自主时间校准] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Electrostatic drift & noise limit scalability<br>静电漂移和噪声限制可扩展性]\n        C --> C1[Track transitions in CSDs for feedback<br>通过CSD中的跃迁线进行反馈跟踪]\n        D --> D1[Stable operation on 10-QD device<br>在10-Q点器件上实现稳定操作]\n        D --> D2[Noise spectroscopy & correlation analysis<br>噪声谱与相关性分析]"
    },
    {
      "title": "SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware",
      "authors": "Mahadev Sunil Kumar, Arnab Raha, Debayan Das, Gopakumar G, Amitava Mukherjee",
      "institution": "Accenture PLC, Intel Corporation, Indian Institute of Science, Amrita Vishwa Vidyapeetham, Birla Institute of Technology and Science",
      "link": "https://arxiv.org/pdf/2512.22136",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "Structured Pruning",
        "Multi-Objective Optimization",
        "Edge Inference",
        "MVCNN",
        "View-Adaptive Compression"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7687c3e58bfa2b22573745dffb608fb7c36a0c339dd9216829f78a284f51e662_w640_q70.webp",
      "contributions": "1. Proposes a framework for lightweight DNN deployment that integrates structured pruning with multi-objective optimization to meet heterogeneous hardware constraints. 2. Demonstrates the framework on MVCNN by quantifying the contribution of individual views to accuracy for view-adaptive pruning budget allocation. 3. Shows experimentally that the compressed models meet user-specified accuracy and memory bounds while achieving 1.2x to 5.0x inference speedup across diverse hardware.",
      "summary": "The paper addresses the challenge of deploying large DNNs on resource-constrained edge devices. It proposes SlimEdge, a method that combines structured pruning and multi-objective optimization to compress models like MVCNN while preserving task performance. The results show that this approach successfully meets specified accuracy and memory constraints while significantly reducing inference latency on various edge hardware platforms.",
      "mindmap": "graph TB\n        A[SlimEdge: Lightweight Distributed DNN Deployment] --> B(核心问题/Problem: DNN部署在资源受限的边缘设备上/DNN deployment on resource-constrained edge devices)\n        A --> C(主要方法/Method: 结构化剪枝与多目标优化/Structured Pruning & Multi-Objective Optimization)\n        A --> D(关键结果/Results: 满足精度与内存约束，推理延迟降低1.2x-5.0x/Meets accuracy & memory bounds, 1.2x-5.0x latency reduction)"
    },
    {
      "title": "SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models",
      "authors": "Jiesong Lian, Ruizhe Zhong, Zixiang Zhou, Xiaoyue Mi, Yixue Hao, Yuan Zhou, Qinglin Lu, Long Hu, Junchi Yan",
      "institution": "Huazhong University of Science and Technology, Shanghai Jiao Tong University, Tencent Hunyuan, University of Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.22170",
      "code": null,
      "tags": [
        "reinforcement learning from human feedback (rlhf)",
        "reward model",
        "video generation",
        "reward hacking",
        "bradley-terry loss",
        "hierarchical attention"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04ea37599d144feb85d3d1e8303d5d59adfabb579e172f06aef976d3783ee4ef_w640_q70.webp",
      "contributions": "1. A data collection and pairing strategy using single-item binary annotations and cross-prompt pairing to reduce labeling noise. 2. A Hierarchical Progressive Query Attention mechanism for better feature aggregation in the reward model architecture. 3. A modified Bradley-Terry loss function that accommodates win-tie scenarios to regularize the score distribution and mitigate reward hacking.",
      "summary": "This paper proposes SoliReward, a systematic framework to improve reward models for video generation alignment. It addresses data noise and reward hacking through a new data annotation strategy, a hierarchical attention architecture, and a modified loss function. The approach shows improved performance on benchmarks for video quality and enhances the effectiveness of post-training video models.",
      "mindmap": "graph TB\n        A[SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[数据标注噪声/Annotation Noise]\n        B --> B2[奖励黑客攻击/Reward Hacking]\n        B --> B3[模型架构设计不足/Under-explored RM Architecture]\n        C --> C1[单项目二元标注与跨提示配对/Single-item Binary Annotations & Cross-prompt Pairing]\n        C --> C2[分层渐进查询注意力/Hierarchical Progressive Query Attention]\n        C --> C3[改进的BT损失函数/Modified BT Loss for Win-Tie]\n        D --> D1[奖励模型评估指标提升/Improved Direct RM Evaluation Metrics]\n        D --> D2[视频生成后训练效果增强/Enhanced Efficacy of Post-training on Video Generation Models]"
    },
    {
      "title": "Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment",
      "authors": "Dawnena Key",
      "institution": "University of Denver",
      "link": "https://arxiv.org/pdf/2512.22177",
      "code": null,
      "tags": [
        "sign language recognition",
        "3D CNN",
        "LSTM",
        "real-time processing",
        "spatial-temporal features",
        "edge deployment"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a6733a99335eeae8c4efc856acc6d6c875e74bc5925841089b5198403d1d3b0_w640_q70.webp",
      "contributions": "1. A hybrid 3D CNN-LSTM architecture for capturing spatial and temporal features in ASL signs. 2. A training methodology using multiple complementary datasets (WLASL, ASL-LEX, expert-annotated signs). 3. A deployment architecture supporting both cloud (AWS) and edge (OAK-D camera) inference.",
      "summary": "This paper presents a real-time American Sign Language recognition system that uses a hybrid 3D CNN and LSTM architecture to process video streams. The system, trained on multiple datasets, achieves high F1-scores and is deployed for practical use on both cloud and edge devices.",
      "mindmap": "graph TB\n        A[Real-Time ASL Recognition Using 3D CNN and LSTM] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[沟通障碍/Communication Barrier]\n        B --> B2[实时翻译挑战/Real-time Translation Challenge]\n        C --> C1[混合架构/Hybrid Architecture]\n        C1 --> C1_1[3D CNN: 时空特征/Spatial-temporal Features]\n        C1 --> C1_2[LSTM: 序列依赖/Sequential Dependencies]\n        C --> C2[多数据集训练/Multi-dataset Training]\n        C --> C3[云边部署/Cloud-Edge Deployment]\n        D --> D1[高F1分数/High F1-scores (0.71-0.99)]\n        D --> D2[实时推理/Real-time Inference]"
    },
    {
      "title": "Characterizing Motion Encoding in Video Diffusion Timesteps",
      "authors": "Vatsal Baherwani, Yixuan Ren, Abhinav Shrivastava",
      "institution": "University of Maryland",
      "link": "https://arxiv.org/pdf/2512.22175",
      "code": null,
      "tags": [
        "video generation",
        "video diffusion models",
        "timestep analysis",
        "motion-appearance disentanglement",
        "motion transfer",
        "one-shot customization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ff2b9cdf8da1e9fbc9ae04ff2d085e89388ee26b1689338abbb853b17970bed_w640_q70.webp",
      "contributions": "1. Proposes a quantitative proxy and conducts a large-scale study to systematically characterize how motion is encoded across the denoising timesteps of video diffusion models, identifying distinct motion-dominant and appearance-dominant regimes. 2. Derives an operational motion-appearance boundary in timestep space, turning a widely used empirical heuristic into a spatiotemporal disentanglement principle. 3. Simplifies the one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives.",
      "summary": "This paper investigates how motion is encoded across the denoising timesteps of text-to-video diffusion models. By quantifying the trade-off between appearance editing and motion preservation when injecting new conditions, the authors identify early timesteps as motion-dominant and later ones as appearance-dominant. This characterization enables a simplified, effective method for one-shot motion transfer without extra modules.",
      "mindmap": "graph TB\n        A[Characterizing Motion Encoding in Video Diffusion Timesteps] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[视频扩散模型中运动编码机制不明确 / Motion encoding in video diffusion is poorly understood]\n        C --> C1[通过条件注入量化运动-外观权衡 / Quantify motion-appearance trade-off via conditional injection]\n        C --> C2[大规模定量研究 / Large-scale quantitative study]\n        D --> D1[识别早期运动主导与后期外观主导阶段 / Identify early motion-dominant and late appearance-dominant regimes]\n        D --> D2[简化单样本运动定制范式 / Simplify one-shot motion customization paradigm]"
    },
    {
      "title": "Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery",
      "authors": "Hassan Khalid, Muhammad Mahad Khaliq, Muhammad Jawad Bashir",
      "institution": "National University of Science and Technology (NUST)",
      "link": "https://arxiv.org/pdf/2512.22182",
      "code": null,
      "tags": [
        "dimensionality reduction",
        "Locally Linear Embedding (LLE)",
        "AI-enhanced LLE",
        "medical data analysis",
        "medical billing",
        "transcription"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d42fb5873195bf570514a88549054269194cfaa817a772eb3decd5c337e47e24_w640_q70.webp",
      "contributions": "1. Proposes an innovative integration of AI with Locally Linear Embedding (LLE) to handle high-dimensional medical data. 2. Develops a comprehensive mathematical model for the AI-enhanced LLE technique. 3. Demonstrates the model's application in real-world healthcare scenarios, showing significant improvements in data processing accuracy and operational efficiency for medical billing and transcription.",
      "summary": "This paper introduces an AI-enhanced Locally Linear Embedding (LLE) model to improve the analysis of high-dimensional medical data. The method is applied to automate and enhance medical billing and transcription services. The results show significant improvements in processing accuracy and operational efficiency.",
      "mindmap": "graph TB\n    A[Enhancing Medical Data Analysis through AI-Enhanced LLE] --> B(核心问题/Problem: Handling complex high-dimensional medical data for billing and transcription)\n    A --> C(主要方法/Method: Integrating AI with Locally Linear Embedding (LLE))\n    A --> D(关键结果/Results: Improved data processing accuracy and operational efficiency)"
    },
    {
      "title": "Unbiased Visual Reasoning with Controlled Visual Inputs",
      "authors": "Zhaonan Li, Shijie Lu, Fei Wang, Jacob Dineen, Xiao Ye, Zhikun Xu, Siyi Liu, Young Min Cho, Bangzheng Li, Daniel Chang, Kenny Nguyen, Qizheng Yang, Muhao Chen, Ben Zhou",
      "institution": "Arizona State University, University of Southern California, University of Pennsylvania, University of California, Davis",
      "link": "https://arxiv.org/pdf/2512.22183",
      "code": null,
      "tags": [
        "multimodal reasoning",
        "vision-language models",
        "spurious correlations",
        "information bottleneck",
        "reinforcement learning",
        "modular reasoning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8f546535b9c36b7873d0f685328a4f4a8e058e6a4788639c170f69e073d8f9e_w640_q70.webp",
      "contributions": "1. Proposes VISTA, a modular framework that decouples visual perception from reasoning using an explicit information bottleneck to control visual inputs. 2. Introduces a training method using reinforcement learning (GRPO) on a small curated dataset to align the reasoner with unbiased visual evidence. 3. Demonstrates improved robustness against spurious correlations, transferability across VLM sensors, and enhanced interpretability in reasoning traces.",
      "summary": "The paper addresses the problem of vision-language models (VLMs) relying on spurious correlations rather than causal visual evidence. It proposes VISTA, a modular framework that separates perception (via a frozen VLM) from reasoning (via an LLM) using controlled queries and trains the reasoner with reinforcement learning. The method shows significant gains in robustness on benchmarks like SpuriVerse while maintaining competitive performance on other tasks.",
      "mindmap": "graph TB\n        A[Unbiased Visual Reasoning with Controlled Visual Inputs] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[VLMs exploit spurious correlations/VLMs利用虚假关联]\n        C --> C1[VISTA: Modular framework decoupling perception & reasoning/VISTA: 解耦感知与推理的模块化框架]\n        C1 --> C2[Frozen VLM sensor + LLM reasoner/冻结VLM感知器 + LLM推理器]\n        C2 --> C3[Train with RL (GRPO)/使用强化学习(GRPO)训练]\n        D --> D1[Improved robustness on SpuriVerse/在SpuriVerse上鲁棒性提升]\n        D --> D2[Competitive on MMVP & SeedBench/在MMVP & SeedBench上保持竞争力]\n        D --> D3[Transferable & interpretable/可迁移且可解释]"
    },
    {
      "title": "HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology",
      "authors": "Xitong Ling, Minxi Ouyang, Xiaoxiao Li, Jiawen Li, Ying Chen, Yuxuan Sun, Xinrui Chen, Tian Guan, Xiaoping Liu, Yonghong He",
      "institution": "Tsinghua University, Xiamen University, Westlake University, Wuhan University",
      "link": "https://arxiv.org/pdf/2512.22188",
      "code": "https://github.com/lingxitong/HookMIL",
      "tags": [
        "computational pathology",
        "Multiple Instance Learning",
        "Hook Tokens",
        "Linear Complexity",
        "Multimodal Initialization",
        "Hook Diversity Loss"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/125d35cff1b2c94d5e736ab81d897743e3d0b37d50d5ba6ce441aa77f8bc620e_w640_q70.webp",
      "contributions": "1. Proposes HookMIL, a context-aware MIL framework using learnable hook tokens for structured contextual aggregation with linear computational complexity. 2. Introduces a multimodal initialization strategy for hook tokens using visual, textual, and spatial priors to accelerate convergence and improve representation. 3. Presents a Hook Diversity Loss and a hook-to-hook communication mechanism to encourage token specialization and refine interactions while minimizing redundancy.",
      "summary": "The paper addresses the loss of context in traditional MIL and the high computational cost of transformer-based MIL for whole-slide image analysis. It proposes HookMIL, a framework that uses learnable hook tokens for efficient, linear-complexity context modeling, enhanced by multimodal initialization and specialized loss functions. Experiments on four public datasets show that HookMIL achieves state-of-the-art performance with improved efficiency and interpretability.",
      "mindmap": "graph TB\n        Root[HookMIL: Revisiting Context Modeling in MIL for Computational Pathology] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: MIL loses context; Transformers are inefficient] --> P1[传统MIL丢失上下文/Traditional MIL loses context]\n        Problem --> P2[基于Transformer的MIL计算复杂/Transformer-based MIL has quadratic complexity]\n        Method[主要方法/Method: HookMIL Framework] --> M1[使用可学习的Hook Tokens/Use learnable Hook Tokens]\n        Method --> M2[多模态初始化/Multimodal Initialization]\n        Method --> M3[Hook多样性损失与通信机制/Hook Diversity Loss & Communication]\n        Results[关键结果/Results] --> R1[SOTA性能/State-of-the-art Performance]\n        Results --> R2[计算高效/Computationally Efficient]\n        Results --> R3[可解释性/Interpretability]"
    },
    {
      "title": "SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening",
      "authors": "Antara Titikhsha, Divyanshu Tak",
      "institution": "Carnegie Mellon University, Harvard Medical School, Mass General Brigham, Dana-Farber Cancer Institute, Brigham and Women’s Hospital",
      "link": "https://arxiv.org/pdf/2512.22185",
      "code": "https://github.com/antitikhsha/SAMM2D",
      "tags": [
        "medical image analysis",
        "dual-encoder",
        "data augmentation",
        "transfer learning",
        "intracranial aneurysm detection",
        "Grad-CAM"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d68de6f079250367c97085c42d9cd0408759f4a0e86134b480f2002a580665_w640_q70.webp",
      "contributions": "1. Introduced SAMM2D, a scale-aware multi-modal 2D dual-encoder framework for high-sensitivity intracranial aneurysm screening. 2. Demonstrated through ablation that data augmentation degrades performance when using a strong pretrained backbone, challenging a common assumption in low-data medical imaging. 3. Showed the model achieves 95% sensitivity (surpassing average radiologist performance) and provides interpretable visualizations (Grad-CAM) with clinically relevant focus.",
      "summary": "This paper introduces SAMM2D, a dual-encoder framework for detecting intracranial aneurysms in medical images. The key finding is that, contrary to common practice, data augmentation harms performance when using a strong ImageNet-pretrained backbone, suggesting robust pretraining is more beneficial than complex augmentation in low-data medical settings. The model achieves high sensitivity and demonstrates cost-saving potential in clinical screening.",
      "mindmap": "graph TB\n        A[SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[颅内动脉瘤检测挑战/Intracranial Aneurysm Detection Challenges]\n        B1 --> B2[形态细微/Subtle Morphology]\n        B1 --> B3[类别不平衡/Class Imbalance]\n        B1 --> B4[标注数据稀缺/Scarce Annotated Data]\n        C --> C1[双编码器框架/Dual-Encoder Framework]\n        C --> C2[使用强预训练骨干/Strong Pretrained Backbone]\n        C --> C3[无数据增强/No Data Augmentation]\n        D --> D1[AUC 0.686 (提升32%)/AUC 0.686 (32% Improvement)]\n        D --> D2[95% 灵敏度 (超越放射科医生)/95% Sensitivity (Surpasses Radiologists)]\n        D --> D3[数据增强降低性能/Augmentation Degrades Performance]\n        D --> D4[可解释性可视化/Interpretable Visualizations (Grad-CAM)]"
    },
    {
      "title": "Tiny-YOLOSAM: Fast Hybrid Image Segmentation",
      "authors": "Kenneth Xu, Songhan Wu",
      "institution": "University of Michigan",
      "link": "https://arxiv.org/pdf/2512.22193",
      "code": null,
      "tags": [
        "image segmentation",
        "Segment Anything Model (SAM)",
        "YOLO",
        "hybrid prompting",
        "inference acceleration",
        "object detection"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a0001bbf996fbac2c566ca31f8e7ca42f4e6abdc06cc1a2d649111b5442d799_w640_q70.webp",
      "contributions": "1. Proposes Tiny-YOLOSAM, a fast hybrid segmentation pipeline that combines YOLO object detection with TinySAM for efficient prompting. 2. Introduces a targeted sparse prompting strategy that samples points only in regions not covered by detector-guided masks to improve coverage. 3. Demonstrates significant improvements in both segmentation coverage (AR, mIoU) and inference speed (4.7x faster) on COCO val2017 compared to the baseline TinySAM \"segment-everything\" mode.",
      "summary": "This paper addresses the high computational cost of the Segment Anything Model (SAM) for full-scene segmentation. It proposes Tiny-YOLOSAM, a hybrid method that uses a YOLO detector to generate box prompts for salient objects and supplements uncovered areas with sparse point prompts. The approach achieves a 4.7x speedup and significantly better coverage compared to the baseline, offering a practical alternative to dense \"segment-everything\" prompting.",
      "mindmap": "graph TB\n        A[Tiny-YOLOSAM: Fast Hybrid Image Segmentation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: SAM/TinySAM ”segment-everything” mode is computationally expensive and slow for full-scene segmentation.]\n        C[主要方法/Method: Hybrid pipeline using YOLO for box prompts on foreground objects and sparse point prompts for uncovered regions.]\n        D[关键结果/Results: 4.7x speedup (10.39s vs 49.20s/image) and improved coverage (mIoU: 67.8% vs 19.2%) on COCO val2017.]"
    },
    {
      "title": "Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy",
      "authors": "Shivum Telang",
      "institution": "University of Pittsburgh Rangos Research Center, North Allegheny Senior High School",
      "link": "https://arxiv.org/pdf/2512.22197",
      "code": null,
      "tags": [
        "medical image analysis",
        "Vision-Language Model (VLM)",
        "Few-Shot Learning",
        "Grad-CAM",
        "Multimodal Explainability",
        "Diabetic Retinopathy"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbd85ade1402c194ad4e8570cc627a2a90022d01c5dd2aebc1d16c608b3b733a_w640_q70.webp",
      "contributions": "1. Proposes a novel multimodal explainability model that combines fundus and OCT images for Diabetic Retinopathy severity classification, mimicking an ophthalmologist's reasoning. 2. Introduces a Vision-Language Model (VLM) with few-shot learning adaptation to analyze lesion distributions within retinal quadrants and generate natural language explanations. 3. Develops a method to generate paired Grad-CAM heatmaps across both imaging modalities to visually highlight regions contributing to the classification decision.",
      "summary": "This paper addresses the lack of explainability in AI models for Diabetic Retinopathy (DR) diagnosis by proposing a multimodal Vision-Language Model (VLM). The model uses few-shot learning to analyze lesion distributions in retinal quadrants from both fundus and OCT images and generates paired Grad-CAM heatmaps and natural language explanations for its severity classifications. This approach aims to provide a more interpretable and clinically practical tool for DR diagnostics.",
      "mindmap": "graph TB\n        A[Quadrant Segmentation VLM for Diabetic Retinopathy] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[AI模型缺乏可解释性 / AI Models Lack Explainability]\n        B --> B2[依赖单一成像模态 / Reliance on Single Imaging Modality]\n        C --> C1[多模态VLM与少样本学习 / Multimodal VLM with Few-Shot Learning]\n        C --> C2[象限分析与Grad-CAM热图 / Quadrant Analysis & Grad-CAM Heatmaps]\n        D --> D1[提供自然语言解释 / Provides Natural Language Explanations]\n        D --> D2[提升临床实用性 / Improves Clinical Practicality]"
    },
    {
      "title": "A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability",
      "authors": "Md. Ismiel Hossen Abir, Awolad Hossain",
      "institution": "Department of Computer Science & Engineering, International Standard University, Dhaka, Bangladesh",
      "link": "https://arxiv.org/pdf/2512.22205",
      "code": null,
      "tags": [
        "medical image classification",
        "Convolutional Neural Network",
        "SHAP",
        "LIME",
        "Saliency Maps",
        "Malaria Diagnosis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cafdabeab6baa067cead631acca4eba73f7c2812fd0d0e97d201544854ddd16b_w640_q70.webp",
      "contributions": "1. Proposes a custom CNN model for automated malaria diagnosis from blood cell images, achieving high accuracy (96%). 2. Compares the performance of the custom CNN with established deep learning architectures like ResNet50 and VGG16. 3. Enhances model interpretability for clinical trust by applying Explainable AI techniques, including SHAP, LIME, and Saliency Maps.",
      "summary": "This paper proposes a deep learning-based system using a custom Convolutional Neural Network (CNN) to automatically diagnose malaria from blood cell images, achieving high accuracy. It compares this model against several established architectures and applies Explainable AI (XAI) techniques like SHAP and LIME to make the model's decisions interpretable. The study concludes that this approach can provide a quick, accurate, and understandable diagnostic tool, particularly valuable in resource-limited settings.",
      "mindmap": "graph TB\n        Root[”A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability”] --> Problem[”核心问题/Problem: Traditional microscopic diagnosis is slow, subjective, and resource-intensive.”]\n        Root --> Method[”主要方法/Method: A custom CNN model for classification, compared with other architectures, enhanced with SHAP, LIME, and Saliency Maps for explainability.”]\n        Root --> Results[”关键结果/Results: Achieves 96% accuracy, high precision/recall, providing a fast and interpretable diagnostic tool.”]"
    },
    {
      "title": "TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting",
      "authors": "Qiang Guo, Rubo Zhang, Bingbing Zhang, Junjie Liu, Jianqing Liu",
      "institution": "Dalian Minzu University, Dalian University of Technology, Dalian Rijia Electronics Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.22203",
      "code": null,
      "tags": [
        "crowd counting",
        "weakly-supervised learning",
        "vision transformer",
        "density-guided aggregation",
        "parameter efficiency",
        "lightweight model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67bff3bf5e0b02cbd2cc6071e68fd191f9adc37c49a6f5dd3f4b89fbc7205ca3_w640_q70.webp",
      "contributions": "1. Proposes TCFormer, an ultra-lightweight transformer-based framework with only 5 million parameters for weakly-supervised crowd counting. 2. Introduces a Learnable Density-Weighted Averaging module to dynamically re-weight local features based on predicted density, compensating for the lack of spatial annotations. 3. Designs a density-level classification loss to discretize crowd density into grades, regularizing training and enhancing performance across varying density levels.",
      "summary": "This paper proposes TCFormer, a tiny transformer-based model for weakly-supervised crowd counting that uses only image-level count labels. It introduces a density-guided feature aggregation module and a density-level classification loss to achieve accurate counting. Experiments show it achieves a superior trade-off between parameter efficiency and accuracy, making it suitable for edge devices.",
      "mindmap": "graph TB\n        Root[TCFormer: 5M参数Transformer用于弱监督人群计数] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[标注成本高/High Annotation Cost]\n        Problem --> P2[计算复杂度高/High Computational Complexity]\n        Method[主要方法/Method] --> M1[高效视觉Transformer特征提取器/Efficient ViT Feature Extractor]\n        Method --> M2[可学习密度加权平均模块/Learnable Density-Weighted Averaging]\n        Method --> M3[密度等级分类损失/Density-Level Classification Loss]\n        Results[关键结果/Results] --> R1[仅5M参数/Only 5M Parameters]\n        Results --> R2[弱监督下竞争性性能/Competitive Performance under Weak Supervision]\n        Results --> R3[适用于边缘设备/Suitable for Edge Devices]"
    },
    {
      "title": "Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA",
      "authors": "Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Arash Akbari, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang",
      "institution": "Northeastern University, Harvard University, Cornell University, Tulane University, University of Washington, Roboraction.ai, Futurewei, AIBAO LLC",
      "link": "https://arxiv.org/pdf/2512.22208",
      "code": "https://github.com/moxin-org/Moxin-LLM",
      "tags": [
        "multimodal large language models",
        "Moxin",
        "open-source LLM",
        "vision-language-action",
        "Model Openness Framework",
        "multimodal models"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79873d0c3143fc2b06f1be1be9b8bc80555fa0aefd4a6f2b8d6bda39bce5f227_w640_q70.webp",
      "contributions": "1. Introduces Moxin 7B, a fully open-source LLM developed under the Model Openness Framework, promoting transparency in training, datasets, and implementation. 2. Develops three specialized variants of Moxin: Moxin-VLM for vision-language tasks, Moxin-VLA for vision-language-action tasks, and Moxin-Chinese for Chinese language capabilities. 3. Demonstrates superior performance of the proposed models in various evaluations using open-source frameworks and data, with all models, code, and data publicly released.",
      "summary": "This paper introduces Moxin 7B, a fully transparent open-source large language model, and extends it into three multimodal variants for vision-language, vision-language-action, and Chinese tasks. The models are trained using open-source frameworks and data. The authors release the models, code, and data, reporting superior performance in evaluations.",
      "mindmap": "graph TB\n        A[Open-Source Multimodal Moxin Models<br/>开源多模态Moxin模型] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Proprietary vs. Open-Source LLMs<br/>闭源与开源大语言模型]\n        B --> B2[Need for transparent, capable open models<br/>需要透明、强大的开源模型]\n        C --> C1[Develop Moxin 7B under Model Openness Framework<br/>基于模型开放框架开发Moxin 7B]\n        C --> C2[Create variants: VLM, VLA, Chinese<br/>创建变体: VLM, VLA, 中文模型]\n        D --> D1[Superior performance in evaluations<br/>在评估中表现优异]\n        D --> D2[Full release of models, code, data<br/>完整发布模型、代码、数据]"
    },
    {
      "title": "VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition",
      "authors": "Abdellah Zakaria Sellam, Salah Eddine Bekhouche, Fadi Dornaika, Cosimo Distante, Abdenour Hadid",
      "institution": "University of Salento, Institute of Applied Sciences and Intelligent Systems - CNR, University of the Basque Country UPV/EHU, IKERBASQUE, Sorbonne University Abu Dhabi",
      "link": "https://arxiv.org/pdf/2512.22217",
      "code": null,
      "tags": [
        "pedestrian attribute recognition",
        "vision-language model",
        "cross-attention fusion",
        "class imbalance",
        "domain generalization",
        "SigLIP"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44bf59cdfc87f0708e9f41a8899fbff5562f87b0b721fe79f7fcfc23fb5bb4d4_w640_q70.webp",
      "contributions": "1. Proposes VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders for Pedestrian Attribute Recognition. 2. Introduces a compact cross-attention fusion mechanism to align image and prompt embeddings by refining visual features. 3. Demonstrates state-of-the-art performance on the imbalanced PA100K benchmark and significant gains on PETA and Market-1501, showing effectiveness against imbalance and domain shift.",
      "summary": "This paper proposes VLM-PAR, a vision-language model framework that uses a frozen SigLIP encoder and a cross-attention fusion module to refine visual features for Pedestrian Attribute Recognition. It achieves new state-of-the-art results on the PA100K benchmark and shows strong performance on other datasets, demonstrating the effectiveness of leveraging large-scale vision-language pretraining to address class imbalance and generalization challenges in PAR.",
      "mindmap": "graph TB\n        A[VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Class Imbalance, Attribute Co-dependencies, Domain Shifts<br>类别不平衡, 属性依赖, 域偏移]\n        C[主要方法/Method<br>Vision-Language Framework with Cross-Attention Fusion<br>视觉语言框架与跨注意力融合]\n        D[关键结果/Results<br>SOTA on PA100K, Gains on PETA & Market-1501<br>PA100K上SOTA, PETA & Market-1501上提升]"
    },
    {
      "title": "Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition",
      "authors": "Naichuan Zheng, Xiahai Lun, Weiyi Li, Yuchen Du",
      "institution": "Beijing University of Posts and Telecommunications",
      "link": "https://arxiv.org/pdf/2512.22214",
      "code": null,
      "tags": [
        "skeleton-based action recognition",
        "Spiking Neural Networks",
        "Graph Convolutional Networks",
        "Time-Frequency Learning",
        "Topology-Aware Learning",
        "Energy Efficiency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6af8fb8807de54b37db40834a79908ad86cf7e159907b658e54986356c4494d4_w640_q70.webp",
      "contributions": "1. Proposes a novel spiking graph network backbone integrating 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. 2. Introduces a Topology-Shift Self-Attention (TSSA) mechanism to adaptively route attention across learned skeletal topologies without increasing computational complexity. 3. Designs an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch with a Topology-Aware Time-Frequency Fusion (TATF) unit to preserve structural priors in multi-resolution spectral fusion.",
      "summary": "This paper proposes Signal-SGN++, a novel spiking graph network for skeleton-based action recognition that integrates topology-aware learning with time-frequency spiking dynamics to capture motion dependencies. The method combines a spiking graph backbone with a topology-shift attention mechanism and a multi-scale wavelet fusion branch. Experiments show it achieves a superior accuracy-efficiency trade-off, outperforming other SNN methods and competing with state-of-the-art GCNs while using significantly less energy.",
      "mindmap": "graph TB\n        A[Signal-SGN++<br/>论文标题/Paper Title] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[GCNs能耗高<br/>GCNs High Energy Cost]\n        B --> B2[SNNs难以捕捉时空-频率与拓扑依赖<br/>SNNs Limited in Capturing Time-Freq & Topology]\n        C --> C1[主干网络: 1D-SGC + FSC<br/>Backbone: 1D-SGC + FSC]\n        C --> C2[拓扑转移自注意力 TSSA<br/>Topology-Shift Self-Attention TSSA]\n        C --> C3[多尺度小波变换融合 MWTF<br/>Multi-Scale Wavelet Transform Fusion MWTF]\n        D --> D1[优于现有SNN方法<br/>Outperforms Existing SNN Methods]\n        D --> D2[与先进GCNs结果相当<br/>Competitive with SOTA GCNs]\n        D --> D3[能耗显著降低<br/>Substantially Reduced Energy Consumption]"
    },
    {
      "title": "On Extending Semantic Abstraction for Efficient Search of Hidden Objects",
      "authors": "Tasha Pais, Nikhilesh Belulkar",
      "institution": "Columbia University",
      "link": "https://arxiv.org/pdf/2512.22220",
      "code": null,
      "tags": [
        "object detection",
        "semantic abstraction",
        "relevancy maps",
        "3D localization",
        "hidden objects",
        "unstructured search"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb1e039b06f845acfd3a644354907fc35eec78f060cf605799b7607c8a20ba8_w640_q70.webp",
      "contributions": "1. Extends the Semantic Abstraction framework to the novel domain of localizing hidden (occluded) objects. 2. Proposes using historical placement data to efficiently guide the unstructured search for hidden objects. 3. Demonstrates a model that can accurately identify a hidden object's complete 3D location faster than a naive random search.",
      "summary": "This paper addresses the problem of efficiently finding hidden or lost objects by extending the Semantic Abstraction framework. The method uses 2D VLM relevancy maps as abstract object representations to learn 3D localization and leverages historical placement data to optimize the search. The result is a model that can locate hidden objects significantly faster than random search, aiming to improve the capabilities of household robots.",
      "mindmap": "graph TB\n        Root(”On Extending Semantic Abstraction for Efficient Search of Hidden Objects”) --> Problem(”核心问题/Problem: Localizing hidden/occluded objects”)\n        Root --> Method(”主要方法/Method: Use VLM relevancy maps & historical data for efficient 3D search”)\n        Root --> Results(”关键结果/Results: Faster and accurate 3D localization vs. random search”)"
    },
    {
      "title": "Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark",
      "authors": "Hieu Minh Nguyen, Tam Le-Thanh Dang, Kiet Van Nguyen",
      "institution": "University of Information Technology, Vietnam National University, Ho Chi Minh City",
      "link": "https://arxiv.org/pdf/2512.22218",
      "code": null,
      "tags": [
        "visual question answering",
        "signboard VQA",
        "OCR-integrated VQA",
        "multimodal dataset",
        "Vietnamese",
        "multi-agent framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d16c0f62d737eb6a3e9a6e55cda2d721775e9d3be82e08e0e0cd03f4d648a93_w640_q70.webp",
      "contributions": "1. Introduces ViSignVQA, the first large-scale Vietnamese dataset for signboard-oriented VQA, capturing diverse linguistic, cultural, and visual characteristics. 2. Benchmarks the task by adapting state-of-the-art VQA models with integrated Vietnamese OCR and language models, demonstrating significant performance gains from OCR-enhanced context. 3. Proposes a novel multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving high accuracy via majority voting.",
      "summary": "This paper addresses the under-explored task of understanding signboard text in natural scenes for Visual Question Answering (VQA), particularly in low-resource languages like Vietnamese. It introduces the ViSignVQA dataset and benchmarks adapted VQA models with OCR integration, showing substantial performance improvements, and proposes a multi-agent framework that achieves high accuracy. The work highlights the importance of domain-specific multimodal resources for enhancing text-based VQA in real-world applications.",
      "mindmap": "graph TB\n        A[”Towards Signboard-Oriented VQA: ViSignVQA Dataset, Method and Benchmark”] --> B[”核心问题/Problem: 理解自然场景中的招牌文本对于VQA的现实应用至关重要，但在低资源语言中仍未充分探索。”]\n        A --> C[”主要方法/Method: 1. 引入首个大规模越南语招牌VQA数据集ViSignVQA。 2. 通过集成越南语OCR和语言模型来适配SOTA VQA模型。 3. 提出结合感知与推理智能体及GPT-4的多智能体VQA框架。”]\n        A --> D[”关键结果/Results: 1. 添加OCR文本使F1分数提升高达209%。 2. 多智能体框架通过多数投票达到75.98%准确率。 3. 创建了首个捕获真实世界场景文本特征的大规模越南语多模态基准。”]"
    },
    {
      "title": "VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs",
      "authors": "Naishan Zheng, Jie Huang, Qingpei Guo, Feng Zhao",
      "institution": "University of Science and Technology of China, Ant Group",
      "link": "https://arxiv.org/pdf/2512.22226",
      "code": "https://github.com/zheng980629/VideoScaffold",
      "tags": [
        "video understanding",
        "streaming video",
        "multimodal large language models",
        "event segmentation",
        "hierarchical representation",
        "elastic-scale"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ccc0f206a787899e1408b9c740b895e26c8c4847a2ecfbe5f88b48c25ce70ca_w640_q70.webp",
      "contributions": "1. Proposes VideoScaffold, a dynamic representation framework for streaming video understanding in MLLMs that adaptively adjusts event granularity. 2. Introduces Elastic-Scale Event Segmentation (EES) for prediction-guided, dynamic boundary refinement. 3. Introduces Hierarchical Event Consolidation (HEC) for progressively aggregating segments into multi-level abstractions.",
      "summary": "This paper addresses the challenge of understanding long, streaming videos with MLLMs by proposing VideoScaffold, a framework that dynamically segments and hierarchically consolidates video events to adapt granularity and preserve semantics. It achieves state-of-the-art performance on benchmarks and can extend image-based MLLMs to video comprehension.",
      "mindmap": "graph TB\n        A[VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs] --> B[核心问题/Problem: Understanding long, streaming videos with MLLMs is challenging due to redundancy and need for temporal coherence.]\n        A --> C[主要方法/Method: Proposes a dynamic framework with Elastic-Scale Event Segmentation (EES) and Hierarchical Event Consolidation (HEC).]\n        A --> D[关键结果/Results: Achieves state-of-the-art performance; framework is modular and plug-and-play.]"
    },
    {
      "title": "KAN-FPN-Stem:A KAN-Enhanced Feature Pyramid Stem for Boosting ViT-based Pose Estimation",
      "authors": "HaoNan Tang",
      "institution": "WuHan University of Technology",
      "link": "https://arxiv.org/pdf/2512.22228",
      "code": null,
      "tags": [
        "pose estimation",
        "KAN",
        "Feature Pyramid Network",
        "Vision Transformer",
        "multi-scale fusion",
        "convolutional layer"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2245ed47610385d8ff0636f54ef0af43582616927b67e58ea212b9d6d195a902_w640_q70.webp",
      "contributions": "1. Identified that the performance bottleneck in ViT front-ends for pose estimation lies in the post-fusion smoothing step, not in feature refinement via attention modules. 2. Proposed a novel KAN-enhanced FPN-Stem architecture that replaces the standard linear 3x3 convolution in the FPN with a KAN-based convolutional layer for superior non-linear smoothing. 3. Demonstrated a significant performance improvement of up to +2.0 AP on COCO dataset over the ViTPose-S baseline, providing a plug-and-play module and a new direction for improving feature fusion quality.",
      "summary": "This paper addresses the performance bottleneck in Vision Transformer (ViT) based pose estimation models, which stems from simplistic front-end designs that cause information loss and poor multi-scale handling. The authors propose KAN-FPN-Stem, an architecture that enhances the Feature Pyramid Network by replacing its final linear smoothing convolution with a KAN-based layer to better rectify fusion artifacts. Experiments show a significant performance boost, revealing that improving feature fusion, not just refinement, is key to advancing ViT-based dense prediction tasks.",
      "mindmap": "graph TB\n        Root[”KAN-FPN-Stem: A KAN-Enhanced Feature Pyramid Stem for Boosting ViT-based Pose Estimation”] --> Problem[”核心问题/Problem: ViT front-end (e.g., ViTPose) has simplistic design causing multi-scale handling issues & information loss”]\n        Root --> Method[”主要方法/Method: Replace FPN's terminal linear 3x3 conv with a KAN-based convolutional layer for adaptive non-linear smoothing”]\n        Root --> Results[”关键结果/Results: Achieves +2.0 AP boost on COCO over ViTPose-S; reveals fusion quality is the key bottleneck”]"
    },
    {
      "title": "Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction",
      "authors": "Mengxiao Geng, Ran Hong, Xiaoling Xu, Bingxuan Li, Qiegen Liu",
      "institution": "Nanchang University, Hefei Comprehensive National Science Center",
      "link": "https://arxiv.org/pdf/2512.22237",
      "code": null,
      "tags": [
        "medical image reconstruction",
        "diffusion model",
        "cross-domain",
        "meta-information",
        "sinogram adapter",
        "low-dose PET"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/974872f0916ad96ffeb1ed9b169c4f627d5c534046b438f10fd015171720864a_w640_q70.webp",
      "contributions": "1. Proposes a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates cross-modal priors for PET reconstruction. 2. Introduces a meta-information encoding module that transforms clinical parameters into semantic prompts for cross-modal alignment. 3. Designs a cross-domain architecture with a specialized sinogram adapter to capture global physical structures in the projection domain.",
      "summary": "The paper addresses the challenge of low-dose PET image reconstruction, which suffers from noise and loss of detail. It proposes a novel diffusion model called MiG-DM that guides the reconstruction using patient-specific meta-information and processes data across both the projection and image domains. Experiments show that MiG-DM outperforms existing methods in improving image quality and preserving physiological details.",
      "mindmap": "graph TB\n        A[Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction] --> B(核心问题/Problem: Low-dose PET imaging faces noise, reduced contrast, and detail loss)\n        A --> C(主要方法/Method: MiG-DM integrates meta-information prompts and cross-domain (projection & image) processing)\n        A --> D(关键结果/Results: Outperforms SOTA on UDPET and clinical datasets, enhancing quality and preserving details)"
    },
    {
      "title": "Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening",
      "authors": "Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf",
      "institution": "Radboud University Medical Center, Radboud University",
      "link": "https://arxiv.org/pdf/2512.22242",
      "code": null,
      "tags": [
        "medical imaging",
        "algorithmic fairness",
        "subgroup performance analysis",
        "JustEFAB framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp",
      "contributions": "1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application.",
      "summary": "This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes.",
      "mindmap": "graph TB\n        Root[Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening<br/>肺癌筛查风险估计模型的公平性评估] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br/>AI肺癌风险模型在不同人口亚组中的性能表现是否公平？<br/>Is AI lung cancer risk model performance fair across demographic subgroups?]\n        Method[主要方法/Method<br/>使用JustEFAB框架评估模型在NLST验证集上的性能差异<br/>Evaluate model performance disparities on NLST validation set using JustEFAB framework]\n        Results[关键结果/Results<br/>发现Sybil和Venkadesh21模型存在显著的、无法用混杂因素解释的性能差异<br/>Found significant, unexplained performance disparities in Sybil and Venkadesh21 models]"
    },
    {
      "title": "Masking Teacher and Reinforcing Student for Distilling Vision-Language Models",
      "authors": "Byung-Kwan Lee, Yu-Chiang Frank Wang, Ryo Hachiuma",
      "institution": "NVIDIA",
      "link": "https://arxiv.org/pdf/2512.22238",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "knowledge distillation",
        "reinforcement learning",
        "vision-language models",
        "progressive masking",
        "offline RL"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72df4c6bab2069771a9955e5dac4af81d2f423474fdaf07bf9980aaea39edeaf_w640_q70.webp",
      "contributions": "1. Proposes Masters, a mask-progressive RL distillation framework that first masks non-dominant teacher weights to reduce complexity and then progressively restores them for stable student learning. 2. Introduces an offline RL stage with complementary accuracy and distillation rewards, leveraging pre-generated responses from masked teachers for efficient guidance. 3. Demonstrates that progressive teacher scaling (e.g., from 14B to 38B) yields smoother convergence and stronger generalization than one-shot distillation, providing a scalable path to efficient VLMs.",
      "summary": "The paper addresses the challenge of distilling large vision-language models (VLMs) into compact ones by proposing Masters, a framework that uses progressive masking of the teacher model and offline reinforcement learning. This method enables stable knowledge transfer and efficient training, resulting in small VLMs that achieve strong performance, sometimes surpassing larger models, while being far more efficient for deployment.",
      "mindmap": "graph TB\n        A[Masking Teacher and Reinforcing Student for Distilling Vision-Language Models] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[大型VLM难以部署到移动/边缘设备/Large VLMs are impractical for mobile/edge deployment]\n        B --> B2[师生模型尺寸差距导致知识蒸馏不稳定/Large size gap causes unstable distillation]\n        C --> C1[掩码渐进式强化学习蒸馏框架/Mask-progressive RL distillation framework]\n        C --> C2[先掩码教师非主导权重，再渐进恢复/First mask non-dominant teacher weights, then progressively restore]\n        C --> C3[离线RL阶段使用准确性和蒸馏奖励/Offline RL stage with accuracy and distillation rewards]\n        D --> D1[在多个基准测试中超越现有紧凑型VLM/Outperforms existing compact VLMs on diverse benchmarks]\n        D --> D2[渐进增加教师尺寸带来更平滑收敛和更强泛化/Gradually increasing teacher size yields smoother convergence & stronger generalization]\n        D --> D3[提供高效、可部署VLM的可扩展路径/Provides a scalable path toward efficient, deployable VLMs]"
    },
    {
      "title": "Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture",
      "authors": "Phi-Hung Hoang, Nam-Thuan Trinh, Van-Manh Tran, Thi-Thu-Hong Phan",
      "institution": "FPT University",
      "link": "https://arxiv.org/pdf/2512.22239",
      "code": null,
      "tags": [
        "model compression",
        "knowledge distillation",
        "lightweight CNN",
        "inverted residual blocks",
        "dense connectivity",
        "multi-objective learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccaf949c91086899f4aa9953236d8ee76957b0a5aaafcda22bf81f403ee1e0a5_w640_q70.webp",
      "contributions": "1. Proposes a hybrid knowledge distillation framework integrating hard-label supervision, feature-level, response-level, and self-distillation for training efficient models. 2. Designs a customized student CNN architecture combining inverted residual blocks with dense connectivity to balance efficiency and accuracy. 3. Demonstrates strong generalization across multiple agricultural datasets (rice seeds and plant leaf diseases) with significant reductions in computational cost and model size while maintaining high accuracy.",
      "summary": "This paper proposes a multi-objective hybrid knowledge distillation method to create a lightweight CNN for smart agriculture, combining inverted residual and dense blocks. The distilled model achieves near-teacher accuracy with drastically reduced computation and parameters, showing robust performance on rice seed and plant disease datasets.",
      "mindmap": "graph TB\n        Root[”Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture<br>面向智慧农业的高效深度学习的多目标混合知识蒸馏”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Deploying deep models on edge devices in smart agriculture<br>在智慧农业中于边缘设备部署深度模型”] --> P1[”挑战/Challenge<br>Trade-off between efficiency and accuracy<br>效率与准确性的权衡”]\n        Method[”主要方法/Method<br>Hybrid knowledge distillation framework<br>混合知识蒸馏框架”] --> M1[”学生模型/Student Model<br>Customized CNN with inverted residual & dense blocks<br>定制化CNN，含倒残差与密集连接块”]\n        Method --> M2[”教师模型/Teacher Model<br>ResNet18 guidance<br>ResNet18教师网络指导”]\n        Method --> M3[”多目标策略/Multi-objective Strategy<br>Integrates hard-label, feature-level, response-level, self-distillation<br>整合硬标签、特征级、响应级与自蒸馏”]\n        Results[”关键结果/Results<br>Experiments on agricultural datasets<br>在农业数据集上的实验”] --> R1[”性能/Performance<br>98.56% accuracy on rice seeds (vs teacher 98.65%)<br>水稻种子分类准确率98.56%（教师模型98.65%）”]\n        Results --> R2[”效率/Efficiency<br>0.68 GFLOPs, ~1.07M parameters (10x smaller than teacher)<br>0.68 GFLOPs，约107万参数（比教师模型小10倍）”]\n        Results --> R3[”泛化/Generalization<br>Consistent gains on plant leaf disease datasets<br>在植物叶片病害数据集上一致性能提升”]"
    },
    {
      "title": "Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models",
      "authors": "Zheng Xing, Weibing Zhao",
      "institution": "Shenzhen University, Shenzhen MSU-BIT University",
      "link": "https://arxiv.org/pdf/2512.22249",
      "code": null,
      "tags": [
        "human motion segmentation",
        "temporal vision semantics",
        "subspace clustering",
        "large language model",
        "temporal regularizer",
        "feedback framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fce87220cc17109bbd9138b27d5aa353003bb134fb0924f68b7fc59fdfd3ee0d_w640_q70.webp",
      "contributions": "1. Proposes a novel method to learn Temporal Vision Semantics (TVS) from human motion sequences by querying a Large Language Model (LLM) to determine motion consistency between consecutive frames. 2. Develops a TVS-integrated subspace clustering framework that incorporates a temporal regularizer and constraint to enforce similarity among temporal neighbors in the subspace embedding and segmentation. 3. Introduces a feedback-enabled optimization framework that iteratively refines the subspace embedding based on the segmentation output to improve performance.",
      "summary": "This paper addresses the problem of unsupervised human motion segmentation by integrating temporal semantic information into subspace clustering. The proposed method uses a Large Language Model to extract textual motion descriptions from consecutive frames and incorporates this learned temporal neighbor information as constraints within the clustering framework. Experimental results show the method outperforms state-of-the-art approaches on four benchmark datasets.",
      "mindmap": "graph TB\n        A[论文标题 / Paper Title: Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models] --> B(核心问题 / Problem: 传统无监督人体运动分割方法忽略了时序语义的作用 / Traditional unsupervised HMS overlooks temporal semantics)\n        A --> C(主要方法 / Method: 利用LLM提取时序视觉语义，并融入子空间聚类框架 / Use LLM to extract TVS and integrate it into subspace clustering)\n        A --> D(关键结果 / Results: 在四个基准数据集上性能超越现有方法 / Outperforms SOTA on four benchmark datasets)\n        C --> C1(LLM查询 / LLM Query: 判断相邻帧是否描述相同运动 / Determine if consecutive frames depict the same motion)\n        C --> C2(时序正则化 / Temporal Regularizer: 诱导相邻帧共享相似子空间嵌入 / Induces similar subspace embeddings for temporal neighbors)\n        C --> C3(反馈优化 / Feedback Optimization: 基于分割结果迭代优化嵌入 / Iteratively optimizes embedding based on segmentation output)"
    },
    {
      "title": "Human-Aligned Generative Perception: Bridging Psychophysics and Generative Models",
      "authors": "Antara Titikhsha, Om Kulkarni, Dharun Muthaiah",
      "institution": "Carnegie Mellon University",
      "link": "https://arxiv.org/pdf/2512.22272",
      "code": "https://github.com/omkul22/16824-Project-Human-Aligned-Generative-Perception",
      "tags": [
        "generative models",
        "text-to-image diffusion",
        "geometric control",
        "human perception embedding",
        "latent guidance",
        "teacher model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da7d63c4f85d42c740b13906960a8e8749c9cb29eda218682db4a1db49adc38f_w640_q70.webp",
      "contributions": "1. Proposing a Human Perception Embedding (HPE) teacher trained on human similarity data to capture geometry-sensitive features. 2. Introducing a model-agnostic latent guidance framework for steering various generative architectures. 3. Demonstrating improved geometric control and semantic alignment in text-to-image synthesis without specialized training.",
      "summary": "This paper addresses the problem where text-to-image models prioritize style over geometric constraints. The proposed method uses a lightweight \"teacher\" model, trained on human perceptual data, to guide the diffusion process towards desired shapes. The results show that this approach significantly improves geometric control across different model architectures without requiring retraining.",
      "mindmap": "graph TB\n        A[Human-Aligned Generative Perception<br>人类对齐的生成式感知] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Generative models ignore geometry for style<br>生成模型为风格忽略几何]\n        C --> C1[Use HPE teacher for guidance<br>使用HPE教师模型进行引导]\n        D --> D1[~80% better alignment<br>对齐度提升约80%]\n        D --> D2[Zero-shot shape transfer<br>零样本形状迁移]"
    },
    {
      "title": "GeCo: A Differentiable Geometric Consistency Metric for Video Generation",
      "authors": "Leslie Gu, Junhwa Hur, Charles Herrmann, Fangneng Zhan, Todd Zickler, Deqing Sun, Hanspeter Pfister",
      "institution": "Harvard University, Google DeepMind, Massachusetts Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22274",
      "code": null,
      "tags": [
        "video generation evaluation",
        "geometric consistency",
        "video generation",
        "differentiable metric",
        "depth prior",
        "motion prior"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e4a287254d3ac04a1aa636a703d4f551fc0d8e920f903da7f174ef77ae6203f_w640_q70.webp",
      "contributions": "1. Proposes GeCo, a novel differentiable metric that fuses motion and depth priors to detect geometric deformation and occlusion inconsistency in generated videos. 2. Introduces two synthetic datasets, WarpBench and OccluBench, to validate the metric's performance on isolated artifacts. 3. Demonstrates GeCo's dual utility for benchmarking state-of-the-art video generation models and as a training-free guidance loss to reduce geometric artifacts during generation.",
      "summary": "The paper introduces GeCo, a differentiable metric for evaluating geometric consistency in generated videos by combining motion and depth cues. It validates the metric on new synthetic datasets and uses it to benchmark models and improve video generation by reducing artifacts as a guidance loss.",
      "mindmap": "graph TB\n        A[GeCo: A Differentiable Geometric Consistency Metric for Video Generation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[生成视频违反3D几何/Generated videos violate 3D geometry]\n        C --> C1[融合运动和深度先验/Fuse motion & depth priors]\n        C --> C2[创建合成数据集/Create synthetic datasets]\n        D --> D1[系统评估模型/Systematically benchmark models]\n        D --> D2[用作无训练引导损失/Used as training-free guidance loss]"
    },
    {
      "title": "Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions",
      "authors": "Aahan Sachdeva, Dhanvinkumar Ganeshkumar, James E. Gallagher, Tyler Treat, Edward J. Oughton",
      "institution": "George Mason University",
      "link": "https://arxiv.org/pdf/2512.22263",
      "code": null,
      "tags": [
        "object detection",
        "RGB-LWIR fusion",
        "multispectral imagery",
        "YOLO",
        "adaptive framework",
        "illumination conditions"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9eac93c181b8ee533ee303243bd2258f5b332ba1744a5e93dc7fa00505d6c4e_w640_q70.webp",
      "contributions": "1. An adaptive framework that dynamically selects the optimal RGB-LWIR fusion ratio and detection model based on real-time illumination conditions. 2. Creation of a comprehensive dataset and model set, training 33 YOLO models on over 22,000 annotated images across three light levels with eleven fusion ratios. 3. Demonstrated significant performance improvements over RGB-only and thermal-only baselines, particularly in full-light and dim-light conditions, enhancing detection reliability for autonomous systems.",
      "summary": "This paper addresses the limitations of RGB and thermal-only object detection in variable lighting by proposing an adaptive framework that fuses RGB and LWIR video streams at multiple ratios and selects the best model for the current illumination. The method, evaluated on a large dataset, showed that optimized fusion models significantly outperformed baseline models in full and dim light, improving detection confidence and reliability for autonomous robotic vision.",
      "mindmap": "graph TB\n        Root[Evaluating an Adaptive Multispectral Turret System] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[RGB在低光下表现差/RGB struggles in low-light]\n        Problem --> P2[热成像缺乏颜色纹理/Thermal lacks color & texture]\n        Method[主要方法/Method] --> M1[自适应RGB-LWIR融合框架/Adaptive RGB-LWIR fusion framework]\n        Method --> M2[训练33个YOLO模型/Trained 33 YOLO models]\n        Method --> M3[11种融合比例/11 fusion ratios]\n        Results[关键结果/Results] --> R1[全光模型: 92.8%置信度/Full-light model: 92.8% confidence]\n        Results --> R2[微光模型: 92.0%置信度/Dim-light model: 92.0% confidence]\n        Results --> R3[无光模型: 71.0%置信度/No-light model: 71.0% confidence]"
    },
    {
      "title": "FETAL-GAUGE: A Benchmark for Assessing Vision-Language Models in Fetal Ultrasound",
      "authors": "Hussain Alasmawi, Numan Saeed, Mohammad Yaqub",
      "institution": "Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)",
      "link": "https://arxiv.org/pdf/2512.22278",
      "code": null,
      "tags": [
        "medical imaging",
        "vision-language models",
        "fetal ultrasound",
        "visual question answering",
        "benchmark",
        "multimodal learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4890359c1dc1ee5f0dc6fae45f7bab8696f5048a1a22998019c7d37eab29b5da_w640_q70.webp",
      "contributions": "1. Introduces Fetal-Gauge, the first and largest visual question answering benchmark for evaluating Vision-Language Models (VLMs) in fetal ultrasound, comprising over 42,000 images and 93,000 question-answer pairs. 2. Systematically evaluates state-of-the-art general-purpose and medical-specific VLMs, revealing a substantial performance gap where the best model achieves only 55% accuracy. 3. Identifies critical limitations of current VLMs in this domain and establishes a foundation for advancing domain-adapted architectures and specialized training for prenatal care.",
      "summary": "This paper addresses the lack of a standardized benchmark for evaluating Vision-Language Models (VLMs) in fetal ultrasound by introducing Fetal-Gauge, a large-scale visual question answering dataset. The authors evaluate multiple VLMs and find their performance (max 55% accuracy) is far below clinical requirements, highlighting the urgent need for specialized models in this medical domain.",
      "mindmap": "graph TB\n        A[FETAL-GAUGE: A Benchmark for Assessing Vision-Language Models in Fetal Ultrasound] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[全球超声医师短缺 / Global Sonographer Shortage]\n        B --> B2[缺乏标准化评估基准 / Lack of Standardized Benchmark]\n        C --> C1[创建胎儿超声VQA基准 / Create Fetal Ultrasound VQA Benchmark]\n        C --> C2[系统评估VLMs / Systematically Evaluate VLMs]\n        D --> D1[最佳模型准确率55% / Best Model Accuracy 55%]\n        D --> D2[性能远低于临床要求 / Performance Far Below Clinical Requirements]"
    },
    {
      "title": "The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency",
      "authors": "Dingyu Wang, Zimu Yuan, Jiajun Liu, Shanggui Liu, Nan Zhou, Tianxing Xu, Di Huang, Dong Jiang",
      "institution": "Peking University Third Hospital, Beihang University",
      "link": "https://arxiv.org/pdf/2512.22275",
      "code": null,
      "tags": [
        "multimodal reasoning",
        "clinical reasoning benchmark",
        "vision-language models",
        "multimodal integration",
        "medical image interpretation",
        "hallucination"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43db8495c9ac46c5ea892f723394bd1e6c9b400003c2c67ace7ac9abe26f0bcf_w640_q70.webp",
      "contributions": "1. Introduced the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework with 1,245 questions derived from real-world patient cases to assess clinical reasoning. 2. Revealed a significant performance gap in VLMs, showing high accuracy on structured tasks but poor performance on open-ended, multimodal reasoning tasks, with severe text-driven hallucinations. 3. Demonstrated that medically fine-tuned models show no consistent advantage over general-purpose models, highlighting a fundamental limitation in current AI for clinical competency.",
      "summary": "The paper introduces the Bones and Joints (B&J) Benchmark to rigorously evaluate the clinical reasoning capabilities of vision-language and large language models. The results show that while models perform well on structured tasks, they struggle significantly with open-ended, multimodal reasoning essential for real-world patient care, indicating they are not yet clinically competent. The authors conclude that safe AI deployment should be limited to supportive roles until fundamental breakthroughs in multimodal integration are achieved.",
      "mindmap": "graph TB\n        A[The Illusion of Clinical Reasoning<br>临床推理的假象] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Current benchmarks fail to capture integrated, multimodal clinical reasoning.<br>现有基准无法捕捉综合、多模态临床推理。]\n        C --> C1[Developed the B&J Benchmark with 1245 real-world questions across 7 tasks.<br>开发了包含1245个真实世界问题、涵盖7项任务的B&J基准。]\n        D --> D1[Large performance gap: high on MCQ, low on open-ended multimodal tasks.<br>巨大性能差距：选择题表现好，开放式多模态任务表现差。]\n        D --> D2[VLMs have limitations in image interpretation and exhibit hallucinations.<br>VLM在图像解释方面存在局限并出现幻觉。]\n        D --> D3[Medically fine-tuned models show no consistent advantage.<br>医学微调模型未显示一致优势。]"
    },
    {
      "title": "Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model",
      "authors": "Renping Zhou, Zanlin Ni, Tianyi Chen, Zeyu Liu, Yang Yue, Yulin Wang, Yuxuan Wang, Jingshu Liu, Gao Huang",
      "institution": "Tsinghua University (Leap Lab), Anyverse Dynamics",
      "link": "https://arxiv.org/pdf/2512.22288",
      "code": "https://co-grpo.github.io",
      "tags": [
        "diffusion models",
        "Masked Diffusion Models",
        "Markov Decision Process",
        "Group Relative Policy Optimization",
        "inference schedule optimization",
        "trajectory-level training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/63c1159878e540d373846dba6e76fb918342cb837f6f15d4e79e21a40dad9e84_w640_q70.webp",
      "contributions": "1. Identifies and addresses the discrepancy between the single-step training and multi-step inference of Masked Diffusion Models (MDMs). 2. Proposes Co-GRPO, a method that reformulates MDM generation as a unified Markov Decision Process to jointly optimize model parameters and inference schedule parameters. 3. Introduces a trajectory-level optimization using Group Relative Policy Optimization that avoids costly backpropagation through the multi-step generation process.",
      "summary": "This paper addresses the misalignment between the training and inference procedures of Masked Diffusion Models (MDMs). It proposes Co-GRPO, a method that jointly optimizes the MDM and its inference schedule as a unified Markov Decision Process using Group Relative Policy Optimization. The approach improves generation quality across multiple benchmarks without requiring expensive backpropagation through the full generation trajectory.",
      "mindmap": "graph TB\n        A[Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[训练与推理不匹配/Mismatch between Training & Inference]\n        B1 --> B2[训练: 单步BERT式/Training: Single-step BERT-style]\n        B1 --> B3[推理: 多步有调度/Inference: Multi-step with Schedule]\n        C --> C1[统一MDP/Unified MDP]\n        C1 --> C2[联合优化模型与调度/Jointly Optimize Model & Schedule]\n        C2 --> C3[组相对策略优化/Group Relative Policy Optimization]\n        D --> D1[提升生成质量/Improved Generation Quality]\n        D1 --> D2[在四个基准上验证/Validated on Four Benchmarks]"
    },
    {
      "title": "A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation",
      "authors": "Philip Xu, David Elizondo, Raouf Hamzaoui",
      "institution": "De Montfort University",
      "link": "https://arxiv.org/pdf/2512.22294",
      "code": null,
      "tags": [
        "multimodal retrieval and generation",
        "3D retrieval",
        "4D generation",
        "cross-modal alignment",
        "multi-head attention",
        "open-vocabulary"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8bb826b31ecaac1f8358869614a5af4e6a0fe9eeceb1eb97ce8bbb0ff8afdcd6_w640_q70.webp",
      "contributions": "1. Proposes Uni4D, a unified framework for large-scale open-vocabulary 3D retrieval and controlled 4D generation. 2. Introduces a structured three-level alignment strategy across text, 3D models, and images to enhance semantic understanding. 3. Presents a 3D-Text Multi-head Attention and Search (ATMS) model to optimize text-to-3D retrieval efficiency and accuracy.",
      "summary": "This paper introduces Uni4D, a framework that uses a three-level alignment strategy across text, 3D, and images to address the challenges of large-scale 3D retrieval and controlled 4D generation. The method employs a novel attention and search model to improve semantic alignment and retrieval efficiency. Experimental results demonstrate that Uni4D achieves high-quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding.",
      "mindmap": "graph TB\n        A[Uni4D: A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[大规模3D检索与可控4D生成的挑战/Challenges in large-scale 3D retrieval and controlled 4D generation]\n        C --> C1[三级对齐框架: 文本-3D-图像/Three-level alignment: text-3D-image]\n        C --> C2[ATMS模型优化检索/ATMS model optimizes retrieval]\n        D --> D1[高质量3D检索/High-quality 3D retrieval]\n        D --> D2[可控4D生成/Controlled 4D generation]"
    },
    {
      "title": "Learning Dynamic Scene Reconstruction with Sinusoidal Geometric Priors",
      "authors": "Tian Guo, Hui Yuan, Philip Xu, David Elizondo",
      "institution": "De Montfort University",
      "link": "https://arxiv.org/pdf/2512.22295",
      "code": null,
      "tags": [
        "3D scene reconstruction",
        "SirenPose",
        "sinusoidal representation networks",
        "geometric priors",
        "spatiotemporal consistency",
        "dynamic 3D reconstruction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09fea4b3f99fd0198624beda8099c963456afad518a9c74cf1532ea8667df82e_w640_q70.webp",
      "contributions": "1. Proposes a novel loss function \"SirenPose\" that combines periodic activation from SIREN networks with geometric priors from keypoint structures. 2. Introduces physics-inspired constraint mechanisms to enforce coherent keypoint predictions across spatial and temporal dimensions. 3. Expands the training dataset to 600,000 annotated instances to support robust learning and demonstrates significant improvements in spatiotemporal consistency metrics.",
      "summary": "This paper addresses the challenge of maintaining motion accuracy and spatiotemporal consistency in dynamic 3D scene reconstruction from monocular videos. It proposes a novel loss function called SirenPose, which integrates sinusoidal representation networks with geometric keypoint priors and physics-based constraints. Experiments show that models using SirenPose achieve superior performance in handling rapid motion and complex scene changes compared to prior methods.",
      "mindmap": "graph TB\n        A[Learning Dynamic Scene Reconstruction with Sinusoidal Geometric Priors] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法在快速运动多目标场景中难以保持运动建模精度和时空一致性/Existing methods struggle with motion accuracy and spatiotemporal consistency in fast-moving multi-target scenes]\n        C --> C1[提出SirenPose损失函数，结合SIREN的周期激活特性和关键点几何先验/Propose SirenPose loss, combining SIREN's periodic activation and keypoint geometric priors]\n        C --> C2[引入物理启发的约束机制，确保时空维度上关键点预测的一致性/Introduce physics-inspired constraints for coherent keypoint predictions across space and time]\n        D --> D1[模型在时空一致性指标上相比现有方法有显著提升/Models show significant improvement in spatiotemporal consistency metrics vs. prior methods]\n        D --> D2[在处理快速运动和复杂场景变化上表现出优越性能/Demonstrates superior performance in handling rapid motion and complex scene changes]"
    },
    {
      "title": "Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware",
      "authors": "Vesal Ahsani, Babak Hossein Khalaj",
      "institution": "Sharif University of Technology",
      "link": "https://arxiv.org/pdf/2512.22298",
      "code": null,
      "tags": [
        "human activity recognition",
        "driver monitoring systems",
        "edge AI",
        "quantization",
        "temporal decision head",
        "confounder-aware labeling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bd52e06dc77080ab346fc3d6fe46b900bf06b5c1eaeb6ae89801ac125ee7c51_w640_q70.webp",
      "contributions": "1. A deployable single-camera driver behavior recognition pipeline optimized for low-cost edge hardware (Raspberry Pi 5 and Google Coral Edge TPU). 2. A confounder-aware label design to reduce false positives from visually similar actions. 3. A temporal decision head that generates stable alerts based on sustained, confident predictions rather than noisy per-frame outputs.",
      "summary": "This paper presents a real-time driver behavior recognition system designed for low-cost edge hardware to address the challenges of compute, power, and cost constraints in vehicles. The method combines a compact vision model, confounder-aware labeling, and a temporal decision head to recognize 17 distraction and drowsiness-related behaviors. The optimized system achieves 16 FPS on a Raspberry Pi 5 and 25 FPS on a Coral Edge TPU, enabling practical deployment for in-cabin monitoring.",
      "mindmap": "graph TB\n        A[Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[实时DMS需求 / Real-time DMS needs low latency, low cost, low power]\n        C --> C1[紧凑单摄像头系统 / Compact single-camera pipeline]\n        C1 --> C2[紧凑视觉模型 / Compact per-frame vision model]\n        C1 --> C3[抗混淆标签设计 / Confounder-aware label design]\n        C1 --> C4[时序决策头 / Temporal decision head]\n        D --> D1[性能: 16 FPS (RPi5), 25 FPS (Edge TPU) / Performance: 16 FPS (RPi5), 25 FPS (Edge TPU)]\n        D --> D2[验证: 真实车辆测试 / Validation: Real in-vehicle tests]"
    },
    {
      "title": "Attack-Aware Deepfake Detection under Counter-Forensic Manipulations",
      "authors": "Noor Fatima, Hasan Faraz Khan, Muzammil Behzad",
      "institution": "King Fahd University of Petroleum and Minerals (KFUPM), SDAIA-KFUPM Joint Research Center for Artificial Intelligence",
      "link": "https://arxiv.org/pdf/2512.22303",
      "code": null,
      "tags": [
        "deepfake detection",
        "counter-forensics",
        "red-team training",
        "test-time defense",
        "two-stream architecture",
        "tamper heatmaps"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45e4389b2e929ec3ecb92d5f6329f2086fd32a88abcf98391c61119cd497cc8f_w640_q70.webp",
      "contributions": "1. Proposes an attack-aware deepfake detection method combining red-team training with randomized test-time defense for robustness against counter-forensic manipulations. 2. Introduces a two-stream architecture with a lightweight residual adapter for fusing semantic and forensic features, and a weakly supervised FPN-style head for generating tamper heatmaps. 3. Establishes a practical, modular, and data-efficient baseline with well-calibrated probabilities and actionable evidence, evaluated on standard and challenging surveillance-style datasets.",
      "summary": "This paper addresses the challenge of robust deepfake detection under realistic counter-forensic attacks. It proposes a two-stream model trained with worst-case adversarial manipulations and defended at test-time with random jitters, which achieves strong performance, reliable probability calibration, and useful localization heatmaps. The method provides a practical and data-efficient baseline for attack-aware detection in real-world conditions.",
      "mindmap": "graph TB\n        A[”Attack-Aware Deepfake Detection under Counter-Forensic Manipulations”] --> B[”核心问题/Problem: Robust detection under realistic counter-forensic attacks”]\n        A --> C[”主要方法/Method: Red-team training + Test-time defense in a two-stream architecture”]\n        A --> D[”关键结果/Results: Near-perfect attack ranking, low calibration error, actionable heatmaps”]"
    },
    {
      "title": "PortionNet: Distilling 3D Geometric Knowledge for Food Nutrition Estimation",
      "authors": "Darrin Bright, Rakshith Raj, Kanchan Keisham",
      "institution": "Vellore Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.22304",
      "code": null,
      "tags": [
        "food volume and nutrition estimation",
        "knowledge distillation",
        "cross-modal learning",
        "point cloud",
        "RGB-to-Geometry Adapter",
        "dual-mode training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fecf7cb96cd72089359940c6705a9ff3efce7eba9810d635e1bbc23891d97d05_w640_q70.webp",
      "contributions": "1. Proposed PortionNet, a cross-modal knowledge distillation framework that enables RGB models to learn 3D geometric features from point clouds during training, eliminating the need for depth sensors at inference. 2. Introduced a dual-mode training strategy with a lightweight RGB-to-Geometry Adapter that learns to generate pseudo-3D features from standard RGB images. 3. Achieved state-of-the-art performance on MetaFood3D for volume and energy estimation and demonstrated strong generalization on SimpleFood45.",
      "summary": "The paper addresses the challenge of estimating food nutrition from a single RGB image, which lacks 3D information. It proposes PortionNet, a framework that uses knowledge distillation to teach an RGB model geometric reasoning from point cloud data during training, requiring only RGB images at test time. The method achieves state-of-the-art accuracy on benchmark datasets, demonstrating effective 3D knowledge transfer without specialized hardware.",
      "mindmap": "graph TB\n        A[PortionNet: Distilling 3D Geometric Knowledge for Food Nutrition Estimation] --> B(核心问题/Problem: 从单张RGB图像进行食物营养估计缺乏3D信息/Accurate food nutrition estimation from single RGB images lacks 3D information.)\n        A --> C(主要方法/Method: 跨模态知识蒸馏，从点云学习几何特征/Cross-modal knowledge distillation learns geometric features from point clouds.)\n        A --> D(关键结果/Results: 在MetaFood3D上取得SOTA，在SimpleFood45上展示强泛化性/Achieves SOTA on MetaFood3D, shows strong generalization on SimpleFood45.)"
    },
    {
      "title": "MoFu: Scale-Aware Modulation and Fourier Fusion for Multi-Subject Video Generation",
      "authors": "Run Ling, Ke Cao, Jian Lu, Ao Ma, Haowei Liu, Runze He, Changwei Wang, Rongtao Xu, Yihua Shao, Zhanjie Zhang, Peng Wu, Guibing Guo, Wei Feng, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law, Xingwei Wang",
      "institution": "JD.com, Inc., Northeastern University, University of Science and Technology of China, Chongqing University of Post and Telecommunications, University of Chinese Academy of Sciences, Northwestern Polytechnical University",
      "link": "https://arxiv.org/pdf/2512.22310",
      "code": null,
      "tags": [
        "video generation",
        "multi-subject video generation",
        "scale-aware modulation",
        "fourier fusion",
        "permutation invariance"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb2e690ce0802d8b6f90a803d45abda742cd1de636984d46b6cf4117642746da_w640_q70.webp",
      "contributions": "1. Proposes Scale-Aware Modulation (SMO), an LLM-guided module to extract implicit scale cues from text prompts to address scale inconsistency. 2. Introduces a Fourier Fusion strategy using Fast Fourier Transform to process reference features for permutation-invariant generation. 3. Designs a dedicated benchmark and a Scale-Permutation Stability Loss to evaluate and jointly optimize for scale consistency and permutation invariance.",
      "summary": "This paper proposes MoFu, a framework for multi-subject video generation that tackles scale inconsistency and permutation sensitivity. It introduces a Scale-Aware Modulation module and a Fourier Fusion strategy, validated by a new benchmark and a stability loss. Experiments show MoFu outperforms existing methods in preserving natural scale, subject fidelity, and visual quality.",
      "mindmap": "graph TB\n        A[MoFu: Multi-Subject Video Generation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[尺度不一致/Scale Inconsistency]\n        B --> B2[排列敏感性/Permutation Sensitivity]\n        C --> C1[尺度感知调制/Scale-Aware Modulation (SMO)]\n        C --> C2[傅里叶融合/Fourier Fusion]\n        C --> C3[稳定性损失/Scale-Permutation Stability Loss]\n        D --> D1[超越现有方法/Outperforms Existing Methods]\n        D --> D2[保持自然尺度与保真度/Preserves Natural Scale & Fidelity]"
    },
    {
      "title": "LangPrecip: Language-Aware Multimodal Precipitation Nowcasting",
      "authors": "Xudong Ling, Tianxi Huang, Qian Dong, Tao He, Chaorong Li, Guiduo Duan",
      "institution": "University of Electronic Science and Technology of China (UESTC), Chengdu Textile College, Yibin University",
      "link": "https://arxiv.org/pdf/2512.22317",
      "code": null,
      "tags": [
        "weather forecasting",
        "multimodal nowcasting",
        "rectified flow",
        "semantic motion constraint",
        "latent space integration",
        "large-scale dataset"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff97192e450e6a7b03dee1e2aabdfe42754a4d8248f0398395932ec97689a42d_w640_q70.webp",
      "contributions": "1. Proposed LangPrecip, a language-aware multimodal nowcasting framework that uses meteorological text as a semantic motion constraint to guide precipitation evolution. 2. Introduced LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. 3. Formulated nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm for efficient and physically consistent multimodal integration.",
      "summary": "The paper proposes LangPrecip, a novel framework that integrates meteorological text descriptions with radar data to constrain precipitation nowcasting. By formulating the problem as semantically constrained trajectory generation using Rectified Flow, it achieves significant performance gains, especially for heavy rainfall at long lead times, as demonstrated on Swedish and MRMS datasets.",
      "mindmap": "graph TB\n        Root[LangPrecip: Language-Aware Multimodal Precipitation Nowcasting] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: 短期降水临近预报存在不确定性，现有方法依赖视觉条件，未来运动约束弱]\n        Method[主要方法/Method: 提出语言感知多模态框架，将气象文本作为语义运动约束，在Rectified Flow范式下进行潜空间集成]\n        Results[关键结果/Results: 在瑞典和MRMS数据集上超越SOTA，在80分钟预见期，强降水CSI提升超60%和19%]"
    },
    {
      "title": "VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning",
      "authors": "Yang Ding, Yizhen Zhang, Xin Lai, Ruihang Chu, Yujiu Yang",
      "institution": "Tsinghua University, The Chinese University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.22315",
      "code": "https://github.com/zsgvivo/VideoZoomer",
      "tags": [
        "video understanding",
        "agentic framework",
        "temporal zoom",
        "reinforcement learning",
        "long video reasoning",
        "multimodal large language models"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ab9b4b645f4fff1b4f548256b858cb41da2b35db78b1083f110e983d60c3547e_w640_q70.webp",
      "contributions": "1. Proposes VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control visual focus during reasoning for long videos. 2. Introduces a two-stage training strategy combining supervised fine-tuning on distilled trajectories with reinforcement learning to refine the agentic policy. 3. Demonstrates strong performance across long video benchmarks, surpassing open-source models and rivaling proprietary systems with superior efficiency.",
      "summary": "The paper addresses the limitation of Multimodal LLMs in understanding long videos due to context window constraints. It proposes VideoZoomer, an agentic framework that dynamically selects and zooms into key temporal moments for fine-grained evidence gathering, trained with a two-stage strategy. The resulting 7B model achieves state-of-the-art performance on long video reasoning benchmarks with high efficiency.",
      "mindmap": "graph TB\n        A[VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[长视频理解受限/Limited Long Video Understanding]\n        B1 --> B2[上下文窗口限制/Context Window Limitation]\n        B1 --> B3[均匀采样忽略关键证据/Uniform Sampling Overlooks Evidence]\n        C --> C1[代理框架/Agentic Framework]\n        C1 --> C2[动态时间聚焦/Dynamic Temporal Focusing]\n        C2 --> C3[从粗到细推理/Coarse-to-Fine Reasoning]\n        C --> C4[两阶段训练/Two-Stage Training]\n        C4 --> C5[监督微调/Supervised Fine-Tuning]\n        C4 --> C6[强化学习/Reinforcement Learning]\n        D --> D1[性能强劲/Strong Performance]\n        D1 --> D2[超越开源模型/Surpasses Open-Source Models]\n        D1 --> D3[媲美专有系统/Rivals Proprietary Systems]\n        D --> D4[高效推理/Efficient Reasoning]\n        D4 --> D5[低帧预算/Reduced Frame Budget]"
    },
    {
      "title": "SpotEdit: Selective Region Editing in Diffusion Transformers",
      "authors": "Zhibin Qin, Zhenxiong Tan, Zeqing Wang, Songhua Liu, Xinchao Wang",
      "institution": "National University of Singapore, Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.22323",
      "code": "https://biangbiang0321.github.io/SpotEdit.github.io/",
      "tags": [
        "diffusion models",
        "Diffusion Transformers",
        "selective region editing",
        "training-free",
        "perceptual similarity",
        "dynamic fusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4788068dfc021eb102e739694d672f72a617b80ada2a17fbe2ccbc2780fc1287_w640_q70.webp",
      "contributions": "1. Proposes a training-free framework (SpotEdit) for selective region editing in Diffusion Transformers, reducing redundant computation. 2. Introduces SpotSelector to identify stable, unmodified regions via perceptual similarity and skip their denoising. 3. Introduces SpotFusion to adaptively blend reused conditional features with edited tokens, preserving coherence and quality.",
      "summary": "The paper addresses the inefficiency of full-image regeneration in diffusion-based editing when only small regions need modification. It proposes SpotEdit, a training-free framework that selectively updates only modified regions using a selector for stable areas and a fusion mechanism for coherence. This approach reduces computation and maintains fidelity in unedited areas for efficient, precise editing.",
      "mindmap": "graph TB\n        A[SpotEdit: Selective Region Editing in Diffusion Transformers] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[全图去噪冗余/Full-image denoising is redundant for small edits]\n        C --> C1[SpotSelector: 识别稳定区域/Identifies stable regions via perceptual similarity]\n        C --> C2[SpotFusion: 动态特征融合/Dynamically fuses features for coherence]\n        D --> D1[高效编辑/Efficient editing]\n        D --> D2[保持未修改区域保真度/Preserves fidelity in unchanged areas]"
    },
    {
      "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents",
      "authors": "Shaofei Cai, Yulei Qin, Haojia Lin, Zihan Xu, Gang Li, Yuchen Shi, Zongyi Li, Yong Mao, Siqi Cai, Xiaoyu Tan, Yitao Liang, Ke Li, Xing Sun",
      "institution": "Peking University, Tencent",
      "link": "https://arxiv.org/pdf/2512.22322",
      "code": "https://huggingface.co/collections/yolay/smartsnap",
      "tags": [
        "agent system",
        "self-verifying agent",
        "proactive evidence seeking",
        "LLM-as-a-Judge",
        "3C Principles",
        "agentic reinforcement learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61f493094954c71169bd505d339e16726dea8fffb8a79860e20efe7a94cff8ec_w640_q70.webp",
      "contributions": "1. Proposed SmartSnap, a paradigm shift from passive, post-hoc task verification to proactive, in-situ self-verification by the agent itself. 2. Introduced the Self-Verifying Agent, a new agent type with dual missions to complete tasks and prove accomplishment via curated snapshot evidences guided by 3C Principles (Completeness, Conciseness, Creativity). 3. Demonstrated that the SmartSnap paradigm enables scalable training of LLM-driven agents, achieving significant performance gains (up to 26.08% and 16.66%) on mobile tasks and competitive results against larger models.",
      "summary": "The paper addresses the scalability bottleneck in agentic RL caused by costly and unreliable post-hoc task verification. It proposes SmartSnap, a paradigm where agents proactively seek minimal, decisive snapshot evidence to prove task completion during execution, guided by 3C Principles. Experiments show this approach significantly improves agent performance and enables scalable training, achieving competitive results with much larger models.",
      "mindmap": "graph TB\n        A[SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Passive, post-hoc verification is costly and unreliable for agentic RL]\n        C[主要方法/Method: Proactive self-verification via Self-Verifying Agent and 3C Principles]\n        D[关键结果/Results: Performance gains up to 26.08%; competitive with larger models]"
    },
    {
      "title": "DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models",
      "authors": "Jianrong Zhang, Hehe Fan, Yi Yang",
      "institution": "University of Technology Sydney, Zhejiang University",
      "link": "https://arxiv.org/pdf/2512.22324",
      "code": "https://jiro-zhang.github.io/DeMoGen/",
      "tags": [
        "human motion generation",
        "energy-based diffusion model",
        "motion decomposition",
        "compositional training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3a4e4c44b030f7c77d17ff4917930237054b6fbd2ff7b40464abc4563cf33f_w640_q70.webp",
      "contributions": "1. Proposes DeMoGen, a compositional training paradigm using an energy-based diffusion model to decompose holistic motions into semantic sub-components without needing ground-truth for individual concepts., 2. Introduces three training variants (DeMoGen-Exp, DeMoGen-OSS, DeMoGen-SC) to encourage decompositional understanding and disentangle reusable motion primitives., 3. Constructs a text-decomposed dataset to support compositional training and demonstrates that decomposed concepts can be recombined to generate novel motions beyond the training distribution.",
      "summary": "This paper addresses the problem of decomposing holistic human motions into simpler, reusable primitives. It proposes DeMoGen, an energy-based diffusion model training paradigm with three variants to learn this decomposition without individual concept supervision. The method successfully disentangles motion concepts, which can then be flexibly recombined to generate diverse and novel motions.",
      "mindmap": "graph TB\n        A[DeMoGen: Decompositional Human Motion Generation] --> B[核心问题/Problem: 如何将整体运动分解为语义子组件/How to decompose holistic motion into semantic sub-components?]\n        A --> C[主要方法/Method: 基于能量的扩散模型与三种训练变体/Energy-based diffusion model with three training variants]\n        A --> D[关键结果/Results: 解耦可重用运动基元并支持重组生成/Disentangle reusable motion primitives and support recombination generation]"
    },
    {
      "title": "The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma",
      "authors": "Mariya Miteva, Maria Nisheva-Pavlova",
      "institution": "Not explicitly stated in provided content.",
      "link": "https://arxiv.org/pdf/2512.22331",
      "code": null,
      "tags": [
        "medical image analysis",
        "multi-view learning",
        "variational autoencoder",
        "latent representation learning",
        "radiomics",
        "glioblastoma"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bde275b3d90b700f19b613a2539cb5c16f7fb3637de09a820e24738011c2f8da_w640_q70.webp",
      "contributions": "1. Proposed a multi-view latent representation learning framework based on VAEs for integrating complementary MRI radiomic features. 2. Introduced independent probabilistic encoders for each modality to preserve modality-specific structure before fusion in a compact latent space. 3. Applied the learned latent embeddings for the non-invasive classification of MGMT promoter methylation status in glioblastoma.",
      "summary": "This paper addresses the challenge of non-invasively predicting MGMT promoter methylation in glioblastoma from MRI scans. It proposes a multi-view framework using variational autoencoders to integrate features from T1Gd and FLAIR MRI sequences by fusing them in a latent space, aiming to better preserve modality-specific information. The resulting latent embeddings are used for classification, offering a potential improvement over conventional unimodal or early-fusion radiomics approaches.",
      "mindmap": "graph TB\n        A[The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Non-invasive prediction of MGMT methylation in glioblastoma from MRI]\n        C[主要方法/Method: Multi-view VAE framework for latent fusion of T1Gd and FLAIR radiomic features]\n        D[关键结果/Results: Latent embeddings used for MGMT promoter methylation classification]"
    },
    {
      "title": "VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement",
      "authors": "Zhengfei Kuang, Rui Lin, Long Zhao, Gordon Wetzstein, Saining Xie, Sanghyun Woo",
      "institution": "Stanford University, Google, Google DeepMind, New York University",
      "link": "https://arxiv.org/pdf/2512.22351",
      "code": "vulcan-3d.github.io",
      "tags": [
        "3D scene understanding and manipulation",
        "Multimodal Large Language Models (MLLMs)",
        "3D object arrangement",
        "tool-augmented agents",
        "MCP-based API",
        "multi-agent framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79364171e71fa2e99be3aaae7f42a6f8bb502acdf59cc5033e98f9a1d424f8bb_w640_q70.webp",
      "contributions": "1. Introduced an MCP-based API to shift interaction from raw code to robust function-level updates, addressing MLLMs' weak visual grounding in 3D. 2. Augmented MLLMs with specialized visual tools for scene analysis, spatial information gathering, and action validation, creating a perceptual feedback loop. 3. Proposed a collaborative multi-agent framework with designated planning, execution, and verification roles to manage iterative, error-prone updates in complex tasks.",
      "summary": "This paper tackles the underexplored challenge of applying Multimodal Large Language Models (MLLMs) to complex 3D object arrangement. The proposed VULCAN system uses an MCP-based API, a suite of visual tools, and a multi-agent framework to enable robust, iterative 3D scene manipulation. The approach significantly outperforms baselines on a diverse set of 25 complex arrangement tasks.",
      "mindmap": "graph TB\n        A[VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>MLLMs在复杂3D场景操控中的应用未被充分探索<br>Application of MLLMs to complex 3D scene manipulation is underexplored]\n        C[主要方法/Method<br>引入MCP API、视觉工具套件和多智能体协作框架<br>Introduces MCP-based API, visual tool suite, and multi-agent collaborative framework]\n        D[关键结果/Results<br>在25个复杂任务上显著超越基线<br>Significantly outperforms baselines on 25 complex tasks]"
    },
    {
      "title": "Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides",
      "authors": "Olaide N. Oyelade, Oliver Hoxey, Yulia Humrye",
      "institution": "North Carolina A&T State University, University of Chichester, Yale University",
      "link": "https://arxiv.org/pdf/2512.22335",
      "code": null,
      "tags": [
        "medical image analysis",
        "vision transformer",
        "whole slide image",
        "HER2 scoring",
        "multi-modality",
        "tumor classification"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03d448dc8fb7520382bb6ea1a3e317817181ebbce215c0d5c387ed2268b2f407_w640_q70.webp",
      "contributions": "1. Proposed a novel mapping function to correlate malignant regions in H&E whole slide images with corresponding regions in IHC images for joint analysis. 2. Developed an end-to-end pipeline using a multi-stage vision transformer system for automatic pixel-level annotation of 4-way HER2 status scoring (0, 1+, 2+, 3+). 3. Embedded a clinically inspired HER2 scoring mechanism that accurately classifies HER2-negative and HER2-positive cases from whole slide images.",
      "summary": "This paper proposes an end-to-end pipeline using multi-stage vision transformers to jointly analyze H&E and IHC whole slide images for HER2 status scoring and tumor classification. The method introduces a novel mapping function to align modalities and provides pixel-level HER2 scoring. The results demonstrate high accuracy (0.94) for HER2 status prediction, showing the method's effectiveness comparable to human pathologists.",
      "mindmap": "graph TB\n    A[Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[挑战: 联合分析H&E和IHC图像进行HER2评分/Challenge: Jointly analyzing H&E and IHC images for HER2 scoring]\n    B --> B2[难点: 现有方法无法提供像素级HER2状态定位/Issue: Existing methods lack pixel-level HER2 status localization]\n    C --> C1[方法: 端到端多阶段视觉Transformer管道/Method: End-to-end multi-stage Vision Transformer pipeline]\n    C --> C2[创新: 新颖的映射函数关联H&E与IHC区域/Innovation: Novel mapping function to correlate H&E and IHC regions]\n    C --> C3[机制: 临床启发的4级HER2评分机制/Mechanism: Clinically inspired 4-way HER2 scoring mechanism]\n    D --> D1[结果: 肿瘤定位分类准确率高/Result: Good classification accuracy for tumor localization]\n    D --> D2[结果: HER2状态预测准确率0.94/Result: 0.94 accuracy for HER2 status prediction]\n    D --> D3[结论: 端到端ViT模型可用于联合评估H&E和IHC图像/Conclusion: End-to-end ViT models usable for jointly evaluating H&E and IHC images]"
    },
    {
      "title": "Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data",
      "authors": "Alaa Alahmadi, Mohamed Hasan",
      "institution": "Newcastle University, University of Leeds",
      "link": "https://arxiv.org/pdf/2512.22349",
      "code": null,
      "tags": [
        "medical image analysis",
        "pseudo-colouring",
        "few-shot learning",
        "prototypical networks",
        "ResNet-18",
        "explainability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4573948678ad6f25faec7ec6be8baccee385ce760fef63e3980935e84e21be0_w640_q70.webp",
      "contributions": "1. Introduces a perception-informed pseudo-colouring technique to encode clinically salient temporal ECG features (like QT-interval) into structured colour representations. 2. Demonstrates that this technique enables effective few-shot and one-shot learning for a complex physiological data task (drug-induced LQTS) using prototypical networks and ResNet-18. 3. Shows that the method improves model explainability by guiding attention to clinically meaningful features and that aggregating multiple cardiac cycles (mirroring human perceptual averaging) further boosts performance.",
      "summary": "This paper addresses the problems of data inefficiency and poor interpretability in deep learning models for physiological signal analysis. It proposes a human-inspired pseudo-colouring technique to encode ECG features, enabling effective few-shot learning and improving model explainability by focusing on clinically relevant signal components. The results demonstrate that incorporating human-like perceptual encoding can bridge data efficiency and interpretability in medical AI.",
      "mindmap": "graph TB\n        A[Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data] --> B1\n        A --> B2\n        A --> B3\n        B1[核心问题/Problem] --> C1[数据效率低/Lack of data efficiency]\n        B1 --> C2[可解释性差/Limited explainability]\n        B1 --> C3[临床可靠性受限/Constrained clinical reliability]\n        B2[主要方法/Method] --> D1[感知启发的伪着色技术/Perception-informed pseudo-colouring]\n        D1 --> E1[编码临床特征/Encode clinical features (e.g., QT-interval)]\n        D1 --> E2[结构化颜色表示/Structured colour representations]\n        B2 --> D2[原型网络与ResNet-18/Prototypical networks & ResNet-18]\n        B2 --> D3[聚合多个心跳周期/Aggregate multiple cardiac cycles]\n        B3[关键结果/Results] --> F1[实现少样本与单样本学习/Achieve few-shot & one-shot learning]\n        B3 --> F2[提升可解释性/Improve explainability (guide attention)]\n        B3 --> F3[桥接数据效率与因果推理/Bridge data efficiency & causal reasoning]"
    },
    {
      "title": "Self-Evaluation Unlocks Any-Step Text-to-Image Generation",
      "authors": "Xin Yu, Xiaojuan Qi, Zhengqi Li, Kai Zhang, Richard Zhang, Zhe Lin, Eli Shechtman, Tianyu Wang, Yotam Nitzan",
      "institution": "The University of Hong Kong, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.22374",
      "code": null,
      "tags": [
        "diffusion models",
        "text-to-image generation",
        "flow matching",
        "self-evaluation",
        "any-step inference",
        "from-scratch training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8074c4ce26dcd6ff20416ce7ac5c3c013208374f66871d4f0a55abd9bb7e52e9_w640_q70.webp",
      "contributions": "1. Introduces the Self-Evaluating Model (Self-E), a novel from-scratch training framework that combines local flow matching with a self-evaluation mechanism, eliminating the need for a pretrained teacher model. 2. Enables \"any-step\" inference, allowing the same model to perform both high-quality few-step and many-step generation, bridging the gap between local supervision and global matching paradigms. 3. Demonstrates competitive performance with state-of-the-art flow matching models at high step counts while excelling at very low step counts, offering a unified and scalable solution.",
      "summary": "The paper addresses the problem that traditional diffusion/flow models require many inference steps, and distillation methods need a pretrained teacher. It proposes Self-E, a model trained from scratch that uses self-evaluation as a dynamic teacher to enable high-quality generation at any number of steps. The results show Self-E excels at few-step generation and is competitive at many steps, providing a unified framework for efficient and scalable text-to-image synthesis.",
      "mindmap": "graph TB\n        Root[Self-Evaluation Unlocks Any-Step Text-to-Image Generation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Traditional models need many steps or a teacher model] --> Problem_Sub1[传统模型需要多步或教师模型/Traditional models need many steps or a teacher]\n        Method[主要方法/Method: Self-Evaluating Model (Self-E)] --> Method_Sub1[结合流匹配与自评估/Combines Flow Matching & Self-Evaluation]\n        Results[关键结果/Results: Unified any-step model] --> Results_Sub1[少步与多步均表现优异/Excels at both few-step and many-step]"
    },
    {
      "title": "LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition",
      "authors": "Elsen Ronando, Sozo Inoue",
      "institution": "Kyushu Institute of Technology, Universitas 17 Agustus 1945 Surabaya",
      "link": "https://arxiv.org/pdf/2512.22385",
      "code": null,
      "tags": [
        "few-shot learning",
        "exemplar selection",
        "large language model",
        "human activity recognition",
        "facility-location optimization",
        "PageRank"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/90a149428a6ff84d38e1ab6a741982394b05c9d5b98eaaa538dcd12183dbe7bf_w640_q70.webp",
      "contributions": "1. Proposes an LLM-Guided Exemplar Selection framework that incorporates semantic reasoning via LLM-generated knowledge priors (feature importance, inter-class confusability, budget multipliers) for HAR. 2. Integrates these semantic priors with multiple geometric and structural cues (margin-based validation, PageRank centrality, hubness penalization, facility-location optimization) for a unified exemplar scoring and selection process. 3. Demonstrates superior performance (88.78% macro F1-score on UCI-HAR) under strict few-shot conditions compared to classical selection methods like random sampling, herding, and k-center.",
      "summary": "This paper addresses the limitation of relying on large labeled datasets and purely geometric exemplar selection in Human Activity Recognition (HAR) by proposing an LLM-Guided Exemplar Selection framework. The method uses an LLM to generate semantic knowledge priors, which are combined with structural and geometric cues to select a compact, informative set of exemplars for few-shot learning. Evaluated on the UCI-HAR dataset, the framework outperforms classical selection approaches, showing that integrating semantic reasoning improves representative exemplar selection for wearable-sensor HAR.",
      "mindmap": "graph TB\n        A[LLM-Guided Exemplar Selection for Few-Shot HAR] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[依赖大数据集与几何选择 / Reliance on large datasets & geometric selection]\n        B --> B2[难以区分相似活动 / Hard to distinguish similar activities]\n        C --> C1[LLM生成语义先验 / LLM-generated semantic priors]\n        C --> C2[结合多线索优化 / Combine multiple cues for optimization]\n        D --> D1[性能超越基线 / Outperforms baselines (88.78% F1)]\n        D --> D2[语义先验有效 / Semantic priors are effective]"
    },
    {
      "title": "iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI",
      "authors": "Himanshu Naidu, Yuxiang Zhang, Sachin Mehta, Anat Caspi",
      "institution": "University of Washington",
      "link": "https://arxiv.org/pdf/2512.22392",
      "code": null,
      "tags": [
        "semantic segmentation",
        "on-device AI",
        "LiDAR depth estimation",
        "fused GPS/IMU",
        "sidewalk mapping",
        "accessibility"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7ffc517e9052bb4ade596ae33e68c84c5942243e638f1f15055e1560eb0b6f0_w640_q70.webp",
      "contributions": "1. Introduces iOSPointMapper, a mobile app for real-time, privacy-conscious sidewalk mapping using iPhones/iPads. 2. Leverages on-device semantic segmentation, LiDAR depth, and fused GPS/IMU to detect and localize sidewalk features like signs and poles. 3. Incorporates a user-guided annotation interface for validation and integrates collected data with the Transportation Data Exchange Initiative (TDEI).",
      "summary": "The paper addresses the costly and fragmented collection of sidewalk data by proposing iOSPointMapper, a mobile application that uses on-device AI, LiDAR, and sensor fusion for real-time detection and mapping of pedestrian infrastructure. The system includes a user validation interface and integrates data into a broader transportation dataset. Evaluations show its potential for scalable and enhanced pedestrian mapping.",
      "mindmap": "graph TB\n        Root(iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI) --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1(人行道数据收集成本高、碎片化且难以扩展/Sidewalk data collection is costly, fragmented, and hard to scale)\n        Method --> M1(移动应用使用设备端AI、LiDAR和传感器融合/Mobile app uses on-device AI, LiDAR, and sensor fusion)\n        Method --> M2(用户引导的标注界面进行验证/User-guided annotation interface for validation)\n        Results --> R1(评估显示人行道特征检测和空间映射的潜力/Evaluation shows potential for feature detection and spatial mapping)\n        Results --> R2(为可扩展的、以用户为中心的数据收集提供方法/Offers a scalable, user-centered approach to data collection)"
    },
    {
      "title": "DeFloMat: Detection with Flow Matching for Stable and Efficient Generative Object Localization",
      "authors": "Hansang Lee, Chaelin Lee, Nieun Seo, Joon Seok Lim, Helen Hong",
      "institution": "Seoul Women's University, Severance Hospital, Yonsei University College of Medicine",
      "link": "https://arxiv.org/pdf/2512.22406",
      "code": null,
      "tags": [
        "object detection",
        "Conditional Flow Matching",
        "Generative Object Detection",
        "Rectified Flow",
        "Ordinary Differential Equation",
        "Magnetic Resonance Enterography"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a84d309b1834782fd71a3208d1783483482053483ffa20811f690fd91c6683d9_w640_q70.webp",
      "contributions": "1. Proposes DeFloMat, a novel generative object detection framework that replaces the slow multi-step stochastic denoising of diffusion models with a fast, deterministic flow field based on Conditional Flow Matching and Rectified Flow. 2. Achieves state-of-the-art accuracy with only 3 inference steps, significantly outperforming prior diffusion-based detectors in speed and performance on a clinical MRE dataset. 3. Demonstrates superior localization stability and recall in the few-step regime, effectively resolving the trade-off between generative accuracy and inference efficiency for time-sensitive applications.",
      "summary": "The paper addresses the high latency of diffusion-based object detectors, which is impractical for clinical use. It proposes DeFloMat, a new framework that uses Conditional Flow Matching to create a deterministic flow for fast, stable object localization via an ODE solver. The method achieves superior accuracy with only 3 inference steps on a medical imaging dataset, setting a new standard for efficient generative detection.",
      "mindmap": "graph TB\n        A[DeFloMat: Detection with Flow Matching] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Diffusion-based detectors are too slow for clinical applications (e.g., Crohn's Disease detection in MRE)]\n        C[主要方法/Method: Replaces stochastic diffusion with deterministic flow field (Conditional Flow Matching, Rectified Flow) solved via ODE]\n        D[关键结果/Results: Achieves SOTA accuracy in only 3 steps, 1.4x performance gain, superior stability and recall]"
    },
    {
      "title": "Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy",
      "authors": "Amil Khan, Matheus Palhares Viana, Suraj Mishra, B.S. Manjunath",
      "institution": "UC Santa Barbara, Allen Institute for Cell Sciences",
      "link": "https://arxiv.org/pdf/2512.22423",
      "code": null,
      "tags": [
        "medical image segmentation",
        "hyperspherical learning",
        "native sparse attention",
        "anisotropic patch embed"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7af1d13c6f2fcc3e8d27fe1157700eff79fd4e0fccc0c4ba8b37ebb62c2043fd_w640_q70.webp",
      "contributions": "1. A 4B-parameter foundation model (Bright-4B) that learns on the unit hypersphere for segmenting subcellular structures directly from 3D brightfield volumes. 2. Novel architectural components: a hardware-aligned Native Sparse Attention mechanism, depth-width residual HyperConnections, and a soft Mixture-of-Experts for adaptive capacity. 3. A plug-and-play anisotropic patch embed that respects confocal point-spread and axial thinning for geometry-faithful 3D tokenization.",
      "summary": "The paper introduces Bright-4B, a 4-billion parameter foundation model designed for volumetric segmentation of subcellular structures directly from label-free 3D brightfield microscopy images. It employs novel architectural components like hyperspherical learning, native sparse attention, and an anisotropic patch embed to handle 3D context and anisotropic sampling. The model outperforms contemporary baselines in preserving fine structural detail across depth and cell types without requiring fluorescence or heavy post-processing.",
      "mindmap": "graph TB\n        A[Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy] --> B(核心问题/Problem: Robust 3D segmentation in brightfield microscopy depends on fluorescence or heavy post-processing.)\n        A --> C(主要方法/Method: A 4B-parameter foundation model with hyperspherical learning, native sparse attention, hyperconnections, mixture-of-experts, and anisotropic patch embed.)\n        A --> D(关键结果/Results: Produces accurate segmentations from brightfield alone, outperforms baselines, preserves detail across depth and cell types.)"
    },
    {
      "title": "FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning",
      "authors": "Ujunwa Mgboh, Rafi Ibn Sultan, Joshua Kim, Kundan Thind, Dongxiao Zhu",
      "institution": "Wayne State University, Henry Ford Health",
      "link": "https://arxiv.org/pdf/2512.22425",
      "code": null,
      "tags": [
        "medical image analysis",
        "transformer",
        "fluence map prediction",
        "physics-informed loss",
        "two-stage regression",
        "Swin UNETR"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d0e329d1bfd4c1f892f7333af6a3a4e76c5219cd192f715a25e502a35ef178_w640_q70.webp",
      "contributions": "1. Proposed FluenceFormer, a backbone-agnostic transformer framework for direct, geometry-aware fluence map regression. 2. Introduced a unified two-stage design (dose prior prediction followed by geometry-conditioned fluence regression) and the physics-informed Fluence-Aware Regression (FAR) loss. 3. Demonstrated the framework's generality across multiple transformer backbones and achieved state-of-the-art performance, significantly reducing energy error.",
      "summary": "This paper introduces FluenceFormer, a transformer-based framework for automating radiotherapy planning by predicting multi-beam fluence maps. The method uses a two-stage, geometry-aware regression approach with a novel physics-informed loss function. The results show that FluenceFormer outperforms existing methods, achieving a low energy error and improved structural fidelity in fluence map prediction.",
      "mindmap": "graph TB\n        A[FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Ill-posed inverse problem: complex anatomy-beam relationship / 病态逆问题: 解剖结构与射束强度的复杂关系]\n        B --> B2[CNN struggles with long-range dependencies / CNN难以捕捉长程依赖]\n        C --> C1[Two-stage transformer framework / 两阶段Transformer框架]\n        C1 --> C1_1[Stage 1: Global dose prior / 阶段1: 全局剂量先验]\n        C1 --> C1_2[Stage 2: Geometry-conditioned fluence regression / 阶段2: 几何条件化的注量图回归]\n        C --> C2[Fluence-Aware Regression (FAR) loss / 注量感知回归损失]\n        D --> D1[Reduced Energy Error to 4.5% / 能量误差降低至4.5%]\n        D --> D2[Improved structural fidelity (p<0.05) / 结构保真度显著提升]\n        D --> D3[Outperformed benchmark CNN & single-stage methods / 超越基准CNN与单阶段方法]"
    },
    {
      "title": "SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems",
      "authors": "Khalfalla Awedat, Mohamed Abidalrekab, Gurcan Comert, Mustafa Ayad",
      "institution": "SUNY Morrisville College, Portland State University, North Carolina A&T State University, SUNY Oswego",
      "link": "https://arxiv.org/pdf/2512.22439",
      "code": null,
      "tags": [
        "point cloud processing",
        "Graph Attention Networks",
        "LiDAR reconstruction",
        "beam dropout",
        "gated residual fusion",
        "sparse point cloud"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68e019420f70e8da99107deedb1af90b7e95ec95b9487f8fc6c872f9994d15e1_w640_q70.webp",
      "contributions": "1. Proposes SuperiorGAT, a novel graph attention-based framework for reconstructing missing elevation data in sparse LiDAR point clouds. 2. Introduces a beam-aware graph modeling approach for LiDAR scans combined with gated residual fusion and feed-forward refinement to achieve accurate reconstruction without increasing network depth. 3. Demonstrates superior performance in reconstruction error and geometric consistency across diverse environments compared to PointNet and deeper GAT baselines, validated through structured beam dropout simulation.",
      "summary": "This paper addresses the problem of LiDAR beam dropout and sparse resolution in autonomous systems by proposing SuperiorGAT, a graph attention network framework that reconstructs missing elevation information. The method models LiDAR scans as beam-aware graphs and uses gated residual fusion for accurate reconstruction without deeper networks. The results show it achieves lower error and better geometric consistency than baselines, offering a computationally efficient way to improve LiDAR resolution.",
      "mindmap": "graph TB\n        A[SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction] --> B[核心问题/Problem: LiDAR垂直分辨率固定与光束丢失导致点云稀疏]\n        A --> C[主要方法/Method: 基于光束感知图与门控残差融合的图注意力网络]\n        A --> D[关键结果/Results: 重建误差更低，几何一致性更好，结构完整性保持]"
    },
    {
      "title": "EmoCtrl: Controllable Emotional Image Content Generation",
      "authors": "Jingyuan Yang, Weibin Luo, Hui Huang",
      "institution": "Shenzhen University",
      "link": "https://arxiv.org/pdf/2512.22437",
      "code": null,
      "tags": [
        "image generation",
        "controllable generation",
        "emotion-aware generation",
        "diffusion models",
        "affective computing",
        "multi-modal learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1083bd0b74d6006ca00b929998df65fb57106e86c8784a14b3c3bc1e8cc5ce7a_w640_q70.webp",
      "contributions": "1. Introduces the novel task of Controllable Emotional Image Content Generation (C-EICG), which aims to generate images faithful to a content description while expressing a target emotion. 2. Proposes the EmoCtrl model, which incorporates textual and visual emotion enhancement modules to bridge abstract emotions to visual cues using learned emotion tokens. 3. Constructs a supporting dataset annotated with content, emotion, and affective prompts, and demonstrates through experiments and user studies that EmoCtrl outperforms existing methods in achieving both content faithfulness and expressive emotion control.",
      "summary": "The paper addresses the gap between content-faithful and emotion-expressive image generation by proposing EmoCtrl, a model for Controllable Emotional Image Content Generation. EmoCtrl uses textual and visual modules to learn emotion tokens that enrich affective expression while maintaining semantic content. Experiments and user studies show it outperforms existing methods in balancing content accuracy and emotional tone.",
      "mindmap": "graph TB\n        A[EmoCtrl: Controllable Emotional Image Content Generation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有模型缺乏情感控制或内容失真<br>Existing models lack emotion control or distort content]\n        C --> C1[提出EmoCtrl模型与数据集<br>Propose EmoCtrl model and dataset]\n        C1 --> C2[文本与视觉情感增强模块<br>Textual and visual emotion enhancement modules]\n        C2 --> C3[学习情感令牌<br>Learn emotion tokens]\n        D --> D1[定量定性实验表现优异<br>Quantitative and qualitative experiments show superiority]\n        D --> D2[用户研究符合人类偏好<br>User studies align with human preference]\n        D --> D3[泛化至创意应用<br>Generalizes to creative applications]"
    },
    {
      "title": "LECalib: Line-Based Event Camera Calibration",
      "authors": "Zibin Liu, Banglei Guana, Yang Shanga, Zhenbao Yu, Yifei Bian, Qifeng Yu",
      "institution": "National University of Defense Technology, Wuhan University",
      "link": "https://arxiv.org/pdf/2512.22441",
      "code": "https://github.com/Zibin6/line_based_event_camera_calib",
      "tags": [
        "camera calibration",
        "event camera",
        "line detection",
        "geometric calibration",
        "non-linear optimization",
        "stereo calibration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbbac4b87dc7bfa86714585a9bb2fe18a9dc39835aac2892d57f5396cbffbfc6_w640_q70.webp",
      "contributions": "1. Proposes a line-based calibration framework for event cameras that uses geometric lines from common man-made objects, eliminating the need for dedicated flashing patterns or calibration boards. 2. Introduces a method to detect lines directly from raw event streams and leverages an event-line calibration model to generate an initial parameter guess suitable for both planar and non-planar lines. 3. Validates the method's feasibility and accuracy through both simulation and real-world experiments on monocular and stereo event camera setups.",
      "summary": "This paper addresses the problem of time-consuming and manually intensive calibration for event cameras. It proposes LECalib, a framework that calibrates event cameras by detecting geometric lines directly from event streams and using them in a linear initialization and non-linear refinement process. The method is validated as feasible and accurate for both monocular and stereo setups.",
      "mindmap": "graph TB\n        A[LECalib: Line-Based Event Camera Calibration] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法耗时且需人工标定物 / Existing methods are time-consuming and require manual calibration objects]\n        C --> C1[从事件流直接检测线特征 / Detect lines directly from event streams]\n        C --> C2[使用事件-线标定模型初始化 / Use event-line model for initial guess]\n        C --> C3[非线性优化精修参数 / Refine parameters with non-linear optimization]\n        D --> D1[仿真与真实实验验证 / Validated in simulation and real-world]\n        D --> D2[支持单目与立体事件相机 / Works for monocular and stereo event cameras]"
    },
    {
      "title": "Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework",
      "authors": "Zhicheng Zhao, Yuancheng Xu, Andong Lu, Chenglong Li, Jin Tang",
      "institution": "Anhui University, China Electronics Technology Group Corporation (38th Research Institute)",
      "link": "https://arxiv.org/pdf/2512.22447",
      "code": null,
      "tags": [
        "object detection",
        "optical-SAR fusion",
        "missing modality",
        "quality-aware fusion",
        "dynamic fusion",
        "orthogonal constraint"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69b5db28a2b2a43b3d55d1592c7f26e5ce4421dd707ae3e19282c69c4e894e50_w640_q70.webp",
      "contributions": "1. Proposed a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection under missing or degraded modalities. 2. Designed a Dynamic Modality Quality Assessment (DMQA) module that uses learnable reference tokens to iteratively assess feature reliability and identify degraded regions. 3. Developed an Orthogonal Constraint Normalization Fusion (OCNF) module that uses orthogonal constraints to preserve modality independence and dynamically adjust fusion weights based on reliability scores to suppress unreliable features.",
      "summary": "This paper addresses the problem of robust object detection using optical and SAR images when one modality is missing or degraded. The proposed QDFNet method dynamically assesses feature quality and adaptively fuses information using learnable tokens and orthogonal constraints. Experiments show it outperforms other methods, especially when modalities are partially missing.",
      "mindmap": "graph TB\n        Root[”Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework”] --> Problem[”核心问题/Problem: Optical-SAR image pairs are often misaligned or missing, degrading fusion-based detection.”]\n        Root --> Method[”主要方法/Method: Proposes QDFNet with DMQA (quality assessment) and OCNF (orthogonal fusion) modules.”]\n        Root --> Results[”关键结果/Results: Superior performance on SpaceNet6-OTD and OGSOD-2.0 datasets, especially under missing data.”]"
    },
    {
      "title": "SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues",
      "authors": "Md Abu Obaida Zishan, Annajiat Alim Rasel",
      "institution": "BRAC University",
      "link": "https://arxiv.org/pdf/2512.22449",
      "code": "https://github.com/MohammedZ666/SonoVision",
      "tags": [
        "object detection",
        "EfficientDet-D2",
        "sound cues",
        "on-device inference",
        "assistive technology",
        "Flutter"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e6b53eaa499e058f61b5d29ec05bfaf1db6d84a82c9c2ee9d18d6a68f4bad882_w640_q70.webp",
      "contributions": "1. Developed SonoVision, a smartphone application that uses real-time object detection and spatialized sound cues to help visually impaired individuals locate objects independently. 2. Implemented the system using the Flutter framework and the EfficientDet-D2 model, enabling it to function completely offline for safety and user-friendliness. 3. Designed an intuitive auditory interface where object location (left, center, right) is indicated by playing sinusoidal sounds in the corresponding ear(s) of the user's headphones.",
      "summary": "The paper presents SonoVision, a smartphone app that helps visually impaired people locate objects by using the EfficientDet-D2 model for real-time detection and providing directional sound cues through headphones. The application is built with Flutter and works offline, aiming to increase user independence and safety. The authors conclude that this approach can significantly assist users in a user-friendly manner.",
      "mindmap": "graph TB\n        A[SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues] --> B(核心问题/Problem: Visually impaired individuals struggle to locate objects, hindering independence and safety.)\n        A --> C(主要方法/Method: Smartphone app uses EfficientDet-D2 for object detection and provides directional sound cues (sinusoidal tones) via headphones.)\n        A --> D(关键结果/Results: An offline, user-friendly application that can help visually impaired users locate objects more independently.)"
    },
    {
      "title": "SAM 3D for 3D Object Reconstruction from Remote Sensing Images",
      "authors": "Junsheng Yao, Lichao Mou, Qingyu Li",
      "institution": "The Chinese University of Hong Kong, Shenzhen; MedAI Technology",
      "link": "https://arxiv.org/pdf/2512.22452",
      "code": null,
      "tags": [
        "3D reconstruction",
        "SAM 3D",
        "monocular reconstruction",
        "urban modeling",
        "remote sensing",
        "foundation model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9750167169c225301e2e37f4bf4b32716990121a9c6c455a260b102eb2835f5c_w640_q70.webp",
      "contributions": "1. Presents the first systematic evaluation of the general-purpose foundation model SAM 3D for monocular remote sensing building reconstruction. 2. Benchmarks SAM 3D against TRELLIS on the NYC urban dataset using FID and CMMD metrics, showing superior roof geometry and boundary sharpness. 3. Extends SAM 3D to urban scene reconstruction via a novel segment-reconstruct-compose pipeline, demonstrating its potential for broader urban modeling.",
      "summary": "This paper evaluates the SAM 3D foundation model for reconstructing 3D buildings from single remote sensing images. It benchmarks SAM 3D against TRELLIS, finding it produces more coherent geometry and sharper boundaries. The work also proposes a pipeline to extend the model for urban scene reconstruction, highlighting its potential and practical limitations for scalable urban modeling.",
      "mindmap": "graph TB\n        Root(”SAM 3D for 3D Object Reconstruction from Remote Sensing Images<br>论文标题”) --> Problem(”Monocular 3D building reconstruction is essential but challenging<br>核心问题：单目3D建筑重建重要但困难”)\n        Root --> Method(”Systematic evaluation & benchmark of SAM 3D; Segment-Reconstruct-Compose pipeline<br>主要方法：系统评估SAM 3D；分割-重建-组合流程”)\n        Root --> Results(”SAM 3D outperforms TRELLIS; Potential for urban scene modeling demonstrated<br>关键结果：SAM 3D优于TRELLIS；展示了城市场景建模潜力”)"
    },
    {
      "title": "Comparing Object Detection Models for Electrical Substation Component Mapping",
      "authors": "Haley Mody, Namish Bansal, Dennies Kiprono Bor, Edward J. Oughton",
      "institution": "None (No affiliations or email domains provided in the given content)",
      "link": "https://arxiv.org/pdf/2512.22454",
      "code": null,
      "tags": [
        "object detection",
        "YOLOv8",
        "YOLOv11",
        "RF-DETR",
        "substation mapping",
        "computer vision"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/347825774ed597bd332295edbcb741eaa0167e22f948cf6387f16bda5dec5bd2_w640_q70.webp",
      "contributions": "1. Training and comparative evaluation of three state-of-the-art object detection models (YOLOv8, YOLOv11, RF-DETR) on a manually labeled dataset of US electrical substation images. 2. Analysis of model performance based on detection accuracy, precision, and efficiency to identify strengths and limitations for the specific application. 3. Demonstration of a practical use case by utilizing the best-performing model(s) to effectively map substation components across the United States, showcasing autonomous infrastructure assessment.",
      "summary": "This paper addresses the labor-intensive problem of manually mapping electrical substation components by training and comparing three computer vision models (YOLOv8, YOLOv11, RF-DETR) on a labeled dataset of US substation images. The models are evaluated on accuracy, precision, and efficiency to determine the most reliable solution for large-scale, autonomous substation component mapping. The research concludes by identifying the best model and demonstrating its application for mapping substation infrastructure across the United States.",
      "mindmap": "graph TB\n        A[Comparing Object Detection Models for Electrical Substation Component Mapping] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Manual substation mapping is time-consuming and labor-intensive] --> B1[关键影响/Impact<br>Grid failure has economic & safety implications]\n        C[主要方法/Method<br>Train & compare 3 CV models (YOLOv8, YOLOv11, RF-DETR)] --> C1[评估指标/Evaluation<br>Accuracy, Precision, Efficiency]\n        D[关键结果/Results<br>Identify best model for reliable, large-scale mapping] --> D1[应用案例/Use Case<br>Map US substation components]"
    },
    {
      "title": "Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing",
      "authors": "Sukhyun Jeong, Yong-Hoon Choi",
      "institution": "Kwangwoon University",
      "link": "https://arxiv.org/pdf/2512.22464",
      "code": "https://github.com/jayze3736/PGR2M",
      "tags": [
        "motion generation and editing",
        "residual vector quantization (RVQ)",
        "pose code",
        "transformer",
        "text-to-motion",
        "motion editing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fc4d2cdfd610913fc29db703161a253780380e59619b49c5b160c01670eabac_w640_q70.webp",
      "contributions": "1. Proposes a hybrid motion representation (PGR²M) that augments interpretable pose codes with residual codes learned via RVQ to capture both coarse structure and fine-grained details. 2. Introduces a pose-guided RVQ tokenizer and a two-stage Transformer architecture (base and refine) for generating and refining motion from text. 3. Demonstrates improved performance in generation and editing tasks over baselines through quantitative metrics and user studies, while preserving semantic alignment and editability.",
      "summary": "This paper addresses the limitation of pose-code-based motion generation in capturing subtle temporal dynamics by introducing PGR²M, a hybrid representation combining interpretable pose codes with residual codes via RVQ. The method uses a two-stage Transformer to generate pose codes and then refine them with residual details, conditioned on text. Experiments show it outperforms baselines in both generation and editing while enabling intuitive, structure-preserving motion edits.",
      "mindmap": "graph TB\n        A[Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Pose-code frameworks struggle to capture subtle temporal dynamics and high-frequency details.]\n        C[主要方法/Method: Hybrid representation (PGR²M) with pose codes and residual codes (RVQ), using a two-stage Transformer.]\n        D[关键结果/Results: Improves FID and reconstruction metrics; enables intuitive, structure-preserving edits.]"
    },
    {
      "title": "Event-based high temporal resolution measurement of shock wave motion field",
      "authors": "Taihang Lei, Banglei Guan, Minzu Liang, Pengju Sun, Jing Tao, Yang Shang, Qifeng Yu",
      "institution": "National University of Defense Technology",
      "link": "https://arxiv.org/pdf/2512.22474",
      "code": null,
      "tags": [
        "event-based vision",
        "event cameras",
        "shock wave measurement",
        "motion field reconstruction",
        "asymmetry estimation",
        "polar coordinate encoding"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c33eedd7cdaa2bde1d47f4aaa6a4e3d7b22bc50cf22f53cba8d989485748b0ee_w640_q70.webp",
      "contributions": "1. A novel framework using multiple event cameras to measure shock wave asymmetry with high spatiotemporal resolution. 2. A method involving polar coordinate event encoding and adaptive ROI extraction for revealing propagation patterns. 3. The derivation of a geometric model for event-based shock wave parameter estimation and 3D motion field reconstruction.",
      "summary": "This paper proposes a method for high-resolution shock wave measurement using multiple event cameras. It establishes a polar coordinate system to encode events, extracts shock fronts via iterative slope analysis, and derives models for parameter estimation and 3D reconstruction. The method achieves high-precision measurements with errors as low as 0.06% compared to pressure sensors, demonstrating significant progress in the field.",
      "mindmap": "graph TB\n        A[Event-based high temporal resolution measurement of shock wave motion field] --> B(核心问题/Problem: Fast, uneven shock wave propagation under unstable conditions)\n        A --> C(主要方法/Method: Multi-event-camera framework with polar encoding, adaptive ROI, iterative slope analysis)\n        A --> D(关键结果/Results: High-precision measurement, max error 5.20%, min error 0.06%)"
    },
    {
      "title": "Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection",
      "authors": "Zihan Liu, Xiangning Ren, Dezhang Kong, Yipeng Zhang, Meng Han",
      "institution": "Chengdu University, China University of Geosciences (Beijing), Zhejiang University, Zhejiang Lab",
      "link": "https://arxiv.org/pdf/2512.22483",
      "code": null,
      "tags": [
        "object detection",
        "Segment Anything Model",
        "semi-supervised learning",
        "knowledge distillation",
        "Mixture of Experts",
        "infrared small object detection"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c7412ec4948ed592cc4d01249cc14bc919c674a295820a0048ad8ccd18def86_w640_q70.webp",
      "contributions": "1. Proposed a Hierarchical Mixture of Experts (MoE) Adapter to adapt the Segment Anything Model (SAM) to the infrared domain and encode physical priors. 2. Introduced a novel two-stage semi-supervised paradigm (Scalpel-SAM) for knowledge distillation and transfer, requiring only 10% labeled data. 3. Demonstrated that the paradigm enables training lightweight downstream models with pseudo-labels, achieving performance comparable to or surpassing fully supervised models.",
      "summary": "This paper addresses the data scarcity and domain gap challenges in Infrared Small Object Detection (IR-SOT) by proposing Scalpel-SAM, a semi-supervised paradigm. It adapts the Segment Anything Model (SAM) using a Hierarchical MoE Adapter and a two-stage knowledge distillation/transfer process, enabling efficient downstream models to be trained with minimal annotations. Experiments show the method achieves performance on par with or better than fully supervised counterparts.",
      "mindmap": "graph TB\n        A[Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[高标注成本与领域鸿沟/High annotation cost & domain gap]\n        C --> C1[两阶段半监督范式/Two-stage semi-supervised paradigm]\n        C1 --> C1_1[先验引导知识蒸馏/Prior-Guided Knowledge Distillation]\n        C1 --> C1_2[部署导向知识迁移/Deployment-Oriented Knowledge Transfer]\n        C1_1 --> C1_1a[使用MoE适配器蒸馏SAM/Use MoE Adapter to distill SAM]\n        D --> D1[下游模型性能媲美全监督/Downstream models match or surpass fully supervised performance]"
    },
    {
      "title": "Tracking by Predicting 3-D Gaussians Over Time",
      "authors": "Tanish Baranwal, Himanshu Gaurav Singh, Jathushan Rajasegaran, Jitendra Malik",
      "institution": "University of California, Berkeley",
      "link": "https://arxiv.org/pdf/2512.22489",
      "code": "https://github.com/tekotan/video-gmae",
      "tags": [
        "video representation learning",
        "self-supervised learning",
        "Gaussian splatting",
        "masked autoencoder",
        "point tracking",
        "video understanding"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f366f3c8e3f11bb196fa35702e462eaeb3b1cbb9de970853f5ab467656a2947c_w640_q70.webp",
      "contributions": "1. Proposes Video-GMAE, a novel self-supervised method that learns video representations by encoding a sequence into a set of moving 3D Gaussians, enforcing temporal correspondence as an inductive bias. 2. Discovers that tracking emerges naturally from this pretraining, enabling zero-shot point tracking performance comparable to state-of-the-art methods. 3. Demonstrates superior performance after fine-tuning, achieving significant improvements on Kinetics and Kubric datasets over existing self-supervised video approaches.",
      "summary": "This paper introduces Video-GMAE, a self-supervised learning method that represents a video as a set of 3D Gaussian primitives moving over time. This representation enforces temporal consistency, allowing the model to learn strong correspondences and enabling zero-shot point tracking. The method outperforms existing self-supervised approaches on video understanding and tracking benchmarks after fine-tuning.",
      "mindmap": "graph TB\n        Root(”Tracking by Predicting 3-D Gaussians Over Time”) --> Problem(”核心问题/Problem: Self-supervised video representations lack strong temporal correspondence for tracking.”)\n        Root --> Method(”主要方法/Method: Video-GMAE: Masked Autoencoder predicts moving 3D Gaussian splats from video frames.”)\n        Root --> Results(”关键结果/Results: Emergent zero-shot tracking; SOTA performance after fine-tuning on Kinetics & Kubric.”)"
    },
    {
      "title": "Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment",
      "authors": "Hassan Wasswa, Timothy Lynar",
      "institution": "University of New South Wales",
      "link": "https://arxiv.org/pdf/2512.22488",
      "code": null,
      "tags": [
        "network intrusion detection",
        "concept drift",
        "latent space alignment",
        "graph neural network (GNN)",
        "IoT botnet detection",
        "variational autoencoder"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4c1a80d42c79bd3adbbc2c072359068d65253efabadc2bd6d3617f632439f72_w640_q70.webp",
      "contributions": "1. Proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining, reducing computational overhead and catastrophic forgetting. 2. Introduces a method using latent space representation learning and an alignment model to map new traffic to a historical latent space, preserving knowledge of past attacks. 3. Transforms latent representations into a graph structure and uses a Graph Attention Network (GAT) to capture inter-instance relationships among attack samples for improved detection.",
      "summary": "This paper addresses the problem of concept drift in IoT botnet detection by proposing a framework that trains a classifier once on historical traffic representations. It uses an alignment model to map new traffic to this learned latent space and a graph neural network to classify the data, maintaining robust performance without retraining. Experimental results on real-world datasets show the framework's effectiveness in dynamic IoT environments.",
      "mindmap": "graph TB\n        Root[”Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection / 面向真实世界物联网安全：概念漂移鲁棒的物联网僵尸网络检测”]\n        Root --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”AI模型依赖静态数据集 / AI models rely on stationary datasets”]\n        Problem --> P2[”真实流量存在概念漂移 / Real-world traffic suffers concept drift”]\n        Problem --> P3[”现有方案重训练开销大 / Existing solutions have high retraining cost”]\n        Method --> M1[”学习历史流量的潜在空间表示 / Learn latent space of historical traffic”]\n        Method --> M2[”对齐模型映射新流量 / Alignment model maps new traffic”]\n        Method --> M3[”图神经网络分类 / Graph Neural Network for classification”]\n        Results --> R1[”保持鲁棒检测性能 / Maintains robust detection performance”]\n        Results --> R2[”适用于动态大规模环境 / Suitable for dynamic, large-scale environments”]"
    },
    {
      "title": "SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration",
      "authors": "Xin Chen, Kang Luo, Yangyi Xiao, Hesheng Wang",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.22503",
      "code": null,
      "tags": [
        "object detection",
        "multimodal fusion",
        "BEV perception",
        "coordinate attention",
        "feature alignment",
        "small object detection"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8464280aa00fec5a6e285de4af1a67a7ca3564b134ff0090e0f402afdb80f4b9_w640_q70.webp",
      "contributions": "1. A Section-aware Coordinate Attention (SCA) module to enhance feature discrimination for small, irregular objects. 2. A parameter-efficient Cognitive Adapter for efficient camera backbone tuning. 3. A Contrastive Alignment Module (CAM) to enforce consistency between camera and LiDAR features.",
      "summary": "This paper proposes SCAFusion, a multimodal 3D object detection framework built upon BEVFusion to address the challenge of detecting small objects like rocks in lunar exploration. It introduces several modules including a Section-aware Coordinate Attention mechanism to improve small object detection, achieving significant performance gains over the baseline on both standard and simulated lunar datasets.",
      "mindmap": "graph TB\n        Root(SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration) --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1(现有方法在月球环境表现不佳/Existing methods underperform in lunar environments)\n        Problem --> P2(特征未对齐，小物体检测弱/Poor feature alignment, weak small-object detection)\n        Method --> M1(基于BEVFusion构建/Built upon BEVFusion)\n        Method --> M2(集成认知适配器、对比对齐模块、相机辅助训练分支/Integrates Cognitive Adapter, Contrastive Alignment Module, Camera Auxiliary Training Branch)\n        Method --> M3(引入分段坐标注意力机制/Introduces Section-aware Coordinate Attention mechanism)\n        Results --> R1(在nuScenes上mAP 69.7%, NDS 72.1%/69.7% mAP, 72.1% NDS on nuScenes)\n        Results --> R2(在模拟月球环境上mAP 90.93%/90.93% mAP on simulated lunar environment)"
    },
    {
      "title": "DreamOmni3: Scribble-based Editing and Generation",
      "authors": "Bin Xia, Bohao Peng, Jiyang Liu, Sitong Wu, Jingyao Li, Junjia Huang, Xu Zhao, Yitong Wang, Ruihang Chu, Bei Yu, Jiaya Jia",
      "institution": "The Chinese University of Hong Kong (CUHK), ByteDance Inc, The Hong Kong University of Science and Technology (HKUST)",
      "link": "https://arxiv.org/pdf/2512.22525",
      "code": "https://github.com/dvlab-research/DreamOmni3",
      "tags": [
        "image editing and generation",
        "scribble-based editing",
        "unified model",
        "joint input scheme",
        "data synthesis",
        "multimodal instruction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7cc62335a6a71c25dc235748c19992c217446a470fba2f00837c04f3be83d92_w640_q70.webp",
      "contributions": "1. Proposes two new tasks—scribble-based editing and generation—that combine text, images, and freehand sketches for more flexible and precise user control. 2. Introduces a comprehensive data synthesis pipeline to create training data for these tasks, including multiple sub-tasks like doodle editing and image fusion. 3. Designs a novel joint input scheme that feeds both original and scribbled images to the model, using color and shared encodings to precisely localize edit regions without relying on binary masks.",
      "summary": "The paper introduces DreamOmni3, a unified model for scribble-based image editing and generation. It addresses the limitations of text-only instructions by allowing users to specify edits with freehand sketches, proposes a data creation pipeline and a joint input scheme for precise localization, and demonstrates outstanding performance on new benchmarks.",
      "mindmap": "graph TB\n        A[DreamOmni3: Scribble-based Editing and Generation] --> B[核心问题/Problem: Text prompts fail to capture precise edit locations and fine-grained details.]\n        A --> C[主要方法/Method: Proposes scribble-based tasks, a data synthesis pipeline, and a joint input scheme using colored scribbles.]\n        A --> D[关键结果/Results: Achieves outstanding performance; establishes benchmarks; releases models and code.]"
    },
    {
      "title": "CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation",
      "authors": "Qinglin Zeng, Kaitong Cai, Ruiqi Chen, Qinhan Lv, Keze Wang",
      "institution": "Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.22536",
      "code": null,
      "tags": [
        "video generation",
        "closed-loop framework",
        "entity-level memory",
        "vision-language verification",
        "pacing-aware editing",
        "multi-agent collaboration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfaf9ffbd76c531ee1d089b472175957646bd5b08a39cad1cce07b642bd237a4_w640_q70.webp",
      "contributions": "1. Proposes CoAgent, a collaborative closed-loop framework that formulates video generation as a plan-synthesize-verify-edit process. 2. Introduces a Global Context Manager to maintain entity-level memory for cross-shot identity and appearance consistency. 3. Employs a Verifier Agent with vision-language reasoning to evaluate intermediate results and trigger selective regeneration for quality control.",
      "summary": "The paper addresses the challenge of maintaining narrative coherence and visual consistency in long-form video generation. It proposes CoAgent, a collaborative multi-agent framework that uses a closed-loop plan-synthesize-verify-edit pipeline with entity memory and consistency verification. Experiments show that CoAgent significantly improves coherence, consistency, and narrative quality in generated videos.",
      "mindmap": "graph TB\n        A[CoAgent: Coherent Video Generation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Open-domain video generation lacks coherence and consistency/开放域视频生成缺乏连贯性和一致性]\n        C --> C1[Plan-Synthesize-Verify-Edit Pipeline/计划-合成-验证-编辑流程]\n        C1 --> C2[Storyboard Planner/故事板规划器]\n        C1 --> C3[Global Context Manager/全局上下文管理器]\n        C1 --> C4[Verifier Agent/验证智能体]\n        C1 --> C5[Pacing-aware Editor/节奏感知编辑器]\n        D --> D1[Improves coherence, consistency, narrative quality/提升连贯性、一致性、叙事质量]"
    },
    {
      "title": "VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models",
      "authors": "Borong Zhang, Jiahao Li, Jiachen Shen, Yishuai Cai, Yuhao Zhang, Yuanpei Chen, Juntao Dai, Jiaming Ji, Yaodong Yang",
      "institution": "Peking University, State Key Laboratory of General Artificial Intelligence (Peking University), PKU-PsiBot Joint Lab, Beijing Academy of Artificial Intelligence",
      "link": "https://arxiv.org/pdf/2512.22539",
      "code": "this",
      "tags": [
        "embodied ai / robot learning",
        "vision-language-action models",
        "benchmark",
        "generalization",
        "robustness",
        "structured task design"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72fbe811b1b247e0cabad5619ed5d23d6155ddda1e84493fde30e54c1e9e1092_w640_q70.webp",
      "contributions": "1. Introduces VLA-Arena, a comprehensive benchmark with a novel structured task design framework to quantify difficulty across three orthogonal axes (Task Structure, Language Command, Visual Observation). 2. Provides systematic robustness evaluation via decoupled language and visual perturbations, enabling precise analysis of model failure modes. 3. Releases a complete open-source framework including an end-to-end toolchain, datasets (VLA-Arena-S/M/L), and a leaderboard to foster reproducible research.",
      "summary": "This paper introduces VLA-Arena, an open-source benchmark and framework designed to systematically evaluate the capabilities and failure modes of Vision-Language-Action models. It proposes a structured task design with fine-grained difficulty levels across four dimensions and orthogonal perturbations to measure model robustness. The evaluation reveals critical limitations in current VLAs, such as memorization over generalization and poor safety consideration, and the released framework aims to address these challenges.",
      "mindmap": "graph TB\n        A[VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Difficult to quantitatively understand the limits and failure modes of VLAs]\n        C[主要方法/Method: Structured benchmark with orthogonal difficulty axes (Task Structure, Language, Visual) and systematic perturbations]\n        D[关键结果/Results: Revealed critical VLA limitations (memorization, asymmetric robustness); Provided open-source framework for research]"
    },
    {
      "title": "Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains",
      "authors": "Jesen Zhang, Ningyuan Liu, Kaitong Cai, Sidi Liu, Jing Yang, Ziliang Chen, Xiaofei Sun, Keze Wang",
      "institution": "Sun Yat-sen University, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.22545",
      "code": null,
      "tags": [
        "multimodal reasoning",
        "self-rewarded learning",
        "process alignment",
        "multimodal large language models",
        "reasoning coherence",
        "visual grounding"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/629536dd38b71477a18bd2e7208023831047c11b4eafeff5c5c094c124751f98_w640_q70.webp",
      "contributions": "1. A lightweight, label-free framework (SR-MCR) that aligns multimodal reasoning by constructing a self-reward from intrinsic process signals (semantic alignment, lexical fidelity, non-redundancy, visual grounding, step consistency). 2. A normalized, reliability-weighted reward mechanism that adaptively combines multiple self-referential cues to provide fine-grained, process-level guidance. 3. A critic-free GRPO objective enhanced with a confidence-aware cooling mechanism to stabilize training and suppress trivial or overconfident generations.",
      "summary": "The paper addresses the problem of multimodal LLMs producing fluent but unreliable reasoning with poor step coherence and visual grounding. It proposes SR-MCR, a self-rewarded framework that uses multiple intrinsic process signals from model outputs to create a fine-grained reward for alignment, achieving state-of-the-art accuracy and improved reasoning coherence on visual benchmarks.",
      "mindmap": "graph TB\n        Root[Self-Rewarded Multimodal Coherent Reasoning<br>自我奖励多模态连贯推理] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>Fluent but unreliable reasoning,<br>weak coherence & grounding] --> P1[现有方法缺陷/Existing Method Flaws<br>Supervises only final answer]\n        Method[主要方法/Method<br>SR-MCR Framework] --> M1[自我奖励/Self-Reward<br>Five intrinsic process cues]\n        Method --> M2[优化目标/Optimization<br>GRPO with cooling mechanism]\n        Results[关键结果/Results<br>Evaluation & Ablation] --> R1[性能提升/Performance Gain<br>SOTA accuracy (81.4%)]\n        Results --> R2[消融研究/Ablation Study<br>Confirms contributions]"
    },
    {
      "title": "ReFRM3D: A Radiomics-enhanced Fused Residual Multiparametric 3D Network with Multi-Scale Feature Fusion for Glioma Characterization",
      "authors": "Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Arefin Ittesafun Abian, Yan Zhang, Mirjam Jonkman, Sami Azam",
      "institution": "United International University, Monash University, Charles Darwin University",
      "link": "https://arxiv.org/pdf/2512.22570",
      "code": null,
      "tags": [
        "medical image segmentation",
        "3D U-Net",
        "Multi-scale Feature Fusion",
        "Radiomics",
        "Hybrid Upsampling",
        "Residual Skip Mechanism"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5667074cd5b9a705381cd4273a457f043c3fd17da8d6bb9335a88a6f7f75338_w640_q70.webp",
      "contributions": "1. Proposes ReFRM3D, a novel radiomics-enhanced fused residual multiparametric 3D network for brain tumor characterization, based on a 3D U-Net with multi-scale feature fusion, hybrid upsampling, and an extended residual skip mechanism. 2. Introduces a multi-feature tumor marker-based classifier that leverages radiomic features extracted from the segmented tumor regions. 3. Demonstrates state-of-the-art segmentation performance on multiple BraTS datasets (2019, 2020, 2021), achieving high Dice Similarity Coefficients for whole tumor, enhancing tumor, and tumor core.",
      "summary": "This paper addresses challenges in glioma segmentation and classification from multi-parametric MRI data by proposing ReFRM3D, a novel 3D network architecture enhanced with radiomics and multi-scale feature fusion. The method achieves superior segmentation performance on standard BraTS benchmarks, demonstrating its effectiveness for accurate brain tumor characterization.",
      "mindmap": "graph TB\n        A[ReFRM3D: Glioma Characterization] --> B[核心问题/Problem: Glioma diagnosis challenges: data variability, inefficient segmentation & classification]\n        A --> C[主要方法/Method: ReFRM3D network: 3D U-Net, multi-scale fusion, hybrid upsampling, residual skip, radiomics classifier]\n        A --> D[关键结果/Results: High DSC scores on BraTS2019/2020/2021 datasets for WT, ET, TC segmentation]"
    },
    {
      "title": "KV-Tracker: Real-Time Pose Tracking with Transformers",
      "authors": "Marwan Taher, Ignacio Alzugaray, Kirill Mazur, Xin Kong, Andrew J. Davison",
      "institution": "Imperial College London",
      "link": "https://arxiv.org/pdf/2512.22581",
      "code": "https://marwan99.github.io/kv_tracker/",
      "tags": [
        "pose tracking and reconstruction",
        "key-value caching",
        "real-time tracking",
        "multi-view geometry",
        "transformer",
        "online reconstruction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba3e802fd09ff1999dbcceb5bbbd5d8a3b724730eaf05f3ff5d3d96e25a7c964_w640_q70.webp",
      "contributions": "1. A novel method for adapting slow, multi-view 3D geometry networks for real-time online use by caching key-value pairs from the transformer's self-attention block. 2. A model-agnostic caching strategy that can be applied to off-the-shelf multi-view networks without retraining, enabling significant inference speedup. 3. Demonstration of the system on challenging tasks like on-the-fly object tracking and reconstruction from monocular RGB video without depth or object priors.",
      "summary": "The paper addresses the problem that powerful multi-view 3D geometry networks are too slow for real-time applications. It proposes KV-Tracker, which adapts these networks for online use by selecting keyframes, running a full multi-view model on them, and then caching the transformer's key-value pairs to serve as a compact scene representation for fast, real-time pose tracking. This approach achieves up to 15x speedup and high frame rates (e.g., ~27 FPS) on standard datasets while preventing drift and catastrophic forgetting.",
      "mindmap": "graph TB\n        Root(”KV-Tracker: Real-Time Pose Tracking with Transformers”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”多视角3D网络速度慢/Multi-view 3D networks are slow”)\n        Problem --> P2(”难以实时应用/Difficult for real-time use”)\n        Method --> M1(”选择并管理关键帧/Select & manage keyframes”)\n        Method --> M2(”缓存KV对作为场景表示/Cache KV pairs as scene representation”)\n        Method --> M3(”模型无关的在线跟踪/Model-agnostic online tracking”)\n        Results --> R1(”15倍推理加速/15x inference speedup”)\n        Results --> R2(”高达27 FPS/Up to ~27 FPS”)\n        Results --> R3(”防止漂移和遗忘/Prevents drift & forgetting”)"
    },
    {
      "title": "PTalker: Personalized Speech-Driven 3D Talking Head Animation via Style Disentanglement and Modality Alignment",
      "authors": "Bin Wang, Yang Xu, Huan Zhao, Hao Zhang, Zixing Zhang",
      "institution": "Hunan University, Central South University",
      "link": "https://arxiv.org/pdf/2512.22602",
      "code": null,
      "tags": [
        "3D facial animation",
        "style disentanglement",
        "modality alignment",
        "Graph Attention Networks",
        "cross-attention",
        "contrastive learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2b91879e2c31bbc71f6f461d0b3368225219946fef95227be54fd0978166a06f_w640_q70.webp",
      "contributions": "1. A novel framework for personalized 3D talking head animation that preserves individual speaking styles through style disentanglement from audio and motion sequences. 2. A three-level modality alignment mechanism (spatial, temporal, feature) to enhance lip-synchronization accuracy between speech and 3D mesh. 3. Extensive experiments demonstrating superior performance in generating realistic, stylized animations compared to state-of-the-art methods.",
      "summary": "This paper proposes PTalker, a framework for personalized speech-driven 3D talking head animation. It addresses the lack of individual speaking style in existing methods by disentangling style and content from audio/motion and improves lip-sync via a three-level audio-mesh alignment mechanism. Experiments show PTalker outperforms state-of-the-art methods in generating realistic and stylized animations.",
      "mindmap": "graph TB\n        A[PTalker: Personalized Speech-Driven 3D Talking Head Animation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法忽略个性化说话风格 / Existing methods overlook individual speaking styles]\n        C --> C1[风格解耦 / Style Disentanglement]\n        C --> C2[模态对齐 / Modality Alignment]\n        C1 --> C1a[从音频和动作序列中分离风格与内容 / Separate style & content from audio & motion]\n        C2 --> C2a[空间对齐 / Spatial Alignment: Graph Attention Networks]\n        C2 --> C2b[时间对齐 / Temporal Alignment: Cross-Attention]\n        C2 --> C2c[特征对齐 / Feature Alignment: Contrastive Loss & KL Divergence]\n        D --> D1[生成逼真、个性化的3D说话头 / Generates realistic, personalized 3D talking heads]\n        D --> D2[超越现有方法 / Outperforms state-of-the-art methods]"
    },
    {
      "title": "Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation",
      "authors": "Junshu Dai, Yu Wang, Tongya Zheng, Wei Ji, Qinghong Guo, Ji Cao, Jie Song, Canghong Jin, Mingli Song",
      "institution": "Zhejiang University, Hangzhou City University, Nanjing University",
      "link": "https://arxiv.org/pdf/2512.22605",
      "code": "https://anonymous.4open.science/r/M3ob-62EF",
      "tags": [
        "location-based recommendation",
        "multi-modal learning",
        "spatial-temporal knowledge graph",
        "cross-modal alignment"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de0c70167dfab9ddb767e6d0d1161ce4f31f482e064a77521ffa2e25c598f1d2_w640_q70.webp",
      "contributions": "1. Proposes a unified spatial-temporal relational graph (STRG) for multi-modal representation, enhanced by LLMs. 2. Designs a gating mechanism to fuse spatial-temporal graph representations from different modalities. 3. Introduces an STKG-guided cross-modal alignment method to inject dynamic knowledge into static image representations.",
      "summary": "This paper addresses the limited generalization of next location recommendation methods by proposing M³ob, a framework that leverages multi-modal spatial-temporal knowledge. It constructs a unified graph representation and uses a gating mechanism with cross-modal alignment to capture mobility dynamics. Experiments on six datasets show the method improves performance in both normal and abnormal scenarios.",
      "mindmap": "graph TB\n    A[”Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation”] --> B[”核心问题/Problem: Existing methods have limited generalization; unimodal suffers from sparsity, multi-modal struggles with semantic gap.”]\n    A --> C[”主要方法/Method: Constructs LLM-enhanced spatial-temporal knowledge graph (STKG) and unified STRG; uses gating fusion and STKG-guided cross-modal alignment.”]\n    A --> D[”关键结果/Results: Achieves consistent improvements on six datasets and shows strong generalization in abnormal scenarios.”]"
    },
    {
      "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone",
      "authors": "Jiacheng Ye, Shansan Gong, Jiahui Gao, Junming Fan, Shuang Wu, Wei Bi, Haoli Bai, Lifeng Shang, Lingpeng Kong",
      "institution": "The University of Hong Kong, Huawei Technologies",
      "link": "https://arxiv.org/pdf/2512.22615",
      "code": null,
      "tags": [
        "multi-modal training",
        "diffusion language model",
        "vision-language-action",
        "parallel generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc3fa176585576be4f4bd1035cfa0a21e63722654274b464e83bb35c7ee5bc0c_w640_q70.webp",
      "contributions": "1. Introduces Dream-VL, a state-of-the-art open diffusion-based Vision-Language Model (dVLM) that matches top AR-based VLMs on benchmarks and excels at visual planning. 2. Introduces Dream-VLA, a diffusion-based Vision-Language-Action model built upon Dream-VL, leveraging the bidirectional nature of diffusion for superior action chunking and faster fine-tuning convergence. 3. Demonstrates that diffusion-based VLMs/VLAs outperform autoregressive baselines on downstream tasks, achieving top-tier performance on robotic benchmarks like LIBERO, SimplerEnv-Bridge, and SimplerEnv-Fractal.",
      "summary": "This paper proposes building Vision-Language and Vision-Language-Action models on diffusion-based language models to overcome the limitations of autoregressive models in complex planning and control. The introduced models, Dream-VL and Dream-VLA, leverage the bidirectional, parallel generation nature of diffusion for superior performance in visual planning and robotic tasks, achieving state-of-the-art results on key benchmarks.",
      "mindmap": "graph TB\n        A[Dream-VL & Dream-VLA<br>论文标题/Paper Title] --> B[AR模型在视觉规划与机器人控制中存在局限<br>核心问题/Problem]\n        A --> C[基于扩散语言模型构建VLM和VLA模型<br>主要方法/Method]\n        A --> D[在多个基准测试中取得SOTA性能<br>关键结果/Results]"
    },
    {
      "title": "Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer",
      "authors": "Dafeng Zhang, Yongqi Song, Shizhuo Liu",
      "institution": "Samsung R&D Institute China-Beijing (SRC-B)",
      "link": "https://arxiv.org/pdf/2512.22612",
      "code": null,
      "tags": [
        "face clustering",
        "Sparse Differential Transformer",
        "Top-K Jaccard similarity",
        "noise resilience"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f97a4b274ef0481f36b8e16c722fdbe08401dff5cc2fb6792723e930e0c3d67e_w640_q70.webp",
      "contributions": "1. Proposed a prediction-driven Top-K Jaccard similarity coefficient to enhance neighbor purity and similarity measurement reliability. 2. Developed a Transformer-based model to predict relationships near the Top-K boundary for more accurate similarity estimation. 3. Introduced a Sparse Differential Transformer (SDT) to eliminate noise from irrelevant feature relationships and improve the model's anti-noise capability.",
      "summary": "This paper addresses the problem of noise in face clustering caused by irrelevant nodes in Jaccard similarity measurements. The authors propose a Sparse Differential Transformer (SDT) to predict and refine Top-K Jaccard similarity, enhancing noise resilience. Experiments on datasets like MS-Celeb-1M show the method achieves state-of-the-art performance.",
      "mindmap": "graph TB\n        A[Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法引入过多无关节点，Jaccard系数判别力有限/Existing methods introduce too many irrelevant nodes, Jaccard coefficient has limited discriminative power]\n        C --> C1[提出预测驱动的Top-K Jaccard相似度系数/Propose prediction-driven Top-K Jaccard similarity coefficient]\n        C --> C2[开发基于Transformer的预测模型/Develop a Transformer-based prediction model]\n        C --> C3[提出稀疏差分Transformer (SDT) 消除噪声/Propose Sparse Differential Transformer (SDT) to eliminate noise]\n        D --> D1[在多个数据集上达到SOTA性能/Achieves SOTA performance on multiple datasets]\n        D --> D2[为面部聚类提供更鲁棒的解决方案/Provides a more robust solution for face clustering]"
    },
    {
      "title": "Rethinking Memory Design in SAM-Based Visual Object Tracking",
      "authors": "Mohamad Alansari, Muzammal Naseer, Hasan Al Marzouqi, Naoufel Werghi, Sajid Javed",
      "institution": "Khalifa University",
      "link": "https://arxiv.org/pdf/2512.22624",
      "code": "https://github.com/HamadYA/SAM3_Tracking_Zoo",
      "tags": [
        "visual object tracking",
        "Segment Anything Model",
        "memory mechanism",
        "hybrid memory",
        "distractor-resolving",
        "occlusion robustness"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a51c2fc4ae4647cc17c52c77088a6dfc36a196eed861608fcdf64ee98185d453_w640_q70.webp",
      "contributions": "1. Conducted a systematic analysis of memory design in SAM-based trackers, revealing that methods primarily differ in short-term memory frame selection. 2. Reimplemented and evaluated existing memory mechanisms within the SAM3 framework across ten benchmarks for a controlled analysis. 3. Proposed a unified hybrid memory framework that decomposes memory into short-term appearance and long-term distractor-resolving components, improving robustness.",
      "summary": "This paper systematically studies memory design in SAM-based visual object tracking. It analyzes existing methods, reimplements their memory mechanisms in SAM3, and proposes a unified hybrid memory framework. The framework improves tracking robustness in challenging scenarios like occlusion and distractors for both SAM2 and SAM3 backbones.",
      "mindmap": "graph TB\n        Root[Rethinking Memory Design in SAM-Based Visual Object Tracking] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有方法对内存的设计缺乏系统性理解/Lack of systematic understanding of memory design in SAM-based tracking]\n        Problem --> P2[内存机制在更强基础模型上的迁移效果未知/Unclear how memory mechanisms transfer to next-gen models like SAM3]\n        Method[主要方法/Method] --> M1[分析代表性SAM2跟踪器/Analyze representative SAM2-based trackers]\n        Method --> M2[在SAM3中忠实复现内存机制/Faithfully reimplement memory mechanisms in SAM3]\n        Method --> M3[提出统一的混合内存框架/Propose a unified hybrid memory framework]\n        Results[关键结果/Results] --> R1[实现大规模基准评估/Conduct large-scale benchmark evaluations]\n        Results --> R2[在遮挡、复杂运动等场景提升鲁棒性/Improve robustness under occlusion, complex motion, distractors]"
    },
    {
      "title": "Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion",
      "authors": "Yuming Gu, Yizhi Wang, Yining Hong, Yipeng Gao, Hao Jiang, Angtian Wang, Bo Liu, Nathaniel S. Dennler, Zhengfei Kuang, Hao Li, Gordon Wetzstein, Chongyang Ma",
      "institution": "University of Southern California, ByteDance, Stanford University, Massachusetts Institute of Technology, MBZUAI",
      "link": "https://arxiv.org/pdf/2512.22626",
      "code": "https://envision-paper.github.io/",
      "tags": [
        "embodied visual planning",
        "video diffusion",
        "goal-conditioned generation",
        "embodied agents",
        "visual imagination",
        "first-and-last-frame-conditioned model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ca10604ad58b7959dc688e00e5d0eefe89420321b67b21d1c67cd72d8941c11_w640_q70.webp",
      "contributions": "1. Proposes Envision, a two-stage diffusion framework for embodied visual planning that explicitly uses a goal image to constrain trajectory generation. 2. Introduces a Goal Imagery Model that synthesizes a coherent goal image by identifying task-relevant regions and performing region-aware cross attention. 3. Develops an Env-Goal Video Model based on a first-and-last-frame-conditioned video diffusion model (FL2V) to interpolate smooth, physically plausible video trajectories between start and goal states.",
      "summary": "The paper addresses the problem of spatial drift and goal misalignment in embodied visual planning by proposing Envision, a two-stage diffusion framework that first generates a goal image and then creates a video trajectory connecting the initial scene to that goal. This method enforces goal consistency and physical plausibility. Experiments show it outperforms baselines in goal alignment and spatial consistency, providing reliable visual plans for robotic control.",
      "mindmap": "graph TB\n        Root[”Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion”] --> Problem[”核心问题/Problem”]\n        Root[”Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion”] --> Method[”主要方法/Method”]\n        Root[”Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion”] --> Results[”关键结果/Results”]\n        Problem --> P1[”现有方法为前向预测/Existing approaches are forward predictive”]\n        Problem --> P2[”导致空间漂移和目标错位/Leading to spatial drift & goal misalignment”]\n        Method --> M1[”两阶段框架/Two-stage framework”]\n        M1 --> M1_1[”目标图像模型/Goal Imagery Model”]\n        M1_1 --> M1_1a[”合成目标图像/Synthesizes goal image”]\n        M1 --> M1_2[”环境-目标视频模型/Env-Goal Video Model”]\n        M1_2 --> M1_2a[”基于FL2V的插值/FL2V-based interpolation”]\n        Results --> R1[”目标对齐更优/Superior goal alignment”]\n        Results --> R2[”空间一致性更好/Better spatial consistency”]\n        Results --> R3[”支持机器人规划/Supports robotic planning”]"
    },
    {
      "title": "FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution",
      "authors": "Yidi Liu, Zihao Fan, Jie Huang, Jie Xiao, Dong Li, Wenlong Zhang, Lei Bai, Xueyang Fu, Zheng-Jun Zha",
      "institution": "University of Science and Technology of China, Shanghai AI Laboratory",
      "link": "https://arxiv.org/pdf/2512.22647",
      "code": null,
      "tags": [
        "image super-resolution",
        "reinforcement learning from human feedback (RLHF)",
        "reward hacking",
        "perceptual quality",
        "curriculum learning",
        "fine-grained assessment"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1738fd28b1410f3f5c393bd5d70851ae8df2e1196df6379de62b5d7d48c736b_w640_q70.webp",
      "contributions": "1. Proposes a Fine-grained Perceptual Reward Model (FinPercep-RM) with an encoder-decoder architecture that outputs both a global quality score and a Perceptual Degradation Map to localize defects. 2. Introduces the FGR-30k dataset containing diverse and subtle distortions from real-world super-resolution models for training the reward model. 3. Designs a Co-evolutionary Curriculum Learning (CCL) mechanism that synchronizes the progressive training of the reward model and the ISR model to ensure stable training and suppress reward hacking.",
      "summary": "This paper addresses the problem of reward hacking in RLHF-based Image Super-Resolution, where traditional Image Quality Assessment models are insensitive to local distortions. The authors propose a fine-grained reward model (FinPercep-RM) and a co-evolutionary curriculum learning strategy to provide localized feedback and stabilize training. Experiments show the method improves both global quality and local realism in generated images.",
      "mindmap": "graph TB\n        A[FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>传统IQA模型对局部失真不敏感，导致奖励欺骗/Reward Hacking]\n        C[主要方法/Method<br>1. 细粒度感知奖励模型 (FinPercep-RM)<br>2. 协同进化课程学习 (CCL)]\n        D[关键结果/Results<br>提升全局质量与局部真实感，实现稳定训练/Improves global quality & local realism, enables stable training]"
    },
    {
      "title": "Visual Autoregressive Modelling for Monocular Depth Estimation",
      "authors": "Amir El-Ghoussani, André Kaup, Nassir Navab, Gustavo Carneiro, Vasileios Belagiannis",
      "institution": "Friedrich-Alexander University Erlangen-Nuremberg, Technical University of Munich, University of Surrey",
      "link": "https://arxiv.org/pdf/2512.22653",
      "code": "https://github.com/AmirMaEl/VAR-Depth",
      "tags": [
        "monocular depth estimation",
        "visual autoregressive modelling",
        "classifier-free guidance",
        "scale-wise conditional upsampling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/410796d30bc05e7bf6addcc414bee3b744b04c373092f698970e2770057a5f53_w640_q70.webp",
      "contributions": "1. Proposes a monocular depth estimation method based on visual autoregressive (VAR) priors as an alternative to diffusion models. 2. Introduces a scale-wise conditional upsampling mechanism with classifier-free guidance for the task. 3. Demonstrates the method's efficiency (10-stage inference, 74K fine-tuning samples) and strong performance, achieving state-of-the-art results on indoor benchmarks under constrained training.",
      "summary": "This paper proposes a new method for monocular depth estimation that uses visual autoregressive (VAR) priors instead of diffusion models. The approach adapts a large text-to-image VAR model with a novel scale-wise upsampling mechanism and achieves competitive results with efficient fine-tuning. The work establishes autoregressive models as a complementary, data-scalable family of generative models for 3D vision tasks.",
      "mindmap": "graph TB\n        A[Visual Autoregressive Modelling for Monocular Depth Estimation] --> B(核心问题/Problem: Monocular depth estimation as an ill-posed task)\n        A --> C(主要方法/Method: Adapt VAR priors with scale-wise upsampling & classifier-free guidance)\n        A --> D(关键结果/Results: SOTA indoor performance, strong outdoor results, efficient fine-tuning)"
    },
    {
      "title": "INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading",
      "authors": "Mert Ikinci, Luna Toma, Karin U. Loeffler, Leticia Ussem, Daniela Süsskind, Julia M. Weller, Yousef Yeganeh, Martina C. Herwig-Carl, Shadi Albarqouni",
      "institution": "University Hospital Bonn, Technical University of Munich",
      "link": "https://arxiv.org/pdf/2512.22666",
      "code": null,
      "tags": [
        "medical image analysis",
        "multi-task learning",
        "inter-task consistency",
        "digital pathology",
        "foundation models",
        "combinatorial partial supervision"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ed4ee67160f114b6db4089d38a1fbeae9ea01e9231d6d4df14bea6d6144469c_w640_q70.webp",
      "contributions": "1. Introduces INTERACT-CMIL, a multi-head deep learning framework for jointly predicting five histopathological axes for CMIL grading. 2. Proposes a Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss to enforce cross-task consistency. 3. Curates and evaluates on a new multi-center dataset, showing significant performance improvements over baselines and providing a reproducible benchmark.",
      "summary": "The paper addresses the difficult problem of grading Conjunctival Melanocytic Intraepithelial Lesions (CMIL) by introducing INTERACT-CMIL, a multi-task deep learning framework that uses shared feature learning and an inter-dependence loss to jointly and consistently predict five diagnostic criteria. Evaluated on a new multi-center dataset, the method outperforms CNN and foundation model baselines, offering a coherent and interpretable computational tool for standardized diagnosis.",
      "mindmap": "graph TB\n        Root[”INTERACT-CMIL: CMIL分级<br>INTERACT-CMIL: CMIL Grading”]\n        Root --> Problem[”核心问题/Problem<br>CMIL分级困难，主观性强<br>CMIL grading is difficult and subjective”]\n        Root --> Method[”主要方法/Method<br>多任务共享学习与任务间一致性<br>Multi-task Shared Learning & Inter-Task Consistency”]\n        Root --> Results[”关键结果/Results<br>性能显著提升，提供可解释预测<br>Significant performance gains, interpretable predictions”]"
    },
    {
      "title": "Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains",
      "authors": "Qiankun Li, Feng He, Huabao Chen, Xin Ning, Kun Wang, Zengfu Wang",
      "institution": "University of Science and Technology of China, AnnLab (Institute of Semiconductors, Chinese Academy of Sciences), Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.22664",
      "code": "https://github.com/qklee-lz/CLAdapter",
      "tags": [
        "transfer learning / domain adaptation",
        "Cluster Attention Adapter",
        "adapter tuning",
        "data-limited domains",
        "vision foundation models",
        "adaptive transfer"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d174c6a31de34c34ee98111995fd6b43142db3342aa6c7a647cb2a8121615532_w640_q70.webp",
      "contributions": "1. Proposes a novel Cluster Attention Adapter (CLAdapter) that refines and adapts pre-trained representations to data-limited downstream tasks using attention mechanisms and cluster centers. 2. Designs a unified interface for seamless integration with diverse model architectures (CNNs, Transformers) in both 2D and 3D contexts. 3. Demonstrates state-of-the-art performance through extensive experiments on 10 datasets spanning diverse scientific domains.",
      "summary": "This paper addresses the challenge of adapting large-scale pre-trained vision foundation models to specialized, data-limited scientific domains. It proposes a novel Cluster Attention Adapter (CLAdapter) that personalizes feature enhancement for different downstream tasks, enabling effective adaptive transfer. Extensive experiments across 10 diverse datasets show that CLAdapter achieves state-of-the-art performance, demonstrating its effectiveness in unleashing the potential of foundation models for scientific applications.",
      "mindmap": "graph TB\n        A[Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[下游任务数据稀缺/Data-limited downstream tasks]\n        C --> C1[提出CLAdapter/Propose CLAdapter]\n        C1 --> C2[注意力与聚类中心/Attention & Cluster Centers]\n        C2 --> C3[个性化特征增强/Personalized Feature Enhancement]\n        D --> D1[10个数据集实验/Experiments on 10 datasets]\n        D1 --> D2[实现SOTA性能/Achieves SOTA performance]"
    },
    {
      "title": "Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos",
      "authors": "Shravan Saranyan, Pramit Saha",
      "institution": "Branham High School, University of Oxford",
      "link": "https://arxiv.org/pdf/2512.22657",
      "code": null,
      "tags": [
        "medical image analysis",
        "3D Inception",
        "two-stream networks",
        "CNN-RNN",
        "EchoNet-Dynamic",
        "video analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80971653578a1ff466eaae0a7007615dc054e9d7579f8916b800791a4f77026e_w640_q70.webp",
      "contributions": "1. A systematic investigation and comparison of multiple deep learning architectures (3D Inception, two-stream, CNN-RNN) for automated LVEF estimation from echocardiography videos. 2. Identification that modified 3D Inception architectures achieve the best performance (RMSE of 6.79%) on the EchoNet-Dynamic dataset. 3. Key insights on model design, including the tendency for smaller, simpler models to generalize better and the high sensitivity of performance to hyperparameters like kernel size and normalization.",
      "summary": "This paper investigates deep learning models for automating the estimation of left ventricular ejection fraction (LVEF) from echocardiography videos to address the time-consuming and variable nature of manual assessment. The authors systematically evaluate and modify several architectures, including 3D Inception, two-stream, and CNN-RNN models, finding that a modified 3D Inception model performs best. The study concludes that while these models show promise, they are prone to overfitting and their performance is highly sensitive to specific hyperparameter choices.",
      "mindmap": "graph TB\n        A[Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[手动评估LVEF耗时且存在观察者间差异/Manual LVEF assessment is time-consuming and has inter-observer variability]\n        C --> C1[系统评估3D Inception、双流和CNN-RNN架构/Systematically evaluate 3D Inception, two-stream, and CNN-RNN architectures]\n        C --> C2[在EchoNet-Dynamic数据集上训练和评估/Train and evaluate on the EchoNet-Dynamic dataset]\n        D --> D1[改进的3D Inception架构表现最佳，RMSE为6.79%/Modified 3D Inception achieves best performance (RMSE 6.79%)]\n        D --> D2[更小、更简单的模型泛化能力更好/Smaller, simpler models generalize better]"
    },
    {
      "title": "CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation",
      "authors": "ZhenQi Chen, TsaiChing Ni, YuanFu Yang",
      "institution": "National Yang Ming Chiao Tung University",
      "link": "https://arxiv.org/pdf/2512.22681",
      "code": null,
      "tags": [
        "text-to-image generation",
        "inference-time refinement",
        "semantic critique",
        "spectral fusion",
        "diffusion models",
        "vision-language model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/270ae7b15be98cd6466e9deec0c96f336f351042ec08bab6084e768799a7b763_w640_q70.webp",
      "contributions": "1. Introduces CritiCore, a multimodal semantic critique module using VLM and LLMs to provide high-level feedback for better prompt alignment. 2. Proposes SpecFusion, a frequency-domain method to merge intermediate generation states, preserving high-frequency details while injecting structure. 3. Presents CritiFusion as a plug-in, training-free framework compatible with existing diffusion backbones, improving both semantic correspondence and visual quality.",
      "summary": "The paper addresses the problem of text-to-image diffusion models struggling with semantic alignment to complex prompts. It proposes CritiFusion, an inference-time framework that uses a semantic critique module and spectral fusion to refine generated images, improving faithfulness and detail without requiring additional training. The method achieves results competitive with state-of-the-art reward optimization approaches on standard benchmarks.",
      "mindmap": "graph TB\n        A[CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation] --> B(核心问题/Problem: Diffusion models struggle with semantic alignment to complex prompts.)\n        A --> C(主要方法/Method: Inference-time framework with CritiCore for semantic critique and SpecFusion for spectral refinement.)\n        A --> D(关键结果/Results: Improves text-image correspondence and visual quality; competitive with SOTA.)"
    },
    {
      "title": "Autoregressive Flow Matching for Motion Prediction",
      "authors": "Johnathan Xie, Stefan Stojanov, Cristobal Eyzaguirre, Daniel L. K. Yamins, Jiajun Wu",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.22688",
      "code": "https://github.com/Johnathan-Xie/arfm-motion-prediction",
      "tags": [
        "motion prediction",
        "autoregressive flow matching",
        "point tracks",
        "probabilistic modeling",
        "video conditioning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d8e5a909ec49727b601e446f309c509adc85c5184989119d290d0736109e261_w640_q70.webp",
      "contributions": "1. Proposes autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data. 2. Develops benchmarks for evaluating motion prediction models on human and robot motion. 3. Demonstrates that conditioning downstream tasks (robot action prediction, human motion prediction) on predicted future tracks improves performance.",
      "summary": "The paper addresses the challenge of predicting future motion from videos, which requires understanding agent behavior and physics. It proposes Autoregressive Flow Matching (ARFM), a method trained on diverse video datasets to probabilistically generate future point track locations. The model shows the ability to predict complex motions and its predictions can significantly improve downstream task performance in robotics and human motion analysis.",
      "mindmap": "graph TB\n        Root(”Autoregressive Flow Matching for Motion Prediction”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”现有模型泛化性差或运动建模不准 / Existing models lack generality or accurate motion modeling”)\n        Method --> M1(”提出自回归流匹配 / Propose Autoregressive Flow Matching (ARFM)”)\n        Method --> M2(”在大规模多样化视频数据上训练 / Train on large-scale diverse video datasets”)\n        Method --> M3(”预测未来点轨迹位置 / Predict future point track locations”)\n        Results --> R1(”能预测复杂运动 / Capable of predicting complex motions”)\n        Results --> R2(”提升下游任务性能 / Improves downstream task performance”)"
    },
    {
      "title": "Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors",
      "authors": "Salvador Rodriguez-Sanz, Monica Hernandez",
      "institution": "University of Zaragoza, Aragon Institute for Engineering Research (I3A)",
      "link": "https://arxiv.org/pdf/2512.22689",
      "code": null,
      "tags": [
        "medical image registration",
        "Neural ODEs",
        "Structural Descriptors",
        "Diffeomorphic Registration",
        "Multimodal",
        "Local Mutual Information"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/064c09ec95bfe97d740e2afb1b657324c426af97aabecb55dcfa4db080514f55_w640_q70.webp",
      "contributions": "1. Proposes an instance-specific multimodal diffeomorphic registration framework using Neural ODEs, eliminating the need for large training datasets and avoiding performance drop on unseen modalities. 2. Introduces three method variants that integrate modality-agnostic structural descriptors (image-based or feature-based) with local mutual information for similarity measurement. 3. Demonstrates superior performance, robustness to varying regularization, suitability for different deformation scales, and computational efficiency compared to state-of-the-art baselines.",
      "summary": "This paper addresses the limitations of existing nonrigid registration methods, which often require intensity correlation and are limited to monomodal settings. It proposes a multimodal diffeomorphic registration method that combines Neural ODEs with structural descriptors and local mutual information. The method shows superior accuracy, robustness, and efficiency in aligning medical images from different modalities without requiring extensive training data.",
      "mindmap": "graph TB\n        A[Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[传统算法假设强度相关，限于单模态/Traditional methods assume intensity correlation, limited to monomodal]\n        B --> B2[学习模型需要大量数据，泛化性差/Learning-based models need large datasets, poor generalization]\n        C --> C1[基于实例的框架/Instance-specific framework]\n        C --> C2[使用神经ODE与结构描述符/Using Neural ODEs & Structural Descriptors]\n        C --> C3[整合局部互信息/Integrating Local Mutual Information]\n        D --> D1[超越SOTA结果/Surpassing SOTA results]\n        D --> D2[对正则化鲁棒/Robust to varying regularization]\n        D --> D3[高效且适用于不同尺度/Efficient & suitable for varying scales]"
    },
    {
      "title": "Mesquite MoCap: Democratizing Real-Time Motion Capture with Affordable, Bodyworn IoT Sensors and WebXR SLAM",
      "authors": "Poojan Vanani, Darsh Patel, Danyal Khorami, Siva Munaganuru, Pavan Reddy, Varun Reddy, Bhargav Raghunath, Ishrat Lallmamode, Romir Patel, Assegid Kidané, Tejaswi Gowda",
      "institution": "Arizona State University",
      "link": "https://arxiv.org/pdf/2512.22690",
      "code": null,
      "tags": [
        "motion capture",
        "wearable computing",
        "human-computer interaction",
        "IMU",
        "WebXR",
        "SLAM",
        "IoT",
        "edge computing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/471e89e8fb0b1b1a76d2b789c6196559fd43adfdff5430691fa6cbaa29efc55b_w640_q70.webp",
      "contributions": "1. An open-source, low-cost inertial motion capture system using 15 body-worn IMU sensors and a smartphone for SLAM-based position tracking. 2. A fully browser-based application built on modern web technologies (WebGL, WebXR, WebSerial) for cross-platform, real-time visualization and recording. 3. Demonstrated performance comparable to commercial optical systems (2-5° joint-angle error) at approximately 5% of the cost, with low latency and high reliability.",
      "summary": "The paper presents Mesquite, an affordable motion capture system that uses wearable IMU sensors and a smartphone with WebXR for SLAM. It operates entirely in a web browser using modern web technologies for real-time tracking. The system achieves accuracy close to expensive commercial systems at a fraction of the cost, aiming to democratize motion capture technology.",
      "mindmap": "graph TB\n        Root[Mesquite MoCap: Democratizing Real-Time Motion Capture] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Motion capture is costly and complex, limiting accessibility] --> P1[昂贵且复杂/Expensive & Complex]\n        Problem --> P2[局限于专业实验室/Limited to Specialized Labs]\n        Method[主要方法/Method: Open-source, low-cost system using IoT sensors and web tech] --> M1[身体佩戴IMU传感器网络/Body-worn IMU Sensor Network]\n        Method --> M2[智能手机WebXR SLAM定位/Smartphone WebXR SLAM for Positioning]\n        Method --> M3[基于浏览器的应用/Web-Browser-Based Application]\n        Results[关键结果/Results: Affordable, accurate, and real-time performance] --> R1[成本约为商业系统的5%/~5% Cost of Commercial System]\n        Results --> R2[平均关节角度误差2-5度/Mean Joint-Angle Error 2-5°]\n        Results --> R3[实时低延迟/Real-Time with Low Latency]"
    },
    {
      "title": "SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis",
      "authors": "Paul Dobre, Jackson Cooper, Xin Wang, Hongzhou Yang",
      "institution": "University of Calgary",
      "link": "https://arxiv.org/pdf/2512.22706",
      "code": null,
      "tags": [
        "novel view synthesis",
        "3D Gaussian Splatting",
        "diffusion models",
        "autonomous driving simulation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95f8be4c9d4efa079fef04457695c6277096cd0bb166f918ab1f0cd81c2c5a16_w640_q70.webp",
      "contributions": "1. A unified framework (SCPainter) that jointly handles realistic 3D asset insertion and novel view synthesis for autonomous driving simulation. 2. Integration of 3D Gaussian Splatting representations for assets with 3D scene point clouds and a diffusion model for high-quality image generation. 3. Demonstration of the framework's capability to create diverse and realistic driving data on the Waymo Open Dataset.",
      "summary": "The paper proposes SCPainter, a unified framework that combines 3D Gaussian Splatting for asset representation with diffusion models to jointly perform realistic 3D asset insertion and novel view synthesis for autonomous driving simulation. The method projects 3D assets and scene point clouds into novel views and uses them to condition a diffusion model to generate high-quality images. Evaluation shows the framework can create diverse and realistic driving scenarios for training data.",
      "mindmap": "graph TB\n        A[SCPainter: 统一框架 / Unified Framework] --> B[核心问题 / Problem: 现有方法孤立处理资产插入与NVS / Existing methods treat asset insertion and NVS in isolation]\n        A --> C[主要方法 / Method: 集成3D高斯溅射与扩散模型 / Integrates 3D Gaussian Splatting and diffusion models]\n        A --> D[关键结果 / Results: 在Waymo数据集上实现真实感生成 / Enables realistic generation on Waymo dataset]"
    },
    {
      "title": "Memento-II: Learning by Stateful Reflective Memory",
      "authors": "Jun Wang",
      "institution": "University College London (UCL)",
      "link": "https://arxiv.org/pdf/2512.22716",
      "code": null,
      "tags": [
        "reinforcement learning",
        "stateful reflective decision process",
        "episodic memory",
        "policy iteration",
        "continual learning",
        "retrieval-augmented generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e8cff3c4a9f9d7b57c397010c69f5ae95897e34df30b8e86c43079e22a76db_w640_q70.webp",
      "contributions": "1. Introduces the Stateful Reflective Decision Process (SRDP), a formal theoretical framework that models continual learning in LLM agents as a two-stage read-write interaction with episodic memory, linking it to policy evaluation and improvement. 2. Provides a theoretical analysis showing that the reflective learning process induces an equivalent Markov Decision Process, enabling the use of classical dynamic programming and RL tools, and establishes convergence guarantees when instantiated with entropy-regularised policy iteration. 3. Unifies heuristic approaches like case-based reasoning and retrieval-augmented generation with principled reinforcement learning, offering a rigorous mathematical foundation for building memory-augmented agents capable of online adaptation without parameter updates.",
      "summary": "This paper proposes a theoretical framework for continual learning in LLM agents that uses episodic memory and reflection instead of back-propagation. The core method formalizes learning as a Stateful Reflective Decision Process, where writing to memory is policy evaluation and reading from it is policy improvement. The main conclusion is that this framework provides a principled, convergent foundation for agents to self-improve through interaction without fine-tuning.",
      "mindmap": "graph TB\n        A[Memento-II: Learning by Stateful Reflective Memory] --> B[核心问题/Problem: 缺乏理论解释/Lack of theoretical explanation for memory-based continual learning in LLM agents]\n        A --> C[主要方法/Method: 状态化反思决策过程/Stateful Reflective Decision Process (SRDP) with read-write episodic memory]\n        A --> D[关键结果/Results: 提供理论框架与收敛保证/Provides theoretical framework and convergence guarantees for optimal policy]\n        C --> E[写入对应策略评估/Writing corresponds to policy evaluation]\n        C --> F[读取对应策略改进/Reading corresponds to policy improvement]"
    },
    {
      "title": "Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning",
      "authors": "Youssef Megahed, Robin Ducharme, Inok Lee, Inbal Willner, Olivier X. Miguel, Kevin Dick, Adrian D. C. Chan, Mark Walker, Steven Hawken",
      "institution": "Carleton University, Ottawa Hospital Research Institute, University of Ottawa, Children's Hospital of Eastern Ontario Research Institute",
      "link": "https://arxiv.org/pdf/2512.22730",
      "code": null,
      "tags": [
        "medical image classification",
        "self-supervised learning",
        "masked autoencoding",
        "Score-CAM",
        "prenatal ultrasound",
        "cystic hygroma"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/99e72cff0ee8a4f27fb9382b2b663077b0e2b4a00e62b6b2d32980df58b008c0_w640_q70.webp",
      "contributions": "1. Applied and fine-tuned a large-scale, ultrasound-specific self-supervised model (USF-MAE) for the task of cystic hygroma detection. 2. Demonstrated that this self-supervised pre-training approach significantly outperforms a supervised baseline (DenseNet-169) trained from scratch on a small labeled dataset. 3. Provided qualitative interpretability analysis using Score-CAM visualizations to show the model's focus on clinically relevant anatomical regions.",
      "summary": "This paper addresses the challenge of automated cystic hygroma detection in prenatal ultrasound, where labeled data is scarce. The authors propose fine-tuning a self-supervised model (USF-MAE) pre-trained on a large corpus of unlabeled ultrasound images. Their method significantly outperforms a supervised baseline, demonstrating that ultrasound-specific self-supervised learning enables accurate, robust, and data-efficient detection for early prenatal screening.",
      "mindmap": "graph TB\n        Root[”Improved cystic hygroma detection using ultrasound-specific self-supervised learning<br>基于超声特异性自监督学习的囊性水瘤检测改进”]\n        Root --> Problem[”核心问题/Problem<br>Supervised deep learning for cystic hygroma detection is limited by small labeled datasets.<br>监督式深度学习受限于小规模标注数据集。”]\n        Root --> Method[”主要方法/Method<br>Fine-tune USF-MAE, a self-supervised model pre-trained on 370k+ unlabeled ultrasound images.<br>微调在37万+无标签超声图像上预训练的USF-MAE自监督模型。”]\n        Root --> Results[”关键结果/Results<br>Outperformed DenseNet-169 baseline (Accuracy: 0.96 vs 0.93). Performance gains are statistically significant.<br>性能超越DenseNet-169基线，提升具有统计显著性。”]"
    },
    {
      "title": "Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation",
      "authors": "Yongzhen Hu, Yihui Yang, Haotong Lin, Yifan Wang, Junting Dong, Yifu Deng, Xinyu Zhu, Fan Jia, Hujun Bao, Xiaowei Zhou, Sida Peng",
      "institution": "Zhejiang University, Ant Group, Shanghai AI Lab",
      "link": "https://arxiv.org/pdf/2512.22745",
      "code": null,
      "tags": [
        "4D scene reconstruction",
        "Freetime FeatureGS",
        "contrastive loss",
        "streaming feature learning",
        "Gaussian primitives",
        "decomposed reconstruction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23d1f40ecd27989546bd142e33db7b3761940a690e2422a0d6fefac0e033e539_w640_q70.webp",
      "contributions": "1. Proposes a novel decomposed 4D scene reconstruction method that eliminates the need for unstable video segmentation by relying only on per-image segmentation maps. 2. Introduces Freetime FeatureGS, a representation using Gaussian primitives with learnable features and linear motion, and a contrastive loss to enforce instance-level feature consistency. 3. Designs a temporally ordered streaming training strategy to propagate features over time, avoiding local minima and achieving accurate 4D segmentation.",
      "summary": "This paper addresses decomposed 4D scene reconstruction from multi-view videos by proposing a method that avoids reliance on video segmentation. The core idea is to represent the scene with a novel Freetime FeatureGS model and train it with a contrastive loss and a streaming strategy using only per-frame segmentation. Experiments show the method outperforms recent state-of-the-art approaches in reconstruction quality.",
      "mindmap": "graph TB\n        A[Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation] --> B[核心问题/Problem: 依赖视频分割的4D重建方法不稳定/Reliance on video segmentation leads to unstable 4D reconstruction]\n        A --> C[主要方法/Method: 提出Freetime FeatureGS与流式特征学习/Propose Freetime FeatureGS and streaming feature learning]\n        A --> D[关键结果/Results: 在多个数据集上超越现有方法/Outperforms recent methods on several datasets]"
    },
    {
      "title": "TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts",
      "authors": "Hao Zhang, Mengsi Lyu, Bo Huang, Yulong Ao, Yonghua Lin",
      "institution": "Beijing Academy of Artificial Intelligence (BAAI)",
      "link": "https://arxiv.org/pdf/2512.22748",
      "code": null,
      "tags": [
        "multi-modal inference",
        "visual token pruning",
        "long context",
        "adaptive budget allocation",
        "intra-image diversity",
        "inter-image variation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0de6ebee8f5094e1c2e0115398dde4fca37120436baa74c8f17b1c0de33ca18_w640_q70.webp",
      "contributions": "1. Identifies and analyzes the specific challenges of visual token pruning in long-context, multi-image scenarios. 2. Proposes a novel two-stage adaptive pruning method that decomposes redundancy into intra-image and inter-image components, guided by diversity and variation metrics. 3. Introduces a Pareto selection procedure in the inter-image stage to balance global token diversity with text alignment.",
      "summary": "This paper addresses the high inference cost of Large Multimodal Models (LMMs) in long-context, multi-image settings by proposing TrimTokenator-LC, an adaptive visual token pruning method. The method uses a two-stage process to dynamically allocate token budgets based on intra-image diversity and inter-image variation, then selects tokens by balancing diversity and text relevance. Experiments show the approach significantly reduces the number of visual tokens while maintaining strong performance in long-context tasks.",
      "mindmap": "graph TB\n        A[TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[视觉令牌冗余导致长上下文多图像推理成本高/High inference cost from redundant visual tokens in long-context multi-image settings]\n        C --> C1[分解冗余为图像内和图像间/Decompose redundancy into intra-image and inter-image]\n        C --> C2[两阶段自适应剪枝/Two-stage adaptive pruning]\n        C2 --> C2_1[图像内阶段：内容感知预算与贪婪选择/Intra-image: content-aware budget & greedy selection]\n        C2 --> C2_2[图像间阶段：全局多样性过滤与帕累托选择/Inter-image: global diversity filtering & Pareto selection]\n        D --> D1[在长上下文中保持强性能/Maintains strong performance in long context]\n        D --> D2[显著减少视觉令牌数量/Significantly cuts down visual tokens]"
    },
    {
      "title": "Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers",
      "authors": "Yunge Li, Lanyu Xu",
      "institution": "Oakland University",
      "link": "https://arxiv.org/pdf/2512.22760",
      "code": "https://github.com/Yunge6666/NAP-MAT",
      "tags": [
        "vision transformer efficiency",
        "token reduction",
        "Hilbert curve",
        "neighbor-aware pruning",
        "token merging",
        "spatial continuity"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4ca51a47453a80fb378823f123202bcb915998b4809942ea05464a170da615b_w640_q70.webp",
      "contributions": "1. Proposes a novel neighbor-aware token reduction framework for Vision Transformers that explicitly preserves spatial continuity and local context. 2. Introduces Neighbor-Aware Pruning (NAP), which incorporates the influence of neighboring tokens into importance scoring for selective retention. 3. Introduces Merging by Adjacent Token similarity (MAT), a localized token aggregation strategy that computes similarity and merges tokens only within adjacent regions.",
      "summary": "This paper addresses the computational inefficiency of Vision Transformers caused by redundant token representations. It proposes a novel token reduction method based on Hilbert curve reordering, which includes Neighbor-Aware Pruning (NAP) and Merging by Adjacent Token similarity (MAT) to preserve spatial neighbor structure. The approach achieves state-of-the-art accuracy-efficiency trade-offs, highlighting the importance of spatial continuity for ViT optimization.",
      "mindmap": "graph TB\n        Root(”Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”ViT计算效率低/ViT Computational Inefficiency”)\n        Problem --> P2(”现有方法忽略空间连续性/Existing Methods Overlook Spatial Continuity”)\n        Method --> M1(”Hilbert曲线重排序/Hilbert Curve Reordering”)\n        Method --> M2(”邻居感知剪枝(NAP)/Neighbor-Aware Pruning (NAP)”)\n        Method --> M3(”相邻令牌合并(MAT)/Merging by Adjacent Token similarity (MAT)”)\n        Results --> R1(”SOTA精度-效率权衡/SOTA Accuracy-Efficiency Trade-off”)\n        Results --> R2(”强调空间连续性/Highlights Spatial Continuity”)"
    },
    {
      "title": "Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting",
      "authors": "Yiqian Li, Wen Jiang, Kostas Daniilidis",
      "institution": "University of Pennsylvania",
      "link": "https://arxiv.org/pdf/2512.22771",
      "code": null,
      "tags": [
        "3d reconstruction",
        "3D Gaussian Splatting",
        "Next Best View",
        "Active Learning",
        "Fisher Information",
        "Dynamic Scene Modeling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/512528158b99bf9d8d5519331a5d1557baee5b3050273bff66df842767a73963_w640_q70.webp",
      "contributions": "1. Formulates the next-best-view selection problem for dynamic and semantic 3D scenes as an active learning problem. 2. Proposes an active learning algorithm using Fisher Information to quantify view informativeness for both semantic Gaussian parameters and deformation networks. 3. Provides a unified framework that jointly handles semantic reasoning and dynamic scene modeling, outperforming heuristic and random baselines.",
      "summary": "This paper addresses the challenge of selecting the most informative camera views for training dynamic and semantic 3D Gaussian Splatting models. It proposes an active learning method based on Fisher Information to prioritize frames that maximize information gain for both geometry and semantics. The approach improves rendering quality and segmentation performance compared to random or uncertainty-based selection strategies.",
      "mindmap": "graph TB\n    A[Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting] --> B(核心问题/Problem: Data redundancy in dynamic & semantic scene understanding, need for efficient view selection)\n    A --> C(主要方法/Method: Active learning with Fisher Information to quantify informativeness of views for semantic Gaussians & deformation networks)\n    A --> D(关键结果/Results: Improved rendering quality & semantic segmentation, outperforms random & heuristic baselines)"
    },
    {
      "title": "Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization",
      "authors": "Truong Son Nguyen",
      "institution": "Arizona State University",
      "link": "https://arxiv.org/pdf/2512.22774",
      "code": null,
      "tags": [
        "quantum-inspired machine learning",
        "spectral decomposition",
        "Hamiltonian learning",
        "semantic wavefunctions",
        "operator calculus",
        "emergent manifolds"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58825c1201e17641b4e19a581a742615913539c66fbf08e5c113620bf7eec6de_w640_q70.webp",
      "contributions": "1. A unified spectral-dynamical framework for machine learning inspired by quantum mechanics, comprising a time-independent wave-energy solver, a time-dependent dynamical solver, and a low-rank operator calculus. 2. Demonstration of emergent semantic manifolds that reflect class relations without explicit supervision, dynamic reasoning that adapts to changing environments, and exact operator generalization on symbolic tasks. 3. Proposing a new foundational direction for ML where learning is cast as discovering and navigating an underlying semantic energy landscape, offering an alternative to cross-entropy training and transformer attention.",
      "summary": "This paper introduces Schrödinger AI, a novel machine learning framework inspired by quantum mechanics that treats perception as spectral decomposition and reasoning as wavefunction dynamics. The method demonstrates robust generalization, interpretable semantics, and emergent topology on tasks like classification, maze navigation, and modular arithmetic. The results suggest a promising new paradigm for AI that learns by discovering an underlying semantic energy landscape.",
      "mindmap": "graph TB\n        Root[”Schrödinger AI: A Unified Spectral-Dynamical Framework<br>薛定谔AI: 统一谱-动力学框架”] --> Problem[”核心问题/Problem<br>Limitations of conventional ML<br>传统机器学习的局限”]\n        Root --> Method[”主要方法/Method<br>Quantum-inspired framework<br>量子启发的框架”]\n        Root --> Results[”关键结果/Results<br>Empirical demonstrations<br>实证演示”]\n        Problem --> P1[”Struggle with uncertainty & adaptation<br>难以处理不确定性与适应性”]\n        Problem --> P2[”Brittle symbolic reasoning<br>脆弱的符号推理”]\n        Method --> M1[”Time-independent wave-energy solver<br>时间无关波能求解器”]\n        Method --> M2[”Time-dependent dynamical solver<br>时间相关动力学求解器”]\n        Method --> M3[”Low-rank operator calculus<br>低秩算子演算”]\n        Results --> R1[”Emergent semantic manifolds<br>涌现的语义流形”]\n        Results --> R2[”Dynamic reasoning adaptation<br>动态推理适应”]\n        Results --> R3[”Exact operator generalization<br>精确算子泛化”]"
    },
    {
      "title": "Plug In, Grade Right: Psychology-Inspired AGIQA",
      "authors": "Zhicheng Liao, Baoliang Chen, Hanwei Zhu, Lingyu Zhu, Shiqi Wang, Weisi Lin",
      "institution": "South China Normal University, City University of Hong Kong, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.22780",
      "code": null,
      "tags": [
        "image quality assessment",
        "AGIQA",
        "Graded Response Model",
        "semantic drift",
        "quality grading",
        "plug-and-play"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e5502866af39224d2021cd83326b1c76e8d4a8dfa3a4f757b8f51a03362124c_w640_q70.webp",
      "contributions": "1. Identifies and defines the \"semantic drift\" problem in existing AGIQA models where image embeddings show inconsistent similarity to multi-grade text descriptions. 2. Proposes a psychology-inspired, improved Graded Response Model (GRM) for AGIQA, framing quality as an image's \"ability\" to meet \"difficulty\" levels. 3. Designs a novel Arithmetic GRM based Quality Grading (AGQG) module that enforces a unimodal, interpretable quality distribution and demonstrates plug-and-play performance gains across various frameworks.",
      "summary": "This paper addresses the \"semantic drift\" issue in AI-generated image quality assessment (AGIQA), where inconsistent similarities between image and text embeddings degrade reliability. The authors propose a novel Arithmetic GRM based Quality Grading (AGQG) module, inspired by psychometrics, which models image quality as an ability to overcome graded difficulty levels. The plug-and-play module consistently improves state-of-the-art AGIQA models and generalizes to both natural and screen content images.",
      "mindmap": "graph TB\n        Root[”Plug In, Grade Right: Psychology-Inspired AGIQA”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”语义漂移/Semantic Drift”]\n        Problem --> P2[”多模态相似度分布/Multimodal Similarity”]\n        Method --> M1[”分级响应模型/Graded Response Model (GRM)”]\n        Method --> M2[”双分支模块/Two-branch Module”]\n        Method --> M3[”算术难度生成/Arithmetic Difficulty”]\n        Results --> R1[”即插即用提升/Plug-and-Play Improvement”]\n        Results --> R2[”泛化能力/Generalization”]"
    },
    {
      "title": "VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM",
      "authors": "Jingchao Wang, Kaiwen Zhou, Zhijian Wu, Kunhua Ji, Dingjiang Huang, Yefeng Zheng",
      "institution": "East China Normal University, Westlake University",
      "link": "https://arxiv.org/pdf/2512.22799",
      "code": "https://github.com/jcwang0602/VPTracker",
      "tags": [
        "object tracking",
        "vision-language tracking",
        "multimodal large language model",
        "visual prompt",
        "global search",
        "location-aware"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a9617d5ed5f0e1ed9ff8bb7b07da944102f01b36dcfa1122d537a24b6a20916e_w640_q70.webp",
      "contributions": "1. Introduces the first global vision-language tracking framework based on Multimodal Large Language Models (MLLMs)., 2. Proposes a location-aware visual prompting mechanism to incorporate spatial priors and suppress distractions., 3. Demonstrates enhanced tracking stability and target disambiguation in challenging scenarios, opening a new avenue for MLLM integration in visual tracking.",
      "summary": "This paper proposes VPTracker, a novel global vision-language tracking framework that leverages Multimodal Large Language Models (MLLMs) for robust target localization across the entire image. To address distractions from global search, it introduces a location-aware visual prompting mechanism that uses the target's previous location as a spatial prior. Experiments show the method significantly improves tracking stability and disambiguation under challenging conditions like occlusions and rapid motion.",
      "mindmap": "graph TB\n        A[VPTracker: Global Vision-Language Tracking] --> B[核心问题/Problem: 现有局部搜索方法在视角变化、遮挡、快速运动下易失效/Existing local search methods prone to failure under viewpoint changes, occlusions, rapid motion]\n        A --> C[主要方法/Method: 基于MLLM的全局跟踪框架 + 位置感知视觉提示/Global tracking via MLLM + Location-aware visual prompt]\n        A --> D[关键结果/Results: 显著提升跟踪稳定性和目标辨别能力/Significantly enhances tracking stability and target disambiguation]"
    },
    {
      "title": "Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation",
      "authors": "Bin Liu, Wenyan Tian, Huangxin Fu, Zizheng Li, Zhifen He, Bo Li",
      "institution": "Nanchang Hangkong University, Guilin University Of Electronic Technology",
      "link": "https://arxiv.org/pdf/2512.22800",
      "code": null,
      "tags": [
        "3D reconstruction",
        "3D Gaussian Representation",
        "tri-plane representation",
        "sparse reconstruction",
        "semantic segmentation",
        "medical image analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c719633132efd891759bf317060eb9e7deff2cc44fa35a7c33b3b080dba098fb_w640_q70.webp",
      "contributions": "1. Proposes an efficient 3D reconstruction method for medical images by combining 3D Gaussian and tri-plane representations. 2. Enhances structural continuity and semantic consistency under sparse slice conditions, addressing a key limitation of traditional methods. 3. Demonstrates high-quality, anatomically coherent reconstruction on multimodal medical datasets (US, MRI) with improved efficiency.",
      "summary": "This paper addresses the challenges of computationally expensive and detail-losing 3D reconstruction from sparse medical image slices. It proposes a novel method that integrates 3D Gaussian and tri-plane representations to efficiently generate high-quality, anatomically coherent 3D visualizations. Experiments on US and MRI data confirm the method's effectiveness in improving reconstruction quality and efficiency under sparse data conditions.",
      "mindmap": "graph TB\n        A[Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation] --> B(核心问题/Problem: 稀疏切片下传统3D重建方法计算昂贵、结构不连续/Traditional 3D reconstruction is computationally expensive and structurally discontinuous with sparse slices)\n        A --> C(主要方法/Method: 基于3D高斯表示与三平面表示的高效重建方法/Efficient reconstruction method based on 3D Gaussian and tri-plane representations)\n        A --> D(关键结果/Results: 在稀疏数据下生成高质量、解剖一致且高效的3D医学图像/Generates high-quality, anatomically coherent, and efficient 3D medical images under sparse data)"
    },
    {
      "title": "ReDiF: Reinforced Distillation for Few Step Diffusion",
      "authors": "Amirhossein Tighkhorshid, Zahra Dehghanian, Gholamali Aminian, Chengchun Shi, Hamid R. Rabiee",
      "institution": "Sharif University of Technology, Alan Turing Institute, London School of Economics",
      "link": "https://arxiv.org/pdf/2512.22802",
      "code": null,
      "tags": [
        "diffusion models",
        "reinforcement learning",
        "knowledge distillation",
        "policy optimization",
        "denoising paths",
        "model agnostic"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73d47eb5443f9f8848454e65825da3569c70d954239d9794e6cff086fa5bc17a_w640_q70.webp",
      "contributions": "1. Proposes a novel reinforcement learning framework for distilling diffusion models, treating distillation as a policy optimization problem. 2. Introduces a reward signal based on alignment with teacher outputs, allowing the student model to explore multiple denoising paths and take longer, optimized steps. 3. Demonstrates a model-agnostic framework that achieves superior performance with fewer inference steps and computational resources compared to existing distillation techniques.",
      "summary": "This paper addresses the slow sampling problem in diffusion models by proposing ReDiF, a reinforcement learning-based distillation framework. Instead of using fixed losses, it treats distillation as policy optimization, using a reward signal to guide the student to take longer, optimized steps. The method achieves better performance with fewer steps and is applicable to various diffusion models.",
      "mindmap": "graph TB\n        A[ReDiF: Reinforced Distillation for Few Step Diffusion] --> B(核心问题/Problem: Diffusion模型采样慢/Slow sampling in diffusion models)\n        A --> C(主要方法/Method: 基于强化学习的蒸馏框架/Reinforcement learning based distillation framework)\n        A --> D(关键结果/Results: 更少步骤，性能更优/Fewer steps, superior performance)"
    },
    {
      "title": "Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image",
      "authors": "Po-Chih Wu",
      "institution": "National Yang Ming Chiao Tung University",
      "link": "https://arxiv.org/pdf/2512.22801",
      "code": "https://github.com/gohakushi1118/Low-quality-image-dataset",
      "tags": [
        "object detection",
        "open-vocabulary object detection",
        "low-quality image",
        "image degradation",
        "benchmark dataset",
        "OWLv2"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3523d24298701084e07c062fb88e9e451f2e2a096e82a3cfc75f02cf63d866ab_w640_q70.webp",
      "contributions": "1. Constructed a new benchmark dataset simulating real-world low-quality images with four types of degradation (lossy compression, image intensity, noise, blur). 2. Evaluated six state-of-the-art open-vocabulary object detection models on this benchmark to assess their robustness. 3. Provided analysis showing varying model sensitivity to degradation types and levels, with OWLv2 demonstrating more consistent performance.",
      "summary": "This paper evaluates the robustness of open-vocabulary object detection models on low-quality images by creating a new benchmark dataset with various image degradations. The experiments show that while models are resilient to mild degradation, severe degradation causes significant performance drops, with OWLv2 models being the most robust.",
      "mindmap": "graph TB\n    A[Evaluating Open-Vocabulary Object Detection in Low-quality Image] --> B[核心问题/Problem: 真实世界低质量图像对开放词汇目标检测的影响/Impact of real-world low-quality images on open-vocabulary object detection]\n    A --> C[主要方法/Method: 构建包含四种退化类型的低质量图像数据集/Build a low-quality image dataset with four degradation types]\n    A --> D[关键结果/Results: 模型对严重退化敏感，OWLv2表现最稳健/Models are sensitive to severe degradation, OWLv2 is the most robust]"
    },
    {
      "title": "Parallel Diffusion Solver via Residual Dirichlet Policy Optimization",
      "authors": "Ruoyu Wang, Ziyu Li, Beier Zhu, Liangyu Yuan, Hanwang Zhang, Xun Yang, Xiaojun Chang, Chi Zhang",
      "institution": "Westlake University, University of Illinois Urbana-Champaign, Nanyang Technological University, Shanghai Jiao Tong University, University of Science and Technology of China",
      "link": "https://arxiv.org/pdf/2512.22796",
      "code": null,
      "tags": [
        "diffusion models",
        "ODE solver",
        "parallel gradient evaluation",
        "reinforcement learning fine-tuning",
        "low-latency sampling",
        "Dirichlet policy"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2678a61b07c4f5b5cfdd2006673a05c8a4699c07dee6180c47750600496f796_w640_q70.webp",
      "contributions": "1. Proposes EPD-Solver, a novel ODE solver that uses multiple parallel gradient evaluations per step to reduce truncation errors while maintaining low latency. 2. Introduces a two-stage optimization framework, including a parameter-efficient RL fine-tuning scheme that reformulates the solver as a stochastic Dirichlet policy to avoid reward hacking. 3. Demonstrates the method's flexibility as a plugin (EPD-Plugin) to enhance existing ODE samplers and shows state-of-the-art performance in both unconditional and text-to-image generation benchmarks.",
      "summary": "This paper addresses the high sampling latency of diffusion models by proposing EPD-Solver, a novel ODE solver that incorporates parallel gradient evaluations to reduce errors without increasing latency. The method uses a two-stage optimization, including RL fine-tuning with a Dirichlet policy, and can be used as a plugin. Experiments show it achieves superior image quality at low step counts and improves human preference scores in text-to-image generation.",
      "mindmap": "graph TB\n        A[Parallel Diffusion Solver via Residual Dirichlet Policy Optimization] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[扩散模型采样延迟高 / High sampling latency of DMs]\n        B --> B2[现有求解器在低步数下质量下降 / Existing solvers degrade quality at low NFEs]\n        C --> C1[EPD-Solver: 集成并行方向求解器 / Ensemble Parallel Direction solver]\n        C --> C2[两阶段优化: 蒸馏 + RL微调 / Two-stage optimization: Distillation + RL fine-tuning]\n        C --> C3[作为插件提升现有求解器 / Plugin (EPD-Plugin) for existing samplers]\n        D --> D1[低延迟下SOTA FID分数 / SOTA FID scores at low latency]\n        D --> D2[在T2I任务中提升人类偏好分数 / Improved human preference scores in T2I]"
    },
    {
      "title": "EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation",
      "authors": "Libo Zhang, Zekun Li, Tianyu Li, Zeyu Cao, Rui Xu, Xiaoxiao Long, Wenjia Wang, Jingbo Wang, Yuan Liu, Wenping Wang, Daquan Zhou, Taku Komura, Zhiyang Dou",
      "institution": "THU, Brown, Georgia Tech, Cambridge, HKU, NJU, CUHK, HKUST, TAMU, PKU, MIT",
      "link": "https://arxiv.org/pdf/2512.22808",
      "code": null,
      "tags": [
        "human motion generation",
        "egocentric video",
        "3D human reaction",
        "autoregressive generation",
        "VQ-VAE",
        "GPT"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/157e088cb49368b1dbc239ff6380ef8897576c9c3fa92a5bf6737c3a5029e927_w640_q70.webp",
      "contributions": "1. Constructed the Human Reaction Dataset (HRD), a spatially aligned egocentric video-reaction dataset to address data scarcity and misalignment in existing resources. 2. Proposed EgoReAct, the first autoregressive framework for real-time, 3D-aligned human reaction motion generation from streaming egocentric video. 3. Incorporated 3D dynamic features (metric depth, head dynamics) into the generation pipeline to enhance spatial grounding and realism.",
      "summary": "This paper tackles the challenge of generating realistic and spatially aligned 3D human reactions from egocentric video streams. The authors propose EgoReAct, an autoregressive framework that uses a VQ-VAE and a GPT to generate motions in real-time, enhanced by 3D features. Experiments show the method achieves superior realism, spatial consistency, and efficiency while maintaining strict causality.",
      "mindmap": "graph TB\n        A[EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有数据空间不一致/Existing data spatial misalignment]\n        B --> B2[因果生成与3D对齐的挑战/Causal generation & 3D alignment challenge]\n        C --> C1[构建HRD数据集/Build HRD dataset]\n        C --> C2[VQ-VAE压缩运动/VQ-VAE compresses motion]\n        C --> C3[GPT自回归生成/GPT autoregressive generation]\n        C --> C4[融入3D动态特征/Incorporate 3D dynamic features]\n        D --> D1[更高的真实感与空间一致性/Higher realism & spatial consistency]\n        D --> D2[实时生成效率/Real-time generation efficiency]\n        D --> D3[保持严格因果性/Maintains strict causality]"
    },
    {
      "title": "3D Scene Change Modeling With Consistent Multi-View Aggregation",
      "authors": "Zirui Zhou, Junfeng Ni, Shujie Zhang, Yixin Chen, Siyuan Huang",
      "institution": "Tsinghua University, State Key Laboratory of General Artificial Intelligence (BIGAI)",
      "link": "https://arxiv.org/pdf/2512.22830",
      "code": "https://zr-zhou0o0.github.io/SCaR3D/",
      "tags": [
        "3D scene understanding / change detection",
        "3D Gaussian Splatting",
        "signed-distance field",
        "multi-view aggregation",
        "continual reconstruction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78ed554a37f9d36201f10d441a5e94d0905658a4c25e8528463fcb824b31d7b3_w640_q70.webp",
      "contributions": "1. Proposes SCaR-3D, a novel 3D scene change detection framework that uses a signed-distance-based 2D differencing module and multi-view aggregation with voting/pruning to robustly separate pre- and post-change states. 2. Develops a continual scene reconstruction strategy that selectively updates dynamic regions while preserving unchanged areas. 3. Contributes CCS3D, a challenging synthetic dataset for flexible and controlled evaluation of 3D change types.",
      "summary": "This paper addresses the problem of detecting object-level changes in 3D scenes from multi-view images, where existing methods suffer from spatial inconsistency and cannot separate pre- and post-change states. The proposed SCaR-3D framework leverages 3D Gaussian Splatting for consistent multi-view aggregation and includes a strategy for continual scene reconstruction. Experiments show the method outperforms existing approaches in both accuracy and efficiency.",
      "mindmap": "graph TB\n        A[3D Scene Change Modeling With Consistent Multi-View Aggregation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有3D变化检测方法存在空间不一致性且无法分离变化前后状态/Existing 3D change detection methods exhibit spatial inconsistency and fail to separate pre- and post-change states]\n        C --> C1[提出SCaR-3D框架: 基于符号距离的2D差分与多视图聚合投票剪枝/Propose SCaR-3D: Signed-distance-based 2D differencing and multi-view aggregation with voting & pruning]\n        C --> C2[开发持续场景重建策略: 选择性更新动态区域/Develop continual reconstruction: Selectively update dynamic regions]\n        C --> C3[贡献CCS3D合成数据集/Contribute CCS3D synthetic dataset]\n        D --> D1[高精度与高效率/High accuracy and efficiency]\n        D --> D2[优于现有方法/Outperforms existing methods]"
    },
    {
      "title": "KANO: Kolmogorov-Arnold Neural Operator for Image Super-Resolution",
      "authors": "Chenyu Li, Danfeng Hong, Bing Zhang, Zhaojie Pan, Jocelyn Chanussot",
      "institution": "Southeast University, Aerospace Information Research Institute (Chinese Academy of Sciences), Univ. Grenoble Alpes",
      "link": "https://arxiv.org/pdf/2512.22822",
      "code": null,
      "tags": [
        "image super-resolution",
        "Kolmogorov-Arnold Neural Operator",
        "B-spline functions",
        "interpretability",
        "spectral fitting",
        "degradation modeling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96c01412f6132ddd4983bd966141d1e72338121a3cf338d50ea6ab033a26816c_w640_q70.webp",
      "contributions": "1. Proposes the novel Kolmogorov-Arnold Neural Operator (KANO) for interpretable image super-resolution, inspired by the Kolmogorov-Arnold theorem. 2. Employs an additive structure of B-spline functions to model the degradation process transparently, capturing key spectral characteristics like local trends and peak-valley structures. 3. Provides a systematic comparative study between MLPs and KANs for complex sequence fitting, offering insights into interpretable SR model design.",
      "summary": "This paper addresses the challenge of interpretability in single-image super-resolution by proposing a new model called KANO, which uses B-spline functions based on the Kolmogorov-Arnold theorem to transparently model the image degradation process. The method provides physically interpretable results by learning spectral characteristics. The authors demonstrate its effectiveness and compare it with other models across different image types.",
      "mindmap": "graph TB\n        Root[KANO: Kolmogorov-Arnold Neural Operator for Image Super-Resolution] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Interpretable Super-Resolution] --> P1[SR是病态逆问题/SR is Ill-posed]\n        Problem --> P2[现有方法黑盒/Existing Methods are Black-box]\n        Method[主要方法/Method: KANO Model] --> M1[基于KAT定理/Based on KAT]\n        Method --> M2[B样条函数拟合/B-spline Fitting]\n        Method --> M3[学习形状参数/Learn Shape Parameters]\n        Results[关键结果/Results] --> R1[透明降解表示/Transparent Degradation Representation]\n        Results --> R2[物理可解释性/Physical Interpretability]\n        Results --> R3[系统模型比较/Systematic Model Comparison]"
    },
    {
      "title": "Depth Anything in $360^$: Towards Scale Invariance in the Wild",
      "authors": "Hualie Jiang, Ziyang Song, Zhiqiang Lou, Rui Xu, Minglang Tan",
      "institution": "Insta360 Research",
      "link": "https://arxiv.org/pdf/2512.22819",
      "code": "https://insta360-research-team.github.io/DA360",
      "tags": [
        "depth estimation",
        "panoramic depth estimation",
        "scale invariance",
        "zero-shot generalization",
        "circular padding",
        "ViT backbone"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f48de8ab79cd62e59a71f803895b0da94c4bc7e241ab05b9ebd97bf7af12448_w640_q70.webp",
      "contributions": "1. Proposes DA360, a panoramic-adapted version of Depth Anything V2 that learns a shift parameter to transform scale- and shift-invariant output into scale-invariant disparity for direct 3D point cloud generation. 2. Integrates circular padding into the DPT decoder to eliminate seam artifacts and ensure spatially coherent depth maps respecting spherical continuity. 3. Introduces a new outdoor panoramic depth dataset, Metropolis, for evaluation and demonstrates state-of-the-art zero-shot performance on indoor and outdoor benchmarks.",
      "summary": "This paper addresses the gap in zero-shot generalization for panoramic depth estimation by proposing DA360, an adaptation of Depth Anything V2. The method learns a shift parameter for scale-invariant output and uses circular padding to prevent seam artifacts, enabling direct conversion to 3D point clouds. The results show significant error reduction compared to the base model and other panoramic methods, establishing new state-of-the-art performance.",
      "mindmap": "graph TB\n        Root[”Depth Anything in 360°: Towards Scale Invariance in the Wild”] --> Problem[”核心问题/Problem: Panoramic depth estimation lags in zero-shot generalization compared to perspective images.”]\n        Root --> Method[”主要方法/Method: Adapt Depth Anything V2 with learned shift parameter for scale invariance and circular padding for seam removal.”]\n        Root --> Results[”关键结果/Results: Achieves >50% and >10% error reduction on indoor/outdoor benchmarks, outperforms PanDA by ~30%.”]"
    },
    {
      "title": "A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences",
      "authors": "Zhenbao Yu, Shirong Ye, Ronghe Jin, Shunkun Liang, Zibin Liu, Huiyun Zhang, Banglei Guan",
      "institution": "National University of Defense Technology, Wuhan University, Henan University",
      "link": "https://arxiv.org/pdf/2512.22833",
      "code": null,
      "tags": [
        "relative pose estimation",
        "affine correspondences",
        "polynomial eigenvalue method",
        "minimal solver",
        "inertial measurement unit",
        "focal length estimation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd3f71922ab7368b3202c0df07c429238e34d3301688ac8ba9116c190b808c1b_w640_q70.webp",
      "contributions": "1. Proposes a new minimal solver for estimating 3DOF relative pose and focal length from only two affine correspondences when the vertical direction is known from an IMU. 2. Derives a system of four constraint equations that reduce the problem to solving for only two parameters: focal length and relative rotation angle. 3. Utilizes the polynomial eigenvalue method to efficiently solve the derived equations, demonstrating superior performance over state-of-the-art solvers on synthetic and real-world datasets.",
      "summary": "This paper introduces a new minimal solver for estimating the relative pose and focal length between two camera views. The method uses only two affine correspondences and known vertical direction (from an IMU) to formulate constraint equations, which are then solved using a polynomial eigenvalue approach. Experimental results show the proposed solver outperforms existing state-of-the-art methods.",
      "mindmap": "graph TB\n        Root[”A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences<br>基于两个仿射对应的未知焦距相对位姿估计最小求解器”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Estimate relative pose & focal length from minimal data<br>从最少数据估计相对位姿和焦距”] --> P1[”已知垂直方向/Known vertical direction (from IMU)<br>自由度从5DOF降至3DOF”]\n        Problem --> P2[”输入/Input<br>两个仿射对应/Two affine correspondences”]\n        Method[”主要方法/Method<br>建立约束方程并求解/Establish constraints and solve”] --> M1[”建立方程/Establish equations from two ACs”]\n        Method --> M2[”推导系统/Derive 4 equations for 2 parameters<br>焦距和旋转角/focal length & rotation angle”]\n        Method --> M3[”求解方法/Solution Method<br>多项式特征值方法/Polynomial eigenvalue method”]\n        Results[”关键结果/Results<br>评估与比较/Evaluation & Comparison”] --> R1[”合成与真实数据/Synthetic & real-world datasets”]\n        Results --> R2[”性能更好/Performs better than state-of-the-art solvers”]"
    },
    {
      "title": "ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning",
      "authors": "Bangya Liu, Xinyu Gong, Zelin Zhao, Ziyang Song, Yulei Lu, Suhui Wu, Jun Zhang, Suman Banerjee, Hao Zhang",
      "institution": "University of Wisconsin-Madison, ByteDance, Georgia Institute of Technology, The Hong Kong Polytechnic University",
      "link": "https://arxiv.org/pdf/2512.22854",
      "code": "https://neutrinoliu.github.io/byteloom/",
      "tags": [
        "video generation",
        "Human-Object Interaction",
        "Diffusion Transformer",
        "Relative Coordinate Maps",
        "Progressive Curriculum Learning",
        "Geometry Consistency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b9f5ff16308904b889be6695aafd24bfc28b798f080cc6c723a25066bcdc2bdf_w640_q70.webp",
      "contributions": "1. Proposes ByteLoom, a Diffusion Transformer-based framework for generating realistic HOI videos with geometrically consistent objects. 2. Introduces the RCM-cache mechanism using Relative Coordinate Maps to maintain object geometry consistency and control 6-DoF transformations. 3. Designs a progressive training curriculum to compensate for HOI dataset scarcity and relax the need for fine-grained hand mesh annotations.",
      "summary": "This paper addresses the challenges of poor cross-view consistency and reliance on hand mesh annotations in Human-Object Interaction (HOI) video generation. It proposes ByteLoom, a framework that uses a novel RCM-cache mechanism for geometry consistency and a progressive curriculum learning strategy for training. The method effectively preserves human identity and object geometry while generating smooth motion.",
      "mindmap": "graph TB\n        A[ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法缺乏多视图信息注入机制/Existing methods lack multi-view injection]\n        B --> B2[严重依赖手部网格标注/Heavy reliance on hand mesh annotations]\n        C --> C1[提出RCM-cache机制/Propose RCM-cache mechanism]\n        C --> C2[设计渐进式课程学习/Design progressive curriculum learning]\n        D --> D1[保持物体几何一致性/Preserves object geometry consistency]\n        D --> D2[生成平滑运动视频/Generates smooth motion videos]"
    },
    {
      "title": "MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments",
      "authors": "Zhuonan Liu, Xinyu Zhang, Zishuo Wang, Tomohito Kawabata, Xuesu Xiao, Ling Xiao",
      "institution": "Hokkaido University, George Mason University",
      "link": "https://arxiv.org/pdf/2512.22867",
      "code": null,
      "tags": [
        "embodied navigation",
        "socially compliant navigation",
        "multimodal dataset",
        "chain-of-thought",
        "vision language models",
        "benchmark"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5248e75c8017605d3d39791503fba58a4cce5e655f4d900abeee425ea863b824_w640_q70.webp",
      "contributions": "1. Introduces MUSON, a new multimodal dataset for socially compliant navigation with structured five-step Chain-of-Thought annotations (perception, prediction, reasoning, action, explanation). 2. Addresses limitations of prior datasets by explicitly modeling static physical constraints and providing a rationally balanced discrete action space to overcome long-tailed action distributions. 3. Establishes MUSON as an effective benchmark, demonstrating its utility by benchmarking state-of-the-art Small Vision Language Models, with Qwen2.5-VL-3B achieving the highest decision accuracy.",
      "summary": "The paper introduces MUSON, a reasoning-oriented multimodal dataset designed to address the lack of explicit reasoning supervision and imbalanced action distributions in existing social navigation datasets. It features structured Chain-of-Thought annotations and a balanced action space. Benchmarking results show that MUSON serves as an effective benchmark, with Qwen2.5-VL-3B achieving the highest accuracy, demonstrating its utility for training and evaluating socially compliant navigation models.",
      "mindmap": "graph TB\n        Root[MUSON: 面向推理的多模态城市社会合规导航数据集] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有数据集缺乏显式推理监督/Lack explicit reasoning supervision]\n        Problem --> P2[动作分布高度长尾/Highly long-tailed action distribution]\n        Method[主要方法/Method] --> M1[引入MUSON数据集/Introduce MUSON dataset]\n        M1 --> M1_Sub1[五步思维链标注/Five-step Chain-of-Thought annotation]\n        M1 --> M1_Sub2[平衡的离散动作空间/Balanced discrete action space]\n        Results[关键结果/Results] --> R1[Qwen2.5-VL-3B取得最高精度/Qwen2.5-VL-3B achieves highest accuracy]\n        Results --> R2[数据集作为有效基准/Dataset serves as effective benchmark]"
    },
    {
      "title": "Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs",
      "authors": "Ziyu Zhou, Haozhe Luo, Mohammad Reza Hosseinzadeh Taher, Jiaxuan Pang, Xiaowei Ding, Michael B. Gotway, Jianming Liang",
      "institution": "Shanghai Jiao Tong University, Arizona State University, University of Bern, Mayo Clinic",
      "link": "https://arxiv.org/pdf/2512.22872",
      "code": "GitHub.com/JLiangLab/Lamps",
      "tags": [
        "self-supervised learning",
        "self-supervised learning",
        "foundation model",
        "chest radiographs",
        "anatomical consistency",
        "multi-perspective learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f086b74ae968cdc69d3bc3132f42d25c77e16078cfd4f970417ab16219c8be5e_w640_q70.webp",
      "contributions": "1. Proposes a novel self-supervised learning framework (Lamps) that explicitly leverages the consistency, coherence, and hierarchy of human anatomy as supervision signals. 2. Demonstrates superior performance and robustness across 10 chest X-ray datasets compared to 10 baseline models. 3. Releases code and pre-trained models to facilitate research in anatomy-aware foundation models for medical imaging.",
      "summary": "The paper addresses the limitation of existing self-supervised learning methods in medical imaging, which often overlook anatomical structures. It proposes Lamps, a method that learns from multiple anatomical perspectives (consistency, coherence, hierarchy) using self-supervision on chest radiographs. The results show that Lamps achieves superior robustness and transferability, offering a promising foundation model aligned with human anatomy.",
      "mindmap": "graph TB\n        Root[”Lamps: Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs”] --> Problem[”核心问题/Problem: Existing SSL methods overlook anatomical consistency, coherence, and hierarchy in medical images.”]\n        Root --> Method[”主要方法/Method: Lamps uses anatomical consistency, coherence, and hierarchy as self-supervision signals for pre-training on chest X-rays.”]\n        Root --> Results[”关键结果/Results: Superior robustness and transferability across 10 datasets vs. 10 baselines; code and models released.”]"
    },
    {
      "title": "Let Samples Speak: Mitigating Spurious Correlation by Exploiting the Clusterness of Samples",
      "authors": "Weiwei Li, Junzhuo Liu, Yuanyuan Ren, Yuchen Zheng, Yahao Liu, Wen Li",
      "institution": "University of Electronic Science and Technology of China, Shihezi University",
      "link": "https://arxiv.org/pdf/2512.22874",
      "code": "https://github.com/davelee-uestc/nsf_debiasing",
      "tags": [
        "debiasing",
        "spurious correlation",
        "feature transformation",
        "worst group accuracy",
        "bias-invariant representation",
        "empirical risk minimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e28dca63104c78d5b10b26c867dca0089d6d3d5a4a9c58d62e325089a82e7214_w640_q70.webp",
      "contributions": "1. Proposes a data-oriented pipeline for mitigating spurious correlation without requiring prior annotation of bias attributes. 2. Introduces a method to identify spurious features by observing the dispersed distribution of biased samples in the feature space. 3. Develops a feature transformation learning process to align with a bias-invariant representation, leading to an unbiased classifier.",
      "summary": "This paper addresses the problem of deep learning models learning spurious correlations from biased training data. It proposes a method that identifies, neutralizes, and eliminates spurious features by exploiting the clusteredness of samples and aligning feature transformations with a bias-invariant representation. The approach significantly improves worst-group accuracy by over 20% compared to standard training on image and NLP benchmarks.",
      "mindmap": "graph TB\n        A[Let Samples Speak: Mitigating Spurious Correlation<br>让样本说话：利用样本聚类性缓解伪相关] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Deep models learn spurious features<br>深度模型学习伪相关特征]\n        C --> C1[Identify spurious features via sample distribution<br>通过样本分布识别伪特征]\n        C --> C2[Neutralize & eliminate via feature transformation<br>通过特征变换中和与消除]\n        C --> C3[Update classifier<br>更新分类器]\n        D --> D1[>20% worst-group accuracy improvement vs ERM<br>相比ERM最差组准确率提升>20%]"
    },
    {
      "title": "Hash Grid Feature Pruning",
      "authors": "Yangzhi Ma, Bojun Liu, Jie Li, Li Li, Dong Liu",
      "institution": "University of Science and Technology of China",
      "link": "https://arxiv.org/pdf/2512.22882",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "hash grid",
        "Gaussian splatting",
        "feature pruning",
        "rate-distortion",
        "implicit neural field"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bcac938c69d5ba62c79db746401a4102eb5d9b05e4f19058904cf65b316672f9_w640_q70.webp",
      "contributions": "1. Identifies the problem of invalid features in hash grids due to the non-uniform distribution of Gaussian splats, leading to storage and transmission redundancy. 2. Proposes a hash grid feature pruning method that identifies and removes invalid features based on input Gaussian splat coordinates before encoding. 3. Demonstrates improved rate-distortion performance with an average 8% bitrate reduction in standardized tests, without compromising model reconstruction quality.",
      "summary": "The paper addresses the problem of redundant storage in hash grids used for Gaussian splatting compression, caused by invalid features in sparse 3D regions. It proposes a pruning method that removes these invalid features before encoding, reducing bitrate. The method achieves an average 8% bitrate reduction without affecting model performance, improving rate-distortion efficiency.",
      "mindmap": "graph TB\n        A[Hash Grid Feature Pruning] --> B[核心问题/Problem: Hash grid中存在大量无效特征，导致存储和传输冗余]\n        A --> C[主要方法/Method: 基于高斯泼溅坐标识别并剪枝无效特征，仅编码有效特征]\n        A --> D[关键结果/Results: 在保持性能的同时，平均比特率降低8%]"
    },
    {
      "title": "Guided Path Sampling: Steering Diffusion Models Back on Track with Principled Path Guidance",
      "authors": "Haosen Li, Wenshuo Chen, Shaofeng Liang, Lei Wang, Haozhe Jia, Yutao Yue",
      "institution": "The Hong Kong University of Science and Technology (Guangzhou), Griffith University, Data61/CSIRO",
      "link": "https://arxiv.org/pdf/2512.22881",
      "code": null,
      "tags": [
        "diffusion models",
        "Guided Path Sampling",
        "Classifier-Free Guidance",
        "iterative refinement",
        "off-manifold",
        "path stability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c84aa59d1a3cb3e65a9467831b10684291f67a9acaa193d7066d5e5ed9a57831_w640_q70.webp",
      "contributions": "1. Identifies a fundamental limitation where Classifier-Free Guidance (CFG) causes iterative refinement methods to diverge by pushing the sampling path off the data manifold. 2. Proposes Guided Path Sampling (GPS), a new paradigm that replaces CFG's extrapolation with a principled, manifold-constrained interpolation to ensure path stability. 3. Devises an optimal scheduling strategy to dynamically adjust guidance strength, aligning semantic injection with the model's natural coarse-to-fine generation process.",
      "summary": "This paper identifies that standard Classifier-Free Guidance (CFG) causes instability in iterative refinement for diffusion models by pushing the sampling path off the data manifold. To solve this, the authors propose Guided Path Sampling (GPS), a method that uses manifold-constrained interpolation to keep the path stable and includes an optimal guidance schedule. Experiments show GPS improves image quality and prompt adherence, establishing path stability as key for effective refinement.",
      "mindmap": "graph TB\n        A[Guided Path Sampling] --> B[核心问题/Problem: CFG导致采样路径偏离数据流形/CFG pushes sampling path off data manifold]\n        A --> C[主要方法/Method: 提出GPS，使用流形约束插值/Propose GPS with manifold-constrained interpolation]\n        A --> D[关键结果/Results: 提升图像质量与提示对齐/Improves image quality & prompt adherence]"
    },
    {
      "title": "M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models",
      "authors": "Ju-Hsuan Weng, Jia-Wei Liao, Cheng-Fu Chou, Jun-Cheng Chen",
      "institution": "National Taiwan University, Academia Sinica (Research Center for Information Technology Innovation)",
      "link": "https://arxiv.org/pdf/2512.22877",
      "code": null,
      "tags": [
        "diffusion models",
        "concept erasure",
        "multimodal evaluation",
        "inference-time robustness",
        "cross-attention",
        "latent perturbation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/017a1c0b9ad339359323b5dc5a3742c3451a832f8f2f62b44559e5a1d6ae4a66_w640_q70.webp",
      "contributions": "1. Introduces M-ErasureBench, the first comprehensive multimodal benchmark for evaluating concept erasure methods across text prompts, learned embeddings, and inverted latents under white-box and black-box settings. 2. Identifies a critical vulnerability where existing erasure methods fail against non-textual input modalities, with concept reproduction rates exceeding 90%. 3. Proposes IRECE, a plug-and-play inference-time module that enhances robustness by localizing concepts via cross-attention and perturbing associated latents, reducing CRR by up to 40% while preserving image quality.",
      "summary": "This paper identifies that existing concept erasure methods for diffusion models are vulnerable to attacks using non-textual inputs like learned embeddings or inverted latents. To address this, the authors propose a new multimodal benchmark (M-ErasureBench) and a plug-and-play defense module (IRECE) that perturbs latents during inference. Experiments show IRECE significantly reduces concept reproduction rates under challenging attacks while maintaining visual quality.",
      "mindmap": "graph TB\n        Root[M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有方法仅针对文本提示/Existing methods only target text prompts]\n        Problem --> P2[其他模态成为攻击面/Other modalities become attack surfaces]\n        Method[主要方法/Method] --> M1[提出多模态基准M-ErasureBench/Propose multimodal benchmark M-ErasureBench]\n        Method --> M2[提出推理时增强模块IRECE/Propose inference-time module IRECE]\n        Results[关键结果/Results] --> R1[现有方法在非文本模态下失败/Existing methods fail under non-text modalities]\n        Results --> R2[IRECE将CRR降低达40%/IRECE reduces CRR by up to 40%]"
    },
    {
      "title": "SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation",
      "authors": "Hasan Faraz Khan, Noor Fatima, Muzammil Behzad",
      "institution": "King Fahd University of Petroleum and Minerals, SDAIA-KFUPM Joint Research Center for Artificial Intelligence",
      "link": "https://arxiv.org/pdf/2512.22878",
      "code": null,
      "tags": [
        "medical image segmentation",
        "multimodal fusion",
        "text-guided segmentation",
        "transformer-based architecture",
        "lightweight model",
        "3D segmentation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/991651742357d8ab55dc27a29fa78a0c7b0f65e13d3fe1d6d260f8acea2238d9_w640_q70.webp",
      "contributions": "1. Proposes SwinTF3D, a lightweight multimodal fusion model for text-guided 3D medical image segmentation, integrating visual and linguistic representations. 2. Introduces an efficient fusion mechanism to align semantic text prompts with spatial structures in volumetric medical images. 3. Demonstrates competitive performance and significant efficiency gains on the BTCV dataset, offering a practical and interpretable paradigm for interactive clinical segmentation.",
      "summary": "The paper proposes SwinTF3D, a lightweight multimodal model that uses a transformer-based visual encoder and a text encoder to perform text-guided 3D medical image segmentation. It achieves competitive accuracy on the BTCV dataset with low computational overhead, establishing a practical paradigm for interactive, resource-efficient clinical imaging.",
      "mindmap": "graph TB\n        Root[SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Existing 3D segmentation models lack semantic understanding and adaptability to user-defined tasks] --> Problem_Sub[问题细节/Problem Details: Rely on visual-only learning, ineffective for flexible objectives]\n        Method[主要方法/Method: Lightweight multimodal fusion of transformer-based visual encoder and compact text encoder] --> Method_Sub[方法细节/Method Details: Efficient fusion mechanism aligns semantic cues with spatial structures]\n        Results[关键结果/Results: Achieves competitive Dice/IoU scores on BTCV dataset with low computational overhead] --> Results_Sub[结果细节/Results Details: Generalizes well, offers efficiency gains, establishes an interpretable paradigm]"
    },
    {
      "title": "HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery",
      "authors": "Yaping Zhang, Qixuan Zhang, Xingquan Zhang, Zhiyuan Chen, Wenwen Zhuang, Yupu Liang, Lu Xiang, Yang Zhao, Jiajun Zhang, Yu Zhou, Chengqing Zong",
      "institution": "Institute of Automation, Chinese Academy of Sciences; University of the Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.22899",
      "code": null,
      "tags": [
        "benchmark evaluation",
        "scientific intelligence",
        "hierarchical benchmark",
        "multi-disciplinary evaluation",
        "multimodal inputs",
        "dependency-aware framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5d6954386f880faf48b86cfbd5d72eb78413ee39a02fe0f600306713ecc9bda_w640_q70.webp",
      "contributions": "1. Introduces HiSciBench, a novel hierarchical benchmark spanning five levels (Scientific Literacy to Scientific Discovery) to evaluate the complete scientific workflow. 2. Provides a comprehensive, multi-disciplinary dataset of 8,735 instances across six scientific fields, supporting multimodal and cross-lingual inputs. 3. Establishes an integrated, dependency-aware evaluation framework that reveals significant performance gaps in foundation models, especially on higher-order discovery tasks.",
      "summary": "The paper introduces HiSciBench, a hierarchical and multi-disciplinary benchmark designed to evaluate the full spectrum of scientific intelligence in foundation models, from basic literacy to creative discovery. It contains thousands of multimodal instances across six disciplines and uses a dependency-aware framework for evaluation. The evaluation of leading models shows a sharp performance decline on complex discovery tasks, highlighting a key capability gap and setting a new standard for assessing scientific AI.",
      "mindmap": "graph TB\n        A[HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Existing benchmarks are fragmented and fail to reflect the hierarchical, multi-disciplinary nature of real scientific inquiry.]\n        C[主要方法/Method: Proposes HiSciBench, a 5-level hierarchical benchmark covering six disciplines with multimodal support and an integrated evaluation framework.]\n        D[关键结果/Results: Models show a large performance gap (69% on basic tasks vs. 25% on discovery), establishing a new evaluation standard.]"
    },
    {
      "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
      "authors": "Kai Liu, Jungang Li, Yuchong Sun, Shengqiong Wu, Jianzhang Gao, Daoan Zhang, Wei Zhang, Sheng Jin, Sicheng Yu, Geng Zhan, Jiayi Ji, Fan Zhou, Liang Zheng, Shuicheng Yan, Hao Fei, Tat-Seng Chua",
      "institution": "National University of Singapore (NUS), Zhejiang University (ZJU), Renmin University of China (RUC), University of Rochester (UR), Hong Kong University of Science and Technology (Guangzhou) (HKUST(GZ))",
      "link": "https://arxiv.org/pdf/2512.22905",
      "code": "https://JavisVerse.github.io/JavisGPT-page",
      "tags": [
        "multimodal learning",
        "audio-video fusion",
        "instruction tuning",
        "diffusion transformer"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bf62ed65dd48eaa9ab99c17e036a7566d58f5dc20a79de9bbd8c2622972c5c14_w640_q70.webp",
      "contributions": "1. Proposes JavisGPT, the first unified multimodal LLM for joint audio-video comprehension and generation. 2. Introduces a novel architecture with a SyncFusion module and synchrony-aware queries to bridge a pretrained JAV-DiT generator for coherent output. 3. Constructs a large-scale, high-quality instruction dataset (JavisInst-Omni) with over 200K GPT-4o-curated dialogues for training.",
      "summary": "This paper introduces JavisGPT, a unified multimodal LLM designed for joint audio-video understanding and generation. It features a novel encoder-LLM-decoder architecture with a SyncFusion module and is trained using a three-stage pipeline on a newly created large-scale instruction dataset. Experiments show it outperforms existing models, especially in complex, temporally synchronized tasks.",
      "mindmap": "graph TB\n        Root[JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[缺乏统一的音频-视频理解与生成模型/Lack of unified audio-video comprehension and generation model]\n        Method[主要方法/Method] --> M1[简洁的编码器-LLM-解码器架构/Concise encoder-LLM-decoder architecture]\n        M1 --> M1_1[SyncFusion模块用于时空融合/SyncFusion module for spatio-temporal fusion]\n        M1 --> M1_2[同步感知可学习查询/Synchrony-aware learnable queries]\n        Method --> M2[三阶段训练流程/Three-stage training pipeline]\n        M2 --> M2_1[多模态预训练/Multimodal pretraining]\n        M2 --> M2_2[音频-视频微调/Audio-video fine-tuning]\n        M2 --> M2_3[大规模指令调优/Large-scale instruction-tuning]\n        Method --> M3[构建JavisInst-Omni指令数据集/Construct JavisInst-Omni instruction dataset]\n        Results[关键结果/Results] --> R1[在音频-视频理解与生成基准测试中表现优异/Outperforms existing MLLMs on benchmarks]\n        R1 --> R1_1[在复杂和时间同步场景中表现突出/Particularly strong in complex and temporally synchronized settings]"
    },
    {
      "title": "ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving",
      "authors": "Qihang Peng, Xuesong Chen, Chenye Yang, Shaoshuai Shi, Hongsheng Li",
      "institution": "Tsinghua University, CUHK MMLab, Voyager Research (Didi Chuxing)",
      "link": "https://arxiv.org/pdf/2512.22939",
      "code": null,
      "tags": [
        "autonomous driving",
        "vision-language-action",
        "latent reasoning",
        "hierarchical parallel planning",
        "trajectory generation",
        "cognitive reasoning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c251a1d307ff03613dd19778f6efc36d76e43f6a1b9c6003437556f7e294898_w640_q70.webp",
      "contributions": "1. A unified vision-language-action framework that transfers reasoning from discrete text to a continuous latent space, bridging the gap between VLM reasoning and control. 2. A Cognitive Latent Reasoner that compresses scene understanding into decision-oriented meta-action embeddings efficiently using only two VLM forward passes. 3. A Hierarchical Parallel Planner that generates multi-scale, causality-consistent trajectories in a single forward pass, enabling real-time performance.",
      "summary": "The paper proposes ColaVLA, a framework for autonomous driving that uses cognitive latent reasoning to efficiently translate vision-language understanding into continuous control actions, coupled with a hierarchical parallel planner for trajectory generation. It addresses key challenges in VLM-based planning, such as latency and the text-control mismatch. Experiments on nuScenes show state-of-the-art performance in both open-loop and closed-loop settings with improved efficiency and robustness.",
      "mindmap": "graph TB\n        A[ColaVLA] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[VLM规划器挑战: 文本与连续控制不匹配, 高延迟, 非因果/VLM Planner Challenges: Text-Control Mismatch, High Latency, Non-causal]\n        C --> C1[认知潜在推理器: 将场景理解压缩为元动作嵌入/Cognitive Latent Reasoner: Compresses scene to meta-action embeds]\n        C --> C2[分层并行规划器: 单次前向生成多尺度轨迹/Hierarchical Parallel Planner: Single-pass multi-scale trajectory gen]\n        D --> D1[SOTA性能: 在nuScenes开环和闭环测试中/SOTA Performance: On nuScenes open & closed-loop]\n        D --> D2[高效与鲁棒: 有利的效率与鲁棒性/Favorable Efficiency & Robustness]"
    },
    {
      "title": "Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects",
      "authors": "Zhicheng Zhao, Xuanang Fan, Lingma Sun, Chenglong Li, Jin Tang",
      "institution": "Anhui University, Hefei University, 38th Research Institute of China Electronics Technology Group Corporation",
      "link": "https://arxiv.org/pdf/2512.22949",
      "code": null,
      "tags": [
        "object detection",
        "density map",
        "dense tiny objects",
        "remote sensing",
        "cross-attention",
        "feature fusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/46c8b52e74aab3f56417eac731b52218ad491a572ba57a0c07a3ea636702a32d_w640_q70.webp",
      "contributions": "1. Proposed a Density Generation Branch (DGB) to model object distribution and provide spatial priors for focusing on dense regions. 2. Designed a Dense Area Focusing Module (DAFM) that uses density maps to efficiently focus computational resources on dense areas for local-global feature interaction. 3. Introduced a Dual Filter Fusion Module (DFFM) that uses discrete cosine transform and density-guided cross-attention to disentangle and fuse multi-scale features, enhancing complementarity and suppressing background.",
      "summary": "This paper addresses the challenge of detecting dense, tiny objects in remote sensing imagery by proposing DRMNet, a network that uses density maps to guide adaptive feature learning. The method focuses computational resources on dense regions and fuses multi-scale features effectively. Experiments on AI-TOD and DTOD datasets show it outperforms existing methods, especially in high-density and occluded scenarios.",
      "mindmap": "graph TB\n        Root[”Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects<br>论文标题”] --> Problem[”高分辨率遥感图像中密集微小物体检测的挑战<br>Problem: Detecting dense tiny objects in high-resolution remote sensing imagery”]\n        Root --> Method[”提出DRMNet，利用密度图指导自适应特征学习<br>Method: Propose DRMNet using density maps to guide adaptive feature learning”]\n        Root --> Results[”在AI-TOD和DTOD数据集上超越SOTA<br>Results: Outperforms SOTA on AI-TOD and DTOD datasets”]\n        Problem --> P1[”相互遮挡严重，像素足迹有限<br>Severe mutual occlusion, limited pixel footprints”]\n        Problem --> P2[”现有方法资源分配均匀，无法聚焦密集区域<br>Existing methods allocate resources uniformly, failing to focus on dense regions”]\n        Method --> M1[”密度生成分支 (DGB)<br>Density Generation Branch (DGB)”]\n        Method --> M2[”密集区域聚焦模块 (DAFM)<br>Dense Area Focusing Module (DAFM)”]\n        Method --> M3[”双滤波器融合模块 (DFFM)<br>Dual Filter Fusion Module (DFFM)”]\n        Results --> R1[”在复杂高密度和遮挡场景中表现优异<br>Excels in complex high-density and occlusion scenarios”]"
    },
    {
      "title": "Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection",
      "authors": "Runwei Guan, Jianan Liu, Shaofeng Liang, Fangqiang Ding, Shanliang Yao, Xiaokai Bai, Daizong Liu, Tao Huang, Guoqiang Mao, Hui Xiong",
      "institution": "Hong Kong University of Science and Technology (Guangzhou)",
      "link": "https://arxiv.org/pdf/2512.22972",
      "code": null,
      "tags": [
        "3D object detection",
        "4D radar-camera fusion",
        "wavelet attention",
        "geometry-guided progressive fusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f316c737148800a364d90dba1fd5c27af3b2bbf070f93951908aadb83414d6f_w640_q70.webp",
      "contributions": "1. Proposes WRCFormer, a novel framework for fusing raw 4D radar tensors with camera data using multi-view representations. 2. Designs a Wavelet Attention Module within a wavelet-based Feature Pyramid Network to enhance sparse signal representation. 3. Introduces a two-stage query-based, modality-agnostic Geometry-guided Progressive Fusion mechanism for efficient multi-view feature integration.",
      "summary": "This paper addresses the challenges of sparse data and high computational cost in 4D radar-camera fusion for 3D object detection by proposing WRCFormer. The method fuses raw radar tensors and camera inputs using a wavelet-based feature enhancement module and a geometry-guided progressive fusion mechanism. Experiments on the K-Radar benchmark show state-of-the-art performance, particularly highlighting robustness in adverse weather conditions like sleet.",
      "mindmap": "graph TB\n        Root[”Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Sparsity & computational cost of 4D radar”] --> P1[”信息损失/Information loss in point-cloud processing”]\n        Problem --> P2[”计算成本高/High computational cost of raw data”]\n        Method[”主要方法/Method<br>WRCFormer framework”] --> M1[”小波注意力模块/Wavelet Attention Module”]\n        Method --> M2[”几何引导渐进融合/Geometry-guided Progressive Fusion”]\n        Results[”关键结果/Results<br>SOTA on K-Radar”] --> R1[”整体性能提升 +2.4%/Overall +2.4%”]\n        Results --> R2[”恶劣天气鲁棒性/Robust in sleet +1.6%”]"
    },
    {
      "title": "RealCamo: Boosting Real Camouflage Synthesis with Layout Controls and Textual-Visual Guidance",
      "authors": "Chunyuan Chen, Yunuo Cai, Shujuan Li, Weiyun Liang, Bin Wang, Jing Xu",
      "institution": "Nankai University, Fudan University",
      "link": "https://arxiv.org/pdf/2512.22974",
      "code": null,
      "tags": [
        "camouflaged object detection",
        "camouflaged image generation",
        "out-painting",
        "layout control",
        "textual-visual guidance",
        "distribution divergence metric"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d516b18958a8e5fdfa40b3447ee12adaeb3b8e9ebb310665a607e78e4f770c8c_w640_q70.webp",
      "contributions": "1. Proposes a unified out-painting framework with explicit layout controls to improve semantic coherence between foreground objects and generated backgrounds. 2. Constructs a multi-modal textual-visual condition combining fine-grained textual descriptions with texture-oriented background retrieval to enhance visual fidelity. 3. Introduces a background-foreground distribution divergence metric for the quantitative assessment of camouflage quality.",
      "summary": "The paper addresses the gap between generated and real camouflaged images by proposing RealCamo, a framework that uses layout controls and multi-modal textual-visual guidance for realistic out-painting. The method improves both visual similarity and semantic consistency in generated backgrounds. Experiments demonstrate its effectiveness in producing high-quality camouflaged images.",
      "mindmap": "graph TB\n        A[RealCamo: Boosting Real Camouflage Synthesis<br>RealCamo: 提升真实伪装合成] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Existing CIG methods have a gap to real imagery.<br>现有CIG方法与真实图像存在差距。]\n        C[主要方法/Method<br>Proposes RealCamo with layout controls & textual-visual guidance.<br>提出带布局控制和文本-视觉引导的RealCamo。]\n        D[关键结果/Results<br>Improves camouflage quality & realism.<br>提升了伪装质量和真实感。]"
    },
    {
      "title": "CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision",
      "authors": "Behnam Raoufi, Hossein Sharify, Mohamad Mahdee Ramezanee, Khosrow Hajsadeghi, Saeed Bagheri Shouraki",
      "institution": "Sharif University of Technology",
      "link": "https://arxiv.org/pdf/2512.22969",
      "code": null,
      "tags": [
        "object detection",
        "CLIP",
        "contrastive learning",
        "joint training",
        "learnable text embeddings",
        "InfoNCE loss"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/22774054f225017727182e405a72ff7328bc0b14e0b1cf60711f822efbf76eff_w640_q70.webp",
      "contributions": "1. Proposes CLIP-Joint-Detect, a detector-agnostic framework for end-to-end joint training of object detectors with CLIP-style contrastive vision-language supervision. 2. Introduces a lightweight parallel head that aligns visual features with learnable class-specific text embeddings using a combination of InfoNCE contrastive loss and an auxiliary cross-entropy term. 3. Demonstrates consistent performance improvements on standard benchmarks (Pascal VOC, MS COCO) with both two-stage (Faster R-CNN) and one-stage (YOLO) architectures while preserving real-time inference speed.",
      "summary": "The paper addresses the vulnerability of conventional object detectors to class imbalance and label noise by proposing CLIP-Joint-Detect. This framework integrates contrastive vision-language supervision into object detectors via a lightweight parallel head and joint training, aligning visual features with learnable text embeddings. The method improves detection performance across different architectures and datasets without sacrificing inference speed.",
      "mindmap": "graph TB\n        Root(”CLIP-Joint-Detect”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”传统检测器依赖交叉熵分类/Traditional detectors rely on cross-entropy classification”)\n        Problem --> P2(”易受类别不平衡和标签噪声影响/Vulnerable to class imbalance & label noise”)\n        Method --> M1(”轻量级并行头/Lightweight parallel head”)\n        Method --> M2(”将特征投影到CLIP空间/Projects features to CLIP space”)\n        Method --> M3(”与可学习的文本嵌入对齐/Aligns with learnable text embeddings”)\n        Method --> M4(”使用InfoNCE和交叉熵损失/Uses InfoNCE & cross-entropy loss”)\n        Method --> M5(”端到端联合训练/End-to-end joint training”)\n        Results --> R1(”在Pascal VOC和MS COCO上性能提升/Performance gains on Pascal VOC & MS COCO”)\n        Results --> R2(”适用于两阶段和一阶段检测器/Works for two-stage & one-stage detectors”)\n        Results --> R3(”保持实时推理速度/Preserves real-time inference speed”)"
    },
    {
      "title": "Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation",
      "authors": "Linglin Liao, Qichuan Geng, Yu Liu",
      "institution": "Capital Normal University",
      "link": "https://arxiv.org/pdf/2512.22981",
      "code": null,
      "tags": [
        "medical image segmentation",
        "symmetric optimal transport",
        "spatial-aware guidance",
        "multimodal alignment"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/948f766c4a0c8be35a0a1f60d4115b4d951d7647f2f0f9cad9ba51469c19d161_w640_q70.webp",
      "contributions": "1. Proposes a Spatial-aware Symmetric Alignment (SSA) framework for handling hybrid medical texts containing locational, descriptive, and diagnostic information. 2. Introduces a Dual-granularity Symmetric Optimal Transport (DSOT) alignment algorithm to establish bi-directional fine-grained multimodal correspondences. 3. Devises a composite directional guidance strategy that explicitly introduces spatial constraints by constructing region-level guidance masks.",
      "summary": "This paper addresses the limitations of existing text-guided medical image segmentation methods, which struggle with hybrid texts and spatial constraints. It proposes the Spatial-aware Symmetric Alignment (SSA) framework, which uses a symmetric optimal transport mechanism and spatial guidance strategy to improve alignment. Experiments show SSA achieves state-of-the-art performance, especially for lesions with spatial relational constraints.",
      "mindmap": "graph TB\n        A[论文标题 / Paper Title: Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation] --> B(核心问题 / Problem)\n        A --> C(主要方法 / Method)\n        A --> D(关键结果 / Results)\n        B --> B1[现有方法瓶颈 / Bottlenecks of Existing Methods]\n        B1 --> B2[难以处理混合文本 / Struggles with Hybrid Texts]\n        B1 --> B3[忽略空间约束 / Ignores Spatial Constraints]\n        C --> C1[SSA框架 / SSA Framework]\n        C1 --> C2[对称最优传输对齐 / Symmetric Optimal Transport Alignment]\n        C1 --> C3[复合方向性引导策略 / Composite Directional Guidance Strategy]\n        D --> D1[SOTA性能 / SOTA Performance]\n        D --> D2[准确分割空间约束病灶 / Accurate Segmentation of Spatially Constrained Lesions]"
    },
    {
      "title": "PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects",
      "authors": "Huiming Yang, Linglin Liao, Fei Ding, Sibo Wang, Zijian Zeng",
      "institution": "Renmin University of China",
      "link": "https://arxiv.org/pdf/2512.22979",
      "code": null,
      "tags": [
        "pose estimation",
        "6DoF pose estimation",
        "multi-modal fusion",
        "event camera",
        "high-speed tracking",
        "unseen objects"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9f0bfab0dc1a30f5b4f232838a293d0e91da6ca620be227db20abf46e9dd7e3_w640_q70.webp",
      "contributions": "1. Proposes PoseStreamer, a multi-modal framework integrating an Adaptive Pose Memory Queue, Object-centric 2D Tracker, and Ray Pose Filter for robust 6DoF pose estimation in high-speed scenarios. 2. Introduces a novel multi-modal dataset, MoCapCube6D, for benchmarking performance under rapid motion. 3. Demonstrates superior accuracy and generalizability as a template-free framework for unseen moving objects.",
      "summary": "The paper addresses the challenge of 6DoF pose estimation for unseen objects in high-speed, low-light scenarios where RGB cameras suffer from motion blur. It proposes PoseStreamer, a multi-modal framework that fuses RGB and event camera data with three novel components for temporal consistency, 2D tracking priors, and geometric refinement. Experiments show the framework achieves superior accuracy in high-speed scenarios and strong generalizability for unseen objects.",
      "mindmap": "graph TB\n        Root[PoseStreamer: 多模态6DoF姿态估计框架] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[RGB相机运动模糊/Motion Blur in RGB]\n        Problem --> P2[高速场景性能不佳/Poor High-speed Performance]\n        Method --> M1[自适应姿态记忆队列/Adaptive Pose Memory Queue]\n        Method --> M2[以对象为中心的2D跟踪器/Object-centric 2D Tracker]\n        Method --> M3[射线姿态滤波器/Ray Pose Filter]\n        Method --> M4[多模态数据集/Multi-modal Dataset (MoCapCube6D)]\n        Results --> R1[高速场景精度更高/Superior High-speed Accuracy]\n        Results --> R2[对未见物体泛化性强/Strong Generalizability for Unseen Objects]"
    },
    {
      "title": "YOLO-IOD: Towards Real Time Incremental Object Detection",
      "authors": "Shizhou Zhang, Xueqiang Lv, Yinghui Xing, Qirui Wu, Di Xu, Chen Zhao, Yanning Zhang",
      "institution": "Northwestern Polytechnical University, Huawei",
      "link": "https://arxiv.org/pdf/2512.22973",
      "code": null,
      "tags": [
        "object detection",
        "incremental learning",
        "catastrophic forgetting",
        "knowledge distillation",
        "YOLO",
        "pseudo-labeling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/47f65afece41f4627aa9576946c71bab7a7f85810eb8f6c061f75f4e70de8de8_w640_q70.webp",
      "contributions": "1) Identifies three key knowledge conflicts (foreground-background confusion, parameter interference, misaligned knowledge distillation) causing forgetting in YOLO-based incremental detectors. 2) Proposes the YOLO-IOD framework with three novel components (CPR, IKS, CAKD) to address these conflicts. 3) Introduces a new benchmark, LoCo COCO, designed to prevent data leakage for more realistic incremental learning evaluation.",
      "summary": "This paper addresses the problem of catastrophic forgetting in real-time incremental object detection (IOD) for YOLO detectors. It proposes YOLO-IOD, a framework built on YOLO-World that uses pseudo-label refinement, kernel selection, and asymmetric knowledge distillation to mitigate forgetting. The method shows superior performance with minimal forgetting on both conventional and a newly introduced, more realistic benchmark.",
      "mindmap": "graph TB\n        A[YOLO-IOD: Towards Real Time Incremental Object Detection] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有IOD方法不适用于实时YOLO框架 / Existing IOD methods do not accommodate real-time YOLO]\n        B --> B2[YOLO增量检测存在知识冲突导致灾难性遗忘 / Knowledge conflicts cause catastrophic forgetting in YOLO-based IOD]\n        C --> C1[基于YOLO-World的阶段式参数高效微调 / Stage-wise parameter-efficient fine-tuning on YOLO-World]\n        C --> C2[三个核心组件 / Three Core Components]\n        C2 --> C2_1[CPR: 冲突感知伪标签细化 / Conflict-Aware Pseudo-Label Refinement]\n        C2 --> C2_2[IKS: 基于重要性的核选择 / Importance-based Kernel Selection]\n        C2 --> C2_3[CAKD: 跨阶段非对称知识蒸馏 / Cross-Stage Asymmetric Knowledge Distillation]\n        C --> C3[引入新基准LoCo COCO / Introduce new benchmark LoCo COCO]\n        D --> D1[在传统和LoCo COCO基准上性能优越 / Superior performance on conventional and LoCo COCO benchmarks]\n        D --> D2[实现最小的遗忘 / Achieves minimal forgetting]"
    },
    {
      "title": "Reverse Personalization",
      "authors": "Han-Wei Kung, Tuomas Varanka, Nicu Sebe",
      "institution": "University of Trento, University of Oulu",
      "link": "https://arxiv.org/pdf/2512.22984",
      "code": "https://github.com/hanweikung/reverse-personalization",
      "tags": [
        "face anonymization",
        "diffusion inversion",
        "identity-guided conditioning",
        "attribute-controllable anonymization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc89346c2839fea97adb56c783375419bc47c5a059c9a550f5f7d39c86d33657_w640_q70.webp",
      "contributions": "1. Introduces a reverse personalization framework for face anonymization using conditional diffusion inversion, eliminating the need for text prompts or subject-specific fine-tuning. 2. Incorporates an identity-guided conditioning branch to generalize anonymization to subjects not present in the model's pre-training data. 3. Enables attribute-controllable anonymization, allowing users to preserve or modify specific facial attributes while removing identity, a capability lacking in prior methods.",
      "summary": "This paper addresses the problem of removing identity-specific features from facial images while preserving other attributes and scene context. It proposes a reverse personalization method based on conditional diffusion inversion and an identity-guided conditioning branch, which allows for direct image manipulation without text prompts. The method achieves state-of-the-art performance in balancing identity removal, attribute preservation, and image quality, while also offering user control over retained attributes.",
      "mindmap": "graph TB\n        Root[Reverse Personalization] --> Problem[核心问题/Problem: Removing identity from faces while preserving attributes]\n        Root --> Method[主要方法/Method: Conditional diffusion inversion with identity-guided conditioning]\n        Root --> Results[关键结果/Results: State-of-the-art balance of identity removal, attribute preservation, and quality]"
    },
    {
      "title": "A Low-Cost UAV Deep Learning Pipeline for Integrated Apple Disease Diagnosis,Freshness Assessment, and Fruit Detection",
      "authors": "Soham Dutta, Soham Banerjee, Sneha Mahata, Anindya Sen, Sayantani Datta",
      "institution": "Heritage Institute of Technology, Kolkata",
      "link": "https://arxiv.org/pdf/2512.22990",
      "code": null,
      "tags": [
        "object detection",
        "UAV",
        "YOLOv8",
        "ResNet50",
        "VGG16",
        "Edge Inference"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7de7053032c557a59ed3742c120ae0bca3d789437f306b1fbbaefeaaa1ae6a22_w640_q70.webp",
      "contributions": "1. A unified UAV-based pipeline integrating leaf disease detection, apple freshness classification, and fruit localization. 2. A cost-effective hardware architecture using ESP32-CAM and Raspberry Pi for offline, on-site inference. 3. High-performance results using only RGB sensors as a low-cost alternative to multispectral solutions.",
      "summary": "This paper proposes a low-cost, integrated UAV pipeline using RGB cameras and deep learning models (ResNet50, VGG16, YOLOv8) to perform apple leaf disease diagnosis, fruit freshness assessment, and detection in orchards. The system runs offline on edge devices like Raspberry Pi and ESP32-CAM. Experiments show high accuracy, providing a practical and affordable alternative to expensive multispectral systems for precision agriculture.",
      "mindmap": "graph TB\n        A[”A Low-Cost UAV Deep Learning Pipeline<br>低成本无人机深度学习流程”] --> B[”核心问题/Problem<br>Orchard tasks isolated & costly<br>果园任务孤立且成本高”]\n        A --> C[”主要方法/Method<br>Unified RGB-only UAV pipeline<br>统一的仅RGB无人机流程”]\n        C --> D[”Models: ResNet50, VGG16, YOLOv8<br>模型”]\n        C --> E[”Hardware: ESP32-CAM, Raspberry Pi<br>硬件”]\n        A --> F[”关键结果/Results<br>High accuracy & offline inference<br>高精度与离线推理”]\n        F --> G[”Disease: 98.9%<br>疾病检测”]\n        F --> H[”Freshness: 97.4%<br>新鲜度评估”]\n        F --> I[”Detection F1: 0.857<br>果实检测”]"
    },
    {
      "title": "With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs",
      "authors": "Ciprian Constantinescu, Marius Leordeanu",
      "institution": "National University of Science and Technology POLITEHNICA Bucharest",
      "link": "https://arxiv.org/pdf/2512.23024",
      "code": null,
      "tags": [
        "object recognition",
        "Geo-Semantic Contextual Graph",
        "graph-based classifier",
        "contextual reasoning",
        "panoptic segmentation",
        "metric depth"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4c46fbf20c738319733f8ca7df800fc791d3a2a749f28c23b0751806ad4bb4a_w640_q70.webp",
      "contributions": "1. A novel method for constructing a Geo-Semantic Contextual Graph (GSCG) from a monocular image by fusing metric depth with panoptic and material segmentation. 2. A specialized graph-based classifier that aggregates features from a target object, its neighbors, and the global scene context for classification. 3. Demonstrating that the explicit, structured, and interpretable context of the GSCG significantly outperforms context-agnostic and strong baseline models on object recognition.",
      "summary": "This paper proposes a novel framework for contextual object classification by first constructing a Geo-Semantic Contextual Graph (GSCG) from a single image to represent objects and their relationships, and then using a graph-based classifier to leverage this context. The method achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic models and strong baselines like fine-tuned ResNets and a multimodal LLM, highlighting the power of structured, interpretable scene context.",
      "mindmap": "graph TB\n        A[With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 传统物体识别系统忽略关键上下文信息/Traditional object recognition systems ignore vital contextual information.]\n        C[主要方法/Method: 构建地理语义上下文图并使用图分类器/Build Geo-Semantic Contextual Graph and use a graph-based classifier.]\n        D[关键结果/Results: 上下文感知模型准确率73.4%，显著超越基线/Context-aware model achieves 73.4% accuracy, significantly surpassing baselines.]"
    },
    {
      "title": "OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding",
      "authors": "Wenyuan Huang, Zhao Wang, Zhou Wei, Ting Huang, Fang Zhao, Jian Yang, Zhenyu Zhang",
      "institution": "Nanjing University, China Mobile Zijin Innovation Institute",
      "link": "https://arxiv.org/pdf/2512.23020",
      "code": null,
      "tags": [
        "3D visual grounding",
        "open-world",
        "zero-shot",
        "active cognition-based reasoning",
        "object lookup table",
        "visual language models"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dca85890cab234049b1698ab9f6ade12ea02b16f297cec04cbcb907dcdffb7be_w640_q70.webp",
      "contributions": "1. Proposes OpenGround, a novel zero-shot framework for open-world 3D visual grounding that overcomes the limitation of pre-defined object categories. 2. Introduces the Active Cognition-based Reasoning (ACR) module to progressively augment VLM cognition via a cognitive task chain and a dynamically updated Object Lookup Table (OLT). 3. Presents a new dataset named OpenTarget with over 7000 object-description pairs to evaluate open-world 3D grounding performance.",
      "summary": "This paper addresses the limitation of existing 3D visual grounding methods that rely on a pre-defined object lookup table, which restricts their use in open-world scenarios. The authors propose OpenGround, a zero-shot framework featuring an Active Cognition-based Reasoning module that dynamically expands the model's cognitive scope to handle undefined objects. The method achieves competitive or state-of-the-art results on standard benchmarks and shows a 17.6% improvement on their new OpenTarget dataset.",
      "mindmap": "graph TB\n        A[OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法依赖预定义对象表，无法处理未定义目标/Existing methods rely on pre-defined OLT, limiting open-world application]\n        C --> C1[提出OpenGround框架与主动认知推理模块/Propose OpenGround framework with Active Cognition-based Reasoning (ACR) module]\n        C1 --> C2[通过认知任务链和动态更新的OLT增强VLM认知/Enhance VLM cognition via cognitive task chain and dynamically updated OLT]\n        D --> D1[Nr3D上表现有竞争力，ScanRefer上达到SOTA/Competitive on Nr3D, SOTA on ScanRefer]\n        D --> D2[在OpenTarget数据集上提升17.6%/17.6% improvement on OpenTarget dataset]"
    },
    {
      "title": "Interpretable Gallbladder Ultrasound Diagnosis: A Lightweight Web-Mobile Software Platform with Real-Time XAI",
      "authors": "Fuyad Hasan Bhoyan, Prashanta Sarker, Parsia Noor Ethila, Md. Emon Hossain, Md Kaviul Hossain, Md Humaion Kabir Mehedi",
      "institution": "University of Liberal Arts Bangladesh, BRAC University",
      "link": "https://arxiv.org/pdf/2512.23033",
      "code": null,
      "tags": [
        "medical image classification",
        "MobResTaNet",
        "Explainable AI (XAI)",
        "lightweight model",
        "ultrasound diagnosis",
        "web-mobile platform"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c6a6693ffd9f4375cd5a781c2544fba169bc30bc3cbb7590b1562ea78ba6678_w640_q70.webp",
      "contributions": "1. Proposed a hybrid deep learning model (MobResTaNet) for classifying ten gallbladder conditions from ultrasound images with high accuracy (99.85%) and low parameter count (2.24M). 2. Developed an interpretable diagnostic system with real-time Explainable AI (XAI) visualizations to support transparent clinical decision-making. 3. Deployed the system as an efficient and accessible web-mobile software platform using technologies like HTML, CSS, JavaScript, Bootstrap, and Flutter for point-of-care use.",
      "summary": "This paper addresses the challenge of interpreting gallbladder ultrasound images by developing an AI-driven diagnostic software. The core method is a lightweight hybrid deep learning model called MobResTaNet, which classifies diseases and provides real-time, interpretable predictions via XAI. The main conclusion is that the system achieves high accuracy with a small model size and is successfully deployed as accessible web and mobile applications for clinical support.",
      "mindmap": "graph TB\n        A[Interpretable Gallbladder Ultrasound Diagnosis] --> B[核心问题/Problem: Challenging ultrasound interpretation for gallbladder diseases]\n        A --> C[主要方法/Method: AI software with lightweight MobResTaNet model & real-time XAI]\n        A --> D[关键结果/Results: 99.85% accuracy, 2.24M parameters, deployed web-mobile platform]"
    },
    {
      "title": "An Architecture-Led Hybrid Report on Body Language Detection Project",
      "authors": "Thomson Tong, Diba Darooneh",
      "institution": "None",
      "link": "https://arxiv.org/pdf/2512.23028",
      "code": "BodyLanguageDetection",
      "tags": [
        "video understanding",
        "vision-language models",
        "structured generation",
        "bounding boxes",
        "mixture-of-experts",
        "video analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a618c048bc336ad2ade96a7a97cf301fb10fee2c9c8e7bc16556348f1c0c4b9d_w640_q70.webp",
      "contributions": "1. Provides an architecture-led analysis of two modern VLMs (Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct) for a practical task. 2. Maps model architectural properties to a concrete video-to-artifact pipeline for person detection and attribute extraction. 3. Explicitly defines and analyzes critical system constraints and limitations arising from model behavior, such as semantic vs. syntactic correctness and frame-local identifiers.",
      "summary": "This report analyzes two vision-language models (VLMs) and connects their architectures to a practical system for detecting people and their emotions in video frames. The system prompts VLMs to generate structured outputs like bounding boxes, validates the output structure, and can render annotated videos. The core conclusion is that understanding model architecture is crucial for designing robust interfaces and making defensible claims, as VLMs can produce syntactically correct but semantically incorrect outputs.",
      "mindmap": "graph TB\n        A[An Architecture-Led Hybrid Report on Body Language Detection Project] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[如何基于VLM架构构建可靠的应用系统/How to build reliable application systems based on VLM architecture]\n        C --> C1[分析两种VLM架构并映射到视频处理流程/Analyze two VLM architectures and map to a video processing pipeline]\n        C --> C2[系统采样视频帧，提示VLM生成结构化输出/System samples video frames, prompts VLM for structured output]\n        C --> C3[使用预定义模式验证输出结构/Validate output structure with predefined schema]\n        D --> D1[结构化输出可能语法正确但语义错误/Structured outputs can be syntactically valid but semantically incorrect]\n        D --> D2[模式验证是结构性的，非几何正确性/Schema validation is structural, not geometric]\n        D --> D3[理解架构对设计稳健接口和评估至关重要/Understanding architecture is critical for robust interface design and evaluation]"
    },
    {
      "title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
      "authors": "Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Shiying Wang, Jingming Chen, Congyan Lang, Tengfei Cao, Pin Tao, Yuanchun Shi",
      "institution": "Qinghai University, Beijing Jiaotong University, Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.23035",
      "code": "https://xavierjiezou.github.io/Co2S/",
      "tags": [
        "semantic segmentation",
        "semi-supervised learning",
        "pseudo-label drift",
        "vision-language model",
        "self-supervised model",
        "dual-student architecture"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2c3e7bd7c766cb9388933edf8bea0a465af8034e2f1995e5edc4eaaa3062c87_w640_q70.webp",
      "contributions": "1. Proposed Co2S, a stable semi-supervised RS segmentation framework that fuses priors from vision-language and self-supervised models. 2. Introduced a heterogeneous dual-student architecture with ViT models initialized by CLIP and DINOv3 to mitigate error accumulation. 3. Developed an explicit-implicit semantic co-guidance mechanism and a global-local feature collaborative fusion strategy to enhance semantic consistency and segmentation precision.",
      "summary": "This paper addresses the problem of pseudo-label drift in semi-supervised remote sensing image segmentation. It proposes the Co2S framework, which uses a dual-student architecture combining CLIP and DINOv3 models, along with co-guidance and co-fusion mechanisms, to improve stability and accuracy. Experiments on six datasets show the method achieves leading performance across various scenarios.",
      "mindmap": "graph TB\n        Root(”Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”伪标签漂移/Pseudo-label Drift”)\n        Problem --> P2(”标注负担/Annotation Burden”)\n        Method --> M1(”异构双学生架构/Heterogeneous Dual-Student”)\n        Method --> M2(”显式-隐式语义协同引导/Explicit-Implicit Co-Guidance”)\n        Method --> M3(”全局-局部特征融合/Global-Local Feature Fusion”)\n        M1 --> M1_1(”CLIP ViT”)\n        M1 --> M1_2(”DINOv3 ViT”)\n        Results --> R1(”六个数据集领先性能/Leading Performance on Six Datasets”)\n        Results --> R2(”多种划分协议鲁棒/Robust Across Partition Protocols”)"
    },
    {
      "title": "3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds",
      "authors": "Ryousuke Yamada, Kohsuke Ide, Yoshihiro Fukuhara, Hirokatsu Kataoka, Gilles Puy, Andrei Bursuc, Yuki M. Asano",
      "institution": "AIST, University of Technology Nuremberg, INRIA (Valeo.ai)",
      "link": "https://arxiv.org/pdf/2512.23042",
      "code": null,
      "tags": [
        "3D scene understanding",
        "self-supervised learning",
        "point clouds",
        "video reconstruction",
        "geometric regularization",
        "indoor segmentation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/29450a379b6659077d87333387a81a32e83ef28aa568a2d2618219886ad4f564_w640_q70.webp",
      "contributions": "1. Proposes LAM3C, a self-supervised framework for learning 3D representations from video-generated point clouds without real 3D scans. 2. Introduces RoomTours, a large-scale dataset of 49,219 video-generated point cloud scenes from web videos. 3. Designs a noise-regularized loss to enforce local geometric smoothness and stabilize feature learning on noisy point clouds.",
      "summary": "This paper addresses the high cost of collecting 3D scans by proposing a method to learn 3D representations from unlabeled videos. The core method, LAM3C, learns from video-generated point clouds using a novel noise-regularized loss and a new dataset called RoomTours. The results show that this approach outperforms previous self-supervised methods on indoor segmentation tasks, demonstrating that videos are a viable and abundant source for 3D pre-training.",
      "mindmap": "graph TB\n        Root(”3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”大规模3D扫描昂贵且耗时/Large-scale 3D scan collection is expensive and labor-intensive”)\n        Method --> M1(”提出LAM3C框架/Propose LAM3C framework”)\n        Method --> M2(”引入RoomTours数据集/Introduce RoomTours dataset”)\n        Method --> M3(”使用噪声正则化损失/Use noise-regularized loss”)\n        Results --> R1(”性能优于现有自监督方法/Outperforms previous self-supervised methods”)\n        Results --> R2(”证明视频是丰富的3D数据源/Demonstrates videos as an abundant 3D data source”)"
    },
    {
      "title": "Video-BrowseComp: Benchmarking Agentic Video Research on Open Web",
      "authors": "Zhengyang Liang, Yan Shu, Xiangrui Liu, Minghao Qin, Kaixin Liang, Paolo Rota, Nicu Sebe, Zheng Liu, Lizi Liao",
      "institution": "Singapore Management University, University of Trento, Beijing Academy of Artificial Intelligence, Beijing University of Posts and Telecommunications, Hong Kong Polytechnic University",
      "link": "https://arxiv.org/pdf/2512.23044",
      "code": "https://liang-zhengyang.github.io/video-browsecomp/",
      "tags": [
        "video understanding",
        "agentic video research",
        "open-web benchmark",
        "temporal visual evidence",
        "video browsing",
        "multimodal reasoning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60d4510d9ebf0c25d20191be00b854d890c6942ebcae7fafd2869bb49e23acd9_w640_q70.webp",
      "contributions": "1. Introduces Video-BrowseComp, the first benchmark for open-web agentic video reasoning, comprising 210 questions that require active navigation of video timelines. 2. Enforces a mandatory dependency on temporal visual evidence, ensuring answers cannot be derived from text search alone, thus evaluating true video grounding. 3. Reveals a critical performance bottleneck in state-of-the-art models, showing they rely heavily on textual proxies and fail in metadata-sparse, dynamic video domains.",
      "summary": "The paper identifies a gap in evaluating proactive, agentic video research on the open web. To address this, it introduces the Video-BrowseComp benchmark, which requires models to actively navigate and reason over video timelines to answer questions. The evaluation shows that even advanced models perform poorly, highlighting a critical reliance on text and a failure in visually-grounded video understanding.",
      "mindmap": "graph TB\n        A[Video-BrowseComp: Benchmarking Agentic Video Research on Open Web] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有基准测试仅评估被动视频感知/Existing benchmarks only evaluate passive video perception]\n        B --> B2[缺乏对主动、开放式网络视频研究的评估/Lack of evaluation for agentic, open-web video research]\n        C --> C1[提出Video-BrowseComp基准/Propose Video-BrowseComp benchmark]\n        C --> C2[强制依赖时序视觉证据/Enforce mandatory dependency on temporal visual evidence]\n        C --> C3[包含210个挑战性问题/Comprises 210 challenging questions]\n        D --> D1[最先进模型准确率低/State-of-the-art models achieve low accuracy (e.g., 15.24%)]\n        D --> D2[模型过度依赖文本代理/Models heavily rely on textual proxies]\n        D --> D3[在元数据稀疏的动态环境中失败/Models fail in metadata-sparse, dynamic environments]"
    },
    {
      "title": "Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models",
      "authors": "Mingyuan Zhang, Yue Bai, Yifan Wang, Yiyang Huang, Yun Fu",
      "institution": "Northeastern University",
      "link": "https://arxiv.org/pdf/2512.23073",
      "code": "https://github.com/Ming-K9/MFT-VLM",
      "tags": [
        "vision-language models",
        "Mask Fine-Tuning (MFT)",
        "Parameter Efficient Fine-Tuning (PEFT)",
        "Low-Rank Adaptation (LoRA)",
        "structural reparameterization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/92b8916bd08492d9f9fbfd3b3a163d12c7de7a6a1fe0822bc6d711ca118dfb28_w640_q70.webp",
      "contributions": "1. Proposes applying Mask Fine-Tuning (MFT), a structural reparameterization method, to Vision-Language Models (VLMs) for adaptation., 2. Demonstrates that MFT, which learns gating scores for weights instead of updating them, consistently outperforms LoRA variants and full fine-tuning., 3. Reveals that effective adaptation can emerge from reestablishing connections within a model's existing knowledge, not just from updating weights.",
      "summary": "This paper rethinks fine-tuning for Vision-Language Models (VLMs) by applying Mask Fine-Tuning (MFT), a method that learns to gate existing weights instead of updating them. Experiments show MFT surpasses both Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA and full fine-tuning, demonstrating that reorganizing internal subnetworks is a powerful alternative to weight updates.",
      "mindmap": "graph TB\n        Root[”Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models<br>重新思考微调：解锁视觉语言模型的隐藏能力”]\n        Root --> Problem[”Problem: Traditional fine-tuning overlooks underutilized structures in pre-trained VLMs.<br>核心问题：传统微调忽略了预训练VLM中未充分利用的结构。”]\n        Root --> Method[”Method: Apply Mask Fine-Tuning (MFT) to VLMs for structural reparameterization.<br>主要方法：将掩码微调（MFT）应用于VLM进行结构重参数化。”]\n        Root --> Results[”Results: MFT surpasses LoRA and full fine-tuning without altering the backbone.<br>关键结果：MFT超越了LoRA和全参数微调，且不改变主干网络。”]"
    },
    {
      "title": "MedSAM-based lung masking for multi-label chest X-ray classification",
      "authors": "Brayden Miao, Zain Rehman, Xin Miao, Siming Liu, Jianjie Wang",
      "institution": "Missouri State University",
      "link": "https://arxiv.org/pdf/2512.23089",
      "code": null,
      "tags": [
        "medical image analysis",
        "MedSAM",
        "lung segmentation",
        "multi-label classification",
        "chest X-ray",
        "spatial prior"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78073436289f27d236dc3f5c9f70b55eb6480c3a7ea02e8c30b8eb1b8e59faa9_w640_q70.webp",
      "contributions": "1. Proposes a segmentation-guided CXR classification pipeline that integrates a fine-tuned MedSAM model for lung region extraction. 2. Empirically demonstrates that the effect of lung masking is task-dependent and architecture-dependent, revealing a trade-off between abnormality classification and normal case screening. 3. Suggests that lung masking should be treated as a controllable spatial prior tailored to the model backbone and clinical objective, rather than a uniform preprocessing step.",
      "summary": "This paper proposes a method that uses a fine-tuned MedSAM model to extract lung masks from chest X-rays to guide multi-label abnormality classification. The study finds that the impact of masking depends on the task and model architecture, with loose masking improving normal case screening while tight masking aids training efficiency. The conclusion is that lung masking should be a tunable spatial prior aligned with the specific clinical goal and model, not a fixed step.",
      "mindmap": "graph TB\n        A[MedSAM-based lung masking for multi-label chest X-ray classification] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Automated CXR interpretation is challenging<br>自动CXR解读具有挑战性]\n        C --> C1[Fine-tune MedSAM for lung segmentation<br>微调MedSAM进行肺部分割]\n        C --> C2[Use masks to guide multi-label classification<br>使用掩码指导多标签分类]\n        D --> D1[Masking effect is task/architecture dependent<br>掩码效果依赖于任务和架构]\n        D --> D2[Trade-off: abnormality vs. normal screening<br>权衡：异常检测与正常筛查]\n        D --> D3[Masking is a controllable spatial prior<br>掩码是一种可控的空间先验]"
    },
    {
      "title": "PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion",
      "authors": "Jian Wang, Sixing Rong, Jiarui Xing, Yuling Xu, Weide Liu",
      "institution": "Harvard Medical School, Northeastern University, Yale University, Nanchang University, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.23130",
      "code": null,
      "tags": [
        "medical image synthesis",
        "diffusion model",
        "disentangled representation",
        "pathological residual",
        "anatomical manifold",
        "seam-aware fusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b87a518aaabc4319ec5eba26d8ed0e23b50b1f611b88f2d8241ebecec605cc8c_w640_q70.webp",
      "contributions": "1. Proposes a unified generative framework that reformulates MRI pathology synthesis as a disentangled additive deviation on a stable anatomical manifold. 2. Introduces a Deviation-Space Diffusion Model to learn the conditional distribution of pathological residuals, preserving global structure while modeling local variations. 3. Incorporates a seam-aware fusion strategy and an inference-time stabilization module to suppress boundary artifacts and ensure spatial coherence in synthesized lesions.",
      "summary": "The paper proposes PathoSyn, a novel framework for synthesizing pathological MRI images by decomposing the task into deterministic anatomical reconstruction and stochastic modeling of pathological deviations using a diffusion model. This approach preserves anatomical integrity while generating realistic lesion heterogeneity. Evaluations show it outperforms existing baselines in perceptual realism and anatomical fidelity.",
      "mindmap": "graph TB\n        Root[”PathoSyn: Imaging-Pathology MRI Synthesis<br>PathoSyn: 成像-病理MRI合成”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Feature entanglement in generative models<br>causes corrupted anatomy<br>生成模型中的特征纠缠导致解剖结构损坏”] --> P1[”现有范式/Existing Paradigms<br>Global pixel domain or binary masks<br>全局像素域或二进制掩码”]\n        Method[”主要方法/Method<br>Disentangled Deviation Diffusion<br>解耦偏差扩散”] --> M1[”分解任务/Decompose Task<br>1. Deterministic anatomical reconstruction<br>确定性解剖重建<br>2. Stochastic deviation modeling<br>随机偏差建模”]\n        Method --> M2[”核心模型/Core Model<br>Deviation-Space Diffusion Model<br>偏差空间扩散模型<br>Learns pathological residuals<br>学习病理残差”]\n        Method --> M3[”融合与稳定/Fusion & Stabilization<br>Seam-aware fusion & inference-time<br>stabilization module<br>接缝感知融合与推理时稳定模块”]\n        Results[”关键结果/Results<br>Outperforms baselines<br>超越基线模型”] --> R1[”评估/Evaluation<br>Quantitative & qualitative on tumor benchmarks<br>肿瘤基准上的定量与定性评估”]\n        Results --> R2[”优势/Advantages<br>Higher perceptual realism & anatomical fidelity<br>更高的感知真实性与解剖保真度”]"
    },
    {
      "title": "Domain-Shift Immunity in Deep Deformable Registration via Local Feature Representations",
      "authors": "Mingzhen Shao, Sarang Joshi",
      "institution": "University of Utah",
      "link": "https://arxiv.org/pdf/2512.23142",
      "code": null,
      "tags": [
        "image registration",
        "deformable registration",
        "domain shift",
        "local features",
        "UniReg",
        "cross-modal"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2267b3cdf4a5fdca597653c5a64d1db35cb34367c8cc7c7fcb14b4905e0d492b_w640_q70.webp",
      "contributions": "1. Demonstrates that domain-shift immunity is an inherent property of deep deformable registration models, stemming from their reliance on local feature representations. 2. Introduces UniReg, a universal registration framework that decouples feature extraction from deformation estimation to validate this mechanism. 3. Reveals that failures of conventional CNN-based models under modality shift originate from dataset-induced biases in early convolutional layers.",
      "summary": "This paper investigates the robustness of deep learning models for deformable image registration under domain shift. It proposes UniReg, a framework using fixed feature extractors and a UNet, to show that reliance on local features provides inherent domain-shift immunity. The findings indicate that local feature consistency is key to robustness, and early CNN layers are the source of failure in conventional models.",
      "mindmap": "graph TB\n    A[”Domain-Shift Immunity in Deep Deformable Registration via Local Feature Representations”] --> B[”核心问题/Problem: Are deep deformable registration models robust to domain shift?”]\n    A --> C[”主要方法/Method: Propose UniReg, a framework decoupling feature extraction and deformation estimation.”]\n    A --> D[”关键结果/Results: Models have inherent domain-shift immunity due to local features; UniReg shows robust cross-domain performance.”]"
    },
    {
      "title": "GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection",
      "authors": "Jingyu Li, Xiaolong Zhao, Zhe Liu, Wenxiao Wu, Li Zhang",
      "institution": "Fudan University, Shanghai Innovation Institute, Tongji University, The University of Hong Kong, Huazhong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.23147",
      "code": "https://github.com/SII-Whaleice/GeoTeacher",
      "tags": [
        "3D object detection",
        "semi-supervised learning",
        "teacher-student framework",
        "geometric relation supervision",
        "voxel-wise augmentation",
        "pseudo-labeling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8bb5977bbadb9714d8d0a0863da6b57b54d85977aa07fe111b3071b1ec4bb55_w640_q70.webp",
      "contributions": "1. A keypoint-based geometric relation supervision module that transfers the teacher model's knowledge of object geometry to the student model. 2. A voxel-wise data augmentation strategy with a distance-decay mechanism to increase the diversity of object geometries while preserving distant object integrity. 3. A flexible framework that can be combined with different semi-supervised 3D object detection methods to further improve their performance.",
      "summary": "The paper proposes GeoTeacher, a geometry-guided semi-supervised 3D object detection method. It introduces a geometric relation supervision module and a novel voxel-wise augmentation strategy to enhance the student model's ability to capture object geometries when labeled data is limited. Experiments on ONCE and Waymo datasets show the method achieves state-of-the-art performance and demonstrates good generalization.",
      "mindmap": "graph TB\n        A[GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法忽视几何信息/Existing methods overlook geometric information]\n        C --> C1[关键点几何关系监督/Keypoint-based geometric relation supervision]\n        C --> C2[体素数据增强与距离衰减/Voxel-wise augmentation with distance-decay]\n        D --> D1[在ONCE和Waymo上SOTA/SOTA on ONCE and Waymo]\n        D --> D2[方法具有良好的泛化性/Method shows good generalization]"
    },
    {
      "title": "SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling",
      "authors": "Yufan He, Pengfei Guo, Mengya Xu, Zhaoshuo Li, Andriy Myronenko, Dillan Imans, Bingjie Liu, Dongren Yang, Mingxue Gu, Yongnan Ji, Yueming Jin, Ren Zhao, Baiyong Shen, Daguang Xu",
      "institution": "NVIDIA, The Chinese University of Hong Kong, Sung Kyun Kwan University, Wenzhou Medical University, National University of Singapore, Ruijin Hospital",
      "link": "https://arxiv.org/pdf/2512.23162",
      "code": null,
      "tags": [
        "robotic imitation learning",
        "world model",
        "vision-language-action (VLA) model",
        "inverse dynamics model",
        "surgical robotics",
        "synthetic data generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd5e2ca51f8df31fc2fe20ced16d79b01d4a091736ac738cdfcd0c8fda793a16_w640_q70.webp",
      "contributions": "1. Curated the Surgical Action-Text Alignment (SATA) dataset with detailed text descriptions for surgical robot actions. 2. Built SurgWorld, a generative world model capable of producing diverse and realistic synthetic surgical videos. 3. Pioneered the use of an inverse-dynamics model to infer pseudo-kinematics from synthetic videos, creating synthetic paired video-action data for training.",
      "summary": "This paper addresses the data scarcity problem in autonomous surgical robotics by proposing SurgWorld, a world model that generates realistic synthetic surgical videos. The method uses an inverse dynamics model to infer robot actions from these videos, creating a large-scale paired dataset to train a Vision-Language-Action policy. The resulting policy significantly outperforms models trained only on real demonstrations on a real surgical robot platform.",
      "mindmap": "graph TB\n        A[SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling] --> B[核心问题/Problem: Data scarcity for paired video-action data in surgical robotics]\n        A --> C[主要方法/Method: Build SurgWorld world model to generate synthetic videos; Use inverse dynamics to infer pseudo-kinematics]\n        A --> D[关键结果/Results: Surgical VLA policy trained with augmented data outperforms policy trained only on real data]"
    },
    {
      "title": "REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation",
      "authors": "Fulin Shi, Wenyi Xiao, Bin Chen, Liang Din, Leilei Gan",
      "institution": "Zhejiang University, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.23169",
      "code": null,
      "tags": [
        "text-image alignment evaluation",
        "reinforcement learning",
        "multimodal large language models",
        "visual reasoning",
        "element-level grounding",
        "group relative policy optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/448c922a2b6a208469892f6edb23c470ac37ce4a9bbde51c3f92b2b86f87267e_w640_q70.webp",
      "contributions": "1. Proposes REVEALER, a unified framework for element-level text-image alignment evaluation based on a structured \"grounding-reasoning-conclusion\" visual reasoning paradigm. 2. Introduces a reinforcement learning optimization method using Group Relative Policy Optimization (GRPO) with a composite reward function to enhance judgment quality. 3. Demonstrates state-of-the-art performance and superior inference efficiency across multiple benchmarks compared to existing methods and proprietary models.",
      "summary": "The paper addresses the problem of coarse-grained and non-interpretable evaluation of text-to-image model outputs. It proposes REVEALER, a framework that uses reinforcement-guided visual reasoning with MLLMs to perform fine-grained, element-level alignment assessment. Experiments show REVEALER achieves state-of-the-art performance and is more efficient than existing iterative reasoning methods.",
      "mindmap": "graph TB\n        A[REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有评估方法缺乏细粒度可解释性/Existing evaluation lacks fine-grained interpretability]\n        C --> C1[基于强化学习的结构化视觉推理/Reinforcement-guided structured visual reasoning]\n        C --> C2[”范式: 定位-推理-结论/Paradigm: grounding-reasoning-conclusion”]\n        D --> D1[在多个基准测试中达到SOTA/Achieves SOTA on multiple benchmarks]\n        D --> D2[优于专有模型和基线/Superior to proprietary models & baselines]\n        D --> D3[更高的推理效率/Higher inference efficiency]"
    },
    {
      "title": "Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR",
      "authors": "Will Sebelik-Lassiter, Evan Schubert, Muhammad Alliyu, Quentin Robbins, Excel Olatunji, Mustafa Barry",
      "institution": "Milwaukee School of Engineering, Emory University",
      "link": "https://arxiv.org/pdf/2512.23177",
      "code": null,
      "tags": [
        "medical image classification",
        "vocal cord ultrasound",
        "image segmentation",
        "VIPRnet",
        "vocal cord paralysis",
        "classification model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5abcb457a7fbefffa7d7836af553a8db8fa248fdd34a0459a027299b3ce2ce44_w640_q70.webp",
      "contributions": "1. Developed a machine learning pipeline for automated analysis of vocal cord ultrasound (VCUS) videos. 2. Created a segmentation model to automatically identify vocal cords in ultrasound images with 96% validation accuracy. 3. Proposed VIPRnet, a classification model to distinguish normal vocal cords from vocal cord paralysis (VCP) with 99% validation accuracy.",
      "summary": "This paper proposes a machine learning-assisted system to automate the analysis of vocal cord ultrasound (VCUS) to reduce operator dependency. The method involves segmenting the vocal cords and classifying them as normal or paralyzed using models trained on frames from volunteer videos. The results show high validation accuracy (96% for segmentation, 99% for classification), indicating promise for improving diagnostic accuracy.",
      "mindmap": "graph TB\n    A[Project VIPR: Machine Learning-Assisted Vocal Cord Ultrasound Examination] --> B[核心问题/Problem: VCUS accuracy is operator-dependent]\n    A --> C[主要方法/Method: Use ML models for vocal cord segmentation and VCP classification]\n    A --> D[关键结果/Results: Segmentation accuracy 96%, VIPRnet classification accuracy 99%]"
    },
    {
      "title": "GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection",
      "authors": "Yi Zhang, Yi Wang, Lei Yao, Lap-Pui Chau",
      "institution": "The Hong Kong Polytechnic University",
      "link": "https://arxiv.org/pdf/2512.23176",
      "code": null,
      "tags": [
        "3D object detection",
        "Gaussian Splatting",
        "Voxel Representation",
        "Multi-View Learning",
        "Synergistic Learning",
        "Geometry Extraction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1b5d4bbef1eb01585a47bc411dd2af3afb443450d925569c6ba83c498c225b6_w640_q70.webp",
      "contributions": "1. Proposes a novel synergistic framework (GVSynergy-Det) that integrates continuous Gaussian and discrete voxel representations for complementary geometric feature learning in 3D object detection. 2. Adapts generalizable Gaussian Splatting to extract geometric features for detection and develops a cross-representation enhancement mechanism to enrich voxel features with Gaussian-derived details. 3. Achieves state-of-the-art performance on major indoor benchmarks (ScanNetV2, ARKitScenes) without requiring dense 3D supervision (e.g., depth or point clouds).",
      "summary": "This paper introduces GVSynergy-Det, a novel image-based 3D object detection framework that synergistically combines Gaussian and voxel representations to capture complementary geometric information without dense 3D supervision. The method integrates features from both representations through a learnable mechanism, enabling more accurate object localization. Experiments show it achieves state-of-the-art results on indoor benchmarks, outperforming existing methods while maintaining a compact model size.",
      "mindmap": "graph TB\n        Root[GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[基于图像的3D检测挑战/Image-based 3D Detection Challenges]\n        P1 --> P2[高精度需密集3D监督/High accuracy requires dense 3D supervision]\n        P1 --> P3[无监督难以提取准确几何/Unsupervised struggles with geometry extraction]\n        Method[主要方法/Method] --> M1[双表征协同学习/Dual-Representation Synergistic Learning]\n        M1 --> M2[高斯溅射提取几何特征/Gaussian Splatting for geometric features]\n        M1 --> M3[体素提供空间上下文/Voxels provide spatial context]\n        M1 --> M4[跨表征增强机制/Cross-representation enhancement mechanism]\n        Results[关键结果/Results] --> R1[SOTA性能/State-of-the-art performance]\n        R1 --> R2[ScanNetV2 & ARKitScenes数据集/ScanNetV2 & ARKitScenes datasets]\n        Results --> R3[无需密集3D监督/No dense 3D supervision required]\n        Results --> R4[紧凑模型尺寸/Compact model size]"
    },
    {
      "title": "GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation",
      "authors": "Tianchen Deng, Xuefeng Chen, Yi Chen, Qu Chen, Yuyao Xu, Lijin Yang, Le Xu, Yu Zhang, Bo Zhang, Wuxiong Huang, Hesheng Wang",
      "institution": "Shanghai Jiao Tong University, Tsinghua University, MEGVII Technology, Mach Drive",
      "link": "https://arxiv.org/pdf/2512.23180",
      "code": "this",
      "tags": [
        "autonomous driving",
        "3D scene understanding",
        "3D Gaussian Splatting",
        "Driving World Model",
        "Multi-Modal Generation",
        "Vision-Language Model",
        "Scene Representation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/035656391cb2f9ea8e1b71368c7ca76202f290f6e67f8641414807e7be989eb8_w640_q70.webp",
      "contributions": "1. Proposes a unified Driving World Model framework based on 3D Gaussian scene representation for joint 3D scene understanding and multi-modal generation. 2. Introduces early modality alignment by embedding linguistic features into 3D Gaussian primitives and a task-aware language-guided sampling strategy to inject compact 3D tokens into an LLM. 3. Designs a dual-condition multi-modal generation model that uses high-level language conditions and low-level image conditions to guide the generation process.",
      "summary": "This paper proposes GaussianDWM, a novel Driving World Model framework that uses 3D Gaussians as a scene representation to unify 3D scene understanding and multi-modal generation for autonomous driving. It aligns text with the 3D scene via Gaussian primitives and uses a dual-condition model for generation. The method achieves state-of-the-art performance on nuScenes and NuInteract datasets.",
      "mindmap": "graph TB\n        A[GaussianDWM: 3D Gaussian Driving World Model] --> B1(核心问题/Problem: Existing DWMs lack 3D understanding and precise text-scene alignment)\n        A --> B2(主要方法/Method: Unified framework using 3D Gaussians for representation, with early modality alignment and dual-condition generation)\n        A --> B3(关键结果/Results: Achieves SOTA performance on driving datasets)"
    },
    {
      "title": "ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis",
      "authors": "Maisha Haque, Israt Jahan Ayshi, Sadaf M. Anis, Nahian Tasnim, Mithila Moontaha, Md. Sabbir Ahmed, Muhammad Iqbal Hossain, Mohammad Zavid Parvez, Subrata Chakraborty, Biswajeet Pradhan, Biswajit Banik",
      "institution": "BRAC University, Charles Sturt University, University of Technology Sydney",
      "link": "https://arxiv.org/pdf/2512.23196",
      "code": null,
      "tags": [
        "semantic segmentation",
        "Object-Based Image Analysis (OBIA)",
        "Deep Learning",
        "Sentinel-2",
        "Forest Cover Mapping",
        "UNet"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/477043abcfb8b4e851144d00c2ae33596765e1b2a27375709fcbcfc01f79e3e5_w640_q70.webp",
      "contributions": "1. Proposes \"ForCM\", a novel method that integrates Object-Based Image Analysis (OBIA) with various Deep Learning models for forest cover mapping. 2. Evaluates and compares the performance of multiple DL models (UNet, UNet++, ResUNet, AttentionUNet, ResNet50-Segnet) combined with OBIA against traditional OBIA. 3. Demonstrates the practical application of free tools like QGIS for accurate environmental mapping, achieving improved accuracy (up to 95.64%) over traditional methods.",
      "summary": "This paper proposes ForCM, a method for forest cover mapping that combines Object-Based Image Analysis with Deep Learning models like ResUNet and AttentionUNet using Sentinel-2 imagery. The results show that this integration significantly improves mapping accuracy compared to traditional OBIA alone, demonstrating the potential of accessible tools for environmental monitoring.",
      "mindmap": "graph TB\n        A[ForCM: Forest Cover Mapping] --> B[核心问题/Problem: Accurate forest cover mapping for environmental monitoring]\n        A --> C[主要方法/Method: Integrate OBIA with DL models (e.g., UNet, ResUNet) on Sentinel-2 imagery]\n        A --> D[关键结果/Results: Improved accuracy (95.64% with AttentionUNet-OBIA vs 92.91% traditional OBIA)]"
    },
    {
      "title": "Exploring Syn-to-Real Domain Adaptation for Military Target Detection",
      "authors": "Jongoh Jeong, Youngjin Oh, Gyeongrae Nam, Jeongeun Lee, Kuk-Jin Yoon",
      "institution": "Korea Advanced Institute of Science and Technology (KAIST), LIG Nex1",
      "link": "https://arxiv.org/pdf/2512.23208",
      "code": null,
      "tags": [
        "object detection",
        "domain adaptation",
        "synthetic-to-real",
        "Unreal Engine",
        "military target detection"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b67711a2133943d8083642ed36e63614aae288f161e8fc258230c5d03bacb3_w640_q70.webp",
      "contributions": "1. Proposed generating a synthetic RGB dataset for military target detection using Unreal Engine to address the lack of real-world data. 2. Conducted and benchmarked synthetic-to-real domain adaptation experiments on a new train-val dataset pair for military targets. 3. Found that domain adaptation methods using minimal supervision (e.g., object class hints) substantially outperform unsupervised or semi-supervised methods in this challenging cross-domain setting.",
      "summary": "This paper addresses the challenge of military target detection by generating synthetic RGB data using Unreal Engine to overcome the lack of real datasets and high costs of SAR data. It benchmarks state-of-the-art domain adaptation methods on this synthetic-to-real task and finds that methods using minimal supervision achieve the best performance, highlighting remaining challenges in this area.",
      "mindmap": "graph TB\n        A[Exploring Syn-to-Real Domain Adaptation for Military Target Detection] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[缺乏军事目标数据集/Lack of military target datasets]\n        B --> B2[SAR数据成本高/High cost of SAR data]\n        B --> B3[跨域适应挑战/Cross-domain adaptation challenge]\n        C --> C1[使用Unreal Engine生成合成RGB数据/Generate synthetic RGB data using Unreal Engine]\n        C --> C2[合成到真实域适应实验/Synthetic-to-real domain adaptation experiments]\n        C --> C3[基准测试SOTA方法/Benchmark SOTA DA methods]\n        D --> D1[最小监督方法表现最佳/Minimal supervision methods perform best]\n        D --> D2[识别当前挑战/Identify current challenges]"
    },
    {
      "title": "Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks",
      "authors": "Changgyoon Oh, Jongoh Jeong, Jegyeong Cho, Kuk-Jin Yoon",
      "institution": "KAIST (Korea Advanced Institute of Science and Technology)",
      "link": "https://arxiv.org/pdf/2512.23210",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion timestep selection",
        "few-shot learning",
        "dense prediction",
        "Taskonomy",
        "parameter-efficient fine-tuning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a43fa8fc87c3dacfd338cae33c86a18032f1b16e9c26b532e45a4b6342b44c0_w640_q70.webp",
      "contributions": "1. Proposes a Task-aware Timestep Selection (TTS) module to adaptively select ideal diffusion timesteps for a given task based on losses and similarity scores. 2. Introduces a Timestep Feature Consolidation (TFC) module to consolidate the selected timestep features to improve dense prediction performance. 3. Presents a framework that, with a parameter-efficient fine-tuning adapter, achieves superior few-shot dense prediction performance on unseen tasks.",
      "summary": "This paper addresses the sub-optimal performance of diffusion models in few-shot dense prediction tasks due to heuristic timestep feature selection. It proposes a framework with learnable modules (TTS and TFC) to adaptively select and consolidate diffusion timestep features, which is validated on the Taskonomy dataset, showing superior performance in universal few-shot learning scenarios.",
      "mindmap": "graph TB\n        Root[”Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks<br>面向通用少样本密集任务的面向任务可学习扩散时间步”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”Heuristic diffusion timestep selection leads to sub-optimal, task-biased performance.<br>启发式扩散时间步选择导致次优的、偏向特定任务的性能。”]\n        Method[”Proposes TTS for adaptive timestep selection and TFC for feature consolidation.<br>提出TTS用于自适应时间步选择和TFC用于特征整合。”]\n        Results[”Achieves superior few-shot dense prediction on the Taskonomy dataset.<br>在Taskonomy数据集上实现了优越的少样本密集预测性能。”]"
    },
    {
      "title": "AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding",
      "authors": "Jongoh Jeong, Taek-Jin Song, Jong-Hwan Kim, Kuk-Jin Yoon",
      "institution": "KAIST (Korea Advanced Institute of Science and Technology)",
      "link": "https://arxiv.org/pdf/2512.23215",
      "code": null,
      "tags": [
        "object detection",
        "autonomous driving",
        "adverse conditions",
        "semantic segmentation",
        "depth estimation",
        "multi-task learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1278effa0d2d3d0566d9f768237cda657302c10c260f1b221c5739281af453cb_w640_q70.webp",
      "contributions": "1. Introduces the AVOID dataset, a new simulated driving dataset focused on unexpected small road obstacles captured under diverse adverse weather and time conditions. 2. Provides rich multi-modal annotations per image, including semantic and depth maps, LiDAR data (raw and semantic), and waypoints to support various perception tasks. 3. Benchmarks real-time obstacle detection networks and proposes ablation studies using a comprehensive multi-task network for semantic segmentation, depth, and waypoint prediction.",
      "summary": "This paper introduces the AVOID dataset to address the lack of driving datasets containing small, unexpected road obstacles under varied adverse conditions. The dataset, collected in simulation, provides rich multi-modal annotations and is used to benchmark real-time obstacle detection and multi-task perception models. The work aims to improve the robustness of visual perception for autonomous driving in challenging scenarios.",
      "mindmap": "graph TB\n        Root[AVOID Dataset] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有数据集缺乏不利条件下的意外小障碍物/Existing datasets lack small unexpected obstacles under adverse conditions]\n        Method[主要方法/Method] --> M1[提出AVOID模拟驾驶数据集/Propose AVOID simulated driving dataset]\n        M1 --> M2[包含多模态标注: 语义/深度/LiDAR/路径点/Contains multi-modal annotations: semantic/depth/LiDAR/waypoints]\n        Results[关键结果/Results] --> R1[为实时障碍物检测提供基准/Benchmark for real-time obstacle detection]\n        Results --> R2[提出并评估多任务网络/Propose and evaluate multi-task network]"
    },
    {
      "title": "MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?",
      "authors": "Shiqi Dai, Zizhi Ma, Zhicong Luo, Xuesong Yang, Yibin Huang, Wanyue Zhang, Chi Chen, Zonghao Guo, Wang Xu, Yufei Sun, Maosong Sun",
      "institution": "Tsinghua University, Nankai University, Northwest Polytechnical University, Chinese Academy of Sciences, Harbin Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.23219",
      "code": "https://github.com/MM-UAV/MM-UAVBench",
      "tags": [
        "multimodal evaluation",
        "multimodal large language models",
        "unmanned aerial vehicles",
        "benchmark",
        "low-altitude scenarios",
        "spatial bias"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87d6840ebc06b3a6509f941a74651b1ac51e173bc32596cbc933f96b55e0db61_w640_q70.webp",
      "contributions": "1. Introduces MM-UAVBench, a comprehensive benchmark for evaluating MLLMs in low-altitude UAV scenarios across perception, cognition, and planning dimensions. 2. Constructs a dataset of over 5.7K manually annotated questions derived from real-world UAV data, covering 19 sub-tasks. 3. Through extensive experiments on 16 MLLMs, identifies critical bottlenecks like spatial bias and multi-view understanding that hinder model performance in these scenarios.",
      "summary": "The paper introduces MM-UAVBench, a new benchmark designed to evaluate the capabilities of Multimodal Large Language Models (MLLMs) in low-altitude Unmanned Aerial Vehicle (UAV) scenarios. The benchmark systematically tests models on perception, cognition, and planning using over 5.7K annotated questions from real UAV data. The evaluation reveals that current MLLMs struggle with the complex demands of low-altitude environments, highlighting key bottlenecks like spatial bias.",
      "mindmap": "graph TB\n        A[MM-UAVBench: How Well Do MLLMs See, Think, and Plan in Low-Altitude UAV Scenarios?] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有基准未覆盖低空无人机场景的独特挑战 / Existing benchmarks lack coverage of unique low-altitude UAV challenges]\n        C --> C1[提出多维度评估基准MM-UAVBench / Propose multi-dimensional benchmark MM-UAVBench]\n        C --> C2[包含19个子任务与5.7K+人工标注问题 / Contains 19 sub-tasks and 5.7K+ manually annotated questions]\n        D --> D1[当前MLLMs难以适应低空复杂需求 / Current MLLMs struggle to adapt to complex low-altitude demands]\n        D --> D2[发现空间偏见等关键瓶颈 / Uncover critical bottlenecks like spatial bias]"
    },
    {
      "title": "Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information",
      "authors": "Youngchae Kwon, Jinyoung Choi, Injung Kim",
      "institution": "Handong Global University",
      "link": "https://arxiv.org/pdf/2512.23221",
      "code": null,
      "tags": [
        "object detection",
        "Detection Transformer (DETR)",
        "Contextual Information",
        "Holistic Detection",
        "Fashion Item Detection",
        "Co-occurrence Relationship"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dda1eab87d833571a5feac46be0fc3ba879ccabde53c04f72e0d4fcae137cb6e_w640_q70.webp",
      "contributions": "1. Proposes Holi-DETR, a novel holistic detection framework for fashion items that leverages contextual information to reduce detection ambiguities., 2. Introduces a novel architecture that integrates three distinct types of contextual information (co-occurrence, inter-item spatial arrangements, and item-body keypoint relationships) into DETR-based models., 3. Demonstrates performance improvements over baseline models (vanilla DETR and Co-DETR) in terms of average precision (AP).",
      "summary": "This paper addresses the challenge of fashion item detection, which is difficult due to diverse appearances and similar subcategories. The authors propose Holi-DETR, a holistic Detection Transformer that leverages three types of contextual information—co-occurrence, spatial arrangements, and body keypoints—to improve detection accuracy. The method shows improved performance over baseline DETR models.",
      "mindmap": "graph TB\n        A[Holi-DETR: Holistic Fashion Item Detection<br>Holi-DETR: 整体时尚物品检测] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Fashion item detection is challenging due to diverse appearances and similarities among subcategories.<br>时尚物品检测因外观多样和子类别相似而具有挑战性。]\n        C[主要方法/Method<br>Proposes Holi-DETR, a holistic detector leveraging three contextual cues: co-occurrence, spatial arrangements, and body keypoints.<br>提出Holi-DETR，利用共现、空间布局和身体关键点三种上下文线索的整体检测器。]\n        D[关键结果/Results<br>Improved performance over vanilla DETR (+3.6pp AP) and Co-DETR (+1.1pp AP).<br>性能超越原始DETR (+3.6pp AP) 和 Co-DETR (+1.1pp AP)。]"
    },
    {
      "title": "Anomaly Detection by Effectively Leveraging Synthetic Images",
      "authors": "Sungho Kang, Hyunkyu Park, Yeonho Lee, Hanbyul Lee, Mijoo Jeong, YeongHyeon Park, Injae Lee, Juneho Yi",
      "institution": "Sungkyunkwan University, The University of Texas MD Anderson Cancer Center",
      "link": "https://arxiv.org/pdf/2512.23227",
      "code": null,
      "tags": [
        "anomaly detection",
        "synthetic data",
        "image-to-image translation",
        "image retrieval",
        "two-stage training",
        "MVTec AD"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3546b17641c719167618772aa6e4da085e82a3b6d85cd2abc9940897d3e1f92_w640_q70.webp",
      "contributions": "1. A novel framework that efficiently generates synthetic defect images by leveraging a pre-trained text-guided image-to-image translation model and an image retrieval model for filtering. 2. A two-stage training strategy that pre-trains on a large volume of rule-based synthetic images and then fine-tunes on a smaller set of high-quality generated images. 3. Demonstration of the approach's effectiveness in reducing data collection costs while improving anomaly detection performance on the MVTec AD benchmark dataset.",
      "summary": "This paper addresses the trade-off in synthetic data generation for anomaly detection by proposing a framework that uses a pre-trained image-to-image translation model and an image retrieval filter to efficiently create realistic defect images. It also introduces a two-stage training strategy to leverage both cheap, low-quality and expensive, high-quality synthetic data effectively. Experiments on MVTec AD show this method reduces costs and improves detection performance.",
      "mindmap": "graph TB\n        A[Anomaly Detection by Effectively Leveraging Synthetic Images] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 真实缺陷图像稀缺，现有合成方法在成本与质量间难以权衡/Scarcity of real defect images, trade-off between cost and quality in synthesis]\n        C[主要方法/Method: 1. 使用预训练图像翻译与检索模型高效生成缺陷图/Use pre-trained image-to-image & retrieval models for generation. 2. 两阶段训练策略：先预训练再微调/Two-stage training: pre-train then fine-tune]\n        D[关键结果/Results: 在MVTec AD数据集上验证有效，降低成本并提升性能/Validated on MVTec AD, reduces cost and improves performance]"
    },
    {
      "title": "SURE Guided Posterior Sampling: Trajectory Correction for Diffusion-Based Inverse Problems",
      "authors": "Minwoo Kim, Hongki Lim",
      "institution": "Inha University",
      "link": "https://arxiv.org/pdf/2512.23232",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion-based inverse problems",
        "Stein's Unbiased Risk Estimate (SURE)",
        "posterior sampling",
        "trajectory correction",
        "PCA-based noise estimation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/111900f78e63afd7898adde1bee4891035e48eb062b31baf3ef50535922421b6_w640_q70.webp",
      "contributions": "1. Proposes SURE Guided Posterior Sampling (SGPS), a method that corrects sampling trajectory deviations in diffusion-based inverse problem solvers. 2. Introduces the use of Stein's Unbiased Risk Estimate (SURE) gradient updates and PCA-based noise estimation to mitigate noise-induced errors during early and middle sampling stages. 3. Demonstrates that SGPS maintains high reconstruction quality with fewer than 100 Neural Function Evaluations (NFEs), outperforming existing methods at low NFE counts.",
      "summary": "This paper addresses the challenge of error accumulation in diffusion-based inverse problem solvers, which typically require many steps for high-quality reconstruction. The authors propose SURE Guided Posterior Sampling (SGPS), a method that corrects the sampling trajectory using SURE gradient updates and PCA-based noise estimation. The method reduces error accumulation, enabling high-quality reconstructions in fewer than 100 steps and outperforming existing approaches.",
      "mindmap": "graph TB\n        Root[”SURE Guided Posterior Sampling<br>SURE引导后验采样”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”扩散逆问题求解<br>Diffusion-Based Inverse Problems”]\n        Problem --> P2[”误差累积需大量步数<br>Error Accumulation Requires Many Steps”]\n        Method --> M1[”SURE引导轨迹校正<br>SURE Guided Trajectory Correction”]\n        Method --> M2[”PCA噪声估计<br>PCA-Based Noise Estimation”]\n        Results --> R1[”少于100次NFE<br>Fewer than 100 NFEs”]\n        Results --> R2[”高质量重建<br>High-Quality Reconstruction”]\n        Results --> R3[”优于现有方法<br>Outperforms Existing Methods”]"
    },
    {
      "title": "Bridging Your Imagination with Audio-Video Generation via a Unified Director",
      "authors": "Jiaxu Zhang, Tianshu Hu, Yuan Zhang, Zenan Li, Linjie Luo, Guosheng Lin, Xin Chen",
      "institution": "ByteDance Intelligent Creation, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.23222",
      "code": "https://kebii.github.io/UniMAGE",
      "tags": [
        "video generation",
        "unified director model",
        "Mixture-of-Transformers",
        "interleaved concept learning",
        "disentangled expert learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66a6f11305dbb66573f2efcf30ff0c1abc82467fe78ca1d1328a5beb5932ebd_w640_q70.webp",
      "contributions": "1. Proposes UniMAGE, a unified director model that integrates script drafting and key-shot design into a single framework. 2. Introduces a \"first interleaving, then disentangling\" training paradigm (Interleaved Concept Learning and Disentangled Expert Learning) to enhance narrative logic and keyframe consistency. 3. Employs a Mixture-of-Transformers architecture to unify text and image generation within one model.",
      "summary": "This paper addresses the disjoint treatment of script drafting and key-shot design in AI-driven video creation by proposing UniMAGE, a unified director model. It uses a Mixture-of-Transformers architecture and a novel \"first interleaving, then disentangling\" training paradigm to generate coherent scripts and consistent keyframes. Experiments show UniMAGE achieves state-of-the-art performance among open-source models for long-context, multi-shot film generation.",
      "mindmap": "graph TB\n        A[UniMAGE: Bridging Your Imagination with Audio-Video Generation via a Unified Director] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有系统将脚本起草与关键镜头设计视为独立任务/Existing systems treat script drafting and key-shot design as disjoint tasks]\n        C --> C1[提出统一导演模型UniMAGE/Propose unified director model UniMAGE]\n        C --> C2[采用混合Transformer架构/Employ Mixture-of-Transformers architecture]\n        C --> C3[引入”先交错，后解耦”训练范式/Introduce ”first interleaving, then disentangling” training paradigm]\n        D --> D1[在开源模型中达到SOTA性能/Achieves SOTA performance among open-source models]\n        D --> D2[生成逻辑连贯的脚本和视觉一致的图像/Generates logically coherent scripts and visually consistent keyframes]"
    },
    {
      "title": "Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network",
      "authors": "Dongsheng Li, Chaobo Chen, Siling Wang, Song Gao",
      "institution": "(Inferred from author names and arXiv handle; specific institution not provided in the given text. Could be a Chinese research institution or university.)",
      "link": "https://arxiv.org/pdf/2512.23234",
      "code": null,
      "tags": [
        "object detection",
        "physics-inspired modeling",
        "edge detection",
        "content-adaptive routing",
        "multi-scale feature fusion",
        "infrared gas leak detection"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee948d9a0589e5467502d1d916e59d51137b1253413c1001ade729584fd4bf55_w640_q70.webp",
      "contributions": "1. Proposed a physics-inspired Gas Block module that models gas transport using a diffusion-convection unit with local and large-kernel branches, fused via an edge-gated module to enhance weak plume features. 2. Introduced a novel Adaptive Gradient and Phase Edge Operator (AGPEO) and a Multi-Scale Edge Perception Module (MSEPM) to compute and integrate reliable hierarchical edge priors for boundary reinforcement. 3. Designed a Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN) that uses adaptive modulation to selectively propagate informative features across scales based on content and edge cues, improving efficiency and discriminability.",
      "summary": "The paper proposes PEG-DRNet, a physics-inspired and edge-guided network for detecting faint infrared gas leaks. The method combines a gas transport model, a novel edge detection operator, and a content-adaptive routing mechanism for multi-scale feature fusion. Experiments show PEG-DRNet achieves superior accuracy and computational efficiency on benchmark datasets compared to existing detectors.",
      "mindmap": "graph TB\n        A[Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network] --> B(核心问题/Problem: 红外气体泄漏检测困难/Infrared gas leak detection is difficult due to faint, small, semitransparent plumes with weak boundaries.)\n        A --> C(主要方法/Method: 提出PEG-DRNet/Propose PEG-DRNet)\n        C --> C1(气体块建模气体传输/Gas Block models gas transport)\n        C --> C2(自适应梯度相位边缘算子/Adaptive Gradient and Phase Edge Operator (AGPEO))\n        C --> C3(内容自适应稀疏路由聚合网络/Content-Adaptive Sparse Routing Path Aggregation Network (CASR-PAN))\n        A --> D(关键结果/Results: 在IIG和LangGas数据集上性能优越/Superior performance on IIG and LangGas datasets, achieving higher AP and AP50 with good efficiency.)"
    },
    {
      "title": "RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models",
      "authors": "Fan Wei, Runmin Dong, Yushan Lai, Yixiang Yang, Zhaoyang Luo, Jinxiao Zhang, Miao Yang, Shuai Yuan, Jiyao Zhao, Bin Luo, Haohuan Fu",
      "institution": "Tsinghua University, Sun Yat-sen University, National Supercomputing Center in Shenzhen, Tsinghua Shenzhen International Graduate School, The University of Hong Kong, Peking University",
      "link": "https://arxiv.org/pdf/2512.23239",
      "code": null,
      "tags": [
        "diffusion models",
        "data pruning",
        "remote sensing",
        "diffusion foundation models",
        "entropy-based selection",
        "scene-aware clustering"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9ceb6378a9dfeacd28f04cab82d2b54590992943b1b3cdc469d6c8c32434a9f_w640_q70.webp",
      "contributions": "1. A novel, training-free, two-stage data pruning method for remote sensing diffusion models that jointly considers local information content and global scene-level diversity. 2. A reference-guided, scene-aware clustering strategy that leverages existing classification datasets to efficiently select a high-quality subset from large-scale unlabeled data. 3. Demonstrated effectiveness under high pruning ratios (e.g., 85%), significantly improving model convergence and generation quality, and achieving state-of-the-art performance on downstream tasks like super-resolution.",
      "summary": "This paper addresses the inefficiency of training diffusion-based remote sensing foundation models on large, redundant datasets. It proposes RS-Prune, a training-free, two-stage data pruning method that first removes low-information samples and then performs scene-aware clustering for fine-grained selection. The method enables rapid model convergence even after pruning 85% of data and achieves superior performance on downstream generation tasks.",
      "mindmap": "graph TB\n        A[RS-Prune: Training-Free Data Pruning for RS Diffusion Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[数据冗余与噪声/Data Redundancy & Noise]\n        B --> B2[训练效率低/Low Training Efficiency]\n        C --> C1[两阶段剪枝/Two-Stage Pruning]\n        C1 --> C1a[基于熵的筛选/Entropy-Based Filtering]\n        C1 --> C1b[场景感知聚类/Scene-Aware Clustering]\n        D --> D1[高剪枝率有效/Effective at High Ratios (85%)]\n        D --> D2[提升收敛与质量/Improved Convergence & Quality]\n        D --> D3[下游任务SOTA/SOTA on Downstream Tasks]"
    },
    {
      "title": "Contour Information Aware 2D Gaussian Splatting for Image Representation",
      "authors": "Masaya Takabe, Hiroshi Watanabe, Sujun Hong, Tomohiro Ikai, Zheming Fan, Ryo Ishimoto, Kakeru Sugimoto, Ruri Imichi",
      "institution": "Waseda University, Sharp Corporation",
      "link": "https://arxiv.org/pdf/2512.23255",
      "code": null,
      "tags": [
        "image representation",
        "2D Gaussian Splatting",
        "contour awareness",
        "segmentation priors",
        "warm-up scheme"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1364e59f36054f08d771d6c6a564f9406990fd9cbe9926e23e664a2ebd5a3be4_w640_q70.webp",
      "contributions": "1. A Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors to preserve edge structures. 2. A region-constrained rasterization method that prevents cross-boundary blending of Gaussians. 3. A warm-up training scheme to stabilize optimization and improve convergence.",
      "summary": "This paper addresses the problem of blurry object boundaries in highly compressed 2D Gaussian Splatting (2DGS) for image representation. The proposed method integrates segmentation priors to constrain Gaussians to specific object regions during rasterization, preventing blending across edges. Experiments show it achieves higher reconstruction quality around edges with very few Gaussians while maintaining fast rendering and low memory usage.",
      "mindmap": "graph TB\n        A[Contour Information Aware 2D Gaussian Splatting<br/>轮廓信息感知的2D高斯泼溅] --> B\n        A --> C\n        A --> D\n        B[Problem: Blurry boundaries in compressed 2DGS<br/>问题：压缩2DGS中的模糊边界]\n        C[Method: Region-constrained rasterization with segmentation priors<br/>方法：结合分割先验的区域约束栅格化]\n        D[Results: Better edge quality with few Gaussians<br/>结果：用少量高斯实现更好的边缘质量]"
    },
    {
      "title": "Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism",
      "authors": "Siyu Zhang, Ying Chen, Lianlei Shan, Runhe Qiu",
      "institution": "Sanda University, Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.23243",
      "code": null,
      "tags": [
        "multimodal fusion",
        "Dynamic Resolution Input Strategy (DRIS)",
        "Multi-scale Vision-language Alignment Mechanism (MS-VLAM)",
        "Vision-language Model (VLM)",
        "remote sensing image captioning",
        "cross-modal retrieval"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/26c024de47fb99d79be55abc94c255a70b38cb2c16e81d298d21a49c4c05bd20_w640_q70.webp",
      "contributions": "1. Proposes a Dynamic Resolution Input Strategy (DRIS) that adaptively allocates computational resources based on image content complexity to balance detail and efficiency. 2. Introduces a Multi-scale Vision-language Alignment Mechanism (MS-VLAM) with object, local-region, and global-level alignment to capture cross-modal semantic consistency. 3. Presents an integrated VLM framework that demonstrates superior performance in remote sensing image captioning and cross-modal retrieval tasks on the RS-GPT4V dataset.",
      "summary": "This paper addresses the inefficiency of fixed-resolution inputs and the lack of semantic hierarchy in single-scale alignment for multimodal remote sensing interpretation. It proposes a novel Vision-language Model framework featuring a Dynamic Resolution Input Strategy and a Multi-scale Vision-language Alignment Mechanism. The framework significantly improves both semantic understanding accuracy and computational efficiency in tasks like image captioning and cross-modal retrieval, as validated on the RS-GPT4V dataset.",
      "mindmap": "graph TB\n        A[Multimodal Interpretation of Remote Sensing Images<br>多模态遥感图像解译] --> B1(核心问题/Problem)\n        A --> B2(主要方法/Method)\n        A --> B3(关键结果/Results)\n        B1 --> C1[Fixed resolution fails to balance efficiency and detail<br>固定分辨率无法平衡效率与细节]\n        B1 --> C2[Single-scale alignment lacks semantic hierarchy<br>单尺度对齐缺乏语义层次]\n        B2 --> D1[Dynamic Resolution Input Strategy (DRIS)<br>动态分辨率输入策略]\n        B2 --> D2[Multi-scale Vision-language Alignment Mechanism (MS-VLAM)<br>多尺度视觉-语言对齐机制]\n        B3 --> E1[Improves accuracy in captioning & retrieval<br>提升描述与检索精度]\n        B3 --> E2[Enhances computational efficiency<br>提升计算效率]"
    },
    {
      "title": "ASemConsist: Adaptive Semantic Feature Control for Training-Free Identity-Consistent Generation",
      "authors": "Shin seong Kim, Minjung Shin, Hyunin Cho, Youngjung Uh",
      "institution": "Yonsei University",
      "link": "https://arxiv.org/pdf/2512.23245",
      "code": "https://minjung-s.github.io/asemconsist",
      "tags": [
        "text-to-image generation",
        "identity-consistent generation",
        "text embedding modification",
        "padding embeddings",
        "adaptive feature-sharing",
        "Consistency Quality Score (CQS)"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d92693c5da5c617b2e5cb2836099c658b115dfb4a132adf0caeb861c3ab1f213_w640_q70.webp",
      "contributions": "1. A novel framework (ASemConsist) for identity-consistent generation using selective text embedding modification and a semantic control strategy that repurposes padding embeddings as semantic containers. 2. An adaptive feature-sharing strategy that automatically evaluates textual ambiguity and applies constraints only to ambiguous identity prompts. 3. A unified evaluation protocol, the Consistency Quality Score (CQS), which integrates identity preservation and prompt alignment into a single metric to capture performance imbalances.",
      "summary": "This paper addresses the challenge of generating a sequence of images with consistent character identity while maintaining alignment with diverse per-image prompts. The proposed ASemConsist framework modifies text embeddings to control identity semantics, uses padding embeddings as semantic containers, and adaptively applies constraints based on prompt ambiguity. It achieves state-of-the-art performance by overcoming the trade-off between identity consistency and prompt alignment.",
      "mindmap": "graph TB\n        A[ASemConsist: Adaptive Semantic Feature Control] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[身份一致性生成中的权衡/Trade-off in identity-consistent generation]\n        B1 --> B2[保持身份一致性与确保每张图像提示对齐之间的冲突/Conflict between identity preservation and per-image prompt alignment]\n        C --> C1[选择性文本嵌入修改/Selective text embedding modification]\n        C --> C2[将填充嵌入重新用作语义容器/Repurposing padding embeddings as semantic containers]\n        C --> C3[自适应特征共享策略/Adaptive feature-sharing strategy]\n        D --> D1[提出统一评估协议CQS/Proposed unified evaluation protocol CQS]\n        D --> D2[实现最先进的性能/Achieved state-of-the-art performance]\n        D --> D3[克服了先前的权衡/Overcame prior trade-offs]"
    },
    {
      "title": "Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization",
      "authors": "Tong Shao, Yusen Fu, Guoying Sun, Jingde Kong, Zhuotao Tian, Jingyong Su",
      "institution": "Harbin Institute of Technology, Shenzhen",
      "link": "https://arxiv.org/pdf/2512.23258",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion transformer",
        "inference acceleration",
        "caching",
        "error minimization",
        "dynamic programming"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b2f0b193709fc539d32a8fa74b0405ea99491982cb3f280e0dd10ff89b6b0a3_w640_q70.webp",
      "contributions": "1. Proposes CEM, a novel plugin for optimizing caching strategies in DiT acceleration via cumulative error minimization. 2. Introduces a dynamic programming algorithm guided by a predefined error prior to adaptively minimize caching error. 3. Demonstrates the method's model-agnostic nature, seamless integration into existing frameworks, and significant fidelity improvements across multiple models and tasks.",
      "summary": "This paper addresses the slow inference of Diffusion Transformers (DiTs) by proposing CEM, a plug-and-play fidelity optimization plugin. CEM minimizes cumulative caching error via a dynamic programming algorithm, adapting to error variations during denoising. The method is training-free, model-agnostic, and significantly improves generation fidelity when integrated with existing acceleration techniques.",
      "mindmap": "graph TB\n        Root[”Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization”] --> Problem[”核心问题/Problem: DiT推理慢，现有缓存加速方法存在固定策略导致的累积误差/DiT inference is slow; existing caching-based acceleration suffers from fixed-strategy cumulative error”]\n        Root --> Method[”主要方法/Method: 提出CEM插件，通过累积误差最小化的动态规划优化缓存策略/Propose CEM plugin, optimizing caching strategy via cumulative error minimization with dynamic programming”]\n        Root --> Results[”关键结果/Results: 显著提升生成保真度，模型无关，可无缝集成/Significantly improves generation fidelity, model-agnostic, seamlessly integrable”]"
    },
    {
      "title": "ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing",
      "authors": "Xingwei Ma, Shiyang Feng, Bo Zhang, Bin Wang",
      "institution": "Fudan University, Shanghai Artificial Intelligence Laboratory",
      "link": "https://arxiv.org/pdf/2512.23244",
      "code": null,
      "tags": [
        "change detection",
        "vision-language model",
        "remote sensing",
        "semantic change detection",
        "supervised fine-tuning",
        "reinforcement learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5871856f6e1854dd58df304f783ce8ea57314887e0296e446d48afb30805307_w640_q70.webp",
      "contributions": "1. Proposes ViLaCD-R1, a novel two-stage vision-language framework for semantic change detection in remote sensing, comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). 2. Introduces a training strategy for the VLM using supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks to generate a coarse change mask. 3. Demonstrates that the framework significantly improves semantic change recognition and localization while suppressing non-semantic variations, achieving state-of-the-art performance on multiple benchmarks.",
      "summary": "This paper addresses the limitations of existing remote sensing change detection methods, such as poor semantic understanding and inaccurate localization, by proposing ViLaCD-R1. This two-stage vision-language framework first uses a fine-tuned VLM to generate a coarse change mask from dual-temporal images, then refines it with a decoder to produce a precise change map. The method shows superior performance in recognizing true semantic changes and suppressing irrelevant variations across several benchmarks.",
      "mindmap": "graph TB\n        A[ViLaCD-R1: 遥感语义变化检测的视觉语言框架] --> B1(核心问题/Problem)\n        A --> B2(主要方法/Method)\n        A --> B3(关键结果/Results)\n        B1 --> C1[传统方法语义理解不足/Traditional methods lack semantic understanding]\n        B1 --> C2[现有VLM方法定位不准确/Existing VLM methods have inaccurate localization]\n        B2 --> D1[两阶段框架/Two-stage framework]\n        D1 --> E1[多图像推理器/Multi-Image Reasoner]\n        E1 --> F1[SFT与RL训练/SFT and RL training]\n        E1 --> F2[生成粗变化掩码/Generate coarse change mask]\n        D1 --> E2[掩码引导解码器/Mask-Guided Decoder]\n        E2 --> F3[融合特征与掩码/Fuse features and mask]\n        E2 --> F4[预测精细变化图/Predict precise change map]\n        B3 --> G1[提升语义变化识别/Improves semantic change recognition]\n        B3 --> G2[抑制非语义变化/Suppresses non-semantic variations]\n        B3 --> G3[达到SOTA性能/Achieves SOTA performance]"
    },
    {
      "title": "YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection",
      "authors": "Xu Lin, Jinlong Peng, Zhenye Gan, Jiawen Zhu, Jun Liu",
      "institution": "Tencent Youtu Lab, Singapore Management University",
      "link": "https://arxiv.org/pdf/2512.23273",
      "code": "https://github.com/isLinXu/YOLO-Master",
      "tags": [
        "object detection",
        "Mixture-of-Experts",
        "adaptive computation",
        "real-time detection",
        "dynamic routing",
        "YOLO"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7def7ba8e75e1cc89945c90cd74ab4aec5217397e89bc6d189d86c3260048636_w640_q70.webp",
      "contributions": "1. Proposes YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for real-time object detection. 2. Designs an Efficient Sparse Mixture-of-Experts (ES-MoE) block to dynamically allocate computational resources based on scene complexity. 3. Introduces a lightweight dynamic routing network with a diversity-enhancing objective to encourage complementary expert specialization and efficient inference.",
      "summary": "The paper identifies that existing YOLO models use static computation, leading to inefficiency on simple scenes and poor performance on complex ones. To solve this, it proposes YOLO-Master, which uses a Mixture-of-Experts block and a dynamic router to adaptively allocate computation per input. Experiments show it achieves higher accuracy and faster speed than baselines, especially on dense scenes, while maintaining real-time performance.",
      "mindmap": "graph TB\n        A[YOLO-Master] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[静态计算导致资源错配/Static computation misallocates resources]\n        C --> C1[高效稀疏专家混合块/ES-MoE Block]\n        C --> C2[动态路由网络/Dynamic Routing Network]\n        D --> D1[更高精度与速度/Higher Accuracy & Speed]\n        D --> D2[在密集场景提升显著/Better on Dense Scenes]"
    },
    {
      "title": "Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition",
      "authors": "Arman Martirosyan, Shahane Tigranyan, Maria Razzhivina, Artak Aslanyan, Nazgul Salikhova, Ilya Makarov, Andrey Savchenko, Aram Avetisyan",
      "institution": "Russian-Armenian University, ISP RAS, HSE University, Innopolis University, AIRI, Sber AI Lab",
      "link": "https://arxiv.org/pdf/2512.23291",
      "code": null,
      "tags": [
        "multimodal emotion recognition",
        "micro-gesture recognition",
        "behavior-based emotion prediction",
        "multimodal fusion",
        "Cross-Modal Token Fusion",
        "InterFusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98bb53f61c281fed2a6cb4e682e56bc767703f6a640cfb8f4d681abb07a0e6ce_w640_q70.webp",
      "contributions": "1. Proposed a multimodal framework for micro-gesture classification that fuses RGB (MViTv2-S) and 3D pose (2s-AGCN) embeddings via a Cross-Modal Token Fusion module. 2. Developed a separate multimodal framework for behavior-based emotion prediction that fuses facial (SwinFace) and contextual (MViTv2-S) embeddings via an InterFusion module. 3. Demonstrated robust performance on the iMiGUE dataset, achieving 2nd place in the behavior-based emotion prediction task of the MiGA 2025 Challenge.",
      "summary": "This paper presents two multimodal frameworks to tackle micro-gesture recognition and behavior-based emotion prediction on the iMiGUE dataset. The first framework fuses video and skeletal pose data, while the second fuses facial and contextual embeddings for emotion classification. The method demonstrated strong performance, securing 2nd place in the emotion prediction task of the MiGA 2025 Challenge.",
      "mindmap": "graph TB\n        A[Multi-Track Multimodal Learning on iMiGUE] --> B1(核心问题/Problem)\n        A --> B2(主要方法/Method)\n        A --> B3(关键结果/Results)\n        B1 --> C1(微手势识别/Micro-gesture Recognition)\n        B1 --> C2(行为情绪预测/Behavior-based Emotion Prediction)\n        B2 --> D1(多模态框架/Multimodal Framework)\n        D1 --> E1(微手势: RGB+姿态融合/Micro-gesture: RGB+Pose Fusion)\n        D1 --> E2(情绪: 面部+上下文融合/Emotion: Facial+Context Fusion)\n        E1 --> F1(使用MViTv2-S和2s-AGCN/Use MViTv2-S & 2s-AGCN)\n        E1 --> F2(跨模态令牌融合/Cross-Modal Token Fusion)\n        E2 --> F3(使用SwinFace和MViTv2-S/Use SwinFace & MViTv2-S)\n        E2 --> F4(内部融合模块/InterFusion Module)\n        B3 --> G1(在iMiGUE数据集上评估/Evaluated on iMiGUE Dataset)\n        B3 --> G2(MiGA 2025挑战赛第二名/2nd Place in MiGA 2025 Challenge)"
    },
    {
      "title": "MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images",
      "authors": "Md. Sazzadul Islam Prottasha, Nabil Walid Rafi",
      "institution": "Bangladesh University of Professionals",
      "link": "https://arxiv.org/pdf/2512.23304",
      "code": null,
      "tags": [
        "medical image classification",
        "MedGemma",
        "GPT-4",
        "LoRA",
        "zero-shot classification",
        "multimodal LLM"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58abac32272c08efbbe249ec33f3b4f4aa3a6f4d477b241718ddbde679cce96a_w640_q70.webp",
      "contributions": "1. Conducted a critical comparison between the open-source MedGemma and proprietary GPT-4 for zero-shot medical disease classification from images. 2. Demonstrated that the LoRA-fine-tuned MedGemma model significantly outperformed the untuned GPT-4 in accuracy and sensitivity for high-stakes clinical tasks. 3. Highlighted the essential role of domain-specific fine-tuning in minimizing hallucinations and enabling complex, evidence-based medical reasoning for clinical implementation.",
      "summary": "This study compares the performance of the open-source MedGemma model and the proprietary GPT-4 for zero-shot classification of six diseases from medical images. The MedGemma model, fine-tuned with LoRA, achieved higher mean accuracy and sensitivity than GPT-4. The results show that domain-specific fine-tuning is crucial for reliable clinical applications, positioning MedGemma as a sophisticated tool for medical diagnostics.",
      "mindmap": "graph TB\n        A[MedGemma vs GPT-4: 医学图像零样本疾病分类] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 比较开源与闭源多模态LLM在医学图像诊断中的性能]\n        C[主要方法/Method: 使用LoRA微调的MedGemma与未调优的GPT-4进行零样本分类对比]\n        D[关键结果/Results: MedGemma准确率(80.37%)和敏感性更高，领域微调对减少幻觉至关重要]"
    },
    {
      "title": "PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering",
      "authors": "Sheng-Kai Chen, Jie-Yu Chao, Jr-Yu Chang, Po-Lien Wu, Po-Chiang Lin",
      "institution": "Yuan Ze University",
      "link": "https://arxiv.org/pdf/2512.23318",
      "code": null,
      "tags": [
        "visual SLAM",
        "ORB-SLAM3",
        "YOLOv8",
        "dynamic object filtering",
        "point cloud refinement",
        "CUDA acceleration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e72d53f96b383c73428b67d24fbf2d4163ac5820be1bfed6f370c529922f919_w640_q70.webp",
      "contributions": "1. Proposes PCR-ORB, an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to filter dynamic objects. 2. Implements a multi-stage filtering strategy combining semantic segmentation (YOLOv8), ground plane estimation, sky removal, edge filtering, and temporal consistency for robust dynamic object removal. 3. Achieves real-time performance through CUDA-accelerated processing and demonstrates significant accuracy improvements in specific dynamic sequences on the KITTI dataset.",
      "summary": "This paper introduces PCR-ORB, an enhanced visual SLAM system that improves ORB-SLAM3's robustness in dynamic environments by integrating YOLOv8 for semantic segmentation and a multi-stage point cloud refinement process to filter moving objects. The method achieves real-time performance with CUDA acceleration. Evaluation on KITTI shows scenario-dependent effectiveness, with notable accuracy improvements in some sequences but mixed results overall.",
      "mindmap": "graph TB\n        Root[PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: vSLAM accuracy compromised by dynamic objects]\n        Method[主要方法/Method: ORB-SLAM3 + YOLOv8 segmentation + multi-stage point cloud filtering]\n        Results[关键结果/Results: Mixed performance, notable improvement in specific sequences (e.g., Seq04)]"
    },
    {
      "title": "CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations",
      "authors": "Huan-ang Gao, Zikang Zhang, Tianwei Luo, Kaisen Yang, Xinzhe Juan, Jiahao Qiu, Tianxing Chen, Bingxiang He, Hao Zhao, Hao Zhou, Shilong Liu, Mengdi Wang",
      "institution": "Tsinghua University, Princeton University, Shanghai Jiao Tong University & University of Michigan, The University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.23328",
      "code": null,
      "tags": [
        "agent evaluation",
        "spatial reasoning",
        "long-horizon planning",
        "partial observability",
        "mental simulation",
        "diagnostic benchmark"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/210465a4bf9048c43ec900e17f922e63394d83664c6fe631fec0d54577fd9fb6_w640_q70.webp",
      "contributions": "1. Identifies three core cognitive challenges (spatial reasoning, long-horizon state tracking, active exploration under partial observation) hindering LLM agents in the physical world. 2. Introduces CubeBench, a novel generative benchmark based on the Rubik's Cube with a three-tiered diagnostic framework to isolate and evaluate these capabilities. 3. Provides a diagnostic framework using external solver tools to analyze failure modes and reveals critical limitations of leading LLMs, including a 0.00% pass rate on long-horizon tasks.",
      "summary": "The paper introduces CubeBench, a diagnostic benchmark using a Rubik's Cube to evaluate LLM agents' spatial reasoning and long-horizon planning under partial observation. It employs a three-tiered framework from full symbolic to partial visual states. Experiments show leading LLMs fail completely on long-horizon tasks, highlighting a fundamental gap for physical-world deployment.",
      "mindmap": "graph TB\n        A[CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[LLM智能体缺乏物理世界部署所需的稳健空间心智模型/LLM agents lack robust spatial mental models for physical-world deployment]\n        C --> C1[提出基于魔方的三层诊断基准/CubeBench: A three-tiered diagnostic benchmark using Rubik's Cube]\n        C --> C2[从完整符号状态到部分视觉状态逐步评估/Progressive evaluation from full symbolic to partial visual state]\n        D --> D1[领先LLM在长视野任务上通过率为0%/Leading LLMs have 0.00% pass rate on long-horizon tasks]\n        D --> D2[揭示了长期规划和主动探索的根本性失败/Exposes fundamental failure in long-term planning and active exploration]"
    },
    {
      "title": "CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation",
      "authors": "Ke Niu, Haiyang Yu, Zhuofan Chen, Zhengtao Yao, Weitao Jia, Xiaodong Ge, Jingqun Tang, Benlei Cui, Bin Li, Xiangyang Xue",
      "institution": "Fudan University, ByteDance Inc.",
      "link": "https://arxiv.org/pdf/2512.23333",
      "code": null,
      "tags": [
        "reinforcement learning",
        "CAD code generation",
        "multi-expert reinforcement learning",
        "Chain-of-Thought",
        "CADExpert benchmark",
        "CADQuery"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/854c145ea4394c54526f3cfa5f5b5e6528680bb17418bfc2756a03817bea2de5_w640_q70.webp",
      "contributions": "1. Proposes a novel Heterogeneous Collaborative Multi-Expert Reinforcement Learning (CME-CAD) paradigm for generating precise and editable CAD models., 2. Introduces a two-stage training process: Multi-Expert Fine-Tuning (MEFT) and Multi-Expert Reinforcement Learning (MERL)., 3. Presents CADExpert, an open-source benchmark with 17,299 instances including orthographic projections, CoT processes, CADQuery code, and 3D models.",
      "summary": "This paper addresses the challenge of automating the generation of high-precision, editable CAD models from sketches, which existing methods struggle with. It proposes a new training paradigm called CME-CAD, which uses a two-stage process of Multi-Expert Fine-Tuning and Reinforcement Learning to collaboratively improve model performance. The approach aims to generate accurate, constraint-compatible CAD code and is supported by a new open-source benchmark called CADExpert.",
      "mindmap": "graph TB\n        A[CME-CAD: CAD代码生成] --> B1\n        A --> B2\n        A --> B3\n        B1[核心问题/Problem<br>现有方法生成模型不可编辑、不精确<br>依赖文本/图像输入，标注成本高]\n        B2[主要方法/Method<br>异构协作多专家强化学习<br>两阶段训练: MEFT + MERL]\n        B3[关键结果/Results<br>生成精确、可编辑的CAD模型<br>发布CADExpert基准数据集]"
    },
    {
      "title": "Visual Language Hypothesis",
      "authors": "Xiu Li",
      "institution": "Bytedance",
      "link": "https://arxiv.org/pdf/2512.23335",
      "code": null,
      "tags": [
        "representation learning",
        "visual language hypothesis",
        "fiber bundle",
        "semantic quotient",
        "expand-and-snap",
        "topology change"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d70511c4d2f57ecb4e49170ed73c0a81de0fcfcdbdb84cc7bcbdca254140c3f_w640_q70.webp",
      "contributions": "1. Proposes the \"Visual Language Hypothesis,\" framing visual understanding as requiring a discrete semantic language, leading to a fiber-bundle-like structure for the observation space. 2. Derives a theoretical requirement for semantic invariance, arguing it necessitates a non-homeomorphic, discriminative target (e.g., supervised labels, multimodal alignment) rather than smooth deformation alone. 3. Identifies an architectural requirement for models, proposing an \"expand-and-snap\" process to achieve the necessary topology change for semantic abstraction.",
      "summary": "This paper proposes the \"Visual Language Hypothesis,\" which posits that visual understanding requires a discrete semantic structure, implying the visual observation space is organized like a fiber bundle. From this, the authors theoretically argue that achieving semantic invariance demands discriminative supervision and that model architectures must support a specific \"expand-and-snap\" process to change topology. The framework provides a topological interpretation for empirical patterns in large-scale discriminative and multimodal models.",
      "mindmap": "graph TB\n        A[Visual Language Hypothesis] --> B[核心问题/Problem: What structural properties enable semantic abstraction in vision?]\n        A --> C[主要方法/Method: Propose hypothesis of discrete semantic language, derive geometric (fiber bundle) structure]\n        A --> D[关键结果/Results: Semantic invariance needs discriminative target; Model needs ”expand-and-snap” for topology change]"
    },
    {
      "title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents",
      "authors": "Jiafeng Liang, Hao Li, Chang Li, Jiaqi Zhou, Shixin Jiang, Zekun Wang, Changkai Ji, Zhihao Zhu, Runxuan Liu, Tao Ren, Jinlan Fu, See-Kiong Ng, Xia Liang, Ming Liu, Bing Qin",
      "institution": "Harbin Institute of Technology, Fudan University, Peking University, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.23343",
      "code": "https://github.com/AgentMemory/Huaman-Agent-Memory",
      "tags": [
        "agent system",
        "memory systems",
        "cognitive neuroscience",
        "LLM-driven agents",
        "memory security",
        "multimodal memory"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15773eb4c52c63f2641be869baf3af4b7f6bb74f6e36c67247957bfbd039e9b6_w640_q70.webp",
      "contributions": "1. Provides a systematic synthesis and comparative analysis of memory systems from cognitive neuroscience to LLM-driven autonomous agents. 2. Reviews mainstream benchmarks for evaluating agent memory and explores memory security from attack and defense perspectives. 3. Envisions future research directions, focusing on multimodal memory systems and skill acquisition.",
      "summary": "This survey paper bridges the interdisciplinary gap between cognitive neuroscience and AI by systematically analyzing memory systems for autonomous agents. It compares biological and artificial memory taxonomies, storage, and management, while also reviewing evaluation benchmarks and security issues. The work concludes by outlining future directions, including multimodal memory and skill learning.",
      "mindmap": "graph TB\n        Root[AI Meets Brain: Memory Systems / AI与大脑：记忆系统] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[核心问题/Problem] --> P1[Interdisciplinary Gap / 跨学科鸿沟]\n        P1 --> P2[Existing works struggle to assimilate human memory essence / 现有工作难以吸收人类记忆机制精髓]\n    \n        Method[主要方法/Method] --> M1[Systematic Synthesis / 系统综述]\n        M1 --> M2[Comparative Analysis / 对比分析]\n        M2 --> M3[Review Benchmarks & Security / 回顾基准与安全]\n    \n        Results[关键结果/Results] --> R1[Unified Memory Framework / 统一的记忆框架]\n        R1 --> R2[Future Directions / 未来方向]\n        R2 --> R3[Multimodal Memory & Skill Acquisition / 多模态记忆与技能获取]"
    },
    {
      "title": "CountGD++: Generalized Prompting for Open-World Counting",
      "authors": "Niki Amini-Naieni, Andrew Zisserman",
      "institution": "University of Oxford (Visual Geometry Group)",
      "link": "https://arxiv.org/pdf/2512.23351",
      "code": "https://github.com/niki-amini-naieni/CountGDPlusPlus",
      "tags": [
        "object counting",
        "open-world counting",
        "negative prompting",
        "pseudo-exemplars"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1cf78a83ebaf07de6caf2a7ac0a9fc065838c62bc22493f5dab52e0be563c777_w640_q70.webp",
      "contributions": "1. Extended the counting prompt to include negative specifications (what not to count) via text and/or visual examples. 2. Introduced 'pseudo-exemplars' to automate the annotation of visual examples at inference time. 3. Enabled counting models to accept visual examples from both natural and synthetic external images, and integrated the model as a vision expert agent for an LLM.",
      "summary": "The paper addresses limitations in open-world object counting by introducing CountGD++, a model that significantly expands prompt flexibility. It allows specifying what not to count, automates visual example annotation via pseudo-exemplars, and accepts external visual examples, leading to improved accuracy and generalization across datasets.",
      "mindmap": "graph TB\n        A[CountGD++: Generalized Prompting for Open-World Counting] --> B[核心问题/Problem: Limited prompt flexibility in object counting models]\n        A --> C[主要方法/Method: Negative prompts, pseudo-exemplars, external visual examples]\n        A --> D[关键结果/Results: Improved accuracy, efficiency, and generalization]"
    },
    {
      "title": "SpatialMosaic: A Multiview VLM Dataset for Partial Visibility",
      "authors": "Kanghee Lee, Injae Lee, Minseok Kwak, Kwonyoung Ryu, Jungi Hong, Jaesik Park",
      "institution": "Seoul National University, University College London, POSTECH",
      "link": "https://arxiv.org/pdf/2512.23365",
      "code": null,
      "tags": [
        "multi-view vision-language reasoning",
        "spatial reasoning",
        "multi-view images",
        "partial visibility",
        "instruction-tuning",
        "3D reconstruction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76b593a5c7594c6552e0421062301bf10006b9896666b9446c8fe3d5e0816795_w640_q70.webp",
      "contributions": "1. A scalable multi-view data generation and annotation pipeline for realistic spatial reasoning QA pairs. 2. SpatialMosaic, a comprehensive 2M QA pair instruction-tuning dataset, and SpatialMosaic-Bench, a 1M QA pair benchmark for evaluating multi-view spatial reasoning under challenging conditions. 3. SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning.",
      "summary": "This paper addresses the challenge of enabling Vision-Language Models (VLMs) to perform 3D spatial reasoning directly from multi-view images under realistic conditions like partial visibility and occlusion. It proposes a data generation pipeline to create the SpatialMosaic dataset and benchmark, and introduces a hybrid VLM framework that incorporates 3D reconstruction models. Experiments show the approach effectively enhances spatial reasoning in challenging multi-view scenarios.",
      "mindmap": "graph TB\n        A[SpatialMosaic: A Multiview VLM Dataset for Partial Visibility] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法依赖3D重建，限制可扩展性/Existing methods rely on 3D reconstruction, limiting scalability]\n        B --> B2[真实场景的碎片化视觉线索未被充分探索/Real-world fragmented visual cues are under-explored]\n        C --> C1[可扩展的多视图数据生成与标注流水线/Scalable multi-view data generation & annotation pipeline]\n        C --> C2[构建SpatialMosaic数据集与基准/Build SpatialMosaic dataset & benchmark]\n        C --> C3[提出集成3D重建模型的混合VLM框架/Propose hybrid VLM framework with 3D reconstruction models]\n        D --> D1[生成2M QA对用于指令微调/Generated 2M QA pairs for instruction-tuning]\n        D --> D2[构建1M QA对的评估基准/Built 1M QA pair evaluation benchmark]\n        D --> D3[有效提升挑战性条件下的空间推理能力/Effectively enhances spatial reasoning under challenging conditions]"
    },
    {
      "title": "MGCA-Net: Multi-Graph Contextual Attention Network for Two-View Correspondence Learning",
      "authors": "Shuyuan Lin, Mengtin Lo, Haosheng Chen, Yanjie Liang, Qiangqiang Wu",
      "institution": "Jinan University, Chongqing University of Posts and Telecommunications, Peng Cheng Laboratory, City University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.23369",
      "code": "http://www.linshuyuan.com",
      "tags": [
        "two-view correspondence / outlier rejection",
        "geometric attention",
        "graph neural network",
        "cross-stage consensus",
        "outlier rejection",
        "camera pose estimation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a37bf1bef9021d24530aae70ecb962a50ad8484fa1078db8fab9979ce3832164_w640_q70.webp",
      "contributions": "1. Proposed a Contextual Geometric Attention (CGA) module that dynamically integrates spatial and feature information to capture local and global geometric relationships. 2. Introduced a Cross-Stage Multi-Graph Consensus (CSMGC) module to ensure geometric consistency across different network stages via a sparse graph network. 3. Demonstrated state-of-the-art performance on the YFCC100M and SUN3D datasets for outlier rejection and camera pose estimation tasks.",
      "summary": "The paper addresses the problem of robust two-view correspondence learning for tasks like camera pose estimation. It proposes MGCA-Net, a novel network featuring a Contextual Geometric Attention module and a Cross-Stage Multi-Graph Consensus module to better model geometric constraints and ensure information consistency. Experiments show it outperforms existing methods on standard benchmarks.",
      "mindmap": "graph TB\n        A[MGCA-Net: 多图上下文注意力网络 / Multi-Graph Contextual Attention Network] --> B[核心问题 / Problem]\n        A --> C[主要方法 / Method]\n        A --> D[关键结果 / Results]\n        B --> B1[现有方法在局部几何建模和跨阶段信息优化上存在局限 / Limitations in local geometric modeling and cross-stage information optimization]\n        B --> B2[难以准确捕获匹配对的几何约束，降低模型鲁棒性 / Hard to capture geometric constraints, reducing robustness]\n        C --> C1[上下文几何注意力模块 / Contextual Geometric Attention (CGA) Module]\n        C --> C2[跨阶段多图共识模块 / Cross-Stage Multi-Graph Consensus (CSMGC) Module]\n        C1 --> C1a[动态整合空间位置和特征信息 / Dynamically integrates spatial position and feature information]\n        C1 --> C1b[增强捕获局部和全局几何关系的能力 / Enhances capability to capture local and global geometric relationships]\n        C2 --> C2a[通过跨阶段稀疏图网络建立几何共识 / Establishes geometric consensus via cross-stage sparse graph network]\n        C2 --> C2b[确保不同阶段间几何信息的一致性 / Ensures consistency of geometric information across stages]\n        D --> D1[在YFCC100M和SUN3D数据集上显著优于SOTA方法 / Significantly outperforms SOTA on YFCC100M and SUN3D datasets]\n        D --> D2[在离群点剔除和相机姿态估计任务中表现优异 / Excels in outlier rejection and camera pose estimation tasks]"
    },
    {
      "title": "NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection & Localization",
      "authors": "Yifei Li, Haoyuan He, Yu Zheng, Bingyao Yu, Wenzhao Zheng, Lei Chen, Jie Zhou, Jiwen Lu",
      "institution": "Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.23374",
      "code": "https://github.com/JoeLeelyf/NeXT-IMDL",
      "tags": [
        "image forensics",
        "image manipulation detection",
        "generalization benchmark",
        "cross-dimension evaluation",
        "AIGC-based manipulation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1b0f52b5e0a9a84301b167e1fa374896f0b6eecedcf21edc2aff4e5102296cd6_w640_q70.webp",
      "contributions": "1. Proposes NeXT-IMDL, a large-scale diagnostic benchmark for Image Manipulation Detection and Localization (IMDL) designed to systematically probe the generalization boundaries of detectors. 2. Categorizes AI-generated content (AIGC) manipulations along four fundamental axes (editing models, manipulation types, content semantics, forgery granularity) and implements five rigorous cross-dimension evaluation protocols. 3. Through extensive experiments on 11 representative models, reveals that current models exhibit systemic failures and significant performance degradation under the proposed protocols, challenging the perceived progress in the field.",
      "summary": "This paper identifies a \"benchmark illusion\" in Image Manipulation Detection and Localization (IMDL), where current cross-dataset evaluations overestimate model robustness. To address this, the authors propose NeXT-IMDL, a diagnostic benchmark that systematically categorizes manipulations and introduces rigorous cross-dimension evaluation protocols. Experiments show that 11 state-of-the-art models suffer significant performance drops under these new protocols, highlighting their fragility and the need for more robust next-generation IMDL models.",
      "mindmap": "graph TB\n        A[NeXT-IMDL: Next-Generation Image Manipulation Detection & Localization] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有评估方法高估模型鲁棒性/Current evaluations overestimate model robustness]\n        C --> C1[构建诊断性基准 NeXT-IMDL/Build diagnostic benchmark NeXT-IMDL]\n        C1 --> C2[四维操纵分类/Four-axis manipulation categorization]\n        C1 --> C3[五维交叉评估协议/Five cross-dimension evaluation protocols]\n        D --> D1[11个模型出现系统性失败/11 models exhibit systemic failures]\n        D --> D2[性能显著下降/Significant performance degradation]"
    },
    {
      "title": "SoulX-LiveTalk Technical Report",
      "authors": "Le Shen, Qiao Qian, Tan Yu, Ke Zhou, Tianhang Yu, Yu Zhan, Zhenjie Wang, Ming Tao, Shunshun Yin, Siyuan Liu",
      "institution": "Soul AI Lab, Donghua University",
      "link": "https://arxiv.org/pdf/2512.23379",
      "code": "https://soul-ailab.github.io/soulx-livetalk/",
      "tags": [
        "diffusion models",
        "Self-correcting Bidirectional Distillation",
        "Multi-step Retrospective Self-Correction",
        "hybrid sequence parallelism",
        "Parallel VAE",
        "kernel-level optimizations"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d6b1bb994b3c3273da207d2f19494d99c1ff6bf6663f41b1d85b2f0c69b83bb_w640_q70.webp",
      "contributions": "1. Introduced a Self-correcting Bidirectional Distillation strategy that retains bidirectional attention within video chunks to preserve spatiotemporal correlations and enhance visual fidelity. 2. Proposed a Multi-step Retrospective Self-Correction Mechanism to ensure stability during infinite generation by enabling autonomous recovery from accumulated errors. 3. Engineered a full-stack inference acceleration suite with hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations to achieve real-time performance.",
      "summary": "The paper addresses the challenge of deploying large diffusion models for real-time, audio-driven avatar generation by introducing SoulX-LiveTalk, a 14B-parameter framework. It employs a bidirectional distillation strategy and a self-correction mechanism to maintain high visual quality and stability, while a suite of inference optimizations enables sub-second latency and 32 FPS throughput, setting a new standard for interactive digital humans.",
      "mindmap": "graph TB\n        A[SoulX-LiveTalk] --> B[核心问题/Problem: 实时无限时长音频驱动化身生成中计算负载与低延迟的冲突]\n        A --> C[主要方法/Method: 自校正双向蒸馏与多步回顾自校正机制]\n        A --> D[关键结果/Results: 0.87秒启动延迟，32 FPS实时吞吐]"
    },
    {
      "title": "A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers",
      "authors": "Mohammad Nasirzadeh, Jafar Tahmoresnezhad, Parviz Rashidi-Khazaee",
      "institution": "Urmia University of Technology",
      "link": "https://arxiv.org/pdf/2512.23380",
      "code": "https://github.com/your-repo/CoLog",
      "tags": [
        "log anomaly detection",
        "collaborative transformers",
        "multi-head impressed attention",
        "modality adaptation layer"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c23025b6b24d4efc5cb993659def89fe785700fbc818c9cc638fe55cdfc5b75e_w640_q70.webp",
      "contributions": "1. Proposes CoLog, a unified framework for detecting both point and collective anomalies in OS logs by applying multimodal sentiment analysis concepts. 2. Introduces collaborative transformers and multi-head impressed attention to learn interactions between different log data modalities. 3. Incorporates a modality adaptation layer to handle heterogeneity and adapt representations from different log modalities.",
      "summary": "The paper addresses the challenge of log anomaly detection, where existing methods struggle with the multimodal nature of log data and the interactions between these modalities. It proposes CoLog, a framework that uses collaborative transformers and a modality adaptation layer to learn nuanced patterns across log modalities for comprehensive anomaly detection. Extensive experiments show CoLog achieves state-of-the-art performance, with mean precision, recall, and F1 scores over 99.5% across seven benchmark datasets.",
      "mindmap": "graph TB\n        Root[”A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers”] --> Problem[”核心问题/Problem: Unimodal & multimodal methods fail to handle log data modalities and their interactions”]\n        Root --> Method[”主要方法/Method: CoLog framework with collaborative transformers, multi-head impressed attention, and modality adaptation layer”]\n        Root --> Results[”关键结果/Results: Achieves ~99.6% mean precision, recall, F1 on 7 datasets; superior to SOTA”]"
    },
    {
      "title": "Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment",
      "authors": "Henglin Liu, Nisha Huang, Chang Liu, Jiangpeng Yan, Huijuan Huang, Jixuan Ying, Tong-Yee Lee, Pengfei Wan, Xiangyang Ji",
      "institution": "Tsinghua University, Kuaishou Technology, Pengcheng Laboratory, National Cheng Kung University, E Fund Management Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.23413",
      "code": null,
      "tags": [
        "image aesthetics assessment",
        "aesthetic description",
        "multimodal learning",
        "large language model",
        "hierarchical learning",
        "entropy minimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d47a5d6d4a6ecaa69bca3187b7069584ea4c0191c7344484fef5b2419157e30d_w640_q70.webp",
      "contributions": "1. Introduces the Refined Aesthetic Description (RAD) dataset, a large-scale, multi-dimensional structured dataset generated via an iterative pipeline to address data scarcity and imbalance in artistic aesthetics. 2. Proposes ArtQuant, an aesthetics assessment framework that couples isolated aesthetic dimensions through joint description generation and utilizes LLM decoders to better model long-text semantics. 3. Provides a theoretical analysis showing that the semantic adequacy of the RAD dataset and the generation paradigm of ArtQuant collectively minimize prediction entropy, offering mathematical grounding for the framework.",
      "summary": "This paper addresses the challenges in artistic image aesthetics assessment by introducing a large-scale dataset (RAD) and a novel framework (ArtQuant) that uses hierarchical description learning with LLMs. The method achieves state-of-the-art performance on several benchmarks while requiring significantly fewer training epochs, effectively narrowing the cognitive gap between images and human aesthetic judgment.",
      "mindmap": "graph TB\n        A[Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[数据稀缺与模型碎片化 / Data Scarcity & Model Fragmentation]\n        C --> C1[提出RAD数据集 / Propose RAD Dataset]\n        C --> C2[提出ArtQuant框架 / Propose ArtQuant Framework]\n        C2 --> C2_1[联合描述生成 / Joint Description Generation]\n        C2 --> C2_2[LLM解码器 / LLM Decoder]\n        D --> D1[SOTA性能 / SOTA Performance]\n        D --> D2[减少训练成本 / Reduced Training Cost]\n        D --> D3[缩小认知差距 / Narrow Cognitive Gap]"
    },
    {
      "title": "DriveLaW:Unifying Planning and Video Generation in a Latent Driving World",
      "authors": "Tianze Xia, Yongkang Li, Lijun Zhou, Jingfeng Yao, Kaixin Xiong, Haiyang Sun, Bing Wang, Kun Ma, Hangjun Ye, Wenyu Liu, Xinggang Wang",
      "institution": "Huazhong University of Science and Technology, Xiaomi EV",
      "link": "https://arxiv.org/pdf/2512.23421",
      "code": null,
      "tags": [
        "autonomous driving",
        "world model",
        "video generation",
        "diffusion planner",
        "latent representation",
        "unified planning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ac4b7563a9d00eabc7ad0c94b379593c1add28414cfc8bc68e64145861ae65f_w640_q70.webp",
      "contributions": "1. Proposes DriveLaW, a novel paradigm that unifies video generation and motion planning for autonomous driving by directly injecting latent representations from the video generator into the planner. 2. Introduces a three-stage progressive training strategy to jointly optimize the video generation component (DriveLaW-Video) and the diffusion planning component (DriveLaW-Act). 3. Achieves state-of-the-art performance on both video prediction and motion planning benchmarks, significantly surpassing previous methods in metrics like FID, FVD, and NAVSIM.",
      "summary": "The paper addresses the decoupling of world prediction and motion planning in current autonomous driving world models. It proposes DriveLaW, a unified framework that connects a video generator and a diffusion planner via latent representations, ensuring consistency between future scene generation and trajectory planning. The method achieves new state-of-the-art results on both video forecasting and planning benchmarks.",
      "mindmap": "graph TB\n        A[DriveLaW: Unifying Planning and Video Generation] --> B[核心问题/Problem: Decoupled world prediction and planning in autonomous driving]\n        A --> C[主要方法/Method: Unified paradigm with latent injection from video generator to diffusion planner]\n        A --> D[关键结果/Results: SOTA in video prediction (FID, FVD) and planning (NAVSIM)]"
    },
    {
      "title": "SOFTooth: Semantics-Enhanced Order-Aware Fusion for Tooth Instance Segmentation",
      "authors": "Xiaolan Li, Wanquan Liu, Pengcheng Li, Pengyu Jie, Chenqiang Gao",
      "institution": "Sun Yat-sen University, Hainan University",
      "link": "https://arxiv.org/pdf/2512.23411",
      "code": null,
      "tags": [
        "medical image segmentation",
        "2D-3D fusion",
        "instance segmentation",
        "order-aware matching",
        "semantic feature injection",
        "center-guided refinement"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f08a5acde121d4a512791be08f24d9f39834e6660240bf65661d9f0bfb8c723_w640_q70.webp",
      "contributions": "1. A point-wise residual gating module that injects frozen 2D SAM embeddings into 3D point features to refine boundaries without 2D mask supervision. 2. A center-guided mask refinement mechanism that regularizes consistency between instance masks and geometric centroids to reduce center drift. 3. An order-aware Hungarian matching strategy that integrates anatomical tooth order and center distance for coherent instance labeling, especially for cases with missing or crowded teeth.",
      "summary": "The paper addresses challenges in 3D tooth instance segmentation, such as boundary leakage and inconsistent labeling, by proposing SOFTooth, a framework that fuses 2D semantic features from SAM with 3D geometric data. The method introduces modules for boundary refinement, center stabilization, and order-aware instance assignment. It achieves state-of-the-art performance on a benchmark dataset, demonstrating effective transfer of 2D semantics to 3D segmentation without fine-tuning the 2D model.",
      "mindmap": "graph TB\n        Root[SOFTooth: 语义增强的顺序感知融合用于牙齿实例分割] --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1[拥挤牙弓与模糊边界/Crowded arches & ambiguous boundaries]\n        Problem --> P2[中心漂移与身份不一致/Center drift & inconsistent identities]\n        Problem --> P3[稀有第三磨牙/Rare third molars]\n        Method --> M1[点级残差门控注入2D语义/Point-wise residual gating for 2D semantics]\n        Method --> M2[中心引导的掩码细化/Center-guided mask refinement]\n        Method --> M3[顺序感知匈牙利匹配/Order-aware Hungarian matching]\n        Results --> R1[在3DTeethSeg'22上达到SOTA/SOTA on 3DTeethSeg'22]\n        Results --> R2[在涉及第三磨牙的案例上提升明显/Clear gains on third molar cases]\n        Results --> R3[无需2D微调的有效2D-3D迁移/Effective 2D-3D transfer without fine-tuning]"
    },
    {
      "title": "Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision",
      "authors": "Dohyun Kim, Seungwoo Lyu, Seung Wook Kim, Paul Hongsuck Seo",
      "institution": "Korea University, NVIDIA",
      "link": "https://arxiv.org/pdf/2512.23426",
      "code": "this",
      "tags": [
        "diffusion models",
        "preference optimization",
        "diffusion models",
        "score-space supervision",
        "text-to-image synthesis",
        "denoising trajectory"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed32bc07f3693ab8cf568a880f7a3e3ae639d4fb54fb284f6ea438be183324d5_w640_q70.webp",
      "contributions": "1. Proposes DDSPO, a method for direct score-space preference optimization in diffusion models that provides per-timestep supervision from contrastive policy pairs. 2. Introduces a practical strategy to automatically generate preference signals using a pretrained model and semantically degraded prompts, avoiding costly human-labeled data. 3. Demonstrates improved text-image alignment and visual quality, outperforming or matching existing methods with significantly less supervision.",
      "summary": "The paper introduces Direct Diffusion Score Preference Optimization (DDSPO), a method that optimizes diffusion models by applying preference supervision directly in the score space at each denoising timestep, using automatically generated signals from a reference model. This approach provides dense, stepwise learning signals and reduces reliance on expensive human-labeled data. Empirical results show DDSPO improves text-image alignment and visual quality, matching or outperforming prior preference-based methods with less supervision.",
      "mindmap": "graph TB\n        A[Direct Diffusion Score Preference Optimization <br/> 直接扩散分数偏好优化] --> B\n        A --> C\n        A --> D\n        B[Problem: Diffusion models struggle with user intent alignment & aesthetic consistency <br/> 问题: 扩散模型难以对齐用户意图和保持美学一致性]\n        C[Method: DDSPO provides stepwise score-space supervision via contrastive policy pairs <br/> 方法: DDSPO通过对比策略对提供逐步分数空间监督]\n        D[Results: Improves alignment & quality, reduces supervision need <br/> 结果: 改善对齐与质量，减少监督需求]"
    },
    {
      "title": "Towards Integrating Uncertainty for Domain-Agnostic Segmentation",
      "authors": "Jesse Brouwers, Xiaoyan Xing, Alexander Timans",
      "institution": "UvA-Bosch Delta Lab, University of Amsterdam",
      "link": "https://arxiv.org/pdf/2512.23427",
      "code": "https://github.com/JesseBrouw/UncertSAM",
      "tags": [
        "semantic segmentation",
        "uncertainty quantification",
        "domain-agnostic",
        "Segment Anything Model (SAM)",
        "Laplace approximation",
        "benchmark"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15865ed74ed61443aa69769e51585f4a7341748fc10c0250eef9243be675f215_w640_q70.webp",
      "contributions": "1. Curated UncertSAM, a benchmark of eight datasets to stress-test segmentation models under challenging conditions like shadows and camouflage. 2. Evaluated a suite of lightweight, post-hoc uncertainty estimation methods for segmentation foundation models. 3. Assessed a preliminary uncertainty-guided prediction refinement step, finding that last-layer Laplace approximation yields uncertainty estimates well-correlated with segmentation errors.",
      "summary": "This paper investigates whether uncertainty quantification can improve the robustness of foundation segmentation models like SAM in domain-agnostic settings. The authors propose a benchmark (UncertSAM) and evaluate several post-hoc uncertainty estimation methods, finding that a last-layer Laplace approximation provides meaningful uncertainty signals. The results indicate the potential of integrating uncertainty to enhance model generalizability, though refinement benefits are preliminary.",
      "mindmap": "graph TB\n        A[Towards Integrating Uncertainty for Domain-Agnostic Segmentation] --> B[核心问题/Problem: SAM在域偏移或知识有限场景下脆弱/SAM vulnerable in shifted or limited-knowledge domains]\n        A --> C[主要方法/Method: 构建UncertSAM基准，评估后验不确定性方法，尝试不确定性引导优化/Build UncertSAM benchmark, evaluate post-hoc UQ methods, attempt uncertainty-guided refinement]\n        A --> D[关键结果/Results: 拉普拉斯近似不确定性估计与误差相关，初步验证不确定性整合潜力/Laplace approximation yields correlated uncertainty, preliminary potential of integrating UQ]"
    },
    {
      "title": "Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification",
      "authors": "Mustafa Demetgul, Sanja Lazarova Molnar",
      "institution": "Karlsruhe Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.23436",
      "code": null,
      "tags": [
        "image classification",
        "convolutional neural networks",
        "fuzzy logic",
        "road surface classification",
        "intelligent transport systems",
        "data fusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d74437ab4a94c7ac8b2148bba4e1b6dd8d4706d55db8a2ae146a41ace94ef9f9_w640_q70.webp",
      "contributions": "1. Proposes a real-time system for road surface classification by fusing weather-conditional data and road condition data. 2. Compares the performance of multiple deep learning CNNs (AlexNet, LeNet, VGG, ResNet) on both image-based and acceleration-data-as-image classification tasks. 3. Introduces the use of fuzzy logic to classify road surfaces according to environmental factors like weather and time of day, using sensor data.",
      "summary": "This paper proposes a real-time system for road surface condition monitoring. It employs deep learning CNNs to classify road types from images and acceleration data, achieving over 95% accuracy, and suggests using fuzzy logic to incorporate weather and time-of-day factors. The work aims to enhance vehicle safety and autonomous driving systems.",
      "mindmap": "graph TB\n    A[Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification] --> B[核心问题/Problem: Classical road monitoring is expensive and unsystematic.]\n    A --> C[主要方法/Method: Use deep learning (CNN) on images/acceleration data and fuzzy logic for environmental context.]\n    A --> D[关键结果/Results: Over 95% classification accuracy achieved.]"
    },
    {
      "title": "RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction",
      "authors": "Shuhong Liu, Chenyu Bao, Ziteng Cui, Yun Liu, Xuangeng Chu, Lin Gu, Marcos V. Conde, Ryo Umagami, Tomohiro Hashimoto, Zijian Hu, Tianhan Xu, Yuan Gan, Yusuke Kurose, Tatsuya Harada",
      "institution": "The University of Tokyo, NII, Tohoku University, University of Würzburg, RIKEN",
      "link": "https://arxiv.org/pdf/2512.23437",
      "code": null,
      "tags": [
        "3D reconstruction",
        "benchmark",
        "multi-view",
        "physical degradation",
        "neural radiance field",
        "Gaussian splatting"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ccf1373873a24f24accd40b8329f93e9af6b32bea07b7e7c4b639214553f8a0_w640_q70.webp",
      "contributions": "1. Introduces RealX3D, a real-capture benchmark for evaluating multi-view visual restoration and 3D reconstruction under diverse physical degradations. 2. Proposes a unified acquisition protocol that captures four families of corruptions (illumination, scattering, occlusion, blurring) at multiple severity levels, providing pixel-aligned low-quality and ground-truth views, RAW images, and dense laser scans. 3. Demonstrates through extensive benchmarking that current state-of-the-art optimization-based and feed-forward reconstruction methods suffer substantial quality degradation when faced with these real-world corruptions.",
      "summary": "This paper introduces RealX3D, a benchmark dataset for evaluating 3D reconstruction and novel view synthesis methods under real-world physical degradations like low light and blur. The benchmark provides aligned low-quality and high-quality multi-view data with ground-truth geometry. Experiments show current methods are fragile to these corruptions, highlighting a gap between lab performance and real-world deployment.",
      "mindmap": "graph TB\n        A[RealX3D: A Physically-Degraded 3D Benchmark] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[真实世界退化影响3D重建/Real-world degradations hinder 3D reconstruction]\n        C --> C1[构建真实捕获基准/Build real-capture benchmark]\n        C --> C2[四类退化, 多严重级别/Four corruption families, multiple severity levels]\n        C --> C3[提供对齐的LQ/GT视图, RAW数据, 激光扫描/Provide aligned LQ/GT views, RAW, laser scans]\n        D --> D1[当前方法质量显著下降/Current methods show substantial quality degradation]\n        D --> D2[突出现实挑战性/Underscores fragility in challenging real environments]"
    },
    {
      "title": "Stochastic Siamese MAE Pretraining for Longitudinal Medical Images",
      "authors": "Taha Emre, Arunava Chakravarty, Thomas Pinetz, Dmitrii Lachinov, Martin J. Menten, Hendrik Scholl, Sobha Sivaprasad, Daniel Rueckert, Andrew Lotery, Stefan Sacu, Ursula Schmidt-Erfurth, Hrvoje Bogunović",
      "institution": "Medical University of Vienna, Imperial College London, Technical University of Munich, University College London, University of Southampton",
      "link": "https://arxiv.org/pdf/2512.23441",
      "code": null,
      "tags": [
        "medical image analysis",
        "masked autoencoder",
        "siamese network",
        "stochastic process",
        "longitudinal data",
        "variational inference"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c93396b0ded8fc65364d5714bd12e652a23ffc22eb276cbbca01426ecd0db791_w640_q70.webp",
      "contributions": "1. Proposed STAMP, a novel Siamese MAE framework that incorporates temporal information by conditioning on the time difference between input volumes. 2. Introduced a stochastic learning approach by reframing the MAE reconstruction loss as a conditional variational inference objective to model the uncertainty in disease progression. 3. Demonstrated superior performance of STAMP-pretrained models over existing temporal MAE methods and foundation models on predicting progression of Age-Related Macular Degeneration and Alzheimer's Disease.",
      "summary": "The paper addresses the lack of temporal awareness in self-supervised learning methods like MAE for longitudinal medical images. It proposes STAMP, a stochastic Siamese MAE framework that learns temporal dynamics by conditioning on time differences and using a variational inference objective. The method outperformed existing approaches on disease progression prediction tasks for OCT and MRI datasets.",
      "mindmap": "graph TB\n        A[Stochastic Siamese MAE Pretraining for Longitudinal Medical Images] --> B[核心问题/Problem: MAE lacks temporal awareness for longitudinal medical data.]\n        A --> C[主要方法/Method: STAMP - Stochastic Siamese MAE using conditional variational inference.]\n        A --> D[关键结果/Results: Outperforms existing methods on AMD and AD progression prediction.]"
    },
    {
      "title": "CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models",
      "authors": "Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, Zepeng Wang",
      "institution": "Lenovo (PCIE), University of Minnesota (UMN)",
      "link": "https://arxiv.org/pdf/2512.23453",
      "code": "https://github.com/AI-Researcher-Team/CoFi-Dec",
      "tags": [
        "multi-modal inference",
        "hallucination mitigation",
        "coarse-to-fine conditioning",
        "Wasserstein fusion",
        "generative feedback",
        "training-free decoding"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7cab1d37f48692f41be898afb160456b94e821d8b6ea16ba282cb4ee3ac5046_w640_q70.webp",
      "contributions": "1. Proposes CoFi-Dec, a training-free decoding framework that mitigates hallucinations in LVLMs by integrating generative self-feedback with coarse-to-fine visual conditioning. 2. Introduces a Wasserstein-based fusion mechanism to align predictive distributions from multiple visual conditions into a geometrically consistent decoding trajectory. 3. Demonstrates substantial reduction in both entity-level and semantic-level hallucinations across six benchmarks, showing the framework is model-agnostic and requires no additional training.",
      "summary": "The paper addresses the problem of hallucinated content in Large Vision-Language Models (LVLMs). It proposes CoFi-Dec, a training-free decoding framework that uses coarse-to-fine visual conditioning and generative feedback to create multi-level visual hypotheses, which are then unified via a Wasserstein-based fusion mechanism. The method significantly reduces hallucinations across multiple benchmarks and can be applied to various LVLMs without retraining.",
      "mindmap": "graph TB\n        A[CoFi-Dec: Hallucination-Resistant Decoding] --> B[核心问题/Problem: LVLMs产生与视觉输入不一致的幻觉内容]\n        A --> C[主要方法/Method: 基于粗到细视觉条件的生成式自反馈与Wasserstein融合]\n        A --> D[关键结果/Results: 在六个基准测试中显著减少幻觉，无需训练，模型无关]"
    },
    {
      "title": "Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin",
      "authors": "Kayathri Vigneswaran, Hugo Retief, Jai Clifford Holmes, Mariangel Garcia Andarcia, Hansaka Tennakoon",
      "institution": "International Water Management Institute (IWMI), Association for Water and Rural Development (AWARD)",
      "link": "https://arxiv.org/pdf/2512.23454",
      "code": null,
      "tags": [
        "object detection",
        "YOLOv8-Pose",
        "multimodal LLMs",
        "waterline detection",
        "scale gap estimation",
        "geometric calibration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3eb2461b2254accb76162876e6a7ac7fa51fda24feaacba9e7280b9c3b490a4b_w640_q70.webp",
      "contributions": "1. Proposes a novel hybrid framework combining vision-based waterline detection, YOLOv8-Pose for scale extraction, and multimodal LLMs for automated river gauge reading. 2. Demonstrates that incorporating geometric metadata (scale gap) significantly improves the predictive accuracy of LLMs for water level estimation. 3. Provides a scalable and efficient solution for automated hydrological monitoring, highlighting its sensitivity to image quality and potential for real-time digitization.",
      "summary": "This paper addresses the challenge of automated river water level monitoring by proposing a hybrid framework that integrates computer vision (waterline detection and YOLOv8-Pose) with multimodal large language models (GPT-4o and Gemini 2.0 Flash) to read gauge plates. The method uses geometric calibration from scale gap detection to enhance LLM performance, achieving high accuracy under optimal conditions. The study concludes that combining geometric metadata with multimodal AI offers a robust, scalable solution for real-time hydrological monitoring.",
      "mindmap": "graph TB\n        A[Automated River Gauge Plate Reading<br/>自动化河流水位计读数] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Manual gauge reading errors & environmental constraints<br/>人工读数误差与环境限制]\n        C --> C1[Hybrid Object Detection & Generative AI Framework<br/>混合目标检测与生成式AI框架]\n        C1 --> C2[Vision-based waterline detection<br/>基于视觉的水位线检测]\n        C1 --> C3[YOLOv8-Pose scale extraction<br/>YOLOv8-Pose尺度提取]\n        C1 --> C4[Multimodal LLMs (GPT-4o, Gemini) for reading<br/>多模态大语言模型读数]\n        D --> D1[High precision waterline detection (94.24%)<br/>高精度水位线检测]\n        D --> D2[Improved LLM accuracy with scale gap metadata<br/>尺度间隙元数据提升LLM精度]\n        D --> D3[Gemini Stage 2: MAE=5.43cm, RMSE=8.58cm<br/>Gemini Stage 2最佳性能]"
    },
    {
      "title": "Deterministic Image-to-Image Translation via Denoising Brownian Bridge Models with Dual Approximators",
      "authors": "Bohan Xiao, Peiyong Wang, Qisheng He, Ming Dong",
      "institution": "Wayne State University",
      "link": "https://arxiv.org/pdf/2512.23463",
      "code": "https://github.com/bohan95/dual-app-bridge",
      "tags": [
        "image-to-image translation",
        "Brownian bridge",
        "deterministic translation",
        "denoising diffusion",
        "dual approximators",
        "super-resolution"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/103bceca24bdd1f051193099bcdde9fa2cd561605d76eb8f9b4f42f608212956_w640_q70.webp",
      "contributions": "1. Proposes a novel denoising Brownian bridge model with dual neural network approximators for deterministic I2I translation., 2. Introduces a method that guarantees consistent, predictable outputs with high fidelity to ground truth, addressing the limitations of stochastic models., 3. Demonstrates superior performance in tasks like super-resolution compared to both stochastic and deterministic baseline models.",
      "summary": "This paper proposes Dual-approx Bridge, a novel generative model for deterministic image-to-image translation. It uses Brownian bridge dynamics with two neural approximators to produce high-fidelity, low-variance outputs. Experiments show it outperforms existing baselines in image quality and faithfulness to ground truth.",
      "mindmap": "graph TB\n        A[Deterministic Image-to-Image Translation via Denoising Brownian Bridge Models with Dual Approximators] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[确定性图像转换需求 / Need for deterministic I2I translation (e.g., super-resolution)]\n        C --> C1[双近似器去噪布朗桥模型 / Denoising Brownian Bridge Model with Dual Approximators]\n        D --> D1[输出一致且质量高 / Consistent, high-quality output with high fidelity to GT]\n        D --> D2[优于随机与确定性基线 / Superior to stochastic & deterministic baselines]"
    },
    {
      "title": "HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation",
      "authors": "Yuxin Wen, Qing Shuai, Di Kang, Jing Li, Cheng Wen, Yue Qian, Ningxin Jiao, Changhai Chen, Weijie Chen, Yiran Wang, Jinkun Guo, Dongyue An, Han Liu, Yanyu Tong, Chao Zhang, Qing Guo, Juan Chen, Qiao Zhang, Youyi Zhang, Zihao Yao, Cheng Zhang, Hong Duan, Xiaoping Wu, Qi Chen, Fei Cheng, Liang Dong, Peng He, Hao Zhang, Jiaxin Lin, Chao Zhang, Zhongyi Fan, Yifan Li, Zhichao Hu, Yuhong Liu, Linus, Jie Jiang, Xiaolong Li, Linchao Bao",
      "institution": "Tencent Hunyuan",
      "link": "https://arxiv.org/pdf/2512.23464",
      "code": "https://github.com/Tencent-Hunyuan/HY-Motion-1.0",
      "tags": [
        "motion generation",
        "flow matching",
        "diffusion transformer (DiT)",
        "reinforcement learning from human feedback (RLHF)"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3019219aea0683c229d44ce63a0fed59b5ebb795811dc1b1638ae995c9a8156_w640_q70.webp",
      "contributions": "1. The first successful scaling of DiT-based flow matching models to billion parameters for motion generation. 2. A comprehensive full-stage training paradigm including large-scale pretraining, fine-tuning, and RLHF. 3. A meticulous data processing pipeline enabling extensive coverage of over 200 motion categories.",
      "summary": "This paper introduces HY-Motion 1.0, a large-scale model for generating 3D human motions from text. It scales up Diffusion Transformer-based flow matching and uses a full-stage training pipeline with pretraining, fine-tuning, and RLHF. The model achieves state-of-the-art performance and broad motion coverage, and is released open-source.",
      "mindmap": "graph TB\n        Root[”HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation”]\n        Root --> Problem[”核心问题/Problem: Generating high-quality, text-aligned 3D human motions”]\n        Root --> Method[”主要方法/Method: Scale DiT-based flow matching, Full-stage training (pretrain, fine-tune, RLHF), Meticulous data pipeline”]\n        Root --> Results[”关键结果/Results: SOTA performance, Extensive motion coverage, Open-source release”]"
    },
    {
      "title": "MCI-Net: A Robust Multi-Domain Context Integration Network for Point Cloud Registration",
      "authors": "Shuyuan Lin, Wenwu Peng, Junjie Huang, Qiang Qi, Miaohui Wang, Jian Weng",
      "institution": "Jinan University, Qingdao University of Science and Technology, Shenzhen University",
      "link": "https://arxiv.org/pdf/2512.23472",
      "code": "http://www.linshuyuan.com",
      "tags": [
        "point cloud registration",
        "graph neural network",
        "multi-domain context",
        "dynamic inlier selection",
        "feature aggregation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2dc4a548abb0cfe84934aaf9ac3e41164df500ce5f41043542745cdb8b1984c4_w640_q70.webp",
      "contributions": "1. A graph neighborhood aggregation module that constructs a global graph to capture overall structural relationships in point clouds. 2. A progressive context interaction module that enhances feature discriminability through intra-domain decoupling and inter-domain interaction. 3. A dynamic inlier selection method that optimizes inlier weights using residual information from iterative pose estimation.",
      "summary": "This paper proposes MCI-Net, a novel network for point cloud registration that improves feature learning by integrating contextual cues from multiple domains. The method introduces modules for graph-based neighborhood aggregation, progressive context interaction, and dynamic inlier selection. Experiments show it achieves state-of-the-art performance, including a 96.4% registration recall on the 3DMatch dataset.",
      "mindmap": "graph TB\n        Root[MCI-Net: Point Cloud Registration] --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1(现有方法依赖欧氏邻域/Existing methods rely on Euclidean neighborhoods)\n        Problem --> P2(难以捕捉隐式语义和结构一致性/Struggle to capture implicit semantics and structural consistency)\n        Method --> M1(图邻域聚合模块/Graph Neighborhood Aggregation Module)\n        Method --> M2(渐进式上下文交互模块/Progressive Context Interaction Module)\n        Method --> M3(动态内点选择方法/Dynamic Inlier Selection Method)\n        Results --> R1(在室内外数据集上性能优越/Superior performance on indoor and outdoor datasets)\n        Results --> R2(在3DMatch上达到96.4%的召回率/Achieves 96.4% recall on 3DMatch)"
    },
    {
      "title": "SC-Net: Robust Correspondence Learning via Spatial and Cross-Channel Context",
      "authors": "Shuyuan Lin, Hailiang Liao, Qiang Qi, Junjie Huang, Taotao Lai, Jian Weng",
      "institution": "Jinan University, Qingdao University of Science and Technology, Minjiang University",
      "link": "https://arxiv.org/pdf/2512.23473",
      "code": "http://www.linshuyuan.com",
      "tags": [
        "correspondence learning",
        "adaptive focused regularization",
        "bilateral field adjustment",
        "position-aware recovery"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6a1d4901b95443b2a2afa8148dfe229b7aaa60884a789ba90d1475aba259667_w640_q70.webp",
      "contributions": "1. Proposed the Adaptive Focused Regularization (AFR) module to enhance position-awareness and robustness against spurious motion samples. 2. Proposed the Bilateral Field Adjustment (BFA) module to refine motion fields by modeling long-range relationships across spatial and channel dimensions. 3. Proposed a Position-Aware Recovery (PAR) module to ensure consistent and precise recovery of motion vectors from the refined field.",
      "summary": "The paper addresses the problem of CNN-based two-view correspondence learning failing to aggregate global context and oversmoothing motion fields. It proposes SC-Net, a novel network that integrates spatial and cross-channel context using three key modules (AFR, BFA, PAR). Experiments show SC-Net outperforms state-of-the-art methods on pose estimation and outlier removal tasks.",
      "mindmap": "graph TB\n        A[SC-Net] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[CNN无法有效聚合全局上下文/CNN fails to aggregate global context]\n        B --> B2[大视差场景中的运动场过平滑/Oversmoothing in large disparity scenes]\n        C --> C1[自适应聚焦正则化模块/AFR Module]\n        C --> C2[双边场调整模块/BFA Module]\n        C --> C3[位置感知恢复模块/PAR Module]\n        D --> D1[在YFCC100M和SUN3D上超越SOTA/Outperforms SOTA on YFCC100M & SUN3D]\n        D --> D2[提升姿态估计和离群点去除/Improves pose estimation & outlier removal]"
    },
    {
      "title": "TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding",
      "authors": "Zongsheng Cao, Yangfan He, Anran Liu, Feng Chen, Zepeng Wang, Jun Xie",
      "institution": "Lenovo (PCIE), University of Minnesota (UMN)",
      "link": "https://arxiv.org/pdf/2512.23483",
      "code": "https://github.com/AI-Researcher-Team/TV-RAG",
      "tags": [
        "rag (retrieval-augmented generation)",
        "temporal-aware retrieval",
        "entropy-weighted sampling",
        "training-free framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a241b314650fa36c9f6d3a32e1dfd25ff643fa954ce0210d0462c2c6e84624ca_w640_q70.webp",
      "contributions": "1. A time-decay retrieval module that injects temporal offsets into similarity computation to rank queries by their true multimedia context. 2. An entropy-weighted key-frame sampler that selects information-dense frames to reduce redundancy while preserving representativeness. 3. A lightweight, training-free architecture that can be grafted onto any Large Video Language Model (LVLM) for improved long-video reasoning.",
      "summary": "The paper proposes TV-RAG, a training-free framework to enhance long video understanding by Large Video Language Models (LVLMs). It introduces a temporal-aware retrieval module and an entropy-weighted frame sampler to better capture semantic shifts and temporal dependencies in long videos. The system outperforms leading baselines on multiple benchmarks without requiring model retraining.",
      "mindmap": "graph TB\n        A[TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[LVLMs struggle with long videos: narrow temporal windows, miss fine-grained semantic shifts/LVLMs处理长视频困难：时间窗口窄，忽略细粒度语义变化]\n        B --> B2[Text-based retrieval ignores temporal interdependence among multimodal channels/基于文本的检索忽略了多模态通道间的时间依赖性]\n        C --> C1[Time-decay retrieval module/时间衰减检索模块]\n        C --> C2[Entropy-weighted key-frame sampler/熵加权关键帧采样器]\n        D --> D1[Surpasses leading baselines on Video-MME, MLVU, LongVideoBench/在Video-MME, MLVU, LongVideoBench上超越主流基线]\n        D --> D2[Lightweight, training-free, graftable onto any LVLM/轻量级、无需训练、可适配任何LVLM]"
    },
    {
      "title": "Multi-label Classification with Panoptic Context Aggregation Networks",
      "authors": "Mingyuan Jiu, Hailong Zhu, Wenchuan Wei, Hichem Sahbi, Rongrong Ji, Mingliang Xu",
      "institution": "Zhengzhou University, Sorbonne University, Xiamen University",
      "link": "https://arxiv.org/pdf/2512.23486",
      "code": null,
      "tags": [
        "multi-label classification",
        "context modeling",
        "cross-scale aggregation",
        "attention mechanism",
        "geometric relationships",
        "Hilbert space"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252d9e1fa32aca6156eb50c353f63b3e6265abd48d990c277a30d4dca3d5b654_w640_q70.webp",
      "contributions": "1. Proposes a novel Deep Panoptic Context Aggregation Network (PanCAN) that hierarchically integrates multi-order geometric contexts. 2. Introduces a method combining random walks with attention to learn multi-order neighborhood relationships in a high-dimensional Hilbert space. 3. Demonstrates effective cross-scale modeling by cascading modules and dynamically fusing salient anchor features, significantly improving complex scene understanding.",
      "summary": "This paper addresses the limitation of existing multi-label classification methods in modeling cross-scale contextual interactions. It proposes the Panoptic Context Aggregation Network (PanCAN), which hierarchically aggregates multi-order geometric contexts using attention and random walks in a Hilbert space. Experiments on standard benchmarks show PanCAN outperforms state-of-the-art methods, substantially improving classification performance.",
      "mindmap": "graph TB\n        A[Multi-label Classification with Panoptic Context Aggregation Networks] --> B[核心问题/Problem: 现有方法忽略跨尺度上下文交互/Existing methods neglect cross-scale contextual interactions]\n        A --> C[主要方法/Method: 提出PanCAN，在希尔伯特空间中通过注意力和随机游走进行跨尺度多阶上下文聚合/Proposes PanCAN, aggregates cross-scale multi-order contexts via attention & random walks in Hilbert space]\n        A --> D[关键结果/Results: 在多个基准测试中优于SOTA，提升多标签分类性能/Outperforms SOTA on multiple benchmarks, improves multi-label classification performance]"
    },
    {
      "title": "IdentityStory: Taming Your Identity-Preserving Generator for Human-Centric Story Generation",
      "authors": "Donghao Zhou, Jingyu Lin, Guibao Shen, Quande Liu, Jialin Gao, Lihao Liu, Lan Du, Cunjian Chen, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng",
      "institution": "The Chinese University of Hong Kong, Monash University, The Hong Kong University of Science and Technology (Guangzhou), Kuaishou Technology, Amazon, South China University of Technology",
      "link": "https://arxiv.org/pdf/2512.23519",
      "code": "https://correr-zhou.github.io/IdentityStory",
      "tags": [
        "image generation",
        "identity-preserving generation",
        "iterative identity discovery",
        "re-denoising identity injection",
        "human-centric story generation",
        "character consistency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48c67dbdd79287f3a90a67510ea2e86db0083c44b40ed86b753a884d135ab1b7_w640_q70.webp",
      "contributions": "1. Proposes IdentityStory, a framework for human-centric story generation that ensures consistent character identity across sequential images. 2. Introduces Iterative Identity Discovery to extract cohesive character identities. 3. Presents Re-denoising Identity Injection to inject identities into images while preserving the desired context.",
      "summary": "This paper tackles the challenge of generating a series of images with consistent human characters from text prompts, a task known as human-centric story generation. It proposes the IdentityStory framework, which uses Iterative Identity Discovery and Re-denoising Identity Injection to tame identity-preserving generators. Experiments show it outperforms existing methods in maintaining face consistency and supports multi-character combinations.",
      "mindmap": "graph TB\n        A[IdentityStory: 人类中心故事生成 / Human-Centric Story Generation] --> B[核心问题 / Problem]\n        A --> C[主要方法 / Method]\n        A --> D[关键结果 / Results]\n        B --> B1[保持角色身份一致性 / Maintaining Character Identity Consistency]\n        B --> B2[协调多角色 / Coordinating Multiple Characters]\n        C --> C1[迭代身份发现 / Iterative Identity Discovery]\n        C --> C2[重去噪身份注入 / Re-denoising Identity Injection]\n        D --> D1[优于现有方法 / Outperforms Existing Methods]\n        D --> D2[支持多角色组合 / Supports Multi-Character Combinations]"
    },
    {
      "title": "Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution",
      "authors": "Hexin Zhang, Dong Li, Jie Huang, Bingzhou Wang, Xueyang Fu, Zhengjun Zha",
      "institution": "University of Science and Technology of China",
      "link": "https://arxiv.org/pdf/2512.23532",
      "code": null,
      "tags": [
        "image super-resolution",
        "diffusion models",
        "inference-time scaling",
        "iterative refinement",
        "frequency steering",
        "training-free"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba3e0aea28abd4e2c2b0742af634714a92519c3c67ebaa6960839e16e4a97152_w640_q70.webp",
      "contributions": "1. Proposes IAFS, a training-free framework that combines iterative refinement and frequency-aware particle fusion for diffusion-based super-resolution. 2. Introduces adaptive frequency steering to balance high-frequency perceptual quality and low-frequency structural fidelity. 3. Demonstrates through extensive experiments that IAFS effectively resolves the perception-fidelity conflict and outperforms existing inference-time scaling methods.",
      "summary": "The paper addresses the challenge of balancing perceptual quality and structural fidelity in diffusion-based image super-resolution. It proposes IAFS, a training-free framework that uses iterative inference-time scaling with adaptive frequency steering to progressively refine images. Experiments show IAFS outperforms existing methods in achieving better detail and accuracy.",
      "mindmap": "graph TB\n        Root(”Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”感知质量与结构保真度冲突/Perception-Fidelity Conflict”)\n        Method --> M1(”迭代推理时缩放/Iterative Inference-time Scaling”)\n        Method --> M2(”自适应频率引导/Adaptive Frequency Steering”)\n        M1 --> M1_1(”迭代细化/Iterative Refinement”)\n        M2 --> M2_1(”频率感知粒子融合/Frequency-aware Particle Fusion”)\n        Results --> R1(”解决感知-保真度冲突/Resolves Perception-Fidelity Conflict”)\n        Results --> R2(”超越现有方法/Outperforms Existing Methods”)"
    },
    {
      "title": "AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization",
      "authors": "Binhe Yu, Zhen Wang, Kexin Li, Yuqian Yuan, Wenqiao Zhang, Long Chen, Juncheng Li, Jun Xiao, Yueting Zhuang",
      "institution": "Zhejiang University, HKUST (The Hong Kong University of Science and Technology)",
      "link": "https://arxiv.org/pdf/2512.23537",
      "code": null,
      "tags": [
        "diffusion models",
        "multi-subject customization",
        "layout guidance",
        "attention decoupling",
        "training-free",
        "image adapter"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e5ac220ed9f71a26f20317e74d4ddc9cd5a957b5751dba85f593ddfeaf39640_w640_q70.webp",
      "contributions": "1. Proposes AnyMS, a novel training-free framework for layout-guided multi-subject image customization. 2. Introduces a bottom-up dual-level attention decoupling mechanism (global and local) to balance text alignment, identity preservation, and layout control. 3. Employs pre-trained image adapters to extract subject features without requiring subject-specific training or adapter tuning.",
      "summary": "This paper addresses the challenge of generating coherent images containing multiple user-specified subjects while balancing text alignment, subject identity, and layout control. It proposes AnyMS, a training-free framework that uses a bottom-up attention decoupling mechanism and pre-trained adapters to integrate text, subject images, and layout constraints. The method achieves state-of-the-art performance, supporting complex compositions and scaling to many subjects.",
      "mindmap": "graph TB\n        A[AnyMS: 布局引导免训练多主体定制] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[多主体定制中文本对齐、身份保持与布局控制的平衡问题/Balancing text alignment, identity preservation, and layout control in multi-subject customization]\n        C --> C1[提出免训练框架AnyMS/Proposes training-free framework AnyMS]\n        C1 --> C2[引入自底向上双级注意力解耦机制/Introduces bottom-up dual-level attention decoupling]\n        C2 --> C3[全局解耦确保文本对齐/Global decoupling ensures text alignment]\n        C2 --> C4[局部解耦防止主体冲突/Local decoupling prevents subject conflicts]\n        C --> C5[使用预训练图像适配器提取特征/Uses pre-trained image adapters for feature extraction]\n        D --> D1[实现SOTA性能/Achieves SOTA performance]\n        D --> D2[支持复杂组合与更多主体/Supports complex compositions and scales to more subjects]"
    },
    {
      "title": "PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis",
      "authors": "Shengyi Hua, Jianfeng Wu, Tianle Shen, Kangzhe Hu, Zhongzhen Huang, Shujuan Ni, Zhihong Zhang, Yuan Li, Zhe Wang, Xiaofan Zhang",
      "institution": "Shanghai Jiao Tong University, Fourth Military Medical University, University of Science and Technology of China, Fudan University, Nanjing Medical University",
      "link": "https://arxiv.org/pdf/2512.23545",
      "code": null,
      "tags": [
        "computational pathology",
        "agentic multimodal model",
        "evidence-seeking inference",
        "reinforcement learning",
        "whole-slide images",
        "vision-language model"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/db008475f544af0752cf146b5b6ba57eaf58c0e376cb94807dd3df594fa09037_w640_q70.webp",
      "contributions": "1. Proposed PathFound, an agentic multimodal model that introduces an evidence-seeking inference paradigm for pathological diagnosis, moving beyond static, single-pass analysis. 2. Integrated pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to enable proactive information acquisition and multi-stage diagnosis refinement. 3. Demonstrated that the evidence-seeking strategy consistently improves diagnostic accuracy across models and that PathFound achieves state-of-the-art performance, showing strong potential for discovering subtle pathological details.",
      "summary": "The paper proposes PathFound, an agentic multimodal model that mimics clinical workflows by actively seeking evidence for ambiguous pathological diagnoses through multi-turn interactions. It integrates visual foundation models, vision-language models, and reinforcement learning-based reasoning to refine its initial diagnosis. The method achieves state-of-the-art diagnostic accuracy and demonstrates the effectiveness of evidence-seeking workflows in computational pathology.",
      "mindmap": "graph TB\n        A[PathFound: Agentic Multimodal Model] --> B[核心问题/Problem: Static inference vs. clinical workflow]\n        A --> C[主要方法/Method: Agentic model with VFM, VLM, RL]\n        A --> D[关键结果/Results: SOTA accuracy, discovers subtle details]\n        B --> B1[静态推理范式/Static inference paradigm]\n        B --> B2[缺乏证据再获取/Lacks reassessment & evidence acquisition]\n        C --> C1[多阶段诊断/Multi-stage diagnosis]\n        C --> C2[主动信息获取/Proactive information acquisition]\n        D --> D1[诊断准确性提升/Improved diagnostic accuracy]\n        D --> D2[发现细微特征/Discover subtle pathological features]"
    },
    {
      "title": "PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation",
      "authors": "Zongsheng Cao, Yangfan He, Anran Liu, Jun Xie, Feng Chen, Zepeng Wang",
      "institution": "University of Minnesota, Lenovo PCIE",
      "link": "https://arxiv.org/pdf/2512.23546",
      "code": "https://github.com/AI-Researcher-Team/PurifyGen",
      "tags": [
        "diffusion models",
        "text-to-image generation",
        "prompt purification",
        "semantic distance",
        "null space projection",
        "training-free safety"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce70223ecd8b36ca7561cfb1134bb73a6a3a71d9d1fd6f6e1f2cd067f9a77b4f_w640_q70.webp",
      "contributions": "1. Introduces a novel, training-free, dual-stage strategy for safe text-to-image generation that retains the original model weights. 2. Proposes a fine-grained risk discrimination method using complementary semantic distance to classify prompt tokens without keyword matching. 3. Develops a dual-space transformation for semantic purification, projecting risky embeddings into the null space of toxic concepts and the range space of clean concepts.",
      "summary": "The paper proposes PurifyGen, a training-free method to make text-to-image generation safer. It works by first identifying risky tokens in a prompt using semantic distances and then purifying them by removing harmful semantic components while reinforcing safe ones. Experiments show it effectively reduces unsafe content across multiple datasets and competes with training-dependent approaches.",
      "mindmap": "graph TB\n        A[PurifyGen: Safe Text-to-Image Generation] --> B[核心问题/Problem: Diffusion models risk generating unsafe content]\n        A --> C[主要方法/Method: Dual-stage prompt purification via risk discrimination & semantic transformation]\n        A --> D[关键结果/Results: Outperforms current methods, competes with training-dependent approaches]"
    },
    {
      "title": "RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature",
      "authors": "Hanzheng Li, Xi Fang, Yixuan Li, Chaozheng Huang, Junjie Wang, Xi Wang, Hongzhe Bai, Bojun Hao, Shenyu Lin, Huiqi Liang, Linfeng Zhang, Guolin Ke",
      "institution": "DP Technology, Shanghai Jiao Tong University, Tsinghua University, New York University, Fudan University, Xiamen University, ShanghaiTech University",
      "link": "https://arxiv.org/pdf/2512.23565",
      "code": null,
      "tags": [
        "multimodal large language models",
        "chemical reaction understanding",
        "multimodal benchmark",
        "scientific literature",
        "visual perception",
        "cross-modal integration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e7b73b049eb3232e03516a09f66b0fddafff9357b89527de70340faa28603c6_w640_q70.webp",
      "contributions": "1. Introduces RxnBench, a multi-tiered benchmark for evaluating MLLMs on chemical reaction understanding from scientific PDFs, featuring two tasks (SF-QA and FD-QA). 2. Provides a comprehensive evaluation revealing a critical capability gap in MLLMs, showing they struggle with deep chemical logic and precise structural recognition despite excelling at text extraction. 3. Highlights the importance of inference-time reasoning and underscores the urgent need for domain-specific visual encoders and stronger reasoning engines for autonomous AI chemists.",
      "summary": "This paper introduces RxnBench, a multimodal benchmark to evaluate Large Language Models on understanding chemical reactions from scientific literature. The evaluation reveals that while models are good at extracting text, they struggle with chemical logic and structural recognition, showing the need for better domain-specific visual and reasoning components.",
      "mindmap": "graph TB\n        Root[RxnBench: A Multimodal Benchmark for Evaluating LLMs on Chemical Reaction Understanding from Scientific Literature] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: MLLMs' ability to comprehend dense, graphical reaction language in literature is underexplored.]\n        Method[主要方法/Method: A multi-tiered benchmark with two tasks: Single-Figure QA and Full-Document QA.]\n        Results[关键结果/Results: Models struggle with chemical logic and structure; inference-time reasoning helps but accuracy remains low, highlighting need for domain-specific encoders and reasoning engines.]"
    },
    {
      "title": "Instruction-Following Evaluation of Large Vision-Language Models",
      "authors": "Daiki Shiono, Shumpei Miyawaki, Ryota Tanaka, Jun Suzuki",
      "institution": "Tohoku University, NTT Corporation",
      "link": "https://arxiv.org/pdf/2512.23572",
      "code": null,
      "tags": [
        "instruction-following evaluation",
        "large vision-language models",
        "visual instruction tuning",
        "output format specification"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9a9ab8218c47b2791fe28909d9e490dfaa5d680ecfb209ca796611f893b9430_w640_q70.webp",
      "contributions": "1. Quantitatively demonstrates the decline in instruction-following ability of LVLMs after visual instruction fine-tuning. 2. Constructs new training datasets that highlight whether the output format is specified. 3. Shows that explicitly indicating the output format during fine-tuning helps LVLMs follow instructions more accurately.",
      "summary": "This paper identifies and quantifies a problem where Large Vision-Language Models (LVLMs) lose their instruction-following ability after visual instruction tuning. The authors propose constructing datasets that explicitly specify the output format and find that training with such data mitigates the performance decline. The main conclusion is that including instructions on output format during fine-tuning can help preserve LVLMs' instruction-following capabilities.",
      "mindmap": "graph TB\n        A[Instruction-Following Evaluation of Large Vision-Language Models] --> B(核心问题/Problem: LVLMs lose instruction-following ability after fine-tuning)\n        A --> C(主要方法/Method: Construct datasets highlighting output format specification)\n        A --> D(关键结果/Results: Explicit output format instructions improve instruction-following)"
    },
    {
      "title": "ThinkGen: Generalized Thinking for Visual Generation",
      "authors": "Siyu Jiao, Yiheng Lin, Yujie Zhong, Qi She, Wei Zhou, Xiaohan Lan, Zilong Huang, Fei Yu, Yingchen Yu, Yunqing Zhao, Yao Zhao, Yunchao Wei",
      "institution": "Beijing Jiaotong University, Bytedance",
      "link": "https://arxiv.org/pdf/2512.23568",
      "code": "https://github.com/jiaosiyuu/ThinkGen",
      "tags": [
        "text-to-image generation",
        "Chain-of-Thought (CoT)",
        "Multimodal Large Language Model (MLLM)",
        "Diffusion Transformer (DiT)",
        "reinforcement learning",
        "SepGRPO"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8377ac1537eac2a0f36b9ae8883a51e957cbbeb6e49b280cc20b5c5080e11f_w640_q70.webp",
      "contributions": "1. Proposes ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning for various generation tasks. 2. Introduces a decoupled architecture using a pretrained MLLM to generate instructions and a DiT for image synthesis. 3. Proposes a separable GRPO-based training paradigm (SepGRPO) for alternating reinforcement learning between modules, enabling joint training across diverse datasets.",
      "summary": "This paper introduces ThinkGen, a framework that integrates Chain-of-Thought reasoning from Multimodal LLMs with a Diffusion Transformer for visual generation. It uses a decoupled architecture and a novel separable reinforcement learning training method to generalize across diverse generation scenarios. Experiments show it achieves state-of-the-art performance on multiple benchmarks.",
      "mindmap": "graph TB\n        A[ThinkGen: Generalized Thinking for Visual Generation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[CoT推理在生成任务中应用有限/CoT for generation is nascent and scenario-specific]\n        C --> C1[解耦架构: MLLM + DiT/Decoupled architecture: MLLM + DiT]\n        C --> C2[可分离GRPO训练范式/SepGRPO training paradigm]\n        D --> D1[在多个基准上实现SOTA/Achieves SOTA across multiple benchmarks]"
    },
    {
      "title": "ProGuard: Towards Proactive Multimodal Safeguard",
      "authors": "Shaohan Yu, Lijun Li, Chenyang Si, Lu Sheng, Jing Shao",
      "institution": "Shanghai Artificial Intelligence Laboratory, Nanjing University, Beihang University",
      "link": "https://arxiv.org/pdf/2512.23573",
      "code": "https://yushaohan.github.io/ProGuard",
      "tags": [
        "multimodal safety",
        "proactive guard",
        "out-of-distribution (OOD) detection",
        "reinforcement learning (RL)",
        "multimodal safety taxonomy",
        "synonym-bank reward"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ffca72668094b2c8095387a83d8b49cc465da7d2f333cac7db429f76193be61_w640_q70.webp",
      "contributions": "1. Introduces ProGuard, a vision-language proactive guard model that identifies and describes out-of-distribution safety risks without requiring model adjustments. 2. Constructs a modality-balanced dataset of 87K samples with binary safety labels and hierarchical risk categories to mitigate modality bias. 3. Trains the model purely via reinforcement learning augmented with a synonym-bank-based similarity reward to enhance OOD risk inference and description.",
      "summary": "The paper proposes ProGuard, a proactive multimodal safeguard that uses reinforcement learning on a balanced dataset to detect and describe unseen safety risks. It achieves performance comparable to closed-source models on safety classification and significantly improves OOD risk detection and description by over 50%.",
      "mindmap": "graph TB\n        A[ProGuard: Towards Proactive Multimodal Safeguard] --> B[核心问题/Problem: 生成模型快速发展带来持续的多模态安全风险，现有防御方法存在局限。]\n        A --> C[主要方法/Method: 提出ProGuard，基于强化学习训练视觉语言基础模型，引入OOD类别推断任务和同义词库奖励。]\n        A --> D[关键结果/Results: 在二元安全分类上媲美闭源大模型，OOD风险检测提升52.6%，描述提升64.8%。]"
    },
    {
      "title": "Image Denoising Using Global and Local Circulant Representation",
      "authors": "Zhaoming Kong, Xiaowei Yang, Jiahuan Zhang",
      "institution": "South China University of Technology, Guangdong Provincial People's Hospital, Southern Medical University",
      "link": "https://arxiv.org/pdf/2512.23569",
      "code": "https://github.com/ZhaomingKong/Haar-tSVD",
      "tags": [
        "image denoising",
        "circulant representation",
        "tensor-SVD",
        "Haar transform"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4e8ca1da239d053e9277827699e78e6ac4dd3e3111e0069116ba0071aa405a82_w640_q70.webp",
      "contributions": "1. Established a theoretical connection between PCA and the Haar transform under circulant representation. 2. Proposed a computationally simple, one-step plug-and-play denoiser (Haar-tSVD) that balances speed and performance by capturing global and local correlations. 3. Introduced an adaptive noise estimation scheme and integrated deep neural networks to enhance robustness under severe noise conditions.",
      "summary": "This paper proposes a new image denoising method called Haar-tSVD, which combines tensor singular value decomposition with the Haar transform to efficiently remove noise. It is designed as a fast, parallelizable algorithm that does not require learning and can be integrated with deep networks for better performance. Experiments show the method is effective and efficient for noise removal across various datasets.",
      "mindmap": "graph TB\n        Root[Image Denoising Using Global and Local Circulant Representation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: High demand for efficient and effective image denoising] --> P1[挑战/Challenge: Noise degrades image quality]\n        Method[主要方法/Method: Haar-tSVD] --> M1[理论/Theoretical: Connect PCA and Haar transform]\n        Method --> M2[算法/Algorithm: Unified t-SVD projection with Haar transform]\n        Method --> M3[增强/Enhancement: Adaptive noise estimation & DNN integration]\n        Results[关键结果/Results: Efficient and effective noise removal] --> R1[验证/Validation: Experiments on various datasets]"
    },
    {
      "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation",
      "authors": "Ethan Chern, Zhulin Hu, Bohao Tang, Jiadi Su, Steffi Chern, Zhijie Deng, Pengfei Liu",
      "institution": "SII, SJTU, GAIR",
      "link": "https://arxiv.org/pdf/2512.23576",
      "code": "/githubCode",
      "tags": [
        "multi-modal inference",
        "on-policy distillation",
        "real-time video diffusion",
        "multimodal conditioning",
        "Anchor-Heavy Identity Sinks",
        "autoregressive generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/549213eb2c7dd73c1020df9057c0ed9979c11b408c183a2d3d0bc103f7356da9_w640_q70.webp",
      "contributions": "1. Proposes an improved on-policy distillation recipe for real-time multimodal video diffusion, addressing issues like flickering and quality degradation in prior methods. 2. Develops LiveTalk, a real-time interactive avatar system integrating the distilled model with audio language models and long-form video inference techniques. 3. Demonstrates 20x reduction in inference cost/latency while matching baseline quality, and outperforms SOTA in multi-turn coherence and content quality.",
      "summary": "This paper tackles the challenge of real-time interactive video generation by improving on-policy distillation for multimodal-conditioned diffusion models. The proposed method enhances training stability and output quality, enabling a 20x speedup. The resulting LiveTalk system achieves real-time, coherent multi-turn avatar interactions, significantly outperforming existing models in latency and quality.",
      "mindmap": "graph TB\n        A[LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 扩散模型双向注意力迭代过程阻碍实时交互 / Diffusion model's iterative bidirectional attention prevents real-time interaction]\n        C[主要方法/Method: 改进的按策略蒸馏，强调条件输入质量与优化计划 / Improved on-policy distillation, emphasizing condition input quality & optimization schedule]\n        D[关键结果/Results: 20倍加速，实时延迟，多轮交互质量超越SOTA / 20x acceleration, real-time latency, multi-turn quality surpasses SOTA]"
    },
    {
      "title": "Same or Not? Enhancing Visual Perception in Vision-Language Models",
      "authors": "Damiano Marsili, Aditya Mehta, Ryan Y. Lin, Georgia Gkioxari",
      "institution": "California Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.23592",
      "code": null,
      "tags": [
        "vision-language models",
        "fine-grained visual understanding",
        "dataset",
        "benchmark",
        "visual perception",
        "image-pair queries"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea8f098740ab2b0f35ff068b90bb9d954080b272f70a67a2bd48d7a7771756e0_w640_q70.webp",
      "contributions": "1. Introduced TWIN, a large-scale dataset of 561,000 image-pair queries designed to train VLMs on fine-grained visual perception by determining if two similar images depict the same object. 2. Introduced FGVQA, a benchmark suite of 12,000 queries to evaluate fine-grained VQA capabilities across multiple domains. 3. Demonstrated that fine-tuning VLMs on TWIN significantly improves their fine-grained recognition on FGVQA (up to 19.3%) without harming general VQA performance, and showed the importance of dataset scale.",
      "summary": "The paper addresses the limitation of Vision-Language Models (VLMs) in fine-grained visual perception. It proposes a new training dataset (TWIN) and a benchmark (FGVQA) to enhance VLMs' ability to notice subtle visual details. The results show that fine-tuning on TWIN significantly improves fine-grained recognition on unseen domains without compromising general performance.",
      "mindmap": "graph TB\n        A[Same or Not? Enhancing Visual Perception in Vision-Language Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: VLMs lack fine-grained perception, miss subtle details 视觉语言模型缺乏细粒度感知，忽略细微差别]\n        C[主要方法/Method: Introduce TWIN dataset & FGVQA benchmark 引入TWIN数据集和FGVQA基准]\n        D[关键结果/Results: Fine-tuning on TWIN improves fine-grained recognition by up to 19.3% 在TWIN上微调将细粒度识别提升高达19.3%]"
    },
    {
      "title": "Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging",
      "authors": "Janani Annur Thiruvengadam, Kiran Mayee Nabigaru, Anusha Kovi",
      "institution": "Amazon.com Services LLC",
      "link": "https://arxiv.org/pdf/2512.23597",
      "code": null,
      "tags": [
        "medical image analysis",
        "Scalable Residual Feature Aggregation (SRFA)",
        "Hybrid Metaheuristic Optimization (HHO-BA)",
        "Vision Transformer (ViT) with EfficientNet-B3"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/236616ac709d85c1958734582dc8a1182b385106ea696a4ffc79796016e70db9_w640_q70.webp",
      "contributions": "1. Proposes a Scalable Residual Feature Aggregation (SRFA) framework integrating MAGRes-UNet for segmentation and DenseNet-121 for hierarchical feature extraction. 2. Introduces a hybrid HHO-BA metaheuristic feature selection strategy to refine the optimal feature subset. 3. Develops a novel hybrid classifier combining Vision Transformer (ViT) and EfficientNet-B3, fine-tuned using a dual SSA-GWO optimization mechanism for robust classification.",
      "summary": "This paper addresses the challenge of early pancreatic neoplasm detection in multimodal CT imaging by proposing a Scalable Residual Feature Aggregation (SRFA) framework. The method combines advanced segmentation, feature extraction with residual storage, hybrid metaheuristic feature selection, and a novel ViT-EfficientNet-B3 classifier optimized with SSA and GWO. The proposed system achieves high performance (96.23% accuracy), demonstrating significant improvement over traditional and contemporary models for robust early detection.",
      "mindmap": "graph TB\n        A[Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 胰腺肿瘤早期检测困难/Early pancreatic neoplasm detection is difficult due to subtle, low-contrast lesions and high patient variability in CT scans.]\n        C[主要方法/Method: SRFA框架整合MAGRes-UNet分割、DenseNet-121特征提取、HHO-BA特征选择、ViT-EfficientNet-B3混合分类器，并使用SSA-GWO优化/SRFA framework integrates MAGRes-UNet segmentation, DenseNet-121 feature extraction, HHO-BA feature selection, ViT-EfficientNet-B3 hybrid classifier, optimized with SSA-GWO.]\n        D[关键结果/Results: 模型达到96.23%准确率，性能显著优于传统CNN和当前基于Transformer的模型/Model achieves 96.23% accuracy, significantly outperforming traditional CNNs and contemporary transformer-based models.]"
    },
    {
      "title": "Detection Fire in Camera RGB-NIR",
      "authors": "Nguyen Truong Khai, Luong Duc Vinh",
      "institution": "Viettel (inferred from email domain vinhld@viettel.com)",
      "link": "https://arxiv.org/pdf/2512.23594",
      "code": null,
      "tags": [
        "object detection",
        "YOLOv11",
        "EfficientNetV2",
        "two-stage detection",
        "NIR dataset",
        "Patched-YOLO"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/27f2ac726eb6c9db14e654d748210b2689604a2d35ed6affd7a7ea009d6094bd_w640_q70.webp",
      "contributions": "1. An additional NIR dataset with various data augmentation strategies to address data scarcity. 2. A two-stage detection pipeline combining YOLOv11 and EfficientNetV2-B0 to improve night-time fire detection accuracy and reduce false positives from artificial lights. 3. Patched-YOLO, a patch-based processing method to enhance detection of small and distant fire objects in RGB images.",
      "summary": "This paper addresses the challenge of improving fire detection accuracy, especially at night using NIR cameras and for small objects in RGB images. It proposes a two-stage model (YOLOv11 + EfficientNetV2-B0) for NIR detection and a Patched-YOLO method for RGB, alongside an augmented NIR dataset. The proposed approaches aim to achieve higher accuracy and reduce false positives compared to previous methods.",
      "mindmap": "graph TB\n    A[Detection Fire in Camera RGB-NIR] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[夜间红外火焰检测准确性低 / Low accuracy in nighttime NIR fire detection]\n    B --> B2[误将亮光识别为火焰 / Misclassification of bright lights as fire]\n    B --> B3[RGB图像中小型/远距离火焰检测难 / Difficulty detecting small/distant fire in RGB]\n    C --> C1[新增NIR数据集 / Additional NIR Dataset]\n    C --> C2[两阶段检测模型 / Two-Stage Detection Model]\n    C --> C3[Patched-YOLO / Patched-YOLO]\n    D --> D1[检测精度超越先前模型 / Detection accuracy surpasses previous models]"
    },
    {
      "title": "Memorization in 3D Shape Generation: An Empirical Study",
      "authors": "Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu",
      "institution": "Princeton University, Harvard University",
      "link": "https://arxiv.org/pdf/2512.23628",
      "code": "github.com/zlab-princeton/3d_mem",
      "tags": [
        "3D shape generation",
        "memorization",
        "diffusion models",
        "latent vector-set",
        "evaluation framework",
        "data leakage"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cca470d9a610f6e7172776f63b7bfed02850fdb4a843786976b699c63c87febc_w640_q70.webp",
      "contributions": "1. Proposed a novel evaluation framework to quantify memorization in 3D generative models. 2. Applied the framework to benchmark and quantify memorization in existing 3D generation methods. 3. Conducted controlled experiments to identify how data and modeling factors (e.g., modality, guidance scale, augmentation) influence memorization.",
      "summary": "This paper investigates whether 3D shape generative models memorize training data. The authors design an evaluation framework to measure memorization and conduct experiments with a latent vector-set diffusion model. They find memorization depends on data modality and diversity, peaks at moderate guidance, and can be reduced with longer latent vectors and rotation augmentation without harming quality.",
      "mindmap": "graph TB\n        A[Memorization in 3D Shape Generation: An Empirical Study] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(3D生成模型是否记忆训练数据?/Do 3D generative models memorize training data?)\n        C --> C1(设计量化框架/Design evaluation framework)\n        C --> C2(使用Vecset扩散模型进行控制实验/Use Vecset diffusion model for controlled experiments)\n        D --> D1(数据多样性和细粒度条件增加记忆/Data diversity & fine-grained conditioning increase memorization)\n        D --> D2(适度引导规模峰值记忆/Moderate guidance scale peaks memorization)\n        D --> D3(更长Vecsets和旋转增强可缓解记忆/Longer Vecsets & rotation augmentation mitigate memorization)"
    },
    {
      "title": "OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding",
      "authors": "Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang",
      "institution": "Zhejiang University, Westlake University, Ant Group",
      "link": "https://arxiv.org/pdf/2512.23646",
      "code": "https://kd-tao.github.io/OmniAgent",
      "tags": [
        "agent system",
        "active perception",
        "audio-guided",
        "tool orchestration",
        "coarse-to-fine perception",
        "multimodal alignment"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a339430d662ee6bf5ece3181e2cf4fba493ea56a973aed76a319a939348ef1e3_w640_q70.webp",
      "contributions": "1. Introduces OmniAgent, an audio-guided active perception agent that shifts from passive response to active multimodal inquiry. 2. Proposes a novel coarse-to-fine audio-guided perception paradigm that uses audio cues to localize events and guide reasoning. 3. Demonstrates state-of-the-art performance on audio-video benchmarks, outperforming leading models by 10%-20% accuracy.",
      "summary": "The paper addresses the lack of fine-grained cross-modal understanding in omnimodal LLMs by proposing OmniAgent, an active perception agent that dynamically orchestrates tools using audio cues to guide video analysis. It achieves superior performance on audio-video understanding benchmarks, significantly outperforming existing models.",
      "mindmap": "graph TB\n        A[OmniAgent: Audio-Guided Active Perception Agent] --> B[核心问题/Problem: Omnimodal LLMs lack fine-grained cross-modal understanding and multimodal alignment]\n        A --> C[主要方法/Method: Audio-guided active perception agent with dynamic tool orchestration and coarse-to-fine perception]\n        A --> D[关键结果/Results: Achieves SOTA, outperforms leading models by 10%-20% accuracy]"
    },
    {
      "title": "Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception",
      "authors": "Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu, Yitao Wu, Jiajia Fu, Dixiao Cui, Lijun Zhao, Lining Sun",
      "institution": "Harbin Institute of Technology, IROOTECH",
      "link": "https://arxiv.org/pdf/2512.23635",
      "code": "https://github.com/lixiaoyu2000/HAT",
      "tags": [
        "3D object detection and tracking",
        "spatio-temporal alignment",
        "multi-hypothesis decoding",
        "motion modeling",
        "end-to-end perception",
        "autonomous driving"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fbd0387c83c53b2f59d9e00643e3c40d6bb651484df151a86d870b0e63fc6c53_w640_q70.webp",
      "contributions": "1. Proposes HAT, a novel spatio-temporal alignment module that adaptively decodes optimal alignment from multiple motion hypotheses without direct supervision. 2. Integrates both explicit motion models and semantic cues to address suboptimal alignment caused by varying object motion states and features. 3. Demonstrates consistent improvements in 3D perception and tracking performance across diverse baselines and enhances robustness in end-to-end autonomous driving systems, especially under semantic corruption.",
      "summary": "The paper identifies that existing spatio-temporal alignment methods in end-to-end 3D perception for autonomous driving are suboptimal due to simplified motion models. It proposes HAT, a module that generates multiple motion-aware feature proposals and adaptively selects the best alignment using semantic and motion cues. HAT improves detection and tracking performance on benchmarks and enhances the robustness of end-to-end autonomous driving systems.",
      "mindmap": "graph TB\n        Root[Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>现有对齐方法因简化运动模型而次优<br>Existing alignment is suboptimal due to simplified motion models] --> P1[变化导致挑战/Variations in motion states and features pose challenges]\n        Method[主要方法/Method<br>提出HAT模块/Propose HAT module] --> M1[多假设生成/Multi-hypothesis generation using explicit models]\n        Method --> M2[自适应解码/Adaptive decoding with semantic & motion cues]\n        Results[关键结果/Results<br>性能提升与鲁棒性增强/Performance improvement & enhanced robustness] --> R1[SOTA跟踪结果/SOTA tracking results (46.0% AMOTA)]\n        Results --> R2[提升端到端系统/Improves E2E AD perception and reduces collisions]"
    },
    {
      "title": "RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion",
      "authors": "Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang",
      "institution": "Beijing Academy of Artificial Intelligence (BAAI), University of Sydney, Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.23649",
      "code": null,
      "tags": [
        "robot learning",
        "video-to-locomotion",
        "visual motion intent",
        "diffusion policy"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bb8087d8ec53ebe9a14dbea08d918cbe27838f7e4dc5d178ed65513c16703cd7_w640_q70.webp",
      "contributions": "1. Proposes RoboMirror, the first retargeting-free framework that directly generates humanoid locomotion from raw videos by understanding visual motion intents. 2. Introduces a method that leverages Vision-Language Models (VLMs) to distill videos into semantic motion intents, which condition a diffusion-based policy, bypassing explicit pose estimation. 3. Demonstrates the framework's effectiveness for both egocentric (telepresence) and third-person video control, significantly reducing control latency and improving task success rates.",
      "summary": "This paper addresses the gap between visual understanding and control in humanoid locomotion by proposing RoboMirror, a framework that first understands visual motion intents from raw videos and then uses them to condition a diffusion policy for generating physically plausible actions. The method eliminates the need for explicit pose reconstruction and retargeting. Experiments show it enables effective telepresence, reduces control latency by 80%, and achieves higher task success than baselines.",
      "mindmap": "graph TB\n        A[RoboMirror: Video to Humanoid Locomotion] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[视觉理解与控制存在鸿沟/Gap between visual understanding and control]\n        B --> B2[现有方法缺乏真正的视觉理解/Existing methods lack genuine visual understanding]\n        C --> C1[利用VLM提取视觉运动意图/Use VLMs to distill visual motion intents]\n        C --> C2[基于扩散的策略生成动作/Diffusion-based policy generates actions]\n        C --> C3[无需姿态重建或重定向/No explicit pose reconstruction or retargeting]\n        D --> D1[支持第一与第三人称视频控制/Supports egocentric & third-person control]\n        D --> D2[降低80%控制延迟/Reduces control latency by 80%]\n        D --> D3[任务成功率提升3.7%/3.7% higher task success rate]"
    },
    {
      "title": "IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition",
      "authors": "Kang Du, Yirui Guan, Zeyu Wang",
      "institution": "The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology, Tencent",
      "link": "https://arxiv.org/pdf/2512.23667",
      "code": null,
      "tags": [
        "intrinsic image decomposition",
        "multi-view consistency",
        "transformer",
        "physically grounded model",
        "feed-forward inference",
        "specular shading"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01e9c12974f4da9c82258f09668d7544f5ad1769d8b9ab02956676c6f1a49a81_w640_q70.webp",
      "contributions": "1. Proposes IDT, a feed-forward transformer framework for multi-view intrinsic decomposition that directly outputs view-consistent factors without iterative sampling. 2. Introduces a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. 3. Demonstrates superior performance in producing cleaner decompositions and better multi-view consistency compared to prior methods on synthetic and real-world datasets.",
      "summary": "This paper addresses the challenge of view inconsistency in multi-view intrinsic image decomposition. It proposes IDT, a feed-forward transformer that jointly processes multiple views using a physically grounded model to decompose images into reflectance and shading components. Experiments show IDT achieves more consistent and interpretable decompositions than previous methods.",
      "mindmap": "graph TB\n        Root(”IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition”) --> Problem(”核心问题/Problem: Multi-view intrinsic decomposition suffers from view inconsistency”)\n        Root --> Method(”主要方法/Method: Feed-forward transformer with a physically grounded model for joint multi-view reasoning”)\n        Root --> Results(”关键结果/Results: Achieves view-consistent, cleaner decompositions (diffuse reflectance, diffuse shading, specular shading)”)"
    },
    {
      "title": "Web World Models",
      "authors": "Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, Mengdi Wang",
      "institution": "Princeton University, University of California, Los Angeles, University of Pennsylvania",
      "link": "https://arxiv.org/pdf/2512.23676",
      "code": "https://princeton-ai2-lab.github.io/Web-World-Models/",
      "tags": [
        "agent system",
        "world model",
        "language agent",
        "web framework",
        "structured latent state",
        "deterministic generation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed8017bbc1bd6722a0d7bd0f84c67f735c0f1b24747518e2aa15905d07d1b03c_w640_q70.webp",
      "contributions": "1. Introduced the Web World Model (WWM), a hybrid architecture that uses ordinary web code to enforce logical consistency and LLMs to generate open-ended content. 2. Built a suite of practical WWM demonstrations across diverse domains (travel, fiction, encyclopedia, games) on a realistic web stack. 3. Identified key design principles for WWMs, such as separating code-defined rules from model-driven imagination and representing latent state as typed web interfaces.",
      "summary": "This paper proposes Web World Models (WWMs), a framework that combines the reliability of web code for world \"physics\" with the generative power of LLMs for content and narratives. This hybrid approach aims to provide language agents with controllable, logically consistent, yet open-ended persistent environments. The work demonstrates that standard web stacks can serve as a scalable substrate for building such world models.",
      "mindmap": "graph TB\n        A[Web World Models] --> B[”核心问题/Problem: Language agents need persistent worlds; existing solutions are either too rigid (web frameworks) or too uncontrolled (fully generative models).”]\n        A --> C[”主要方法/Method: Hybrid Web World Model (WWM): Web code defines rules & state; LLMs generate context & narratives on top.”]\n        A --> D[”关键结果/Results: Demonstrates scalable, controllable, open-ended environments; proposes design principles for WWMs.”]"
    },
    {
      "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation",
      "authors": "Shaocong Xu, Songlin Wei, Qizhe Wei, Zheng Geng, Hong Li, Licheng Shen, Qianpu Sun, Shu Han, Bin Ma, Bohan Li, Chongjie Ye, Yuhang Zheng, Nan Wang, Saining Zhang, Hao Zhao",
      "institution": "Beijing Academy of Artificial Intelligence, Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.23705",
      "code": "https://daniellli.github.io/projects/DKT/",
      "tags": [
        "depth estimation",
        "video diffusion",
        "transparent objects",
        "LoRA",
        "depth estimation",
        "normal estimation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4acfd062f7e9aa8f5d75a8134d26d8ef4e00ec2127c12cf0ecfcc6466b48290_w640_q70.webp",
      "contributions": "1. Introduces TransPhy3D, a large-scale synthetic video dataset for transparent/reflective scenes with RGB, depth, and normal maps. 2. Proposes DKT, a method that repurposes a pre-trained video diffusion model via lightweight LoRA adapters for temporally consistent depth and normal estimation from videos. 3. Demonstrates state-of-the-art zero-shot performance on real and synthetic benchmarks and shows practical improvement in robotic grasping tasks.",
      "summary": "This paper addresses the challenging problem of depth and normal estimation for transparent and reflective objects in videos. The proposed method, DKT, repurposes a pre-trained video diffusion model using LoRA adapters and a novel synthetic dataset (TransPhy3D) to achieve temporally consistent predictions. The results show state-of-the-art zero-shot performance on benchmarks and improved robotic manipulation, supporting the claim that generative video priors effectively capture the physics of transparency.",
      "mindmap": "graph TB\n        Root[”Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Transparent/reflective objects break assumptions of traditional depth sensing, causing holes and instability.”]\n        Method[”主要方法/Method<br>Repurpose video diffusion model with LoRA adapters, trained on new synthetic dataset TransPhy3D.”]\n        Results[”关键结果/Results<br>Achieves SOTA zero-shot performance on benchmarks and improves robotic grasping success.”]"
    },
    {
      "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion",
      "authors": "Hau-Shiang Shiu, Chin-Yang Lin, Zhixiang Wang, Chi-Wei Hsiao, Po-Fan Yu, Yu-Chih Chen, Yu-Lun Liu",
      "institution": "National Yang Ming Chiao Tung University, Shanda AI Research Tokyo, MediaTek Inc.",
      "link": "https://arxiv.org/pdf/2512.23709",
      "code": "https://jamichss.github.io/stream-diffvsr-project-page/",
      "tags": [
        "video super-resolution",
        "diffusion models",
        "online processing",
        "low-latency",
        "auto-regressive",
        "temporal guidance"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f48d801921015ff458d12a1cb668a1d0e4be149fbb26d7ce6079b45ed8444d0_w640_q70.webp",
      "contributions": "1. A causally conditioned diffusion framework for online VSR that operates strictly on past frames, enabling low-latency deployment. 2. An Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising to enhance temporal coherence. 3. A lightweight temporal-aware decoder with a Temporal Processor Module (TPM) and a four-step distilled denoiser, achieving fast inference (0.328s per 720p frame) while maintaining high perceptual quality.",
      "summary": "This paper addresses the impracticality of diffusion-based video super-resolution (VSR) for low-latency applications by proposing Stream-DiffVSR, an online framework that uses causal conditioning, a distilled denoiser, and novel temporal modules. The method significantly reduces latency to 0.328 seconds per frame while improving perceptual quality, making it the first diffusion VSR approach suitable for real-time online deployment.",
      "mindmap": "graph TB\n        A[Stream-DiffVSR] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[扩散VSR延迟高/Diffusion VSR has high latency]\n        B --> B2[依赖未来帧/Relies on future frames]\n        C --> C1[因果条件扩散/Causally Conditioned Diffusion]\n        C --> C2[自回归时序引导/Auto-regressive Temporal Guidance (ARTG)]\n        C --> C3[四步蒸馏去噪器/4-step Distilled Denoiser]\n        D --> D1[延迟0.328秒/Latency 0.328s per frame]\n        D --> D2[感知质量提升/Improved perceptual quality (LPIPS)]\n        D --> D3[首个在线扩散VSR/First online diffusion VSR]"
    },
    {
      "title": "AI-Enhanced Virtual Biopsies for Brain Tumor Diagnosis in Low Resource Settings",
      "authors": "Areeb Ehsan",
      "institution": "Georgia State University",
      "link": "https://arxiv.org/pdf/2512.22184",
      "code": null,
      "tags": [
        "medical image classification",
        "MobileNetV2",
        "radiomics",
        "Grad-CAM",
        "RandomForest",
        "feature fusion"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b3ff899544d8a10dd2cc79899902ffd4236afc7cedc8e6e24bedb8927407cf2_w640_q70.webp",
      "contributions": "1. Proposes a hybrid \"virtual biopsy\" pipeline combining a lightweight CNN (MobileNetV2) with handcrafted radiomics features for brain tumor classification. 2. Employs a late fusion strategy using a RandomForest classifier on the concatenated CNN embeddings and radiomics features to improve performance. 3. Provides model explainability through Grad-CAM visualizations and radiomics feature importance analysis, and evaluates robustness under low-resolution and noisy imaging conditions relevant to low-resource settings.",
      "summary": "This paper addresses the challenge of brain tumor diagnosis in low-resource settings by proposing a virtual biopsy pipeline. The method combines a lightweight CNN with interpretable radiomics features and fuses them using a RandomForest classifier. Experiments show the fusion approach improves classification performance and the analysis highlights its sensitivity to image quality issues common in resource-constrained environments.",
      "mindmap": "graph TB\n        Root[”AI-Enhanced Virtual Biopsies for Brain Tumor Diagnosis in Low Resource Settings”] --> Problem[”核心问题/Problem: Brain tumor diagnosis is difficult in low-resource settings due to limited expert access, hardware, and invasive biopsies.”]\n        Root --> Method[”主要方法/Method: Uses a hybrid pipeline with a lightweight CNN (MobileNetV2) and radiomics features, fused via RandomForest.”]\n        Root --> Results[”关键结果/Results: Fusion improves performance; robustness tests reveal sensitivity to low-quality images relevant to low-resource environments.”]"
    },
    {
      "title": "Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images",
      "authors": "Haozhe Jia",
      "institution": "Boston University",
      "link": "https://arxiv.org/pdf/2512.22209",
      "code": null,
      "tags": [
        "medical image super-resolution",
        "diffusion models",
        "SR3",
        "DDPM",
        "capsule endoscopy",
        "HyperKvasir"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70b95a1328af0d9ae7f16ab6cb15a216b2ad914761eed6496beb2ee934202e23_w640_q70.webp",
      "contributions": "1. Applied the SR3 diffusion model framework to the specific domain of capsule endoscopy image super-resolution, addressing hardware-imposed low-resolution constraints. 2. Demonstrated that the diffusion-based approach outperforms traditional interpolation and GAN-based methods (e.g., ESRGAN) in both quantitative metrics (PSNR, SSIM) and qualitative anatomical fidelity. 3. Showed that architectural enhancements like attention mechanisms further improve performance, achieving a PSNR of 29.3 dB and SSIM of 0.71.",
      "summary": "This paper proposes using a diffusion model (SR3/DDPM) for super-resolution enhancement of low-resolution capsule endoscopy images. The method learns a probabilistic mapping from low-resolution to high-resolution images and is evaluated on the HyperKvasir dataset. Results show it outperforms traditional and GAN-based methods, better preserving critical anatomical details for clinical diagnosis.",
      "mindmap": "graph TB\n        Root[”Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Capsule endoscopy images have low resolution, limiting clinical diagnosis.”]\n        Method[”主要方法/Method<br>Use SR3 diffusion model to learn mapping from LR to HR images.”]\n        Results[”关键结果/Results<br>Outperforms bicubic & GAN methods, improves PSNR/SSIM, preserves anatomy.”]"
    },
    {
      "title": "Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging",
      "authors": "Muhammad Ibtsaam Qadir, Duane Schonlau, Ulrike Dydak, Fiona R. Kolbinger",
      "institution": "Purdue University, Indiana University School of Medicine, TUD Dresden University of Technology",
      "link": "https://arxiv.org/pdf/2512.22176",
      "code": null,
      "tags": [
        "medical image segmentation",
        "nnU-Net",
        "MRI field strength",
        "radiomic analysis",
        "UMAP clustering",
        "model generalizability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff8ab0f68e15ce620cdfd03bebfec56b322101c32b9cea5d7a2881045b311bc2_w640_q70.webp",
      "contributions": "1. A systematic quantitative evaluation framework to assess the impact of MRI scanner magnetic field strength (1.5T vs. 3.0T) on the performance and generalizability of deep learning segmentation models. 2. Empirical demonstration that training data field strength significantly influences model performance, especially for soft-tissue segmentation tasks, with models trained on 3.0T data often outperforming others. 3. The use of radiomic analysis and UMAP clustering to provide an interpretable, feature-based explanation for the observed performance differences, linking them to field-strength-dependent image characteristics.",
      "summary": "This study investigates how MRI scanner magnetic field strength affects deep learning-based segmentation models. Using nnU-Net models trained on data from 1.5T, 3.0T, or combined field strengths across three anatomical datasets, the authors found that field strength in training data significantly impacts model performance, particularly for soft tissues. The conclusion is that magnetic field strength should be considered a confounding factor in AI studies for MRI analysis.",
      "mindmap": "graph TB\n        Root(”Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging<br>磁共振成像深度学习分析中场强依赖的性能变异性”) --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem(”核心问题/Problem<br>Impact of MRI field strength on DL model performance & generalizability<br>MRI场强对深度学习模型性能与泛化能力的影响”)\n        Method(”主要方法/Method<br>Train/evaluate nnU-Net models on 1.5T, 3.0T, and combined data; Analyze with UMAP & radiomics<br>在1.5T、3.0T及混合数据上训练/评估nnU-Net模型；使用UMAP和影像组学分析”)\n        Results(”关键结果/Results<br>Field strength in training data substantially influences performance, especially for soft tissues<br>训练数据中的场强显著影响性能，尤其对软组织”)"
    },
    {
      "title": "Complex Swin Transformer for Accelerating Enhanced SMWI Reconstruction",
      "authors": "Muhammad Usman, Sung-Min Gho",
      "institution": "Stanford University, DeepNoid Inc.",
      "link": "https://arxiv.org/pdf/2512.22202",
      "code": null,
      "tags": [
        "medical image reconstruction",
        "Swin Transformer",
        "complex-valued network",
        "k-space undersampling",
        "super-resolution",
        "Parkinson's disease"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9f7b6d093d1496f97cdabeee1991bf51b7ca16e517f4f553ad6598a394e77948_w640_q70.webp",
      "contributions": "1. Proposed a novel complex-valued Swin Transformer network for super-resolution reconstruction of multi-echo MRI data. 2. Demonstrated high-quality SMWI reconstruction from low-resolution/undersampled k-space data, significantly reducing required scan time. 3. Validated the method's ability to preserve critical diagnostic features for Parkinson's Disease, enhancing clinical applicability.",
      "summary": "This paper addresses the long scan time problem of Susceptibility Map Weighted Imaging (SMWI) for Parkinson's disease diagnosis. The authors propose a complex-valued Swin Transformer network to reconstruct high-quality SMWI images from reduced k-space data. Experimental results show the method achieves high reconstruction quality (SSIM 0.9116) while preserving diagnostic details, enabling faster clinical scans.",
      "mindmap": "graph TB\n        Root[Complex Swin Transformer for Accelerating Enhanced SMWI Reconstruction] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br>SMWI full scan time too long] --> P1[关键限制/Key Limitation<br>Long scan time limits clinical use]\n        Method[主要方法/Method<br>Complex Swin Transformer Network] --> M1[技术核心/Technical Core<br>Super-resolve multi-echo MRI data]\n        Results[关键结果/Results<br>High-quality reconstruction from reduced data] --> R1[量化指标/Quantitative Metrics<br>SSIM: 0.9116, MSE: 0.076]\n        Results --> R2[临床价值/Clinical Value<br>Preserves diagnostic features, faster scan]"
    },
    {
      "title": "MEGA-PCC: A Mamba-based Efficient Approach for Joint Geometry and Attribute Point Cloud Compression",
      "authors": "Kai-Hsiang Hsieh, Monyneath Yim, Wen-Hsiao Peng, Jui-Chiu Chiang",
      "institution": "National Chung Cheng University, National Yang Ming Chiao Tung University",
      "link": "https://arxiv.org/pdf/2512.22463",
      "code": null,
      "tags": [
        "point cloud compression",
        "Mamba",
        "end-to-end learning",
        "joint geometry-attribute compression",
        "entropy model",
        "rate-distortion optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11cdb3cb29e92ab87ea1a3d602cc6e1c5d95edb121070a94d9bb554dc9f450b0_w640_q70.webp",
      "contributions": "1. Proposes MEGA-PCC, a fully end-to-end learning-based framework for joint point cloud geometry and attribute compression that eliminates the need for post-hoc recoloring and manual bitrate tuning. 2. Introduces a Mamba-based Entropy Model (MEM) that captures spatial and channel-wise correlations to improve probability estimation for entropy coding. 3. Employs a shared encoder with dual decoders built on the Mamba architecture to model long-range dependencies, enabling data-driven bitrate allocation and superior rate-distortion performance.",
      "summary": "This paper addresses the limitations of existing point cloud compression methods, which rely on complex post-processing and manual bitrate allocation. The authors propose MEGA-PCC, an end-to-end framework using a Mamba-based shared encoder and a specialized entropy model for joint geometry and attribute compression. Experiments show the method outperforms traditional and learning-based baselines in both performance and efficiency.",
      "mindmap": "graph TB\n        A[MEGA-PCC: A Mamba-based Efficient Approach for Joint Geometry and Attribute Point Cloud Compression] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法依赖后处理与手动码率分配/Existing methods rely on recoloring & manual bit allocation]\n        C --> C1[端到端联合压缩框架/End-to-end joint compression framework]\n        C1 --> C2[共享编码器与双解码器/Shared encoder & dual decoders]\n        C1 --> C3[Mamba熵模型/Mamba-based Entropy Model (MEM)]\n        D --> D1[优异的率失真性能/Superior rate-distortion performance]\n        D --> D2[更高的运行时效率/Higher runtime efficiency]"
    },
    {
      "title": "Semantic contrastive learning for orthogonal X-ray computed tomography reconstruction",
      "authors": "Jiashu Dong, Jiabing Xiang, Lisheng Geng, Suqing Tian, Wei Zhao",
      "institution": "Beihang University",
      "link": "https://arxiv.org/pdf/2512.22674",
      "code": null,
      "tags": [
        "medical image reconstruction",
        "semantic contrastive learning",
        "orthogonal CT",
        "U-Net",
        "GAN",
        "sparse-view reconstruction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/194dd7eaf0bdee678b87a34ec7828010e9d00abcbfbc039eed3a5c81ff6f3f72_w640_q70.webp",
      "contributions": "1. Proposes a novel semantic feature contrastive learning loss function for CT reconstruction, 2. Introduces a three-stage U-Net-based architecture for coarse reconstruction, detail refinement, and semantic similarity measurement, 3. Demonstrates superior reconstruction quality and faster processing on a chest dataset with orthogonal projections.",
      "summary": "This paper addresses the problem of streak artifacts in sparse-view X-ray CT reconstruction by proposing a novel semantic contrastive learning loss and a three-stage U-Net architecture. The method improves image quality and processing speed compared to other algorithms, offering a practical solution for orthogonal CT reconstruction.",
      "mindmap": "graph TB\n        Root[”Semantic contrastive learning for orthogonal X-ray computed tomography reconstruction<br/>基于语义对比学习的正交X射线CT重建”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br/>Sparse-view CT leads to streak artifacts<br/>稀疏视图CT导致条纹伪影”] --> P1[”Ill-posed reconstruction<br/>病态重建”]\n        Method[”主要方法/Method<br/>Semantic contrastive learning & three-stage U-Net<br/>语义对比学习与三阶段U-Net”] --> M1[”Novel loss function<br/>新颖损失函数”]\n        Method --> M2[”Coarse & detail refinement<br/>粗重建与细节优化”]\n        Results[”关键结果/Results<br/>Superior quality & faster processing<br/>更优质量与更快处理”] --> R1[”Improved image quality<br/>提升图像质量”]\n        Results --> R2[”Low computational complexity<br/>低计算复杂度”]"
    },
    {
      "title": "JParc: Joint cortical surface parcellation with registration",
      "authors": "Jian Li, Karthik Gopinath, Brian L. Edlow, Adrian V. Dalca, Bruce Fischl",
      "institution": "Athinoula A. Martinos Center for Biomedical Imaging (MGH & HMS), MIT Computer Science and Artificial Intelligence Laboratory",
      "link": "https://arxiv.org/pdf/2512.22485",
      "code": null,
      "tags": [
        "medical image segmentation",
        "cortical parcellation",
        "surface registration",
        "atlas propagation",
        "deep learning",
        "geometric features"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fdaccb394b5688465ec87019fe53cfc2a72a02e29653a5e203b7187ce7f9515_w640_q70.webp",
      "contributions": "1. Proposes JParc, a novel joint framework that integrates cortical surface registration and parcellation into a single learning-based model. 2. Demonstrates that the performance improvement is primarily due to accurate registration and a learned parcellation atlas, providing an explanation for the success of learning-based methods. 3. Achieves state-of-the-art parcellation accuracy (Dice &gt;90%) on the Mindboggle dataset using only basic geometric features like sulcal depth and curvature.",
      "summary": "This paper introduces JParc, a joint framework for cortical surface registration and parcellation. It shows that combining these tasks and learning an atlas leads to superior performance, achieving over 90% Dice score on a standard dataset using simple geometric features. This accuracy can enhance brain mapping studies and clinical applications.",
      "mindmap": "graph TB\n        Root[JParc: Joint cortical surface parcellation with registration] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[学习与注册分离/Learning vs. Registration Gap]\n        Problem --> P2[需要自动精准分割/Need for Accurate Automated Parcellation]\n        Method --> M1[联合框架/Joint Registration & Parcellation Framework]\n        Method --> M2[使用浅层子网络微调/Shallow Subnetwork for Fine-tuning]\n        Method --> M3[仅用几何特征/Using Basic Geometric Features]\n        Results --> R1[Dice分数 >90%/Dice Score >90%]\n        Results --> R2[性能归因于注册与图谱/Performance Attributed to Registration & Atlas]\n        Results --> R3[提升下游应用/Enhances Downstream Applications]"
    },
    {
      "title": "SwinCCIR: An end-to-end deep network for Compton camera imaging reconstruction",
      "authors": "Minghao Dong, Xinyang Luo, Xujian Ouyang, Yongshun Xiao",
      "institution": "Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.22766",
      "code": null,
      "tags": [
        "medical imaging reconstruction",
        "Compton camera",
        "Swin Transformer",
        "end-to-end reconstruction",
        "list-mode data",
        "transposed convolution"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a15759f59069e025e4de49ef67e3dc4981218b18af8104c55acbdf36fd7cdf0_w640_q70.webp",
      "contributions": "1. Proposes SwinCCIR, a novel end-to-end deep learning framework for Compton camera imaging that directly maps list-mode events to source distribution, bypassing traditional back-projection. 2. Introduces the use of Swin Transformer blocks to model the complex relationships in the data, combined with a transposed convolution-based image generation module. 3. Demonstrates the method's effectiveness on both simulated and practical datasets, showing it overcomes artifacts and deformations inherent in conventional reconstruction.",
      "summary": "This paper proposes SwinCCIR, an end-to-end deep learning model using Swin Transformer blocks and transposed convolutions to directly reconstruct images from Compton camera list-mode data. The method bypasses the problematic back-projection step of traditional approaches. Experiments on simulated and real data show it effectively reduces artifacts and deformations, improving image quality for practical applications.",
      "mindmap": "graph TB\n        A[SwinCCIR: 康普顿相机成像重建网络] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[传统反投影导致严重伪影和变形/Back-projection causes severe artifacts & deformation]\n        B --> B2[系统误差难以校准/Systematic errors hard to calibrate]\n        C --> C1[采用Swin Transformer块/Adopts Swin Transformer blocks]\n        C --> C2[转置卷积图像生成模块/Transposed convolution generation module]\n        C --> C3[建立列表模式事件到源分布的映射/Establishes mapping from list-mode events to source distribution]\n        D --> D1[在仿真和真实数据集上验证/Validated on simulated & practical dataset]\n        D --> D2[有效克服传统成像问题/Effectively overcomes conventional imaging problems]"
    },
    {
      "title": "A Rapid GeoSAM-Based Workflow for Multi-Temporal Glacier Delineation: Case Study from Svalbard",
      "authors": "Alexandru Hegyi",
      "institution": "University of Oslo",
      "link": "https://arxiv.org/pdf/2512.22855",
      "code": null,
      "tags": [
        "remote sensing image segmentation",
        "GeoSAM",
        "glacier delineation",
        "multi-temporal mapping",
        "Sentinel-2",
        "spectral-index"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c27fe1438ded3a4be984cfee619922eb0dc9f10dc786702d828f9a22a7355cd4_w640_q70.webp",
      "contributions": "1. Proposes a semi-automatic workflow combining GeoSAM with spectral-index pre-processing for rapid glacier mapping. 2. Demonstrates the method's effectiveness for generating temporally consistent glacier outlines in a challenging Arctic environment (Svalbard). 3. Highlights the workflow's flexibility and transferability to other optical datasets due to its reliance on derived RGB imagery.",
      "summary": "This report presents a semi-automatic workflow for rapidly delineating glaciers from satellite imagery. The method uses GeoSAM guided by spectral-index prompts on Sentinel-2 data to create annual glacier outlines. The results show the approach is fast and produces consistent maps for major glaciers, though it requires some user inspection for refinement.",
      "mindmap": "graph TB\n        A[A Rapid GeoSAM-Based Workflow for Multi-Temporal Glacier Delineation: Case Study from Svalbard] --> B[核心问题/Problem: Consistent glacier boundary delineation is essential but difficult to scale across time and environments.]\n        A --> C[主要方法/Method: Combines image compositing, spectral-index pre-processing, prompt-guided GeoSAM segmentation, and post-processing.]\n        A --> D[关键结果/Results: Produces spatially coherent and temporally consistent outlines; errors are associated with small, complex features; method is fast and transferable.]"
    },
    {
      "title": "Deep Learning for Art Market Valuation",
      "authors": "Jianping Mei, Michael Moses, Jan Waelty, Yucheng Yang",
      "institution": "Cheung Kong Graduate School of Business (CKGSB), University of Zurich, Swiss Finance Institute, Art Market Consultancy",
      "link": "https://arxiv.org/pdf/2512.23078",
      "code": null,
      "tags": [
        "multi-modal learning",
        "multi-modal deep learning",
        "visual embeddings",
        "Grad-CAM",
        "hedonic regression",
        "repeated-sales dataset"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21784cb5e49e3a537b10ebbf9acd8a8be80b39a1879d7fe524e11c8045e1e665_w640_q70.webp",
      "contributions": "1. Introduces and benchmarks multi-modal deep learning models that fuse tabular data (artist, history) with visual embeddings from artwork images for art market valuation. 2. Demonstrates that visual content provides a distinct and economically significant predictive contribution, especially for fresh-to-market works lacking prior transaction history. 3. Provides interpretability analyses using Grad-CAM and embedding visualizations to show models attend to compositional and stylistic visual cues.",
      "summary": "This paper proposes using multi-modal deep learning to improve art market valuation by incorporating visual content from artwork images alongside traditional tabular data. It finds that while artist identity and history are most predictive overall, visual features provide crucial value for first-time sales where historical data is absent. The results show deep learning offers new insights for valuation, particularly in the most challenging scenarios.",
      "mindmap": "graph TB\n        A[Deep Learning for Art Market Valuation<br/>艺术市场估值的深度学习] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>How to improve art market valuation?<br/>如何改进艺术市场估值？]\n        C[主要方法/Method<br/>Multi-modal deep learning fusing tabular & image data<br/>融合表格与图像数据的多模态深度学习]\n        D[关键结果/Results<br/>Visual features help most for fresh-to-market works<br/>视觉特征对首次上市作品最有帮助]"
    },
    {
      "title": "EIR: Enhanced Image Representations for Medical Report Generation",
      "authors": "Qiang Sun, Zongcheng Ji, Yinlong Xiao, Peng Chang, Jun Yu",
      "institution": "University of Science and Technology of China, PAII Inc., Beijing University of Technology",
      "link": "https://arxiv.org/pdf/2512.23185",
      "code": null,
      "tags": [
        "medical image captioning",
        "cross-modal transformer",
        "metadata fusion",
        "domain-specific pre-training"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e359c65576903a65bf75a0600c08943744d6bd91d7b1f2e69d13330e289ce1_w640_q70.webp",
      "contributions": "1. Proposes a novel Enhanced Image Representations (EIR) method for medical report generation. 2. Introduces cross-modal transformers to effectively fuse medical metadata with image features, addressing the information asymmetry problem. 3. Leverages medical domain pre-trained models to encode chest X-ray images, bridging the domain gap between general and medical images.",
      "summary": "This paper addresses the problem of generating medical reports from chest X-ray images. It proposes the EIR method, which uses cross-modal transformers to fuse metadata with visual features and employs medical domain pre-trained models for better image representation. Experiments on MIMIC and Open-I datasets demonstrate the method's effectiveness.",
      "mindmap": "graph TB\n        Root[EIR: Enhanced Image Representations for Medical Report Generation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[报告生成耗时耗力/Report generation is time-consuming]\n        Problem --> P2[信息不对称与领域鸿沟/Information asymmetry & domain gap]\n        Method[主要方法/Method] --> M1[跨模态Transformer融合元数据/Cross-modal transformer for metadata fusion]\n        Method --> M2[医学领域预训练模型/Medical domain pre-trained model]\n        Results[关键结果/Results] --> R1[在MIMIC和Open-I数据集上验证/Validated on MIMIC & Open-I datasets]"
    },
    {
      "title": "Understanding Virality: A Rubric based Vision-Language Model Framework for Short-Form Edutainment Evaluation",
      "authors": "Arnav Gupta, Gurekas Singh Sahney, Hardik Rathi, Abhishek Chandwani, Ishaan Gupta, Pratik Narang, Dhruv Kumar",
      "institution": "Birla Institute of Technology and Science, Pilani; GenimeLabs",
      "link": "https://arxiv.org/pdf/2512.21402",
      "code": null,
      "tags": [
        "video understanding",
        "Vision-Language Models",
        "engagement prediction",
        "multimodal features",
        "YouTube Shorts",
        "regression-based evaluator"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bb4f5ae92f21f2e03de0476c0d49f5d4facf563d20f5bbb707cb9ed5960cd88_w640_q70.webp",
      "contributions": "1. A large-scale curated dataset of 11,000 YouTube Shorts videos for edutainment engagement modeling. 2. An unsupervised multimodal framework using VLMs to extract audiovisual features without handcrafted selection. 3. An explainable, regression-based evaluator that predicts engagement with strong correlation and provides feature-level interpretability.",
      "summary": "This paper addresses the problem of evaluating short-form edutainment videos by moving beyond traditional quality metrics to predict human engagement. The proposed method uses Vision-Language Models to extract unsupervised audiovisual features, clusters them, and trains a regression model to predict engagement. The results show strong correlation with actual engagement, offering a scalable and interpretable evaluation framework.",
      "mindmap": "graph TB\n        Root[Understanding Virality: A Rubric based Vision-Language Model Framework] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[传统指标无法评估真实观众参与度/Traditional metrics fail to assess real viewer engagement]\n        Method[主要方法/Method] --> M1[使用VLM提取无监督视听特征/Use VLM to extract unsupervised audiovisual features]\n        Method --> M2[聚类特征并训练回归评估器/Cluster features and train regression evaluator]\n        Results[关键结果/Results] --> R1[预测与真实参与度强相关/Strong correlation between predicted and actual engagement]\n        Results --> R2[提供可解释且可扩展的评估/Provides interpretable and scalable assessment]"
    },
    {
      "title": "A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding",
      "authors": "Christina Liu, Alan Q. Wang, Joy Hsu, Jiajun Wu, Ehsan Adeli",
      "institution": "California Institute of Technology, Stanford University",
      "link": "https://arxiv.org/pdf/2512.21414",
      "code": "https://github.com/christinaliu2020/tool-bottleneck-framework",
      "tags": [
        "medical image analysis",
        "tool-use framework",
        "vision-language model",
        "interpretability",
        "data-efficiency",
        "tool bottleneck model"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/005a0b27c7be7243a18042195a924af5ace8e6d74a858ee50cacd288da25307e_w640_q70.webp",
      "contributions": "1. Proposed the Tool Bottleneck Framework (TBF) for medical image understanding, which uses a learned Tool Bottleneck Model (TBM) to compose tool outputs instead of text-based composition. 2. Introduced a strategy for the TBM to handle arbitrary VLM tool selections, enabling flexible and robust prediction. 3. Demonstrated that the framework improves performance, interpretability, and data-efficiency, matching or surpassing deep learning classifiers and other tool-use frameworks, especially in data-limited scenarios.",
      "summary": "This paper addresses the problem that existing tool-use frameworks for image understanding perform poorly on medical images due to their reliance on text to compose specialized tools. The authors propose the Tool Bottleneck Framework (TBF), which uses a vision-language model to select tools and a learned neural network (the Tool Bottleneck Model) to fuse their outputs. Evaluations on histopathology and dermatology tasks show TBF performs on par with or better than existing methods, offering gains in data-limited settings and more interpretable predictions.",
      "mindmap": "graph TB\n        A[Tool Bottleneck Framework for Medical Image Understanding] --> B[核心问题/Problem: Text-based tool composition fails for medical images with localized features]\n        A --> C[主要方法/Method: TBF uses VLM to select tools, TBM (neural network) to fuse outputs]\n        A --> D[关键结果/Results: Matches or beats baselines, more interpretable, data-efficient]"
    },
    {
      "title": "Scalable Deep Subspace Clustering Network",
      "authors": "Nairouz Mrabah, Mohamed Bouguessa, Sihem Sami",
      "institution": "University of Quebec at Montreal",
      "link": "https://arxiv.org/pdf/2512.21434",
      "code": null,
      "tags": [
        "subspace clustering",
        "landmark-based approximation",
        "self-expression",
        "spectral clustering",
        "linear complexity",
        "convolutional auto-encoder"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7769a2896887c34d8a77b8e69c13ab64ec5b8827cb1b9e3ff0d7fff7982196e4_w640_q70.webp",
      "contributions": "1. Proposes a deep subspace clustering framework (SDSNet) that achieves linear O(n) computational complexity, breaking the traditional O(n^3) bottleneck. 2. Introduces a landmark-based approximation to avoid constructing the full n×n affinity matrix, combined with joint optimization of auto-encoder reconstruction and self-expression objectives. 3. Enables direct spectral clustering on factorized representations, integrating convolutional auto-encoders with subspace-preserving constraints for efficient and accurate clustering.",
      "summary": "The paper addresses the high computational cost (O(n^3)) of traditional subspace clustering methods. It proposes SDSNet, a scalable deep learning framework that uses landmark approximation and joint optimization to achieve linear O(n) complexity. Experiments show SDSNet maintains clustering quality comparable to state-of-the-art methods while being significantly more efficient.",
      "mindmap": "graph TB\n        Root[”Scalable Deep Subspace Clustering Network”] --> Problem[”核心问题/Problem: O(n^3) 计算复杂度 / O(n^3) Computational Complexity”]\n        Root --> Method[”主要方法/Method: 地标近似与联合优化 / Landmark Approximation & Joint Optimization”]\n        Root --> Results[”关键结果/Results: 线性复杂度与可比性能 / Linear Complexity & Comparable Performance”]"
    },
    {
      "title": "IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset",
      "authors": "Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh",
      "institution": "Simon Fraser University, AIP Labs",
      "link": "https://arxiv.org/pdf/2512.21472",
      "code": "/githubsfu-mial/IMAplusplus",
      "tags": [
        "medical image segmentation",
        "multi-annotator segmentation",
        "skin lesion segmentation",
        "dermoscopy",
        "consensus masks",
        "annotator metadata"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c892cf532aaf3e65ad92c26d2ef6701dc5d629cb5bc4b453893916992cd6089_w640_q70.webp",
      "contributions": "1. Introduces IMA++, the largest publicly available multi-annotator skin lesion segmentation dataset with 17,684 masks across 14,967 dermoscopic images. 2. Provides rich annotator metadata (e.g., skill level, tool) enabling research on annotator-specific modeling and metadata analysis. 3. Offers curated data partitions, consensus segmentation masks, and a detailed dataset analysis.",
      "summary": "This paper introduces IMA++, a large-scale public dataset to address the lack of multi-annotator data for dermoscopic skin lesion segmentation. The dataset contains thousands of images with multiple segmentation masks and annotator metadata, facilitating research into modeling annotator variability. It is presented as the largest publicly available resource of its kind for this medical imaging domain.",
      "mindmap": "graph TB\n        Root[”IMA++: ISIC Archive Multi-Annotator Dermoscopic Skin Lesion Segmentation Dataset”] --> Problem[”核心问题/Problem: Lack of large-scale public multi-annotator skin lesion segmentation datasets”]\n        Root --> Method[”主要方法/Method: Introduce ISIC MultiAnnot++ dataset with multiple masks & annotator metadata”]\n        Root --> Results[”关键结果/Results: Largest public SLS dataset (17,684 masks, 14,967 images), enables new research”]"
    },
    {
      "title": "Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism",
      "authors": "Haotian Lv, Yuhui Zhang, Jiangbo Dai, Hanli Wu, Jiaji Wang, Dawei Wang",
      "institution": "Harbin Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.21452",
      "code": null,
      "tags": [
        "object detection",
        "Ground Penetrating Radar (GPR)",
        "Multi-modal Chain Feature Fusion (MCFF)",
        "Global Attention Mechanism (GAM)",
        "DCGAN",
        "transfer learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d4debe9b33028e70ed06ab5d1f340e5cb76dcb8d09f7adf0d8195a2422c90668_w640_q70.webp",
      "contributions": "1. Proposed a DCGAN-based data augmentation strategy to synthesize high-fidelity GPR images, mitigating data scarcity. 2. Designed a novel Multi-modal Chain and Global Attention Network (MCGA-Net) integrating Multi-modal Chain Feature Fusion (MCFF) and a Global Attention Mechanism (GAM) for enhanced defect representation. 3. Utilized MS COCO transfer learning to fine-tune the backbone network, accelerating convergence and improving model generalization.",
      "summary": "This paper addresses the subjective and inefficient interpretation of Ground Penetrating Radar (GPR) images for road defect detection by proposing a comprehensive framework. The method combines DCGAN-based data augmentation, a novel MCGA-Net architecture with feature fusion and attention mechanisms, and transfer learning. The proposed model achieves high precision, recall, and robustness, establishing a new paradigm for automated GPR-based defect detection.",
      "mindmap": "graph TB\n        Root(”Intelligent recognition of GPR road hidden defect images <br/> GPR道路隐蔽病害图像智能识别”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n    \n        Problem --> P1(”Subjective & inefficient GPR interpretation <br/> GPR图像解释主观且低效”)\n        Problem --> P2(”Data scarcity <br/> 数据稀缺”)\n    \n        Method --> M1(”DCGAN-based Data Augmentation <br/> 基于DCGAN的数据增强”)\n        Method --> M2(”MCGA-Net (MCFF + GAM) <br/> MCGA-Net网络”)\n        Method --> M3(”MS COCO Transfer Learning <br/> MS COCO迁移学习”)\n    \n        Results --> R1(”High Performance (Precision 92.8%, mAP@50 95.9%) <br/> 高性能”)\n        Results --> R2(”Robust to noise & weak signals <br/> 对噪声和弱信号鲁棒”)\n        Results --> R3(”New paradigm for automated detection <br/> 自动化检测新范式”)"
    },
    {
      "title": "CCAD: Compressed Global Feature Conditioned Anomaly Detection",
      "authors": "Xiao Jin, Liang Diao, Qixin Xiao, Yifan Hu, Ziqi Zhang, Yuchen Liu, Haisong Gu",
      "institution": "Columbia University, University of Michigan, University of California at Berkeley, Stevens Institute of Technology, VisionX LLC",
      "link": "https://arxiv.org/pdf/2512.21459",
      "code": "https://github.com/chloeqxq/CCAD",
      "tags": [
        "anomaly detection",
        "global feature conditioning",
        "adaptive compression",
        "reconstruction-based anomaly detection"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6719cd2f5873e49e54895c9bbfc91fe99e3e4a74eaf10b87fc908258f60c443d_w640_q70.webp",
      "contributions": "1. Proposes CCAD, a novel method that synergizes reconstruction-based and representation-based anomaly detection by using compressed global features as a conditioning modality for the reconstruction model. 2. Introduces an adaptive compression mechanism to enhance model generalization and training efficiency. 3. Contributes a reorganized and re-annotated version of the DAGM 2007 dataset to validate the method's effectiveness.",
      "summary": "This paper addresses the challenges of anomaly detection in industrial settings with limited anomalous data by proposing CCAD, a method that combines the strengths of reconstruction-based and representation-based approaches. CCAD conditions a reconstruction model on adaptively compressed global features, leading to improved generalization and training efficiency. Experiments show that CCAD outperforms state-of-the-art methods in AUC and achieves faster convergence.",
      "mindmap": "graph TB\n        A[CCAD: Compressed Global Feature Conditioned Anomaly Detection] --> B[核心问题/Problem: 异常检测在有限异常数据下的挑战，现有方法在泛化性、效率和约束上的不足]\n        A --> C[主要方法/Method: 提出CCAD，融合重建与表征方法，使用压缩的全局特征作为重建模型的条件]\n        A --> D[关键结果/Results: 在AUC上超越SOTA，收敛更快，贡献了重新标注的DAGM 2007数据集]"
    },
    {
      "title": "GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification",
      "authors": "Suncheng Xiang, Xiaoyang Wang, Junjie Jiang, Hejia Wang, Dahong Qian",
      "institution": "Shanghai Jiao Tong University, Peking University, Shanghai Fifth People's Hospital",
      "link": "https://arxiv.org/pdf/2512.21476",
      "code": "https://github.com/JeremyXSC/GPF-Net",
      "tags": [
        "medical image retrieval",
        "polyp re-identification",
        "gated progressive fusion",
        "multimodal feature fusion"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75fe09ad1f753b61f2c30ab37c16d0deef5060ca95658846bc6dcaf6bb4c53f9_w640_q70.webp",
      "contributions": "1) Proposes a novel multimodal feature fusion framework named GPF-Net for polyp re-identification. 2) Introduces a gated progressive fusion strategy for layer-wise refinement of semantic information through multi-level feature interactions. 3) Demonstrates state-of-the-art performance on standard benchmarks, showing the benefit of multimodal fusion over unimodal methods.",
      "summary": "This paper addresses the challenge of colonoscopic polyp re-identification, where coarse high-level features harm small object matching. The authors propose GPF-Net, a Gated Progressive Fusion network that selectively fuses multi-level features using gates. Experiments show this multimodal approach outperforms state-of-the-art unimodal ReID models.",
      "mindmap": "graph TB\n        A[GPF-Net: Gated Progressive Fusion Learning for Polyp Re-Identification] --> B[核心问题/Problem: Coarse high-level features lead to inferior results for small polyps]\n        A --> C[主要方法/Method: Gated Progressive Fusion network for selective, multi-level feature fusion]\n        A --> D[关键结果/Results: Outperforms unimodal models, benefits of multimodal fusion strategy]"
    },
    {
      "title": "Generative Multi-Focus Image Fusion",
      "authors": "Xinzhe Xie, Buyu Guo, Bolin Li, Shuangyan He, Yanzhen Gu, Qingyan Jiang, Peiliang Li",
      "institution": "Zhejiang University, Donghai Laboratory",
      "link": "https://arxiv.org/pdf/2512.21495",
      "code": "https://github.com/Xinzhe99/StackMFF-Series",
      "tags": [
        "image fusion",
        "multi-focus image fusion",
        "latent diffusion models",
        "generative restoration",
        "StackMFF",
        "IFControlNet"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32d197875e157fe73427be3804a8f00aec57f7112b871c69141003d4a9a92477_w640_q70.webp",
      "contributions": "1. Proposes a novel two-stage generative framework (GMFF) for multi-focus image fusion, combining deterministic fusion with generative restoration. 2. Introduces a generative restoration stage using IFControlNet, a latent diffusion model, to reconstruct missing content and eliminate edge artifacts. 3. Demonstrates state-of-the-art performance, particularly in handling complex scenarios where not all scene areas are in focus in any input image.",
      "summary": "The paper proposes GMFF, a two-stage framework for multi-focus image fusion. It first uses StackMFF V4 for deterministic fusion and then employs a latent diffusion model (IFControlNet) for generative restoration to recover missing details and remove artifacts. Experiments show GMFF achieves state-of-the-art performance, especially for complex multi-focal content.",
      "mindmap": "graph TB\n        A[Generative Multi-Focus Image Fusion] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法假设总有对焦图像/Existing methods assume always an in-focus image]\n        B --> B2[边缘伪影/Edge artifacts]\n        C --> C1[阶段一: 确定性融合/Stage 1: Deterministic Fusion (StackMFF V4)]\n        C --> C2[阶段二: 生成式恢复/Stage 2: Generative Restoration (IFControlNet)]\n        D --> D1[SOTA性能/State-of-the-art performance]\n        D --> D2[处理复杂多焦内容/Handles complex multi-focal content]"
    },
    {
      "title": "Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering",
      "authors": "Wenyuan Yang, Jie Xu, Hongqing He, Jiangzhang Gan, Xiaofeng Zhu",
      "institution": "University of Electronic Science and Technology of China, Hainan University, Singapore University of Technology and Design, Guangxi Normal University",
      "link": "https://arxiv.org/pdf/2512.21510",
      "code": null,
      "tags": [
        "multi-view clustering",
        "incomplete multi-view clustering",
        "missing pattern tree",
        "decision ensemble",
        "knowledge distillation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/407ccf539edc974ed5d25aba6f5889d28e73dcff5d62d962f118ffbc46c8c49c_w640_q70.webp",
      "contributions": "1. Proposes a missing-pattern tree model to group data into decision sets for fully utilizing available multi-view pairs, addressing the pair under-utilization issue. 2. Introduces a multi-view decision ensemble module that uses uncertainty-based weighting to aggregate robust clustering decisions from different sets. 3. Designs an ensemble-to-individual knowledge distillation module to transfer ensemble knowledge to view-specific models, promoting mutual optimization between ensemble and individual modules.",
      "summary": "This paper addresses the problem of incomplete multi-view clustering (IMVC) where inconsistent missing patterns hinder performance. It proposes a framework called TreeEIC that groups data by missing pattern, performs clustering within each group, ensembles the results with uncertainty weighting, and uses knowledge distillation to refine individual models. Experiments show TreeEIC achieves state-of-the-art performance and robustness.",
      "mindmap": "graph TB\n        Root[”Missing Pattern Tree based Decision Grouping and Ensemble for Deep Incomplete Multi-View Clustering”] --> Problem[”核心问题/Problem: Inconsistent missing patterns in multi-view data limit clustering performance.”]\n        Root --> Method[”主要方法/Method: TreeEIC framework with missing-pattern tree grouping, decision ensemble, and knowledge distillation.”]\n        Root --> Results[”关键结果/Results: Achieves state-of-the-art IMVC performance and superior robustness.”]"
    },
    {
      "title": "Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art",
      "authors": "Md Ashik Khan, Arafat Alam Jion",
      "institution": "Indian Institute of Technology Kharagpur, Chittagong University of Engineering and Technology",
      "link": "https://arxiv.org/pdf/2512.21512",
      "code": null,
      "tags": [
        "image forensics",
        "fixed-threshold evaluation",
        "CNN-ViT hybrid",
        "gated fusion",
        "frequency-domain features",
        "cross-domain detection"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca89224e09c82fcbaf5bf9db1a5d9fb5df38a1947dd8ae3ddcb0e9c385e126aa_w640_q70.webp",
      "contributions": "1. Introduced a fixed-threshold evaluation protocol for AI-generated image detection to provide deployment-relevant robustness estimates by preventing per-condition threshold retuning. 2. Proposed a lightweight CNN-ViT hybrid model with gated fusion and optional frequency enhancement for cross-domain detection across photos and art. 3. Conducted a systematic analysis revealing a forensic-semantic spectrum, showing CNNs are fragile to compression while ViTs are robust, and that semantic patterns in artistic content are more reliable detection cues.",
      "summary": "This paper addresses the problem of misleading robustness estimates in AI-generated image detection by proposing a fixed-threshold evaluation protocol. The authors develop a hybrid CNN-ViT model with gated fusion and evaluate it across photos and art, finding that ViTs are more robust to post-processing like compression due to semantic pattern recognition, and that detection is fundamentally easier on artistic content than photorealistic images.",
      "mindmap": "graph TB\n        Root[”Fixed-Threshold Evaluation of a Hybrid CNN-ViT for AI-Generated Image Detection Across Photos and Art”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem: Misleading robustness estimates from per-condition threshold retuning in AI-generated image detection.”]\n        Method[”主要方法/Method: Fixed-threshold evaluation protocol & a lightweight CNN-ViT hybrid with gated fusion.”]\n        Results[”关键结果/Results: ViTs robust to compression; detection easier on art; hybrid offers balanced cross-domain performance.”]"
    },
    {
      "title": "SVBench: Evaluation of Video Generation Models on Social Reasoning",
      "authors": "Wenshuo Peng, Gongxuan Wang, Tianmeng Yang, Chuanhao Li, Xiaojie Xu, Hui He, Kaipeng Zhang",
      "institution": "Tsinghua University, Shanghai AI Laboratory, Peking University, Harbin Institute of Technology, The Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.21507",
      "code": "https://github.com/Gloria2tt/SVBench-Evaluation",
      "tags": [
        "video generation",
        "social reasoning",
        "benchmark",
        "agent-based pipeline",
        "VLM judge",
        "multi-agent interaction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415eaf890354c8803ae31303b38b5679ebed73bd981f8860ef00ba636922568d_w640_q70.webp",
      "contributions": "1. Introduces the first benchmark (SVBench) for evaluating social reasoning in video generation, grounded in developmental and social psychology. 2. Develops a fully training-free, agent-based pipeline to synthesize and evaluate diverse social reasoning scenarios. 3. Conducts a large-scale evaluation of seven state-of-the-art video generation models, revealing their systematic failures in social cognition.",
      "summary": "This paper introduces SVBench, the first benchmark for evaluating social reasoning in video generation models. It uses an agent-based pipeline to create and assess videos based on psychological paradigms, finding that while current models are visually realistic, they fail at understanding intentions, beliefs, and social norms.",
      "mindmap": "graph TB\n        A[SVBench: 视频生成模型的社会推理评估<br>SVBench: Evaluation of Video Generation Models on Social Reasoning] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有视频生成模型缺乏社会推理能力<br>Current models lack social reasoning]\n        C --> C1[构建基于社会心理学范式的基准<br>Build benchmark based on social psychology paradigms]\n        C --> C2[使用基于智能体的训练免费流程进行评估<br>Use training-free agent-based pipeline for evaluation]\n        D --> D1[模型在表面合理性上表现良好<br>Models perform well on surface-level plausibility]\n        D --> D2[模型在社会推理维度上系统性失败<br>Models fail systematically on social reasoning dimensions]"
    },
    {
      "title": "Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification",
      "authors": "Md Ashik Khan, Md Nahid Siddique",
      "institution": "Indian Institute of Technology Kharagpur, Florida International University",
      "link": "https://arxiv.org/pdf/2512.21508",
      "code": null,
      "tags": [
        "multi-modal training",
        "parameter-efficient training",
        "frozen encoders",
        "adapters",
        "LoRA",
        "BitFit"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3511bf13e05579029e8458f9b63afdf7f5fb3ed3a02050a524e53a4fcb570084_w640_q70.webp",
      "contributions": "1. Demonstrated that parameter-efficient training (PET) methods with frozen encoders achieve superior performance (AUROC 0.892-0.908) compared to full fine-tuning (AUROC 0.770) for multimodal chest X-Ray classification using only 2.51% of trainable parameters. 2. Conducted controlled, budget-matched experiments revealing that performance gains stem primarily from efficient parameter allocation rather than cross-modal synergy, as vision-only models outperformed multimodal models under the same parameter budget. 3. Identified and quantified a tractable limitation of PET methods—degraded model calibration (ECE: 0.29-0.34)—and suggested post-hoc calibration as a solution for clinical deployment.",
      "summary": "This paper investigates parameter-efficient training (PET) strategies for multimodal chest X-Ray classification to reduce computational costs. By freezing pre-trained encoders and training only small modules like adapters or LoRA under a fixed parameter budget, the methods significantly outperform full fine-tuning. The main conclusion is that frozen encoder PET provides superior discrimination at a fraction of the cost, though calibration correction is needed for clinical use.",
      "mindmap": "graph TB\n        A[Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 多模态胸部X光分析的计算成本高，泛化性差 / Multimodal chest X-Ray analysis is computationally costly and has poor generalization]\n        C[主要方法/Method: 使用冻结编码器和参数高效训练策略 / Use frozen encoders and PET strategies (Adapters, LoRA, BitFit)]\n        D[关键结果/Results: PET方法性能优于全微调，但校准性差 / PET methods outperform full fine-tuning but have degraded calibration]"
    },
    {
      "title": "DiverseGRPO: Mitigating Mode Collapse in Image Generation via Diversity-Aware GRPO",
      "authors": "Henglin Liu, Huijuan Huang, Jing Wang, Chang Liu, Xiu Li, Xiangyang Ji",
      "institution": "Tsinghua University, Kuaishou Technology (Kling Team), Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.21514",
      "code": null,
      "tags": [
        "diffusion models",
        "GRPO",
        "mode collapse",
        "diversity-aware reward",
        "spectral clustering",
        "structure-aware regularization"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/862e58c2beed3d5383235af01560a2dc06bc384250083e4027ddff5a3aa32368_w640_q70.webp",
      "contributions": "1. Identifies and analyzes the mode collapse problem in GRPO-based image generation from both reward modeling and generation dynamics perspectives. 2. Proposes a distributional creativity bonus reward based on semantic grouping via spectral clustering to encourage novel visual modes. 3. Introduces a structure-aware regularization that applies stronger constraints during early-stage denoising to preserve diversity without sacrificing quality optimization.",
      "summary": "This paper addresses the mode collapse problem in GRPO-based image generation, where models produce homogenized outputs. The proposed DiverseGRPO method introduces a diversity-aware reward based on semantic clustering and a structure-aware regularization to preserve generation diversity. Experiments show the method significantly improves semantic diversity while maintaining image quality, establishing a better quality-diversity trade-off.",
      "mindmap": "graph TB\n        A[DiverseGRPO: Mitigating Mode Collapse] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[GRPO导致模式崩溃/GRPO causes mode collapse]\n        B1 --> B2[缺乏视觉多样性/Lacks visual diversity]\n        C --> C1[奖励层面: 分布创造力奖励/Reward Level: Distributional Creativity Bonus]\n        C --> C2[生成层面: 结构感知正则化/Generation Level: Structure-Aware Regularization]\n        C1 --> C3[基于语义分组的谱聚类/Spectral Clustering for Semantic Grouping]\n        D --> D1[语义多样性提升13%-18%/13%-18% Semantic Diversity Improvement]\n        D --> D2[建立新的帕累托前沿/Establishes New Pareto Frontier]"
    },
    {
      "title": "MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions",
      "authors": "Puyun Wang, Kaimin Yu, Huayang He, Xianyu Wu",
      "institution": "Fuzhou University, Research Institute of Highway, Ministry of Transport",
      "link": "https://arxiv.org/pdf/2512.21513",
      "code": "https://github.com/WangPuyun/MuS-Polar3D",
      "tags": [
        "3D reconstruction",
        "polarization imaging",
        "computational imaging",
        "underwater imaging",
        "benchmark dataset",
        "multi-scattering"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1c66fd636f62e6b85894934c19824730a7d5f2125cecd8c2495b556ef0c4c42_w640_q70.webp",
      "contributions": "1. Introduces MuS-Polar3D, the first publicly available benchmark dataset for quantitative turbidity underwater polarization-based 3D imaging, featuring 42 objects captured under seven controlled scattering conditions and five viewpoints. 2. Proposes a two-stage computational imaging pipeline that decouples underwater 3D reconstruction into descattering followed by 3D reconstruction. 3. Provides extensive evaluations using multiple baseline methods, demonstrating the dataset's effectiveness for fair algorithm comparison under complex scattering conditions.",
      "summary": "This paper addresses the lack of diverse public datasets for polarization-based underwater 3D imaging by introducing MuS-Polar3D, a benchmark dataset captured under controlled multi-scattering conditions. The authors also propose a two-stage computational imaging pipeline (descattering then 3D reconstruction) for this task. The dataset enables accurate reconstruction and fair evaluation, with experiments achieving a best mean angular error of 15.49 degrees.",
      "mindmap": "graph TB\n        A[MuS-Polar3D: A Benchmark Dataset for Computational Polarimetric 3D Imaging under Multi-Scattering Conditions] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(现有数据集缺乏多样性的散射和观测条件/Existing datasets lack diversity in scattering and observation conditions)\n        C --> C1(构建包含42个物体、7种散射条件、5个视角的偏振图像基准数据集/Construct a benchmark dataset with 42 objects, 7 scattering conditions, 5 viewpoints)\n        C --> C2(提出解耦的两阶段成像流程：去散射后3D重建/Propose a decoupled two-stage pipeline: descattering then 3D reconstruction)\n        D --> D1(实现最佳平均角度误差15.49度/Achieve best mean angular error of 15.49 degrees)\n        D --> D2(首个公开的定量浑浊水下偏振3D成像基准数据集/First publicly available benchmark for quantitative turbidity underwater polarization-based 3D imaging)"
    },
    {
      "title": "Global-Graph Guided and Local-Graph Weighted Contrastive Learning for Unified Clustering on Incomplete and Noise Multi-View Data",
      "authors": "Hongqing He, Jie Xu, Wenyuan Yang, Yonghua Zhu, Guoqiu Wen, Xiaofeng Zhu",
      "institution": "Guangxi Normal University, University of Electronic Science and Technology of China, Singapore University of Technology and Design, Hainan University",
      "link": "https://arxiv.org/pdf/2512.21516",
      "code": null,
      "tags": [
        "multi-view clustering",
        "contrastive learning",
        "incomplete multi-view data",
        "noise-robust clustering",
        "graph-guided learning",
        "imputation-free"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/529caa0e85beab1a6519998120519b076cf7e6e2b9527a44331340cb6dd9574a_w640_q70.webp",
      "contributions": "1. Proposes a global-graph guided contrastive learning strategy to address the rare-paired sample issue in incomplete multi-view data by constructing a global affinity graph to form new sample pairs. 2. Introduces a local-graph weighted contrastive learning mechanism to mitigate the mis-paired sample issue caused by noise, using local neighbors to generate adaptive weights for pair-wise contrast. 3. Integrates these strategies into a unified, imputation-free framework for effective clustering on both incomplete and noisy multi-view data.",
      "summary": "This paper addresses the challenges of rare-paired and mis-paired samples in contrastive learning for multi-view clustering on incomplete and noisy data. It proposes a unified framework combining global-graph guided contrastive learning to explore complementary information and local-graph weighted contrastive learning to adaptively handle noisy pairs. Experiments show the method outperforms state-of-the-art approaches on both incomplete and noisy data settings.",
      "mindmap": "graph TB\n        A[Global-Graph Guided and Local-Graph Weighted Contrastive Learning<br/>全局-局部图引导对比学习] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>Rare-paired & Mis-paired Samples<br/>样本配对稀少与错误] --> B1[Incomplete & Noise Multi-View Data<br/>不完整与噪声多视图数据]\n        C[主要方法/Method<br/>Unified Contrastive Learning Framework<br/>统一对比学习框架] --> C1[Global-Graph Guided CL<br/>全局图引导对比学习]\n        C --> C2[Local-Graph Weighted CL<br/>局部图加权对比学习]\n        C1 --> C1a[Construct Global Affinity Graph<br/>构建全局亲和力图]\n        C2 --> C2a[Generate Adaptive Weights<br/>生成自适应权重]\n        D[关键结果/Results<br/>Superior Clustering Performance<br/>优越的聚类性能] --> D1[Outperforms SOTA Methods<br/>超越现有最佳方法]\n        D --> D2[Effective on Incomplete & Noise Data<br/>在不完整与噪声数据上有效]"
    },
    {
      "title": "Hierarchy-Aware Fine-Tuning of Vision-Language Models",
      "authors": "Jiayu Li, Rajesh Gangireddy, Samet Akcay, Wei Cheng, Juhua Hu",
      "institution": "University of Washington, Intel",
      "link": "https://arxiv.org/pdf/2512.21529",
      "code": null,
      "tags": [
        "multimodal learning",
        "hierarchical classification",
        "vision-language models",
        "efficient fine-tuning",
        "LoRA",
        "Tree-Path KL Divergence"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/88b60d2fc3dd0ad92b5ac8857f844fed2dbe51e280dfb702c26028d91c14fd92_w640_q70.webp",
      "contributions": "1. Proposes an efficient hierarchy-aware fine-tuning framework for Vision-Language Models (VLMs) that updates only a few parameters. 2. Introduces two novel loss functions: Tree-Path KL Divergence (TP-KL) for vertical consistency along label paths and Hierarchy-Sibling Smoothed Cross-Entropy (HiSCE) for horizontal consistency among sibling classes. 3. Demonstrates consistent improvements in Full-Path Accuracy and reduced Tree-based Inconsistency Error across multiple hierarchical benchmarks with minimal parameter overhead.",
      "summary": "This paper addresses the problem of adapting large Vision-Language Models (VLMs) to hierarchical classification tasks efficiently. The proposed method combines two novel hierarchy-aware loss functions (TP-KL and HiSCE) with lightweight LoRA adaptation to enforce structural consistency in predictions. Experiments show the approach improves accuracy and reduces inconsistency across taxonomy levels with minimal computational cost.",
      "mindmap": "graph TB\n        Root(”Hierarchy-Aware Fine-Tuning of Vision-Language Models”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”VLMs适应层级分类效率低/VLMs inefficient for hierarchical classification”)\n        Problem --> P2(”标准方法预测不一致/Standard methods produce inconsistent predictions”)\n        Method --> M1(”提出层级感知微调框架/Propose hierarchy-aware fine-tuning framework”)\n        Method --> M2(”结合TP-KL与HiSCE损失/Combine TP-KL and HiSCE losses”)\n        Method --> M3(”集成轻量级LoRA适配/Integrate lightweight LoRA adaptation”)\n        Results --> R1(”提升全路径精度/Improves Full-Path Accuracy”)\n        Results --> R2(”降低不一致性错误/Reduces Tree-based Inconsistency Error”)\n        Results --> R3(”参数开销最小/Minimal parameter overhead”)"
    },
    {
      "title": "Vision Transformers are Circulant Attention Learners",
      "authors": "Dongchen Han, Tianyu Li, Ziyi Wang, Gao Huang",
      "institution": "Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.21542",
      "code": "https://github.com/LeapLabTHU/Circulant-Attention",
      "tags": [
        "vision transformers",
        "circulant attention",
        "block circulant matrix with circulant blocks (BCCB)",
        "computational complexity",
        "vision transformers",
        "fast Fourier transform (FFT)"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14a3b6c919de65b8d25da885c52023b90262e94f46956f6aed612d5e62b5f19b_w640_q70.webp",
      "contributions": "1. Identifies that self-attention matrices in vision Transformers often approximate Block Circulant matrices with Circulant Blocks (BCCB). 2. Proposes a novel Circulant Attention paradigm that explicitly models the attention map as its nearest BCCB matrix. 3. Introduces an efficient computation algorithm using 2D Discrete Fourier Transform (DFT) to achieve O(N log N) complexity while largely preserving the modeling capacity of standard self-attention.",
      "summary": "This paper addresses the quadratic computational complexity of self-attention in vision Transformers by proposing Circulant Attention. The method models the attention matrix as a Block Circulant matrix with Circulant Blocks (BCCB) and leverages Fast Fourier Transform for efficient O(N log N) computation. Experiments show it effectively reduces computation cost while maintaining model performance, offering a promising alternative to standard self-attention.",
      "mindmap": "graph TB\n        A[Vision Transformers are Circulant Attention Learners] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Self-attention quadratic complexity O(N²) is computationally heavy for high-resolution vision tasks.]\n        C[主要方法/Method<br>Propose Circulant Attention, modeling attention map as nearest BCCB matrix for O(N log N) computation via FFT.]\n        D[关键结果/Results<br>Reduces complexity to O(N log N), maintains model capacity, validated on diverse vision tasks.]"
    },
    {
      "title": "Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration",
      "authors": "Unnati Saraswat, Tarun Rao, Namah Gupta, Shweta Swami, Shikhar Sharma, Prateek Narang, Dhruv Kumar",
      "institution": "Birla Institute of Technology and Science, Pilani",
      "link": "https://arxiv.org/pdf/2512.21560",
      "code": null,
      "tags": [
        "image editing",
        "context-aware object insertion",
        "sponsor-product logo augmentation",
        "vision-language models",
        "diffusion models"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0959c405e8f65b70f4fa16189d8d2817513788b4aa2715490a9136daff2672b7_w640_q70.webp",
      "contributions": "1. Introduces two new tasks for advertising: context-aware object insertion and sponsor-product logo augmentation. 2. Builds two new datasets with annotations for category, placement, and sponsor-product labels to support these tasks. 3. Identifies and addresses a research gap in jointly performing category reasoning, placement prediction, object generation, and seamless compositing for scene augmentation.",
      "summary": "The paper introduces two new tasks for intelligent image editing in advertising: context-aware object insertion and sponsor-product logo augmentation. It proposes to address these tasks using vision-language models and diffusion models, and builds new datasets to support them. The main conclusion is that a significant research gap exists in systems that can jointly reason about context, placement, generation, and compositing for realistic scene augmentation.",
      "mindmap": "graph TB\n        A[Toward Intelligent Scene Augmentation<br>智能场景增强] --> B(Problem: Existing image editing lacks contextual appropriateness<br>核心问题: 现有图像编辑缺乏上下文合理性)\n        A --> C(Method: Introduce two new tasks & build datasets<br>主要方法: 提出两个新任务并构建数据集)\n        A --> D(Results: Identifies research gap for joint reasoning & generation<br>关键结果: 指出了联合推理与生成的研究空白)\n        B --> E(Task 1: Context-aware object insertion<br>任务1: 上下文感知物体插入)\n        B --> F(Task 2: Sponsor-product logo augmentation<br>任务2: 赞助商产品商标增强)\n        C --> G(Utilize VLMs and diffusion models<br>利用视觉语言模型和扩散模型)\n        C --> H(Build annotated datasets<br>构建带标注的数据集)"
    },
    {
      "title": "EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal",
      "authors": "Sanghyun Jo, Donghwan Lee, Eunji Jung, Seong Je Oh, Kyungsu Kim",
      "institution": "Seoul National University, OGQ",
      "link": "https://arxiv.org/pdf/2512.21545",
      "code": null,
      "tags": [
        "image inpainting",
        "object removal",
        "dataset-free",
        "test-time adaptation",
        "multimodal large-language model",
        "background-aware reasoning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05f86516075c3f56dc02ca42051c7afa7154583c986a395447b42dd8dc4bf354_w640_q70.webp",
      "contributions": "1. Proposes a novel dataset-free framework, EraseLoRA, that replaces attention manipulation with background-aware reasoning and test-time adaptation for object removal. 2. Introduces Background-aware Foreground Exclusion (BFE), which uses an MLLM to separate target foreground, non-target foregrounds, and clean background from a single image-mask pair without supervision. 3. Introduces Background-aware Reconstruction with Subtype Aggregation (BRSA), a test-time optimization that treats background subtypes as complementary pieces and enforces their consistent integration through reconstruction and alignment objectives.",
      "summary": "The paper addresses the challenge of object removal in inpainting, where existing dataset-free methods often regenerate unwanted objects or disrupt details. It proposes EraseLoRA, a framework that uses an MLLM to separate image components and performs test-time optimization to reconstruct the background coherently. The method shows consistent improvements over dataset-free baselines and competitive results against dataset-driven approaches.",
      "mindmap": "graph TB\n        A[EraseLoRA: MLLM-Driven Foreground Exclusion and Background Subtype Aggregation for Dataset-Free Object Removal] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Object removal must prevent target reappearance and reconstruct occluded background with fidelity, unlike common inpainting. Existing attention-redirecting methods regenerate unwanted objects and disrupt details.]\n        C[主要方法/Method: 1. Background-aware Foreground Exclusion (BFE): Uses MLLM to separate target, non-target foregrounds, and clean background. 2. Background-aware Reconstruction with Subtype Aggregation (BRSA): Test-time optimization for consistent background integration.]\n        D[关键结果/Results: Consistent improvements over dataset-free baselines; competitive results against dataset-driven methods; validated as a plug-in to pretrained diffusion models.]"
    },
    {
      "title": "Exploration of Reproducible Generated Image Detection",
      "authors": "Yihang Duan",
      "institution": "Not explicitly stated in the provided content. (Author name only, no affiliation or email domain provided)",
      "link": "https://arxiv.org/pdf/2512.21562",
      "code": null,
      "tags": [
        "image forensics",
        "AIGC detection",
        "reproducibility",
        "generalizability",
        "diffusion models",
        "binary classification"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c9df3d6873df2ea90e378adf52625583637b76319eb9a55ebf11b8f17abf1fc_w640_q70.webp",
      "contributions": "1. Identifies and analyzes the root causes of poor reproducibility in AIGC image detection research, citing omitted experimental details and overfitting to generator-specific features. 2. Provides empirical evidence for the reproducibility issue by constructing a test dataset and reproducing a representative detection method, demonstrating performance drops under cross-generator testing. 3. Proposes reference directions for the research community to improve reproducibility and generalizability, such as more comprehensive disclosure of experimental details.",
      "summary": "This paper investigates the reproducibility and generalizability challenges in AI-Generated Content (AIGC) image detection. By reviewing key literature, building a test dataset, and reproducing a detection method, it identifies causes like omitted experimental details and model overfitting. The study concludes that while basic performance can be reproduced, detection fails when preprocessing disrupts key features or when testing across different generators, highlighting the need for better methodological disclosure and robustness.",
      "mindmap": "graph TB\n        A[Exploration of Reproducible Generated Image Detection] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Poor Reproducibility & Generalizability]\n        C[主要方法/Method<br>Literature Review, Dataset Construction, Method Reproduction]\n        D[关键结果/Results<br>Performance Drops with Preprocessing/Cross-Generator Tests]"
    },
    {
      "title": "Towards Long-window Anchoring in Vision-Language Model Distillation",
      "authors": "Haoyi Zhou, Shuo Li, Tianyu Chen, Qi Song, Chonghan Gao, Jianxin Li",
      "institution": "Beihang University, Zhongguancun Laboratory",
      "link": "https://arxiv.org/pdf/2512.21576",
      "code": null,
      "tags": [
        "multi-modal training",
        "knowledge distillation",
        "long-context",
        "rotary position embeddings (RoPE)",
        "attention mechanism",
        "vision-language models"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4663870a97f8c352e6cd352d0f9f9be365648a5545642796d256fa99c7ddcd4_w640_q70.webp",
      "contributions": "1. Identifies the problem of limited effective context windows in small, distilled vision-language models despite using identical positional embeddings and architectures as their larger counterparts. 2. Proposes LAid, a novel distillation method featuring progressive distance-weighted attention matching and learnable RoPE response gain modulation to transfer long-range attention mechanisms. 3. Demonstrates that LAid-distilled models achieve significantly longer effective context windows (up to 3.2x) while maintaining performance on standard benchmarks, and provides spectral analysis showing successful transfer of low-frequency attention components.",
      "summary": "This paper addresses the problem that small, distilled vision-language models have much shorter effective context windows than their large teacher models. The authors propose LAid, a new distillation method that transfers long-range attention capabilities via progressive attention matching and learnable RoPE modulation. Their method successfully extends the context window of small models by up to 3.2 times while preserving performance on standard benchmarks.",
      "mindmap": "graph TB\n        A[Towards Long-window Anchoring in Vision-Language Model Distillation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Small distilled VLMs have limited effective context windows]\n        C[主要方法/Method: LAid - Progressive attention matching & learnable RoPE modulation]\n        D[关键结果/Results: Achieves up to 3.2x longer context, maintains benchmark performance]"
    },
    {
      "title": "LLM-Free Image Captioning Evaluation in Reference-Flexible Settings",
      "authors": "Shinnosuke Hirano, Yuiga Wada, Kazuki Matsuda, Seitaro Otsuki, Komei Sugiura",
      "institution": "Keio University",
      "link": "https://arxiv.org/pdf/2512.21582",
      "code": null,
      "tags": [
        "image captioning evaluation",
        "LLM-free evaluation",
        "reference-flexible",
        "supervised metric",
        "image-caption similarity",
        "human-annotated dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad4cf942f6b913d144878df5c7f41a2b4ea396044f5286c9aeed1071cfa693a1_w640_q70.webp",
      "contributions": "1. Proposes Pearl, a novel LLM-free supervised metric for image captioning that works in both reference-based and reference-free settings. 2. Introduces a novel mechanism to learn representations of image-caption and caption-caption similarities. 3. Constructs a large-scale human-annotated dataset for metric evaluation, comprising ~333k judgments from over 2k annotators across 75k+ images.",
      "summary": "This paper addresses the issues of bias in LLM-based and the limited performance of existing LLM-free metrics for evaluating image captions. It proposes Pearl, a fast, LLM-free supervised metric that learns joint image-caption and caption-caption similarity representations and is applicable to both reference-based and reference-free settings. Pearl outperforms other LLM-free metrics on multiple benchmarks, demonstrating higher alignment with human judgment without the neutrality and speed concerns of LLM-based approaches.",
      "mindmap": "graph TB\n        Root(”LLM-Free Image Captioning Evaluation in Reference-Flexible Settings”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”LLM-based metrics lack neutrality/LLM指标缺乏中立性”)\n        Problem --> P2(”LLM-free metrics lack performance/无LLM指标性能不足”)\n        Method --> M1(”Propose Pearl metric/提出Pearl指标”)\n        Method --> M2(”Learn image-caption & caption-caption similarity/学习图像-描述与描述-描述相似性”)\n        Method --> M3(”Construct large human-annotated dataset/构建大规模人工标注数据集”)\n        Results --> R1(”Outperforms LLM-free metrics on benchmarks/在基准测试中超越其他无LLM指标”)\n        Results --> R2(”Works in reference-based & reference-free settings/适用于有参考和无参考设置”)\n        Results --> R3(”Fast & aligned with human judgment/快速且与人类判断一致”)"
    },
    {
      "title": "UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation",
      "authors": "Linxuan Fan, Juntao Jiang, Weixuan Liu, Zhucun Xue, Jiajun Lv, Jiangning Zhang, Yong Liu",
      "institution": "Not explicitly stated in the provided content. Could be inferred from author names but no affiliations/domains are given.",
      "link": "https://arxiv.org/pdf/2512.21584",
      "code": "this",
      "tags": [
        "medical image segmentation",
        "Mamba",
        "state-space model",
        "knowledge distillation",
        "lightweight model",
        "U-Net"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089b376dfef7caeeae8830bbdef8bf95a5c6ff909a643481b728550d7ff3d937_w640_q70.webp",
      "contributions": "1. Proposes a lightweight Global–Local Multi-branch Perception Module integrating global context and local features. 2. Demonstrates a bidirectional Mamba with shared weights for enhanced information flow without extra parameters. 3. Introduces a hybrid knowledge distillation strategy to create an ultra-compact, high-performance variant.",
      "summary": "This paper proposes UltraLBM-UNet, a lightweight model for skin lesion segmentation that combines a bidirectional Mamba for global modeling with multi-branch local feature perception. It achieves state-of-the-art accuracy on benchmark datasets with minimal parameters and computational cost, and a distilled variant further compresses the model, making it suitable for point-of-care deployment.",
      "mindmap": "graph TB\n        A[UltraLBM-UNet: Ultralight Bidirectional Mamba-based Model for Skin Lesion Segmentation] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Existing methods have limitations: low performance, high complexity for skin lesion segmentation.]\n        C[主要方法/Method<br>Lightweight U-Net variant with bidirectional Mamba for global modeling and multi-branch local feature perception.]\n        D[关键结果/Results<br>Achieves SOTA accuracy with only 0.034M params; distilled variant has 0.011M params. Suitable for point-of-care.]"
    },
    {
      "title": "From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement",
      "authors": "Jian Lang, Rongpei Hong, Ting Zhong, Leiting Chen, Qiang Gao, Fan Zhou",
      "institution": "University of Electronic Science and Technology of China, Southwestern University of Finance and Economics",
      "link": "https://arxiv.org/pdf/2512.21598",
      "code": null,
      "tags": [
        "multimodal learning",
        "harmful meme detection",
        "large multimodal model",
        "agent self-improvement",
        "label-free adaptation",
        "contrastive learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c54d55d69468e93b3a2275f2a3f89c24499aa76dbe87423ea8069fb10f1df9f1_w640_q70.webp",
      "contributions": "1. Proposes ALARM, the first label-free harmful meme detection framework using LMM agent self-improvement. 2. Introduces a Confidence-based Explicit Meme Identification mechanism to isolate and pseudo-label explicit memes. 3. Introduces a Pairwise Learning Guided Agent Self-Improvement paradigm that uses contrastive pairs to refine the agent's ability to handle complex memes.",
      "summary": "This paper proposes ALARM, a label-free framework for detecting harmful memes by using a Large Multimodal Model agent that self-improves by learning from easy, explicit examples to handle complex, subtle ones. The method outperforms label-driven approaches on three datasets, demonstrating strong adaptability to evolving online content.",
      "mindmap": "graph TB\n        A[From Shallow Humor to Metaphor: Towards Label-Free Harmful Meme Detection via LMM Agent Self-Improvement] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Harmful meme detection requires costly labeled data and struggles to adapt to evolving content.]\n        C[主要方法/Method: ALARM framework uses LMM agent self-improvement via confidence-based explicit meme identification and pairwise contrastive learning.]\n        D[关键结果/Results: Superior performance on three datasets, outperforms label-driven methods, shows strong adaptability.]"
    },
    {
      "title": "GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians",
      "authors": "Bintao He, Yiran Cheng, Hongjia Li, Xiang Gao, Xin Gao, Fa Zhang, Renmin Han",
      "institution": "Shandong University, Ningxia Medical University, Beijing Institute of Technology, King Abdullah University of Science and Technology (KAUST)",
      "link": "https://arxiv.org/pdf/2512.21599",
      "code": null,
      "tags": [
        "cryo-EM image analysis",
        "3D Gaussians",
        "conformational heterogeneity",
        "two-encoder-one-decoder",
        "pseudo-atomic model",
        "cryo-EM"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b95f9f3fcbc3c6429b2ee462e30233f1b388c03440a44273a62612612e5e7244_w640_q70.webp",
      "contributions": "1. Proposes GaussianEM, a novel Gaussian pseudo-atomic framework for modeling compositional and conformational heterogeneity from cryo-EM images. 2. Introduces a two-encoder-one-decoder architecture to map images to individual Gaussian components and represent variability through Gaussian parameter changes. 3. Bridges the gap between density-based models and atomic models, providing an intuitive and interpretable description of conformational changes while preserving local structural consistency.",
      "summary": "The paper introduces GaussianEM, a method that uses 3D Gaussians to model both compositional and conformational heterogeneity in proteins from cryo-EM images. It employs a two-encoder-one-decoder architecture to map images to Gaussian components, representing structural changes through parameter variations. The method is shown to be effective on both simulated and experimental datasets, offering an interpretable bridge between density maps and atomic models.",
      "mindmap": "graph TB\n        A[GaussianEM: Model compositional and conformational heterogeneity using 3D Gaussians] --> B(核心问题/Problem: Analyzing cryo-EM datasets with continuous motions and discrete states is challenging.)\n        A --> C(主要方法/Method: A Gaussian pseudo-atomic framework with a two-encoder-one-decoder architecture to map images to Gaussians.)\n        A --> D(关键结果/Results: Provides interpretable conformational change description, bridges density-atomic model gap, and demonstrates effectiveness.)"
    },
    {
      "title": "Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data in Emergency and Critical Care",
      "authors": "Yusuf Brima, Marcellin Atemkeng",
      "institution": "Osnabrück University, Rhodes University, National Institute for Theoretical and Computational Sciences (NITheCS)",
      "link": "https://arxiv.org/pdf/2512.21602",
      "code": null,
      "tags": [
        "imbalanced classification",
        "XGBoost",
        "TabNet",
        "TabResNet",
        "Bayesian hyperparameter search",
        "class imbalance"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44b64e154deeb74ff2b3607779dfb325b743068924af0e0b1b55c3172d889db8_w640_q70.webp",
      "contributions": "1. Systematic evaluation of robustness and scalability for classical ML and deep learning models on imbalanced clinical tabular data. 2. Introduction of TabResNet, a custom lightweight residual network designed as a computationally efficient alternative to TabNet for real-time clinical use. 3. Evidence-based finding that tree-based ensemble methods (especially XGBoost) offer the most stable and scalable performance for high-stakes, time-sensitive clinical environments.",
      "summary": "This paper evaluates the robustness and scalability of machine learning models on imbalanced clinical data from emergency and critical care settings. It proposes TabResNet, a lightweight deep learning alternative to TabNet, and compares it against tree-based methods like XGBoost. The main conclusion is that tree-based ensemble models, particularly XGBoost, provide the most stable performance and computational scalability for practical clinical deployment, outperforming more complex deep learning architectures in these environments.",
      "mindmap": "graph TB\n        A[Robustness and Scalability Of Machine Learning for Imbalanced Clinical Data<br>不平衡临床数据中机器学习的鲁棒性与可扩展性] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Imbalanced clinical data in emergency/critical care<br>急诊/重症监护中的不平衡临床数据]\n        C[主要方法/Method<br>Systematic evaluation of tree-based models, TabNet, and proposed TabResNet<br>系统评估树模型、TabNet及提出的TabResNet]\n        D[关键结果/Results<br>XGBoost most robust & scalable; deep models degrade under imbalance<br>XGBoost最鲁棒且可扩展；深度学习模型在不平衡下性能下降]"
    },
    {
      "title": "TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant",
      "authors": "Rongpei Hong, Jian Lang, Ting Zhong, Yong Wang, Fan Zhou",
      "institution": "University of Electronic Science and Technology of China, Aiwen Technology Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.21616",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "MLLM personalization",
        "long-context",
        "training-free",
        "state-aware",
        "retrieval-augmented generation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a0916d89499cb78545669c049322f7cc673c2b69f516444a870dd8869bb13521_w640_q70.webp",
      "contributions": "1. Proposes LCMP, the first Long-Context MLLM Personalization benchmark for evaluating personalized dialogues. 2. Introduces TAME, a novel training-free and state-aware framework using double memories to manage concept variations. 3. Presents the RA2G (Retrieve-then-Align Augmented Generation) paradigm to align retrieved knowledge with current queries for better interaction.",
      "summary": "This paper addresses the limitation of existing MLLM personalization methods in handling long-context dialogues. It proposes a training-free, state-aware framework called TAME, which uses double memories and a novel retrieval-augmentation paradigm (RA2G) to manage personalized concepts over time. Experiments on the new LCMP benchmark show TAME achieves the best performance, enabling evolving interactions in long-context scenarios.",
      "mindmap": "graph TB\n        A[TAMEing Long Contexts in Personalization<br>论文标题] --> B[Problem: Existing MLLM personalization lacks long-context support<br>核心问题: 现有MLLM个性化方法缺乏长上下文支持]\n        A --> C[Method: Proposes TAME framework with double memory & RA2G<br>主要方法: 提出TAME框架, 包含双重记忆和RA2G范式]\n        A --> D[Results: TAME achieves best performance on LCMP benchmark<br>关键结果: TAME在LCMP基准测试中取得最佳性能]"
    },
    {
      "title": "SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration",
      "authors": "Zhiyuan Liu, Daocheng Fu, Pinlong Cai, Lening Wang, Ying Liu, Yilong Ren, Botian Shi, Jianqiang Wang",
      "institution": "Tsinghua University, Shanghai Artificial Intelligence Laboratory, Beihang University",
      "link": "https://arxiv.org/pdf/2512.21618",
      "code": null,
      "tags": [
        "novel view synthesis",
        "diffusion models",
        "3D Gaussian Splatting",
        "auto-regressive restoration",
        "context-aware inpainting",
        "view synthesis"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc8335a8d121faeeab0173601f0f5c37fa7781ac9d3431d854039722cd203b51_w640_q70.webp",
      "contributions": "1. Proposes SymDrive, a unified diffusion-based framework for joint high-quality rendering and scene editing in autonomous driving simulation. 2. Introduces a Symmetric Auto-regressive Online Restoration paradigm for recovering fine-grained details and generating consistent lateral views. 3. Leverages the restoration capability for a training-free harmonization mechanism, treating vehicle insertion as context-aware inpainting to ensure lighting and shadow consistency.",
      "summary": "SymDrive addresses the challenge of creating high-fidelity and controllable 3D driving simulators by proposing a unified diffusion-based framework. It uses a symmetric auto-regressive online restoration method to enhance novel-view synthesis and a training-free harmonization mechanism for realistic vehicle insertion. The method achieves state-of-the-art performance in both rendering quality and scene editing realism.",
      "mindmap": "graph TB\n        A[SymDrive: Realistic and Controllable Driving Simulator via Symmetric Auto-regressive Online Restoration] --> B[核心问题/Problem: 现有方法难以同时实现高保真渲染和交互式交通编辑 / Existing methods struggle with joint photorealistic rendering and interactive traffic editing.]\n        A --> C[主要方法/Method: 提出对称自回归在线修复范式与免训练协调机制 / Proposes Symmetric Auto-regressive Online Restoration paradigm and training-free harmonization mechanism.]\n        A --> D[关键结果/Results: 在新视角增强和3D车辆插入中实现最先进性能 / Achieves SOTA performance in novel-view enhancement and realistic 3D vehicle insertion.]"
    },
    {
      "title": "CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective",
      "authors": "Zhiwen Yang, Jinglin Xu, Yuxin Pen",
      "institution": "Peking University, University of Science and Technology Beijing",
      "link": "https://arxiv.org/pdf/2512.21617",
      "code": "https://github.com/PKU-ICST-MIPL/CausalFSFG_TMM",
      "tags": [
        "fine-grained visual categorization",
        "causal intervention",
        "structural causal model",
        "few-shot learning",
        "interventional multi-scale encoder",
        "interventional masked feature reconstruction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdf9390290fc2d16f99be55ee74ab9d2d794c0c3343cf9ea92ae99e679f1d456_w640_q70.webp",
      "contributions": "1. Proposes a causal perspective to address the confounding effect of support samples in FS-FGVC, framing the task as a causal inference problem. 2. Introduces a novel CausalFSFG method with two key components: an Interventional Multi-Scale Encoder (IMSE) for sample-level intervention and an Interventional Masked Feature Reconstruction (IMFR) module for feature-level intervention. 3. Demonstrates state-of-the-art performance on standard FS-FGVC datasets (CUB-200-2011, Stanford Dogs, Stanford Cars) through extensive experiments.",
      "summary": "This paper identifies that the support sample set acts as a confounding variable in Few-Shot Fine-Grained Visual Categorization (FS-FGVC), introducing bias and spurious correlations. To address this, the authors propose CausalFSFG, a method that uses causal intervention via a sample-level and a feature-level module to reveal true causal relationships. The approach achieves new state-of-the-art results on multiple benchmark datasets.",
      "mindmap": "graph TB\n        Root[CausalFSFG: Rethinking Few-Shot Fine-Grained Visual Categorization from Causal Perspective]\n        Root --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[Support set as confounder/支持集作为混淆变量]\n        Problem --> P2[Biased data distribution/有偏数据分布]\n        Problem --> P3[Spurious correlations/虚假关联]\n        Method --> M1[Causal Intervention/因果干预]\n        Method --> M2[IMSE: Sample-level/IMSE: 样本层面]\n        Method --> M3[IMFR: Feature-level/IMFR: 特征层面]\n        Results --> R1[SOTA Performance/最优性能]\n        Results --> R2[Datasets: CUB, Dogs, Cars/数据集: CUB, Dogs, Cars]"
    },
    {
      "title": "Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints",
      "authors": "Mutiara Shabrina, Nova Kurnia Putri, Jefri Satria Ferdiansyah, Sabita Khansa Dewi, Novanto Yudistira",
      "institution": "Universitas Brawijaya",
      "link": "https://arxiv.org/pdf/2512.21637",
      "code": null,
      "tags": [
        "image editing",
        "StyleGAN2",
        "CLIP",
        "L1 regularization",
        "latent space manipulation",
        "attribute disentanglement"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d3d2b6fa78e88b5a6e668468f53e16660057860c14bd7c81ab11e3e9b86d455_w640_q70.webp",
      "contributions": "1. An empirical analysis identifying that L2-based regularization in the PPE framework leads to dense latent updates and attribute leakage. 2. The introduction of a sparsity-based constraint using L1 regularization (referred to as an ultra-strict layer masking strategy) for latent space manipulation. 3. Demonstration that the proposed approach enforces more focused edits, reducing unintended changes and better preserving facial identity.",
      "summary": "This paper addresses the problem of attribute entanglement in text-guided image editing. It analyzes the PPE framework and proposes a new method using L1 regularization to enforce sparsity in latent space updates. The results show this approach achieves more controlled edits with less unintended semantic leakage.",
      "mindmap": "graph TB\n        A[Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints] --> B(核心问题/Problem: Attribute Entanglement in Text-Driven Image Editing)\n        A --> C(主要方法/Method: Sparse Latent Constraints via L1 Regularization)\n        A --> D(关键结果/Results: More Focused Edits, Reduced Unintended Changes)"
    },
    {
      "title": "Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding",
      "authors": "Zhiwang Zhou, Yuandong Pu, Xuming He, Yidi Liu, Yixin Chen, Junchao Gong, Xiang Zhuang, Wanghan Xu, Qinglong Cao, Shixiang Tang, Yihao Liu, Wenlong Zhang, Lei Bai",
      "institution": "Shanghai AI Laboratory, Tongji University, Shanghai Jiao Tong University, Zhejiang University, University of Science and Technology of China, UCLA",
      "link": "https://arxiv.org/pdf/2512.21643",
      "code": "https://github.com/Zhouzone/OmniWeather",
      "tags": [
        "multimodal weather modeling",
        "multimodal foundation model",
        "weather generation",
        "weather understanding",
        "Chain-of-Thought",
        "self-attention"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df6816c446631f92d0b9ba66f4d2f4ddda08200edc161b65555c257b6a7b7a85_w640_q70.webp",
      "contributions": "1. Proposes Omni-Weather, the first unified multimodal foundation model that integrates weather generation and understanding in a single architecture. 2. Introduces a Chain-of-Thought dataset for causal reasoning in weather generation to improve interpretability and perceptual quality. 3. Demonstrates through experiments that generative and understanding tasks in weather can mutually enhance each other, achieving state-of-the-art performance.",
      "summary": "This paper addresses the gap between weather prediction and interpretation by proposing Omni-Weather, a unified multimodal foundation model that combines generation and understanding using a shared self-attention mechanism and a novel Chain-of-Thought dataset. The model achieves state-of-the-art results in both tasks, showing that they can mutually benefit each other.",
      "mindmap": "graph TB\n        A[Omni-Weather: 统一多模态天气基础模型 / Unified Multimodal Foundation Model for Weather] --> B\n        A --> C\n        A --> D\n        B[核心问题 / Problem: 天气建模中生成与理解任务分离 / Separation of generation and understanding in weather modeling]\n        C[主要方法 / Method: 统一架构，共享自注意力，思维链数据集 / Unified architecture, shared self-attention, Chain-of-Thought dataset]\n        D[关键结果 / Results: SOTA性能，任务互增强 / State-of-the-art performance, mutual enhancement of tasks]"
    },
    {
      "title": "TrackTeller: Temporal Multimodal 3D Grounding for Behavior-Dependent Object References",
      "authors": "Jiahong Yu, Ziqi Wang, Hailiang Zhao, Wei Zhai, Xueqiang Yan, Shuiguang Deng",
      "institution": "Zhejiang University, Fudan University, Huawei Technologies Ltd.",
      "link": "https://arxiv.org/pdf/2512.21641",
      "code": null,
      "tags": [
        "3D object grounding",
        "temporal multimodal grounding",
        "LiDAR-image fusion",
        "language-conditioned decoding",
        "UniScene representation",
        "NuPrompt benchmark"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc60054c622874cc83217afc715702b65eb56126f722620ffb7004f46ebe296d_w640_q70.webp",
      "contributions": "1. Proposes TrackTeller, a unified temporal multimodal framework for 3D grounding that integrates LiDAR-image fusion, language-conditioned decoding, and temporal reasoning. 2. Introduces a shared UniScene representation aligned with textual semantics to generate language-aware 3D proposals. 3. Demonstrates significant performance improvements on the NuPrompt benchmark, including a 70% relative gain in Average Multi-Object Tracking Accuracy and a 3.15-3.4x reduction in False Alarm Frequency.",
      "summary": "This paper addresses the problem of grounding natural language references to objects in dynamic 3D driving scenes, which often depend on recent motion or behavior. The authors propose TrackTeller, a framework that fuses LiDAR and camera data with language, builds a unified scene representation, and uses temporal reasoning to refine object identification. Experiments show that TrackTeller significantly outperforms existing baselines in language-grounded tracking accuracy.",
      "mindmap": "graph TB\n        A[TrackTeller: Temporal Multimodal 3D Grounding] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[动态3D场景中的行为依赖语言指代/Dynamic 3D Behavior-Dependent Language Grounding]\n        C --> C1[统一多模态时序框架/Unified Temporal Multimodal Framework]\n        C1 --> C2[LiDAR-图像融合与语言解码/LiDAR-Image Fusion & Language Decoding]\n        C1 --> C3[构建UniScene表示/Build UniScene Representation]\n        C1 --> C4[利用运动历史推理/Reason with Motion History]\n        D --> D1[在NuPrompt上显著提升性能/Significant Improvement on NuPrompt]\n        D1 --> D2[AMOTA提升70%/70% AMOTA Gain]\n        D1 --> D3[误报率降低3.15-3.4倍/3.15-3.4x FA Reduction]"
    },
    {
      "title": "The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds",
      "authors": "Subramanyam Sahoo, Jared Junkin",
      "institution": "University of California, Berkeley, Johns Hopkins University",
      "link": "https://arxiv.org/pdf/2512.21670",
      "code": "https://github.com/SubramanyamSahoo/The-Deepfake-Detective",
      "tags": [
        "deepfake detection",
        "mechanistic interpretability",
        "sparse autoencoder",
        "forensic manifold analysis",
        "feature selectivity",
        "vision-language model"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0975c346afcfa8a9faf58f7e59aefe76bd1a40a3922d1a230951ff391c9a39_w640_q70.webp",
      "contributions": "1. Introduces a novel mechanistic interpretability framework specifically for deepfake detection, combining sparse autoencoder analysis with forensic manifold analysis. 2. Demonstrates that only a small fraction of latent features are actively used in each layer of the model for detection. 3. Shows that geometric properties of the model's feature manifold (e.g., intrinsic dimensionality, curvature) vary systematically with different types of deepfake artifacts, linking learned features to specific forensic cues.",
      "summary": "This paper addresses the \"black box\" nature of deepfake detectors by proposing a mechanistic interpretability framework that analyzes a vision-language model's internal representations. The method uses sparse autoencoders and a novel forensic manifold analysis to probe how the model's features respond to forensic artifacts. The key findings are that detection relies on a sparse set of features and that the geometry of the feature manifold correlates with specific artifact types, providing a step towards more interpretable and robust detectors.",
      "mindmap": "graph TB\n        A[The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[深度伪造检测器是黑盒模型/Deepfake detectors are black boxes]\n        C --> C1[稀疏自编码器分析/Sparse Autoencoder (SAE) Analysis]\n        C --> C2[法证流形分析/Forensic Manifold Analysis]\n        D --> D1[潜在特征稀疏使用/Latent features are sparsely used]\n        D --> D2[流形几何特性揭示伪影/Manifold geometry reveals artifacts]"
    },
    {
      "title": "Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles",
      "authors": "Jalal Khan",
      "institution": "United Arab Emirates University",
      "link": "https://arxiv.org/pdf/2512.21673",
      "code": null,
      "tags": [
        "object detection",
        "YOLO-NAS",
        "YOLOv8",
        "perception",
        "autonomous vehicles",
        "custom dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4a67f4b1902039d3a0f1a96acadea1a1625b1870da583b146bb58337f3c0561_w640_q70.webp",
      "contributions": "1. Conducted a comparative performance analysis of two emerging deep learning models, YOLO-NAS and YOLOv8, for object detection in autonomous vehicle perception. 2. Created and utilized a custom dataset to evaluate the models under real-world use case scenarios. 3. Provided empirical results showing YOLOv8s offers a 75% reduction in training time and a 2% higher object detection accuracy (83% vs 81%) compared to YOLO-NAS.",
      "summary": "This paper compares the performance of YOLO-NAS and YOLOv8 deep learning models for object detection in autonomous vehicle perception using a custom dataset. The analysis finds that the YOLOv8s model is significantly faster to train and achieves slightly higher detection accuracy than the YOLO-NAS model.",
      "mindmap": "graph TB\n    A[Comparative Analysis of Deep Learning Models for Perception in Autonomous Vehicles] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[评估自动驾驶感知中深度学习模型的性能/Evaluate DL model performance for AV perception]\n    C --> C1[使用自定义数据集比较YOLO-NAS与YOLOv8/Compare YOLO-NAS and YOLOv8 using a custom dataset]\n    D --> D1[YOLOv8s训练时间减少75%/YOLOv8s saves 75% training time]\n    D --> D2[YOLOv8s准确率更高(83% vs 81%)/YOLOv8s has higher accuracy (83% vs 81%)]"
    },
    {
      "title": "UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture",
      "authors": "Shuo Cao, Jiayang Li, Xiaohui Li, Yuandong Pu, Kaiwen Zhu, Yuanting Gao, Siqi Luo, Yi Xin, Qi Qin, Yu Zhou, Xiangyu Chen, Wenlong Zhang, Bin Fu, Yu Qiao, Yihao Liu",
      "institution": "Shanghai AI Laboratory, University of Science and Technology of China, Peking University, Shanghai Jiao Tong University, Tsinghua University, Nanjing University, Sun Yat-sen University, Tele-AI",
      "link": "https://arxiv.org/pdf/2512.21675",
      "code": "https://github.com/thunderbolt215/UniPercept",
      "tags": [
        "multimodal understanding",
        "perceptual-level image understanding",
        "multimodal large language models",
        "domain-adaptive pre-training",
        "task-aligned reinforcement learning",
        "unified benchmark"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e93f4678ef30567f8d2c7b2034256f3795d2cf297d9c6c013681b974accd77a7_w640_q70.webp",
      "contributions": "1. Proposes UniPercept-Bench, a unified benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture with hierarchical definitions and large-scale datasets. 2. Develops a strong baseline model, UniPercept, trained via Domain-Adaptive Pre-Training and Task-Aligned Reinforcement Learning for robust generalization. 3. Demonstrates that UniPercept outperforms existing MLLMs on perceptual tasks and can serve as a plug-and-play reward model for text-to-image generation.",
      "summary": "This paper addresses the limited ability of Multimodal Large Language Models (MLLMs) to perceive perceptual-level image features. It introduces UniPercept, a unified framework and benchmark for perceptual-level image understanding across aesthetics, quality, structure, and texture, trained with Domain-Adaptive Pre-Training and Task-Aligned RL. The proposed model outperforms existing MLLMs and can be used as a reward model for image generation.",
      "mindmap": "graph TB\n        Root[”UniPercept: 统一感知级图像理解 / Unified Perceptual-Level Image Understanding”] --> Problem[”MLLMs感知能力有限 / MLLMs' Perceptual Ability is Limited”]\n        Root --> Method[”提出统一基准与模型 / Proposes Unified Benchmark & Model”]\n        Root --> Results[”性能超越现有模型 / Outperforms Existing Models”]\n        Problem --> P1[”感知级特征理解不足 / Limited Perceptual-Level Feature Understanding”]\n        Method --> M1[”UniPercept-Bench基准 / UniPercept-Bench Benchmark”]\n        Method --> M2[”DAPT与Task-Aligned RL训练 / DAPT & Task-Aligned RL Training”]\n        Results --> R1[”在VR与VQA任务上泛化 / Generalizes on VR & VQA Tasks”]\n        Results --> R2[”可作为图像生成奖励模型 / Serves as Reward Model for Generation”]"
    },
    {
      "title": "SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration",
      "authors": "Md Motaleb Hossen Manik, Md Zabirul Islam, Ge Wang",
      "institution": "Rensselaer Polytechnic Institute",
      "link": "https://arxiv.org/pdf/2512.21684",
      "code": null,
      "tags": [
        "multi-modal inference",
        "blockchain provenance",
        "vision-language models",
        "semantic extraction",
        "reproducibility",
        "educational AI"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/369d8533474f19127055b5c4bf7fad048caff38e6ab3e9aca56aa3a6039a79a4_w640_q70.webp",
      "contributions": "1. Introduces SlideChain, a blockchain-backed provenance framework for verifiable integrity in multimodal semantic extraction from educational content. 2. Presents the SlideChain Slides Dataset, a curated corpus of 1,117 medical imaging lecture slides, and conducts the first systematic analysis of semantic disagreement across four state-of-the-art VLMs. 3. Demonstrates practical scalability, perfect tamper detection, and deterministic reproducibility through evaluation of gas usage, throughput, and auditability on a local EVM-compatible blockchain.",
      "summary": "This paper introduces SlideChain, a framework that uses blockchain to provide verifiable provenance for semantic outputs from vision-language models (VLMs) on educational slides. It analyzes discrepancies across VLMs and shows that SlideChain ensures tamper-evident auditability and reproducibility for AI-assisted instructional systems. The results indicate it is a practical step toward trustworthy multimodal educational pipelines.",
      "mindmap": "graph TB\n        A[SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration] --> B[核心问题/Problem: VLM语义输出难以验证、复现和审计，存在不一致性 / VLM semantic outputs are hard to verify, reproduce, and audit, with inconsistencies]\n        A --> C[主要方法/Method: 基于区块链的溯源框架，从幻灯片中提取概念和关系三元组，哈希上链 / Blockchain-backed provenance framework extracting concepts and triples, hashing to chain]\n        A --> D[关键结果/Results: 揭示模型间显著差异，实现完美篡改检测和确定性复现，提供可扩展的完整性 / Reveals cross-model discrepancies, achieves perfect tamper detection and reproducibility, provides scalable integrity]"
    },
    {
      "title": "Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation",
      "authors": "Yuntian Bo, Tao Zhou, Zechao Li, Haofeng Zhang, Ling Shao",
      "institution": "Nanjing University of Science and Technology, University of Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.21683",
      "code": "https://github.com/primebo1/C-Graph",
      "tags": [
        "medical image segmentation",
        "graph neural network",
        "contrastive learning",
        "few-shot learning",
        "cross-domain",
        "structural consistency"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9571022ab0a47d7b3509a2620b8081c3b3ecf53a3d4371337ce5b1b8a68a57c_w640_q70.webp",
      "contributions": "1. Proposes a Structural Prior Graph (SPG) layer to capture and transfer target-category node dependencies for global structure modeling. 2. Introduces a Subgraph Matching Decoding (SMD) mechanism that leverages semantic node relations to guide prediction. 3. Designs a Confusion-minimizing Node Contrast (CNC) loss to reduce node ambiguity and subgraph heterogeneity via contrastive learning.",
      "summary": "This paper addresses the problem of cross-domain few-shot medical image segmentation, where existing methods degrade performance by filtering out domain-specific information. The proposed C-Graph framework leverages the structural consistency of medical images, modeling features as graphs with novel SPG layers, SMD decoding, and a CNC loss. It achieves state-of-the-art cross-domain performance while preserving strong source-domain accuracy.",
      "mindmap": "graph TB\n        A[Contrastive Graph Modeling for Cross-Domain Few-Shot Medical Image Segmentation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法过滤域特定信息，损害性能/Existing methods filter domain-specific info, harming performance]\n        C --> C1[利用结构一致性作为先验/Use structural consistency as prior]\n        C --> C2[图建模: SPG层, SMD解码, CNC损失/Graph Modeling: SPG layer, SMD decoding, CNC loss]\n        D --> D1[跨域性能SOTA/Cross-domain SOTA performance]\n        D --> D2[保持源域精度/Preserves source-domain accuracy]"
    },
    {
      "title": "Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective",
      "authors": "Huan Li, Longjun Luo, Yuling Shi, Xiaodong Gu",
      "institution": "Huazhong University of Science and Technology, Guangdong University of Technology, Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.21691",
      "code": null,
      "tags": [
        "3D reconstruction",
        "attention collapse",
        "degenerate diffusion",
        "token-merging",
        "mean-field PDE",
        "VGGT"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81a116424b9dbb97e00548c11b9b6393d53b336e80df62d4caf4add021d7d599_w640_q70.webp",
      "contributions": "1. Provides a rigorous mathematical explanation for the attention collapse phenomenon in VGGT by modeling it as a degenerate diffusion process. 2. Derives a closed-form mean-field partial differential equation that quantitatively predicts the observed rank profile and convergence rate of token features. 3. Explains why the token-merging remedy delays the collapse by slowing the effective diffusion coefficient, offering a principled lens for future scalable 3D-vision transformers.",
      "summary": "This paper analyzes the attention collapse problem in Visual Geometry Grounded Transformers (VGGT) for 3D reconstruction. It models the global self-attention iteration as a degenerate diffusion process, proving that token features converge to a Dirac measure and deriving a mean-field PDE to predict the collapse. The theory explains the empirical observations and shows how token-merging mitigates the issue by reducing the diffusion rate.",
      "mindmap": "graph TB\n        A[Analyzing the Mechanism of Attention Collapse in VGGT from a Dynamics Perspective] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[VGGT中的注意力崩溃现象/Attention Collapse in VGGT]\n        C --> C1[将注意力迭代建模为退化扩散过程/Model Attention Iteration as Degenerate Diffusion]\n        D --> D1[推导出预测崩溃的均值场PDE/Derive Mean-Field PDE Predicting Collapse]\n        D --> D2[理论解释令牌合并的缓解作用/Theory Explains Token-Merging Remedy]"
    },
    {
      "title": "ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields",
      "authors": "Albert Barreiro, Roger Marí, Rafael Redondo, Gloria Haro, Carles Bosch",
      "institution": "Eurecat, Centre Tecnològic de Catalunya; Universitat Pompeu Fabra; Universitat de Vic - UCC",
      "link": "https://arxiv.org/pdf/2512.21692",
      "code": null,
      "tags": [
        "neural rendering",
        "Neural Radiance Fields",
        "anisotropic specular reflections",
        "Anisotropic Spherical Gaussian",
        "von Mises-Fisher distribution",
        "material editing"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ad12a4dd31b4ec9b89dafef4a6d4d851fe0aa09b5e3d8c6f918d085ee2acda50_w640_q70.webp",
      "contributions": "1. Introduces ShinyNeRF, a novel NeRF framework capable of modeling both isotropic and anisotropic specular reflections. 2. Proposes a method to jointly estimate physical surface properties (normals, tangents, specular concentration, anisotropy) by approximating outgoing radiance with a mixture of isotropic von Mises-Fisher distributions. 3. Achieves state-of-the-art performance in digitizing anisotropic materials and enables interpretable material property editing.",
      "summary": "This paper introduces ShinyNeRF, a novel Neural Radiance Fields framework designed to accurately model anisotropic specular reflections, such as those on brushed metals, which previous methods struggled with. The method learns an approximation of outgoing radiance using a mixture of isotropic von Mises-Fisher distributions to jointly estimate physical surface properties. Experimental results show it achieves state-of-the-art performance and enables plausible material editing.",
      "mindmap": "graph TB\n        A[ShinyNeRF: Digitizing Anisotropic Appearance in Neural Radiance Fields] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法难以建模各向异性高光/Existing methods struggle with anisotropic specular reflections]\n        C --> C1[提出ShinyNeRF框架/Propose ShinyNeRF framework]\n        C1 --> C2[使用各向同性vMF混合近似出射辐射度/Use isotropic vMF mixture to approximate outgoing radiance]\n        C2 --> C3[联合估计法线、切线、高光参数/Jointly estimate normals, tangents, specular parameters]\n        D --> D1[实现SOTA性能/Achieves SOTA performance]\n        D --> D2[提供物理解释和材质编辑/Provides physical interpretation and material editing]"
    },
    {
      "title": "Prior-AttUNet: Retinal OCT Fluid Segmentation Based on Normal Anatomical Priors and Attention Gating",
      "authors": "Li Yang, Yuting Liu",
      "institution": "Wannan Medical College",
      "link": "https://arxiv.org/pdf/2512.21693",
      "code": null,
      "tags": [
        "medical image segmentation",
        "anatomical prior",
        "attention mechanism",
        "variational autoencoder",
        "densely connected blocks",
        "spatial pyramid pooling"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85dc59e9646562af5e8c3595b08a9f5064375d7976961a2ab7b04fe287a243d1_w640_q70.webp",
      "contributions": "1. Proposes a hybrid dual-path architecture integrating a generative prior pathway (using a VAE) with a segmentation network to provide multi-scale normative anatomical priors. 2. Introduces an anatomical prior-guided triple-attention mechanism to dynamically modulate feature importance across decoding stages for improved boundary delineation. 3. Embeds densely connected blocks and spatial pyramid pooling modules in the segmentation backbone to enhance contextual feature extraction.",
      "summary": "This paper introduces Prior-AttUNet, a model for segmenting macular edema fluid in OCT images. It uses a dual-path architecture with generative anatomical priors and a novel triple-attention mechanism to improve accuracy, especially at boundaries, and achieves high Dice scores across multiple imaging devices while maintaining low computational cost.",
      "mindmap": "graph TB\n        A[Prior-AttUNet: 视网膜OCT液体分割 / Prior-AttUNet: Retinal OCT Fluid Segmentation] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[模糊边界与设备差异 / Ambiguous Boundaries & Device Heterogeneity]\n        C --> C1[双路径架构 / Dual-Path Architecture]\n        C --> C2[变分自编码器先验 / VAE Priors]\n        C --> C3[三重注意力机制 / Triple-Attention Mechanism]\n        D --> D1[高Dice分数 / High Dice Scores]\n        D --> D2[低计算成本 / Low Computational Cost]"
    },
    {
      "title": "BeHGAN: Bengali Handwritten Word Generation from Plain Text Using Generative Adversarial Networks",
      "authors": "Md. Rakibul Islam, Md. Kamrozzaman Bhuiyan, Safwan Muntasir, Arifur Rahman Jawad, Most. Sharmin Sultana Samu",
      "institution": "Not explicitly stated in provided text. Affiliation/domain cannot be reliably inferred.",
      "link": "https://arxiv.org/pdf/2512.21694",
      "code": null,
      "tags": [
        "handwritten text generation",
        "Generative Adversarial Networks",
        "Handwritten Text Generation",
        "Bengali",
        "Dataset",
        "Pre-processing"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8b36b4afe805283b88409b54815e1bd251762075786246ae741c57cb65c191a_w640_q70.webp",
      "contributions": "1. Proposed a method for generating Bengali handwritten words from plain text using GANs, addressing a significant research gap. 2. Developed and used a novel, self-collected dataset of Bengali handwriting from approximately 500 diverse individuals. 3. Demonstrated the ability to produce diverse and realistic handwritten outputs through the described approach.",
      "summary": "This paper addresses the lack of research on Bengali handwritten text generation by proposing a GAN-based method to generate words from plain text. The authors created a new dataset of Bengali handwriting samples from hundreds of contributors. The work successfully generates diverse handwritten outputs and contributes to advancing research in this area for the Bengali language.",
      "mindmap": "graph TB\n        A[BeHGAN: Bengali Handwritten Word Generation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[HTG is challenging & understudied for Bengali<br/>孟加拉语手写文本生成研究不足且困难]\n        C --> C1[Propose GAN-based method<br/>提出基于GAN的方法]\n        C --> C2[Use self-collected dataset<br/>使用自收集数据集]\n        C --> C3[Pre-process images<br/>预处理图像]\n        D --> D1[Generates diverse handwritten words<br/>生成多样化手写词]\n        D --> D2[Contributes to Bengali HTG research<br/>推动孟加拉语手写文本生成研究]"
    },
    {
      "title": "FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection",
      "authors": "Md. Zahid Hossain, Most. Sharmin Sultana Samu, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman, Md. Rakibul Islam",
      "institution": "Not explicitly stated in provided content. Affiliation/email domain not present.",
      "link": "https://arxiv.org/pdf/2512.21695",
      "code": null,
      "tags": [
        "ai-generated image detection",
        "Fast Fourier Transform",
        "CLIP",
        "hybrid system",
        "spectral features",
        "semantic features"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9517071d67c92839f8173192ae8096327651e4417f7ed69aefae08dc78c34838_w640_q70.webp",
      "contributions": "1. Proposes FUSE, a novel hybrid detection system that fuses spectral (FFT-based) and semantic (CLIP-based) features into a joint representation. 2. Introduces a progressive two-stage training strategy to effectively learn the fused representation. 3. Demonstrates state-of-the-art robustness and generalization across multiple high-fidelity image generators and diverse benchmarks, particularly on challenging datasets like Chameleon.",
      "summary": "This paper addresses the challenge of robustly detecting AI-generated images by proposing FUSE, a hybrid method that combines spectral features from Fast Fourier Transform with semantic features from CLIP's vision encoder. The fused features are trained progressively in two stages. The approach achieves strong generalization and state-of-the-art performance across multiple datasets and generators, highlighting the benefit of integrating spectral and semantic cues.",
      "mindmap": "graph TB\n        Root[”FUSE: Unifying Spectral and Semantic Cues for Robust AI-Generated Image Detection”] --> Problem[”核心问题/Problem: Reliable detection of AI-generated images”]\n        Root --> Method[”主要方法/Method: Hybrid system fusing FFT spectral features & CLIP semantic features with two-stage training”]\n        Root --> Results[”关键结果/Results: SOTA on Chameleon, strong generalization across datasets”]"
    },
    {
      "title": "Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction",
      "authors": "Zheng Yin, Chengjian Li, Xiangbo Shu, Meiqi Cao, Rui Yan, Jinhui Tang",
      "institution": "Nanjing University of Science and Technology, Nanjing Forestry University",
      "link": "https://arxiv.org/pdf/2512.21707",
      "code": "https://github.com/alanyz106/ST-MoE",
      "tags": [
        "human motion prediction",
        "Mixture of Experts",
        "Mamba",
        "spatiotemporal dependencies",
        "computational efficiency"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfad49ae18e732f3537bb8bddb97086ffd94c3c545ca0a735b6cfe245b14f6f0_w640_q70.webp",
      "contributions": "1. Proposes a Spatiotemporal-Untrammelled Mixture of Experts (ST-MoE) model to flexibly capture complex spatio-temporal dependencies in multi-person motion. 2. Introduces four distinct spatiotemporal experts based on bidirectional spatiotemporal Mamba to achieve parameter sharing and efficiency. 3. Demonstrates superior accuracy, a 41.38% reduction in model parameters, and a 3.6x training speedup on benchmark datasets.",
      "summary": "This paper addresses the limitations of existing multi-person motion prediction methods, which suffer from inflexible spatiotemporal representations and high computational costs. The authors propose the ST-MoE model, which uses a mixture of Mamba-based experts to adaptively capture spatiotemporal patterns. The method achieves higher accuracy while being more parameter-efficient and faster to train than state-of-the-art approaches.",
      "mindmap": "graph TB\n        Root(”Spatiotemporal-Untrammelled Mixture of Experts for Multi-Person Motion Prediction”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”不灵活的时空表示/Inflexible spatiotemporal representation”)\n        Problem --> P2(”高计算成本/High computational cost”)\n        Method --> M1(”提出ST-MoE模型/Propose ST-MoE model”)\n        M1 --> M2(”四种时空专家/Four spatiotemporal experts”)\n        M2 --> M3(”双向时空Mamba/Bidirectional spatiotemporal Mamba”)\n        Results --> R1(”精度超越SOTA/Outperforms SOTA in accuracy”)\n        Results --> R2(”参数减少41.38%/Reduces parameters by 41.38%”)\n        Results --> R3(”训练加速3.6倍/3.6x training speedup”)"
    },
    {
      "title": "RAPTOR: Real-Time High-Resolution UAV Video Prediction with Efficient Video Attention",
      "authors": "Zhan Chen, Zile Guo, Enze Zhu, Peirong Zhang, Xiaoxuan Liu, Lei Wang, Yidan Zhang",
      "institution": "Aerospace Information Research Institute, Chinese Academy of Sciences; University of Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.21710",
      "code": "https://github.com/Thelegendzz/RAPTOR",
      "tags": [
        "video prediction",
        "Efficient Video Attention (EVA)",
        "spatiotemporal factorization",
        "real-time inference",
        "on-device AI",
        "training curriculum"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca32784d3d86e53d15479ff35e2982c66656147309d47e2856d7c2334de35f97_w640_q70.webp",
      "contributions": "1. Introduces RAPTOR, a single-pass video prediction architecture that avoids the latency and error accumulation of iterative models like diffusion., 2. Proposes Efficient Video Attention (EVA), a novel module that factorizes spatiotemporal modeling to achieve linear time and memory complexity, enabling high-resolution (512^2) processing., 3. Presents a 3-stage training curriculum that progressively refines predictions from coarse structure to sharp, temporally coherent details.",
      "summary": "This paper addresses the trilemma in video prediction between speed, resolution, and quality by introducing RAPTOR, a model featuring a novel Efficient Video Attention (EVA) module for linear-complexity spatiotemporal modeling. RAPTOR achieves real-time, high-resolution prediction on edge hardware, setting new state-of-the-art results on benchmark datasets and significantly improving UAV navigation success rates.",
      "mindmap": "graph TB\n        A[RAPTOR: 实时高分辨率无人机视频预测<br>RAPTOR: Real-Time High-Resolution UAV Video Prediction] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>视频预测的”三难困境”: 速度、分辨率、质量<br>Video Prediction Trilemma: Speed, Resolution, Quality]\n        C[主要方法/Method<br>高效视频注意力 (EVA) 模块<br>Efficient Video Attention (EVA) Module]\n        D[关键结果/Results<br>首个在Jetson上512^2视频>30 FPS<br>First >30 FPS for 512^2 video on Jetson]"
    },
    {
      "title": "AstraNav-World: World Model for Foresight Control and Consistency",
      "authors": "Junjun Hu, Jintao Chen, Haochen Bai, Minghua Luo, Shichao Xie, Ziyi Chen, Fei Liu, Zedong Chu, Xinda Xue, Botao Ren, Xiaolong Wu, Mu Xu, Shanghang Zhang",
      "institution": "Amap Alibaba, Peking University (PKU), Tsinghua University (THU)",
      "link": "https://arxiv.org/pdf/2512.21714",
      "code": "https://astra-amap.github.io/AstraNav-World.github.io/",
      "tags": [
        "world models",
        "world model",
        "diffusion model",
        "embodied navigation",
        "foresight control",
        "vision-language policy"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15f89f809a2e6654a0c9645caeda1ab33306e62b8276228d05e7467ef63fc5b8_w640_q70.webp",
      "contributions": "1. Proposes AstraNav-World, an end-to-end world model that jointly reasons about future visual states and action sequences within a unified probabilistic framework. 2. Introduces a training paradigm with bidirectional constraints, optimizing for both action-conditioned visual prediction and visual-conditioned trajectory derivation to ensure executability and physical consistency. 3. Demonstrates improved navigation performance and exceptional zero-shot generalization in real-world testing, showing the model captures transferable spatial and dynamic understanding.",
      "summary": "The paper proposes AstraNav-World, a unified world model that integrates a diffusion-based video generator with a vision-language policy to jointly predict future visual scenes and plan action sequences. This bidirectional, synchronized approach mitigates cumulative errors from decoupled pipelines. Experiments show it improves navigation accuracy and success rates, and exhibits strong zero-shot generalization to real-world scenarios.",
      "mindmap": "graph TB\n        A[AstraNav-World] --> B[核心问题/Problem: Embodied navigation lacks foresight, leading to error accumulation in open, dynamic environments.]\n        A --> C[主要方法/Method: Unified world model with diffusion video generator & vision-language policy for synchronized vision-action rollouts.]\n        A --> D[关键结果/Results: Improved trajectory accuracy, higher success rates, and exceptional zero-shot real-world adaptation.]"
    },
    {
      "title": "Knot Forcing: Taming Autoregressive Video Diffusion Models for Real-time Infinite Interactive Portrait Animation",
      "authors": "Steven Xiao, XIndi Zhang, Dechao Meng, Qi Wang, Peng Zhang, Bang Zhang",
      "institution": "Tongyi Lab, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.21734",
      "code": "https://humanaigc.github.io/knot_forcing_demo_page/",
      "tags": [
        "diffusion models",
        "autoregressive video generation",
        "KV caching",
        "sliding window attention",
        "temporal knot",
        "chunk-wise generation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3db0cbfa541f28563a8f65a9f7c0b4cb0b909e1c62fbec822e091b0ad44811c7_w640_q70.webp",
      "contributions": "1. A chunk-wise generation strategy with global identity preservation via cached KV states and local temporal modeling using sliding window attention. 2. A temporal knot module that overlaps adjacent chunks and uses image-to-video conditioning to smooth inter-chunk motion transitions. 3. A \"running ahead\" mechanism that dynamically updates the reference frame's temporal coordinate to maintain long-term coherence.",
      "summary": "This paper proposes Knot Forcing, a streaming framework for real-time, infinite portrait animation. It addresses error accumulation and motion discontinuities in autoregressive video diffusion models through chunk-wise generation, a temporal knot module, and a running-ahead mechanism. The method achieves high-fidelity, temporally consistent animation with real-time performance on consumer GPUs.",
      "mindmap": "graph TB\n        A[”Knot Forcing: Taming Autoregressive Video Diffusion Models<br>Knot Forcing: 驯服自回归视频扩散模型”] --> B[”核心问题/Problem: Real-time portrait animation needs low latency & consistency, but autoregressive models suffer from error accumulation and motion discontinuities.<br>实时肖像动画需要低延迟和一致性，但自回归模型存在误差累积和运动不连续问题。”]\n        A --> C[”主要方法/Method: Chunk-wise generation with KV caching, Temporal Knot module for smooth transitions, and 'Running Ahead' mechanism.<br>分块生成与KV缓存，用于平滑过渡的时序结模块，以及'超前运行'机制。”]\n        A --> D[”关键结果/Results: Enables high-fidelity, infinite, interactive portrait animation with real-time performance on consumer GPUs.<br>在消费级GPU上实现高保真、无限、交互式的实时肖像动画。”]"
    },
    {
      "title": "SyncAnyone: Implicit Disentanglement via Progressive Self-Correction for Lip-Syncing in the wild",
      "authors": "Xindi Zhang, Dechao Meng, Steven Xiao, Qi Wang, Peng Zhang, Bang Zhang",
      "institution": "Tongyi Lab, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.21736",
      "code": "https://humanaigc.github.io/sync_anyone_demo_page/",
      "tags": [
        "video generation",
        "lip-syncing",
        "diffusion transformer",
        "two-stage learning",
        "inpainting",
        "self-correction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/825b387e09612033170e3bd5c2f7bedd0f32c2a644514ee397fd09b64088653e_w640_q70.webp",
      "contributions": "1. A novel two-stage learning framework (SyncAnyone) for lip-syncing that decouples accurate motion modeling from high-fidelity generation. 2. A Stage 2 mask-free tuning pipeline that uses a self-generated pseudo-dataset to correct artifacts from the initial mask-based training. 3. Demonstrates state-of-the-art performance in visual quality, temporal coherence, and identity preservation for in-the-wild scenarios.",
      "summary": "This paper addresses the problem of generating high-quality, audio-synchronized lip movements in videos without disrupting the spatiotemporal context or background. The proposed method, SyncAnyone, uses a two-stage framework: first training a diffusion-based video transformer for masked mouth inpainting, and then fine-tuning it mask-free on self-generated data to correct artifacts. Experiments show it achieves state-of-the-art results in lip-syncing for challenging, real-world videos.",
      "mindmap": "graph TB\n        A[SyncAnyone] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[”现有方法破坏时空上下文 / Existing methods disrupt spatiotemporal context”]\n        C --> C1[”阶段1: 基于掩码的扩散变换器训练 / Stage 1: Mask-based DiT training”]\n        C --> C2[”阶段2: 无掩码调优与自校正 / Stage 2: Mask-free tuning & self-correction”]\n        D --> D1[”SOTA视觉质量与时间一致性 / SOTA visual quality & temporal coherence”]\n        D --> D2[”更好的身份与背景保持 / Better identity & background preservation”]"
    },
    {
      "title": "Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning",
      "authors": "Hengyi Wu, Zhenyi Wang, Heng Huang",
      "institution": "University of Maryland, College Park, University of Central Florida",
      "link": "https://arxiv.org/pdf/2512.21743",
      "code": null,
      "tags": [
        "continual learning",
        "entropy scaling",
        "catastrophic forgetting",
        "stability-plasticity dilemma",
        "dynamic feedback",
        "layer-wise control"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3fa04d7e7b67fc322ddb75e74ec87a46fd273db39fd194f1fd2bb36fb01c0ec_w640_q70.webp",
      "contributions": "1. Proposes a novel entropy-aware continual learning method that uses a dynamic feedback mechanism to regulate each layer based on its entropy, addressing underfitting and overfitting. 2. Introduces Self-Adaptive Entropy Scaling, which adaptively adjusts regularization strength per layer to encourage convergence to wider local minima for better generalization. 3. Presents a complementary adaptive training mechanism that modulates layer plasticity based on performance, preserving knowledge in high-performing layers while amplifying updates in underperforming ones.",
      "summary": "The paper addresses catastrophic forgetting in continual learning by proposing a dynamic feedback mechanism that regulates each neural network layer based on its entropy. This entropy-aware method reduces entropy in high-entropy layers to prevent underfitting and increases it in low-entropy layers to prevent overfitting, promoting convergence to wider minima for improved generalization. The approach is general and integrates with existing replay- and regularization-based methods, showing substantial performance gains over state-of-the-art baselines.",
      "mindmap": "graph TB\n        Root[Dynamic Feedback Engines: Layer-Wise Control for Self-Regulating Continual Learning] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Catastrophic forgetting in continual learning due to uniform layer treatment] --> P1[高熵层欠拟合/High-entropy layers underfit]\n        Problem --> P2[低熵层过拟合/Low-entropy layers overfit]\n        Method[主要方法/Method: Entropy-aware dynamic feedback for layer-wise control] --> M1[减少高熵层熵值/Reduce entropy in high-entropy layers]\n        Method --> M2[增加低熵层熵值/Increase entropy in low-entropy layers]\n        Results[关键结果/Results: Improved generalization and performance] --> R1[收敛到更宽的局部极小值/Converge to wider local minima]\n        Results --> R2[超越现有基线方法/Outperforms state-of-the-art baselines]"
    },
    {
      "title": "Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG",
      "authors": "Gourav Siddhad, Anurag Singh, Rajkumar Saini, Partha Pratim Roy",
      "institution": "Indian Institute of Technology Roorkee, OP Jindal University, Luleå University of Technology, Indian Institute of Technology Dhanbad",
      "link": "https://arxiv.org/pdf/2512.21747",
      "code": null,
      "tags": [
        "brain-computer interface (BCI)",
        "EEG",
        "TSception",
        "Adaptive Average Pooling",
        "spatiotemporal features",
        "drowsiness detection"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/43d71afc9fcee5064adfe94f1fb1d9f8a1c55ccd8d1c759dcd1b5801b9d23f46_w640_q70.webp",
      "contributions": "1. Proposed a Modified TSception architecture with a five-layer temporal refinement strategy to capture multi-scale brain dynamics. 2. Introduced Adaptive Average Pooling for structural flexibility to handle varying EEG input dimensions and a two-stage fusion mechanism for optimized spatiotemporal feature integration. 3. Demonstrated improved performance stability (reduced confidence interval) on the SEED-VIG dataset and state-of-the-art generalizability on the STEW mental workload dataset.",
      "summary": "This paper proposes a Modified TSception deep learning model for robust EEG-based detection of driver drowsiness and mental workload. The key modifications include a multi-layer temporal refinement strategy and Adaptive Average Pooling, which improve the model's stability and ability to handle varying input sizes. The model achieves comparable accuracy with significantly better stability on a drowsiness dataset and state-of-the-art results on a mental workload dataset, demonstrating its effectiveness for reliable cognitive state monitoring.",
      "mindmap": "graph TB\n        A[Modified TSception for Analyzing Driver Drowsiness and Mental Workload from EEG] --> B(核心问题/Problem: Driver drowsiness detection for road safety)\n        A --> C(主要方法/Method: Modified TSception with temporal refinement & Adaptive Average Pooling)\n        A --> D(关键结果/Results: Improved stability on SEED-VIG, SOTA on STEW)"
    },
    {
      "title": "A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network for Multimodal Liver Tumor Segmentation from Unpaired Datasets",
      "authors": "Arunkumar V, Firos V M, Senthilkumar S, Gangadharan G R",
      "institution": "Anna University, National Institute of Technology Tiruchirappalli",
      "link": "https://arxiv.org/pdf/2512.21760",
      "code": null,
      "tags": [
        "medical image segmentation",
        "Quaternion Neural Networks",
        "Cross-Attention",
        "Unpaired Data",
        "Multimodal Learning",
        "Explainable AI"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebec19949c3fad65f84d0acee71e271ec2d8caca288cc4ecf24768e9533dbbe8_w640_q70.webp",
      "contributions": "1. Proposes an Adaptive Quaternion Cross-Fusion (A-QCF) block for bidirectional knowledge transfer between unpaired CT and MRI data streams., 2. Introduces a unified segmentation model (A-QCF-Net) that leverages Quaternion Neural Networks to build a shared feature space from separate datasets., 3. Demonstrates significant performance gains over strong unimodal baselines and validates clinical relevance through explainability analysis.",
      "summary": "This paper addresses the challenge of training multimodal segmentation models with unpaired datasets. It proposes A-QCF-Net, which uses Quaternion Neural Networks and an adaptive cross-fusion block to enable knowledge transfer between separate CT and MRI data. The method significantly outperforms unimodal baselines and provides a viable paradigm for leveraging large, unpaired medical image archives.",
      "mindmap": "graph TB\n        A[A-QCF-Net: An Adaptive Quaternion Cross-Fusion Network<br>for Multimodal Liver Tumor Segmentation from Unpaired Datasets] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Scarcity of paired & aligned multimodal medical datasets]\n        C[主要方法/Method<br>Adaptive Quaternion Cross-Fusion (A-QCF) block<br>Quaternion Neural Networks for shared feature space]\n        D[关键结果/Results<br>Significant Dice score improvement over nnU-Net<br>Validated by explainability analysis]"
    },
    {
      "title": "BertsWin: Resolving Topological Sparsity in 3D Masked Autoencoders via Component-Balanced Structural Optimization",
      "authors": "Evgeny Alves Limarenko, Anastasiia Studenikina",
      "institution": "Moscow Institute of Physics and Technology",
      "link": "https://arxiv.org/pdf/2512.21769",
      "code": null,
      "tags": [
        "3d medical image analysis",
        "Masked Autoencoder",
        "Swin Transformer",
        "Self-Supervised Learning",
        "3D Vision Transformer",
        "Structural Priority Loss"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18d8b6d7f8a20f3bce77706daf18b554423899bdff962eb21fde56292e0c3fde_w640_q70.webp",
      "contributions": "1. Proposed BertsWin, a hybrid architecture combining full BERT-style token masking with Swin Transformer windows to preserve 3D spatial topology during SSL pre-training. 2. Introduced a structural priority loss function to enhance learning. 3. Demonstrated significant acceleration in semantic convergence (5.8x) and a 15-fold reduction in training epochs to reach SOTA fidelity when combined with the GradientConductor optimizer, without increasing computational FLOPs.",
      "summary": "The paper addresses the difficulty of applying standard Masked Autoencoders to 3D medical images, which lose spatial context. It proposes BertsWin, a hybrid architecture that maintains a full 3D token grid using Swin Transformer windows and a structural loss. The method achieves much faster convergence and state-of-the-art reconstruction fidelity for 3D CBCT scans without extra computational cost.",
      "mindmap": "graph TB\n        Root(”BertsWin: 3D MAE优化”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”3D MAE拓扑稀疏性/Topological Sparsity in 3D MAE”)\n        Problem --> P2(”破坏空间关系/Destroys Spatial Context”)\n        Method --> M1(”BertsWin混合架构/BertsWin Hybrid Architecture”)\n        Method --> M2(”完整3D令牌网格/Full 3D Token Grid”)\n        Method --> M3(”Swin窗口 & 结构损失/Swin Windows & Structural Loss”)\n        Results --> R1(”5.8x语义收敛加速/5.8x Faster Convergence”)\n        Results --> R2(”15倍训练轮次减少/15x Fewer Epochs”)\n        Results --> R3(”FLOPs持平，总资源减少/FLOP Parity, Net Resource Reduction”)"
    },
    {
      "title": "Inference-based GAN Video Generation",
      "authors": "Jingbo Yang, Adrian G. Bors",
      "institution": "University of York",
      "link": "https://arxiv.org/pdf/2512.21776",
      "code": null,
      "tags": [
        "video generation",
        "VAE-GAN",
        "Markov chain",
        "long video generation",
        "temporal consistency",
        "encoder-decoder"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5d6cd59a07e4478b6b21e586a92b15f90e237037b758a29738bf31e46f9843c_w640_q70.webp",
      "contributions": "1. Proposes a new video generator, Encoder GAN3 (EncGAN3), which is a VAE-GAN hybrid structure that incorporates inference capabilities into an adversarial-based unconditional video generator. 2. Introduces a novel, memory-efficient approach to generate long videos (hundreds/thousands of frames) by extending the base VAE-GAN model. 3. Leverages a Markov chain framework with a recall mechanism, where each state is a short VAE-GAN generator, to sequentially connect video sub-sequences and ensure temporal continuity.",
      "summary": "This paper addresses the challenge of generating long, high-quality videos, a task where existing models suffer from quality degradation. The proposed method first introduces a VAE-GAN hybrid video generator (EncGAN3) and then extends it using a Markov chain framework to sequentially generate short video clips, enabling the creation of temporally consistent long videos. The main conclusion is that this approach overcomes the temporal scaling limitation and allows for memory-efficient generation of long video sequences.",
      "mindmap": "graph TB\n        Root[Inference-based GAN Video Generation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[现有模型难以生成长视频/Existing models struggle with long video generation]\n        P1 --> P2[视频长度增加导致质量下降/Increased length degrades quality]\n        Method[主要方法/Method] --> M1[提出VAE-GAN混合视频生成器/Propose VAE-GAN hybrid video generator]\n        M1 --> M2[使用马尔可夫链框架扩展/Extend with Markov chain framework]\n        M2 --> M3[状态代表短视频生成器/Each state is a short video generator]\n        Results[关键结果/Results] --> R1[能够生成长视频序列/Can generate long video sequences]\n        R1 --> R2[确保时序连续性与一致性/Ensures temporal continuity and consistency]"
    },
    {
      "title": "Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models",
      "authors": "Nimrod Berman, Adam Botach, Emanuel Ben-Baruch, Shunit Haviv Hakimi, Asaf Gendler, Ilan Naiman, Erez Yosef, Igor Kviatkovsky",
      "institution": "Ben-Gurion University, Amazon Prime Video, Tel-Aviv University",
      "link": "https://arxiv.org/pdf/2512.21778",
      "code": null,
      "tags": [
        "video scene segmentation",
        "vision-language model",
        "multimodal reasoning",
        "context-focus window",
        "confidence score extraction",
        "explainable AI"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d88d54ab6607412e3eb29cafcea4e88a9b83dcabc3bd75f5b4eca365e6ada2b5_w640_q70.webp",
      "contributions": "1. Introduces Scene-VLM, the first fine-tuned vision-language model framework for video scene segmentation, enabling multimodal reasoning across visual and textual cues. 2. Proposes a method to extract confidence scores from VLM token-level logits, enabling controllable precision-recall trade-offs. 3. Demonstrates the model can be aligned to generate coherent natural-language rationales for its boundary decisions with minimal supervision.",
      "summary": "This paper proposes Scene-VLM, a novel vision-language model framework for segmenting long videos into coherent scenes by jointly processing visual frames, dialogue, and metadata. It predicts boundaries sequentially with a context-focus window and can provide confidence scores and explanations. The method achieves state-of-the-art performance on benchmarks like MovieNet, significantly outperforming previous methods.",
      "mindmap": "graph TB\n        Root(”Scene-VLM: Multimodal Video Scene Segmentation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”现有方法限制/Limitations of existing methods”)\n        P1 --> P1_1(”视觉中心偏见/Visual-centric bias”)\n        P1 --> P1_2(”孤立分类/Isolated shot classification”)\n        P1 --> P1_3(”缺乏可解释性/Lack explainability”)\n        Method --> M1(”微调VLM框架/Fine-tuned VLM framework”)\n        M1 --> M1_1(”多模态推理/Multimodal reasoning”)\n        M1 --> M1_2(”序列预测/Sequential prediction”)\n        M1 --> M1_3(”上下文聚焦窗口/Context-focus window”)\n        Method --> M2(”置信度提取/Confidence score extraction”)\n        Method --> M3(”生成解释/Generate rationales”)\n        Results --> R1(”SOTA性能/State-of-the-art performance”)\n        Results --> R2(”显著提升/Significant improvement on MovieNet”)"
    },
    {
      "title": "Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning",
      "authors": "Ting-Hao K.Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, C. Lee Giles",
      "institution": "The Pennsylvania State University, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.21789",
      "code": null,
      "tags": [
        "image captioning",
        "scientific figure captioning",
        "large-scale dataset",
        "domain-specific training",
        "human evaluation",
        "large language models (LLMs)"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f7ba728eef6969e957e00de058f6caa0b6756df68bc13251efae06aa946322b_w640_q70.webp",
      "contributions": "1. Creation and continuous updating of a large-scale, real-world dataset of scientific figure-caption pairs from arXiv papers. 2. Conducting extensive evaluations, both automatic and human, on generated and author-written captions to assess quality. 3. Developing interactive systems and launching annual challenges to advance the field and help scientists write better captions.",
      "summary": "This paper reviews the SciCap project's first five years, which focused on generating and evaluating captions for scientific figures. The core method involved building a large-scale dataset from arXiv and exploring domain-specific training, similar to models like SciBERT, for captioning. The conclusion outlines key lessons learned and proposes future research directions to address unsolved challenges in the field.",
      "mindmap": "graph TB\n        Root[Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[科学图表说明质量差/Poor quality of scientific figure captions]\n        Problem --> P2[缺乏大规模真实数据集/Lack of large-scale real-world dataset]\n        Method --> M1[构建arXiv图表-说明对数据集/Construct arXiv figure-caption dataset]\n        Method --> M2[领域特定训练与评估/Domain-specific training & evaluation]\n        Method --> M3[应对大语言模型兴起/Navigate rise of LLMs]\n        Results --> R1[总结技术方法经验/Summarize technical & methodological lessons]\n        Results --> R2[提出未来挑战与方向/Outline future challenges & directions]"
    },
    {
      "title": "InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation",
      "authors": "Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang, Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan",
      "institution": "ByteDance Inc., Rutgers University",
      "link": "https://arxiv.org/pdf/2512.21788",
      "code": "https://github.com/yanq095/InstructMoLE",
      "tags": [
        "diffusion models",
        "Parameter-Efficient Fine-Tuning",
        "Mixture of Low-rank Experts",
        "Instruction-Guided Routing",
        "Multi-Conditional Generation",
        "Diffusion Transformers"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11bd8639cb632e86b35367843cf453bbc6a02fb6882f88ff7441c78b42b653ce_w640_q70.webp",
      "contributions": "1. Introduces InstructMoLE, a framework using an Instruction-Guided Mixture of Low-Rank Experts for multi-conditional image generation., 2. Proposes Instruction-Guided Routing (IGR), a global routing signal derived from user instructions to select a coherent expert council for all tokens, addressing spatial fragmentation and semantic drift., 3. Introduces an output-space orthogonality loss to promote expert functional diversity and prevent representational collapse.",
      "summary": "This paper addresses the problem of task interference in multi-conditional image generation when using monolithic PEFT adapters like LoRA. It proposes InstructMoLE, a novel framework that uses global instruction-guided routing to select a consistent mixture of low-rank experts, combined with an orthogonality loss for diversity, which outperforms existing methods on benchmarks.",
      "mindmap": "graph TB\n        A[InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for Multi-Conditional Image Generation] --> B[核心问题/Problem: Task interference & spatial fragmentation in multi-conditional DiT fine-tuning]\n        A --> C[主要方法/Method: Global Instruction-Guided Routing (IGR) & output-space orthogonality loss]\n        A --> D[关键结果/Results: Outperforms LoRA & MoLE variants on benchmarks]"
    },
    {
      "title": "AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge",
      "authors": "Hyam Omar Ali, Sahar Alhesseen, Lamis Elkhair, Adrian Galdran, Ming Feng, Zhixiang Xiong, Zengming Lin, Kele Xu, Liang Hu, Benjamin Keel, Oliver Mills, James Battye, Akshay Kumar, Asra Aslam, Prasad Dutande, Ujjwal Baid, Bhakti Baheti, Suhas Gajre, Aravind Shrenivas Murali, Eung-Joo Lee, Ahmed Fahal, Rachid Jennane",
      "institution": "University of Khartoum, Orleans University, Tongji University, National University of Defense Technology, University of Leeds",
      "link": "https://arxiv.org/pdf/2512.21792",
      "code": null,
      "tags": [
        "medical image analysis",
        "histopathological images",
        "image segmentation",
        "grain detection",
        "deep learning",
        "mycetoma classification"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2608e467b615afdd887a03b7ca003d5d2989f1640e4dc84304a9ee0f5a6e979_w640_q70.webp",
      "contributions": "1. Organized the mAIcetoma challenge to advance AI solutions for mycetoma diagnosis. 2. Provided the standardized Mycetoma database (MyData) for model development. 3. Demonstrated the effectiveness of automated deep learning models for segmenting grains and classifying mycetoma types from histopathological images.",
      "summary": "This paper presents the mAIcetoma challenge, which aimed to develop AI models for automating the diagnosis of mycetoma by segmenting grains and classifying disease types from histopathological images. Multiple teams proposed various deep learning architectures, which were evaluated on a provided standardized dataset. The results showed that the models achieved high segmentation accuracy and significant performance in classification, highlighting the potential of AI to assist in diagnosing this neglected tropical disease.",
      "mindmap": "graph TB\n        Root(”AI for Mycetoma Diagnosis in Histopathological Images: The MICCAI 2024 Challenge<br>AI用于组织病理学图像中的足菌肿诊断：MICCAI 2024挑战赛”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”足菌肿诊断困难，尤其在资源匮乏地区<br>Mycetoma diagnosis is challenging, especially in low-resource settings”)\n        Method --> M1(”组织mAIcetoma挑战赛，开发AI模型<br>Organized mAIcetoma challenge to develop AI models”)\n        Method --> M2(”提供MyData数据集，用于分割和分类<br>Provided MyData dataset for segmentation and classification”)\n        Results --> R1(”模型实现高精度分割<br>Models achieved high segmentation accuracy”)\n        Results --> R2(”顶级模型分类性能显著<br>Top models showed significant classification performance”)"
    },
    {
      "title": "Diffusion Posterior Sampling for Super-Resolution under Gaussian Measurement Noise",
      "authors": "Abu Hanif Muhammad Syarubany",
      "institution": "Korea Advanced Institute of Science & Technology (KAIST)",
      "link": "https://arxiv.org/pdf/2512.21797",
      "code": null,
      "tags": [
        "image super-resolution",
        "diffusion posterior sampling",
        "single-image super-resolution",
        "inverse problems",
        "measurement consistency",
        "unconditional diffusion prior"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc3596dd62ab28bf4cee2aec80e4147a24811ace57a7d9d76802c0ea0767bbdd_w640_q70.webp",
      "contributions": "1. Implementation and evaluation of diffusion posterior sampling (DPS) for single-image super-resolution under a known degradation model. 2. An ablation study analyzing the impact of guidance scale and noise level on reconstruction quality, identifying an optimal configuration. 3. Demonstration that balancing the diffusion prior and measurement-gradient strength enables stable, high-quality reconstructions without model retraining.",
      "summary": "This paper studies the use of diffusion posterior sampling (DPS) for single-image super-resolution under Gaussian noise. The method combines an unconditional diffusion prior with a likelihood-guided update to enforce measurement consistency during sampling. The main finding is that moderate guidance, at a specific scale and noise level, yields the best reconstruction quality, highlighting the importance of balancing prior and data fidelity for stable results.",
      "mindmap": "graph TB\n        Root[”Diffusion Posterior Sampling for Super-Resolution<br>扩散后验采样用于超分辨率”] --> Problem[”核心问题/Problem<br>Single-image super-resolution under Gaussian noise<br>高斯噪声下的单图像超分辨率”]\n        Root --> Method[”主要方法/Method<br>Diffusion Posterior Sampling (DPS)<br>扩散后验采样<br>Unconditional prior + likelihood-guided conditioning<br>无条件先验 + 似然引导的条件化”]\n        Root --> Results[”关键结果/Results<br>Optimal PS scale 0.95, noise σ=0.01<br>最佳PS尺度0.95, 噪声σ=0.01<br>Balancing prior and data yields sharp details<br>平衡先验与数据得到清晰细节”]"
    },
    {
      "title": "CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection",
      "authors": "Ruochen Liu, Yi Tian, Jiahao Wang, Hongbin Liu, Xianxu Hou, Jingxin Liu",
      "institution": "University of Liverpool, National University of Singapore, Xi'an Jiaotong-Liverpool University",
      "link": "https://arxiv.org/pdf/2512.21803",
      "code": null,
      "tags": [
        "object detection",
        "Mamba",
        "Triple-Mapping Adaptive Coupling (TMAC)",
        "Adaptive Mamba Head",
        "biomedical instance detection",
        "VSSD backbone"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fc3fe9b8372a27eedc5a8e2e1150bcdf6cf461c195f9ed154d8890662de191da_w640_q70.webp",
      "contributions": "1. Proposes a novel Triple-Mapping Adaptive Coupling (TMAC) module that splits channels into parallel branches with dual idiosyncratic and one consensus attention map for enhanced spatial discriminability. 2. Designs an Adaptive Mamba Head that fuses multi-scale features via learnable weights to handle varying object sizes robustly. 3. Introduces CellMamba, a lightweight one-stage detector built on a VSSD backbone with CellMamba Blocks, achieving superior accuracy and efficiency on biomedical cell detection datasets.",
      "summary": "This paper proposes CellMamba, a lightweight one-stage detector for cell detection in pathological images. It introduces a novel Triple-Mapping Adaptive Coupling (TMAC) module and an Adaptive Mamba Head to improve spatial discriminability and multi-scale feature fusion. Experiments show CellMamba outperforms CNN, Transformer, and Mamba baselines in accuracy while being more efficient in model size and inference speed.",
      "mindmap": "graph TB\n        A[CellMamba: Adaptive Mamba for Cell Detection] --> B[核心问题/Problem: Cell detection challenges in pathological images]\n        A --> C[主要方法/Method: CellMamba with TMAC module & Adaptive Mamba Head]\n        A --> D[关键结果/Results: Outperforms baselines, lightweight & efficient]"
    },
    {
      "title": "S&P 500 Stock's Movement Prediction using CNN",
      "authors": "Rahul Gupta",
      "institution": "None (No affiliation or email domain provided in the given content)",
      "link": "https://arxiv.org/pdf/2512.21804",
      "code": null,
      "tags": [
        "financial time series forecasting",
        "Convolutional Neural Network (CNN)",
        "multivariate raw data",
        "stock movement prediction",
        "historical data matrices",
        "S&P 500"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d6197655292ac8e55d8c7606c8c3cfe730f7f8dad4004b93d4a9b3a8d8f457_w640_q70.webp",
      "contributions": "1. Proposes the application of Convolutional Neural Networks (CNNs), typically used for image classification, to the problem of stock movement prediction by treating multivariate historical stock data as image-like matrices. 2. Utilizes raw, unprocessed market data including events like stock splits and dividends, instead of relying on pre-engineered financial features. 3. Demonstrates a flexible prediction framework that can be applied at different levels: individual stocks, sectors, or entire portfolios.",
      "summary": "This paper tackles the problem of predicting stock price movements for the S&P 500 index. The core method involves using a Convolutional Neural Network (CNN) to analyze multivariate historical stock data, which is structured as image-like matrices, without extensive feature engineering. The approach shows promising results and offers a flexible model for stock, sector, or portfolio-level predictions.",
      "mindmap": "graph TB\n        Root[”S&P 500 Stock's Movement Prediction using CNN<br>使用CNN预测标普500股票走势”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Predicting stock price movement<br>预测股票价格走势”] --> P1[”传统方法依赖特征工程<br>Traditional methods rely on engineered features”]\n        Problem --> P2[”现有研究多使用单维数据<br>Existing research often uses single-dimension data”]\n        Method[”主要方法/Method<br>Use CNN on raw multivariate data<br>对原始多变量数据使用CNN”] --> M1[”将历史数据矩阵视为图像<br>Treat historical data matrices as images”]\n        Method --> M2[”包含原始市场事件(如拆股)<br>Include raw market events (e.g., splits)”]\n        Results[”关键结果/Results<br>Model achieves promising results<br>模型取得有希望的结果”] --> R1[”支持股票/行业/组合级别预测<br>Supports stock/sector/portfolio prediction”]"
    },
    {
      "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models",
      "authors": "Mengqi He, Xinyu Tian, Xin Shen, Jinhong Ni, Shu Zou, Zhaoyuan Yang, Jing Zhang",
      "institution": "Australian National University, The University of Queensland, GE Research",
      "link": "https://arxiv.org/pdf/2512.21815",
      "code": null,
      "tags": [
        "adversarial attacks",
        "entropy-guided attacks",
        "vision-language models",
        "adversarial robustness",
        "harmful content generation",
        "transferability"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af10d81c75765690cdb206c6e6e648f14a6dcb054a6ac03725ba3a7f9ce27608_w640_q70.webp",
      "contributions": "1. Identifies that only a small fraction (~20%) of high-entropy tokens (critical decision points) disproportionately govern output trajectories in autoregressive VLMs, challenging the prior assumption of equal token importance. 2. Proposes a selective attack strategy that concentrates adversarial perturbations on these high-entropy positions, achieving strong semantic degradation with a smaller perturbation budget and inducing 35-49% of benign outputs to become harmful. 3. Demonstrates that vulnerable high-entropy forks recur across diverse VLMs, enabling feasible transferable attacks (17-26% harmful rates), and proposes the Entropy-bank Guided Adversarial attack (EGA) method which achieves high attack success rates (93-95%) while exposing new safety weaknesses.",
      "summary": "This paper reveals that adversarial attacks on Vision-Language Models (VLMs) can be made more efficient and dangerous by targeting only the critical high-entropy tokens during autoregressive generation, rather than all tokens. The proposed Entropy-bank Guided Adversarial attack (EGA) concentrates perturbations on these positions, achieving high attack success and converting many benign outputs into harmful ones, while also showing significant transferability across models. This exposes a critical and previously overlooked vulnerability in current VLM safety mechanisms.",
      "mindmap": "graph TB\n        Root[Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[VLMs易受对抗攻击/VLMs are vulnerable to adversarial attacks]\n        Problem --> P2[先验攻击假设所有token同等重要/Prior attacks assume all tokens are equally important]\n        Method[主要方法/Method] --> M1[识别高熵关键决策点/Identify high-entropy critical decision points]\n        Method --> M2[提出熵库引导对抗攻击(EGA)/Propose Entropy-bank Guided Adversarial attack (EGA)]\n        Results[关键结果/Results] --> R1[高效攻击:小预算实现强语义退化/Efficient attack: strong degradation with small budget]\n        Results --> R2[高有害转化率:35-49%/High harmful conversion: 35-49%]\n        Results --> R3[可行迁移性:17-26%/Feasible transferability: 17-26%]"
    },
    {
      "title": "End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration",
      "authors": "Zhenwei Yang, Yibo Ai, Weidong Zhang",
      "institution": "University of Science and Technology Beijing, National Center for Materials Service Safety",
      "link": "https://arxiv.org/pdf/2512.21831",
      "code": null,
      "tags": [
        "autonomous driving perception",
        "multimodal fusion",
        "multi-view cooperative perception",
        "spatiotemporal modeling",
        "V2X communication",
        "deformable attention"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2257ca524cd2e350a5c2abf7059415ea35045bbf6cb45991c73d32410116cb9_w640_q70.webp",
      "contributions": "1. Proposes XET-V2X, an end-to-end tracking framework that unifies multi-view multimodal sensing within a shared spatiotemporal representation for V2X collaboration. 2. Introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention to efficiently align heterogeneous viewpoints and modalities. 3. Demonstrates robust performance on real-world and simulated V2X datasets, showing consistent improvements in detection and tracking under varying communication delays.",
      "summary": "This paper addresses the challenge of reliable 3D perception in autonomous driving under occlusions and communication delays by proposing XET-V2X, an end-to-end framework that fuses multi-view images and point clouds using a novel dual-layer spatial cross-attention module. The method achieves effective cross-modal interaction and reduces computational overhead. Experiments show it delivers robust and temporally stable detection and tracking performance in complex V2X scenarios.",
      "mindmap": "graph TB\n    A[End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[感知挑战:遮挡,视角限制,通信延迟/Perception Challenges: Occlusions, Limited Viewpoints, Communication Delays]\n    C --> C1[提出XET-V2X框架/Proposes XET-V2X Framework]\n    C1 --> C2[双层级空间交叉注意力模块/Dual-layer Spatial Cross-Attention Module]\n    C2 --> C3[多视图图像特征聚合/Multi-view Image Feature Aggregation]\n    C2 --> C4[点云融合/Point Cloud Fusion]\n    D --> D1[在V2X-Seq-SPD等数据集上性能提升/Performance Improvements on V2X-Seq-SPD, etc.]\n    D1 --> D2[检测与跟踪性能增强/Enhanced Detection & Tracking Performance]\n    D2 --> D3[鲁棒且时序稳定的感知/Robust & Temporally Stable Perception]"
    },
    {
      "title": "Scalable Class-Incremental Learning Based on Parametric Neural Collapse",
      "authors": "Chuangxin Zhang, Guangfeng Lin, Enhui Zhao, Kaiyang Liao, Yajun Chen",
      "institution": "Not explicitly stated in the provided content. Affiliation information is not included.",
      "link": "https://arxiv.org/pdf/2512.21845",
      "code": "this",
      "tags": [
        "class-incremental learning",
        "parametric neural collapse",
        "equiangular tight frame",
        "knowledge distillation",
        "adaptive expansion",
        "feature alignment"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6233749e62b8335ab812fd2f0b2690d70bb8f1a822bd796c2c3cfa9d1a402cf3_w640_q70.webp",
      "contributions": "1. Proposes a scalable class-incremental learning method (SCL-PNC) based on parametric neural collapse, enabling demand-driven, minimal-cost backbone expansion via an adapt-layer. 2. Introduces a dynamic parametric Equiangular Tight Frame (ETF) classifier to address class misalignment caused by evolving class distributions. 3. Presents a parallel expansion framework with a knowledge distillation algorithm to counteract feature drift and ensure feature consistency across expansion modules.",
      "summary": "The paper addresses challenges in class-incremental learning, such as overfitting and catastrophic forgetting, by proposing SCL-PNC. This method uses a parametric neural collapse framework with an adaptive backbone expansion and a dynamic ETF classifier to efficiently handle growing categories while maintaining feature consistency via distillation. Experiments show the method is effective and efficient.",
      "mindmap": "graph TB\n        Root[”Scalable Class-Incremental Learning Based on Parametric Neural Collapse”] --> Problem\n        Root --> Method\n        Root --> Results\n    \n        Problem[”核心问题 / Problem”] --> P1[”过拟合新数据 / Overfitting to new data”]\n        Problem --> P2[”灾难性遗忘旧数据 / Catastrophic forgetting of old data”]\n        Problem --> P3[”特征差异与类别错位 / Feature difference & Class misalignment”]\n    \n        Method[”主要方法 / Method”] --> M1[”SCL-PNC方法 / SCL-PNC Method”]\n        M1 --> M1_1[”自适应层扩展主干 / Adapt-layer for backbone expansion”]\n        M1 --> M1_2[”动态参数化ETF分类器 / Dynamic Parametric ETF Classifier”]\n        M1 --> M1_3[”并行扩展与知识蒸馏 / Parallel expansion & Knowledge distillation”]\n    \n        Results[”关键结果 / Results”] --> R1[”高效处理类别增长 / Efficiently handles increasing categories”]\n        Results --> R2[”解决类别错位 / Addresses class misalignment”]\n        Results --> R3[”确保特征一致性 / Ensures feature consistency”]"
    },
    {
      "title": "Breaking Alignment Barriers: TPS-Driven Semantic Correlation Learning for Alignment-Free RGB-T Salient Object Detection",
      "authors": "Lupiao Hu, Fasheng Wang, Fangmei Chen, Fuming Sun, Haojie Li",
      "institution": "Dalian Minzu University, Shandong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.21856",
      "code": "https://github.com/HTUTU2/TPS-SCL",
      "tags": [
        "salient object detection",
        "RGB-T",
        "unaligned images",
        "Thin-Plate Spline",
        "MobileViT",
        "Mamba"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dcfc4f61262c973564dfeb3e9591537a78f5165f01a7fece4325e5961bd3fe89_w640_q70.webp",
      "contributions": "1. Proposes a Thin-Plate Spline Alignment Module (TPSAM) to mitigate spatial discrepancies between unaligned RGB-T modalities, addressing local deformations better than homography estimation. 2. Designs a Semantic Correlation Constraint Module (SCCM) to hierarchically constrain salient features and suppress background interference during alignment. 3. Introduces a Cross-Modal Correlation Module (CMCM) to fully explore and integrate inter-modal dependencies, enhancing detection performance in a lightweight architecture using MobileViT and Mamba.",
      "summary": "This paper addresses the performance degradation of RGB-T salient object detection (SOD) methods on real-world, unaligned image pairs. It proposes TPS-SCL, a lightweight network that uses a Thin-Plate Spline module for alignment, semantic correlation constraints, and cross-modal fusion. Experiments show the method achieves state-of-the-art performance among lightweight SOD methods and outperforms mainstream RGB-T SOD approaches.",
      "mindmap": "graph TB\n        A[Breaking Alignment Barriers: TPS-SCL<br>突破对齐壁垒: TPS-SCL] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[现有方法依赖对齐数据集<br>Existing methods rely on aligned datasets]\n        B --> B2[真实场景图像未对齐<br>Real-world images are unaligned]\n        C --> C1[双流MobileViT编码器<br>Dual-stream MobileViT encoder]\n        C --> C2[TPS对齐模块<br>TPS Alignment Module]\n        C --> C3[语义关联约束模块<br>Semantic Correlation Constraint Module]\n        C --> C4[跨模态关联模块<br>Cross-Modal Correlation Module]\n        D --> D1[轻量级SOTA性能<br>Lightweight SOTA performance]\n        D --> D2[超越主流RGB-T方法<br>Outperforms mainstream RGB-T methods]"
    },
    {
      "title": "Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models",
      "authors": "Masayuki Kawarada, Kosuke Yamada, Antonio Tejero-de-Pablos, Naoto Inoue",
      "institution": "CyberAgent",
      "link": "https://arxiv.org/pdf/2512.21860",
      "code": "https://github.com/CyberAgentAILab/DIOR_conditional_image_embeddings",
      "tags": [
        "image embedding",
        "conditional image embedding",
        "large vision-language model",
        "training-free",
        "image similarity",
        "hidden state"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5dc3046fafcb5e9e27821f45ec139d58a5fbb5098fe33ea6a51f89167ca9df74_w640_q70.webp",
      "contributions": "1. Proposes DIOR, a novel training-free framework for generating conditional image embeddings using Large Vision-Language Models (LVLMs)., 2. Introduces a method that prompts an LVLM to describe an image with a single word based on a given condition and uses the last token's hidden state as the embedding., 3. Demonstrates superior performance over existing training-free baselines (e.g., CLIP) and even methods requiring additional training on conditional similarity tasks.",
      "summary": "The paper addresses the problem of generating image embeddings that focus on specific textual conditions, which standard models like CLIP cannot do. It proposes DIOR, a training-free method that uses a Large Vision-Language Model to produce a one-word description based on a condition and extracts its hidden state as the conditional embedding. Experiments show DIOR outperforms both training-free and training-required baselines on conditional image similarity tasks.",
      "mindmap": "graph TB\n        A[Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 全局图像嵌入无法聚焦于特定文本条件]\n        C[主要方法/Method: DIOR - 利用LVLM生成单字描述并提取最后token的隐藏状态]\n        D[关键结果/Results: 在条件图像相似性任务上超越现有方法]"
    },
    {
      "title": "Fast Inference of Visual Autoregressive Model with Adjacency-Adaptive Dynamical Draft Trees",
      "authors": "Haodong Lei, Hongsong Wang, Xin Geng, Liang Wang, Pan Zhou",
      "institution": "Southeast University, Institute of Automation Chinese Academy of Sciences, Singapore Management University",
      "link": "https://arxiv.org/pdf/2512.21857",
      "code": "https://github.com/Haodong-Lei-Ray/ADT-Tree",
      "tags": [
        "multi-modal inference",
        "speculative decoding",
        "draft tree",
        "inference acceleration",
        "autoregressive image generation",
        "dynamic tree structure"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b9ff13149fa2d6733868b2125e7af2ae06239a529152a057b80d0d6f357ccf3_w640_q70.webp",
      "contributions": "1. Identified the key obstacle of applying speculative decoding to visual AR models: inconsistent acceptance rates across draft trees due to spatially varying token prediction difficulty. 2. Proposed ADT-Tree, an adjacency-adaptive dynamic draft tree that dynamically adjusts tree depth and width based on adjacent token states and prior acceptance rates. 3. Demonstrated significant speedups (over 3x) on benchmarks and seamless integration with relaxed sampling methods for further acceleration.",
      "summary": "This paper addresses the slow inference of visual autoregressive models by proposing ADT-Tree, a dynamic draft tree method that adapts its structure to image region complexity. It achieves over 3x speedup on standard benchmarks and can be combined with other sampling techniques for additional gains.",
      "mindmap": "graph TB\n        Root[”Fast Inference of Visual AR Model with ADT-Tree<br>视觉自回归模型快速推理与ADT-Tree”]\n        Root --> Problem[”核心问题/Problem<br>Visual AR models have slow sequential inference.<br>视觉AR模型推理慢”]\n        Root --> Method[”主要方法/Method<br>Propose Adjacency-Adaptive Dynamical Draft Trees (ADT-Tree).<br>提出邻接自适应动态草稿树”]\n        Root --> Results[”关键结果/Results<br>Achieves 3.13x/3.05x speedup on benchmarks.<br>在基准测试上实现3.13x/3.05x加速”]\n        Problem --> P1[”Spatially varying token prediction difficulty.<br>空间变化的token预测难度”]\n        Method --> M1[”Dynamically adjusts tree depth & width.<br>动态调整树深度与宽度”]\n        Method --> M2[”Leverages adjacency & prior acceptance rates.<br>利用邻接关系和先验接受率”]\n        Results --> R1[”Integrates with relaxed sampling.<br>可与松弛采样方法结合”]"
    },
    {
      "title": "Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening",
      "authors": "Md Rafid Islam, Rafsan Jany, Akib Ahmed, Mohammad Ashrafuzzaman Khan",
      "institution": "North South University, Korea Institute of Oriental Medicine, American International University–Bangladesh",
      "link": "https://arxiv.org/pdf/2512.21861",
      "code": null,
      "tags": [
        "medical image classification",
        "feature-level fusion",
        "convolutional neural networks",
        "diabetic retinopathy screening",
        "EfficientNet",
        "DenseNet"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d879fd7c14baee1110e20d8ebdaec476df8f8819b2fc9a74154be1d0a91d7963_w640_q70.webp",
      "contributions": "1. Proposes and evaluates feature-level fusion of complementary CNN backbones (ResNet50, EfficientNet-B0, DenseNet121) for binary diabetic retinopathy screening. 2. Demonstrates that fusion models consistently outperform single backbones in accuracy and generalization across a large, heterogeneous dataset pooled from five public sources. 3. Provides a practical analysis of the accuracy-efficiency trade-off, identifying the EfficientNet-B0 + DenseNet121 fusion as offering the best balance between performance and computational latency.",
      "summary": "This paper investigates feature-level fusion of CNN models to improve binary diabetic retinopathy screening. It finds that fusing EfficientNet-B0 and DenseNet121 achieves the best accuracy (82.89%) with a favorable balance of performance and inference speed, demonstrating that lightweight fusion enhances generalization across diverse datasets for scalable screening.",
      "mindmap": "graph TB\n        A[Balancing Accuracy and Efficiency: CNN Fusion Models for Diabetic Retinopathy Screening] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Large-scale DR screening is constrained by limited specialists and variable image quality.]\n        C[主要方法/Method<br>Feature-level fusion of complementary CNN backbones (e.g., EfficientNet, DenseNet) on pooled fundus images.]\n        D[关键结果/Results<br>Fusion outperforms single models. Eff+Den fusion offers best accuracy-latency balance.]"
    },
    {
      "title": "EasyOmnimatte: Taming Pretrained Inpainting Diffusion Models for End-to-End Video Layered Decomposition",
      "authors": "Yihan Hu, Xuelin Chen, Xiaodong Cun",
      "institution": "Great Bay University, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.21865",
      "code": "https://github.com/GVCLab/EasyOmnimatte",
      "tags": [
        "video matting",
        "video omnimatte",
        "diffusion models",
        "LoRA",
        "DiT blocks",
        "dual-expert"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/200afe63944dc4d9793ba3dadc45f6ca120880dcbf65f86982757bb158120f60_w640_q70.webp",
      "contributions": "1. Proposes the first unified, end-to-end video omnimatte method by finetuning a pretrained video inpainting diffusion model. 2. Introduces a Dual-Expert strategy (Effect Expert and Quality Expert) that selectively applies LoRA to specific DiT blocks to capture foreground-associated effects and refine alpha mattes. 3. Achieves state-of-the-art performance in quality and efficiency by using experts at different denoising steps, eliminating the need for two full diffusion passes.",
      "summary": "The paper introduces EasyOmnimatte, a method for video layered decomposition that finetunes a pretrained video inpainting diffusion model with a novel Dual-Expert strategy to efficiently produce high-quality alpha mattes with associated effects. It significantly outperforms existing methods in both speed and quality, enabling various downstream video editing tasks.",
      "mindmap": "graph TB\n        A[EasyOmnimatte] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法慢且次优/Existing methods are slow and suboptimal]\n        C --> C1[双专家微调/Dual-Expert Finetuning]\n        C1 --> C2[效果专家/Effect Expert]\n        C1 --> C3[质量专家/Quality Expert]\n        D --> D1[高质量分解/High-quality decomposition]\n        D --> D2[高效快速/Efficient and fast]"
    },
    {
      "title": "DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation",
      "authors": "Divyansh Srivastava, Akshay Mehra, Pranav Maneriker, Debopam Sanyal, Vishnu Raj, Vijay Kamarshi, Fan Du, Joshua Kimball",
      "institution": "University of California, San Diego, Dolby Laboratories",
      "link": "https://arxiv.org/pdf/2512.21867",
      "code": null,
      "tags": [
        "multi-modal training",
        "autoregressive image generation",
        "dynamic tokenization",
        "next-token prediction entropy",
        "patch merging",
        "training efficiency"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dac1942675af4bd3638ebec23734e3fed0239b066b673633b094708fa3d2d26f_w640_q70.webp",
      "contributions": "1. Introduces DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into variable-sized patches for efficient generation, using next-token prediction entropy as a merging criterion. 2. Demonstrates that training with dynamic patches yields patch-boundary-robust representations, enabling inference with larger patches for further efficiency. 3. Achieves significant reductions in token count (up to 2.06x) and training FLOPs (up to 40%) while improving image quality (FID) by up to 27.1% compared to fixed-tokenization baselines.",
      "summary": "The paper addresses the computational inefficiency of fixed tokenization in autoregressive image generation, where token count grows quadratically with resolution. It proposes DPAR, a method that dynamically merges image tokens into larger patches based on the information content predicted by a lightweight model's entropy, allocating more compute to complex regions. This approach reduces training costs by up to 40% and improves generated image quality (FID) by up to 27.1% compared to standard models.",
      "mindmap": "graph TB\n        Root[DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Fixed tokenization leads to quadratic token growth and high computational cost in autoregressive image generation.]\n        Method[主要方法/Method: Dynamic patch merging using next-token prediction entropy as a criterion for token aggregation.]\n        Results[关键结果/Results: Reduces token count (1.81x-2.06x), cuts training FLOPs by up to 40%, and improves FID by up to 27.1%.]"
    },
    {
      "title": "Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer",
      "authors": "Tianchen Deng, Wenhua Wu, Kunzhen Wu, Guangming Wang, Siting Zhu, Shenghai Yuan, Xun Chen, Guole Shen, Zhe Liu, Hesheng Wang",
      "institution": "Shanghai Jiao Tong University, Nanyang Technological University, Cambridge University",
      "link": "https://arxiv.org/pdf/2512.21883",
      "code": "https://github.com/dtc111111/Reloc-VGGT",
      "tags": [
        "visual localization",
        "early-fusion",
        "sparse mask attention",
        "pose tokenizer",
        "VGGT backbone",
        "multi-view geometry"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a9b7693fa823e25dd55dc558d5c57f7d31f7adebc941d9ff45f7ef2ad7c6508_w640_q70.webp",
      "contributions": "1. Introduces the first visual localization framework with an early-fusion mechanism for multi-view spatial integration. 2. Proposes a novel sparse mask attention strategy to reduce computational complexity and enable real-time performance. 3. Presents a pose tokenizer and projection module to better exploit spatial relationships from multiple database views.",
      "summary": "This paper proposes Reloc-VGGT, a visual re-localization framework that uses an early-fusion mechanism and a VGGT backbone to integrate multi-view 3D geometry for robust pose estimation. It introduces a sparse mask attention strategy for efficiency and is trained on millions of image pairs. The method achieves strong accuracy, generalization, and real-time performance across diverse datasets.",
      "mindmap": "graph TB\n        A[Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer] --> B[核心问题/Problem: Late-fusion in visual localization is insufficient, degrading accuracy in complex environments.]\n        A --> C[主要方法/Method: Early-fusion framework with VGGT backbone, pose tokenizer, and sparse mask attention.]\n        A --> D[关键结果/Results: Strong accuracy, generalization, and real-time performance validated on public datasets.]"
    },
    {
      "title": "SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis",
      "authors": "Mo Wang, Junfeng Xia, Wenhao Ye, Enyu Liu, Kaining Peng, Jianfeng Feng, Quanying Liu, Hongkai Wen",
      "institution": "Southern University of Science and Technology, University of Warwick, Fudan University",
      "link": "https://arxiv.org/pdf/2512.21881",
      "code": null,
      "tags": [
        "multi-modal training",
        "fMRI",
        "foundation model",
        "data-efficient",
        "training-efficient",
        "hierarchical encoder"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03335d41a7bea8c473f46251d91847fc35908f4f15a206682290034deaf6ef92_w640_q70.webp",
      "contributions": "1. Proposes SLIM-Brain, a novel atlas-free foundation model for fMRI analysis that addresses the dual bottlenecks of data- and training-efficiency. 2. Introduces a two-stage adaptive design featuring a lightweight temporal extractor for saliency ranking and a 4D hierarchical encoder (Hiera-JEPA) that learns from selected windows and uses aggressive masking. 3. Demonstrates state-of-the-art performance across seven benchmarks while requiring significantly less pre-training data (4k sessions) and GPU memory (~30% of traditional methods).",
      "summary": "The paper addresses the data- and training-efficiency bottlenecks in fMRI foundation models. It proposes SLIM-Brain, a two-stage model that uses a temporal extractor to select salient data windows and a hierarchical encoder to learn from them efficiently. The method achieves state-of-the-art results on multiple tasks using far less pre-training data and computational resources than traditional approaches.",
      "mindmap": "graph TB\n        A[SLIM-Brain: A Data- and Training-Efficient Foundation Model for fMRI Data Analysis] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法瓶颈/Bottlenecks of Existing Methods]\n        B1 --> B1_1[图谱方法: 丢失细节, 需大数据/Atlas-based: lose details, need big data]\n        B1 --> B1_2[无图谱方法: 内存计算成本高/Atlas-free: high memory & compute cost]\n        C --> C1[两阶段自适应设计/Two-stage Adaptive Design]\n        C1 --> C1_1[轻量时序提取器: 全局上下文与显著性排序/Lightweight Temporal Extractor: global context & saliency ranking]\n        C1 --> C1_2[4D分层编码器: 从Top-k窗口学习/4D Hierarchical Encoder: learn from top-k windows]\n        D --> D1[性能/Performance]\n        D --> D2[效率/Efficiency]\n        D1 --> D1_1[在七个基准上达到SOTA/Achieves SOTA on seven benchmarks]\n        D2 --> D2_1[仅需4千次预训练会话/Only 4k pre-training sessions]\n        D2 --> D2_2[GPU内存降至30%/GPU memory reduced to ~30%]"
    },
    {
      "title": "CrownGen: Patient-customized Crown Generation via Point Diffusion Model",
      "authors": "Juyoung Bae, Moo Hyun Son, Jiale Peng, Wanting Qu, Wener Chen, Zelin Qiu, Kaixin Li, Xiaojuan Chen, Yifan Lin, Hao Chen",
      "institution": "Hong Kong University of Science and Technology, University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.21890",
      "code": null,
      "tags": [
        "3D shape generation",
        "diffusion model",
        "point cloud",
        "dental crown",
        "generative framework",
        "boundary prediction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2baf8cabd0677f151bf313fb7831de809fe9ecc0a70533ba9d2cffabe95b064b_w640_q70.webp",
      "contributions": "1. A novel generative framework (CrownGen) that automates patient-customized dental crown design. 2. A method using a denoising diffusion model on a tooth-level point cloud representation with a boundary prediction module for spatial priors. 3. Validation through a large-scale quantitative benchmark and a clinical study showing the generated crowns are non-inferior to manual designs and reduce design time.",
      "summary": "This paper presents CrownGen, a framework that automates dental crown design using a point diffusion model. It uses a boundary prediction module and a generative module to create patient-customized crowns in a single pass. Clinical validation shows the generated crowns match manual quality while significantly reducing design time.",
      "mindmap": "graph TB\n        A[CrownGen: Patient-customized Crown Generation<br>基于点扩散模型的个性化牙冠生成] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Digital crown design is labor-intensive<br>数字化牙冠设计费时费力]\n        C --> C1[Uses a point diffusion model<br>使用点扩散模型]\n        C --> C2[Has a boundary prediction module<br>包含边界预测模块]\n        D --> D1[Surpasses SOTA in geometric fidelity<br>几何保真度超越现有方法]\n        D --> D2[Reduces active design time<br>减少主动设计时间]\n        D --> D3[Crowns are clinically non-inferior<br>临床质量不劣于人工设计]"
    },
    {
      "title": "High-Fidelity and Long-Duration Human Image Animation with Diffusion Transformer",
      "authors": "Shen Zheng, Jiaran Cai, Yuansheng Guan, Shenneng Huang, Xingpei Ma, Junjie Cao, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang, Xiao-Ping Zhang",
      "institution": "Guangzhou Quwan Network Technology, Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.21905",
      "code": null,
      "tags": [
        "human image animation",
        "diffusion transformer",
        "hybrid implicit guidance",
        "position shift adaptive module",
        "skeleton alignment"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1dcd7ed4538aa6140dc40b848f4f018648b9407793f582523c563f79c97253f1_w640_q70.webp",
      "contributions": "1. Designed a set of hybrid implicit guidance signals and a sharpness guidance factor to incorporate detailed facial and hand features for high-fidelity generation. 2. Proposed a Position Shift Adaptive Module (time-aware position shift fusion) to enable video generation of arbitrary length. 3. Introduced a novel data augmentation strategy and a skeleton alignment model to mitigate the impact of human shape variations across identities.",
      "summary": "This paper addresses the challenges of generating high-fidelity and long-duration human animation videos by proposing a Diffusion Transformer (DiT)-based framework. The method introduces novel guidance mechanisms for facial/hand details, a module for arbitrary-length generation, and techniques to handle identity shape variations. Experimental results show it outperforms existing state-of-the-art approaches.",
      "mindmap": "graph TB\n        A[”HIGH-FIDELITY AND LONG-DURATION HUMAN IMAGE ANIMATION WITH DIFFUSION TRANSFORMER<br>基于扩散Transformer的高保真长时人体图像动画”] --> B[”核心问题/Problem”]\n        A --> C[”主要方法/Method”]\n        A --> D[”关键结果/Results”]\n        B --> B1[”长视频生成挑战<br>Long-duration Video Generation”]\n        B --> B2[”面部与手部细节合成不足<br>Lack of Fine-grained Facial/Hand Details”]\n        C --> C1[”混合隐式引导信号<br>Hybrid Implicit Guidance”]\n        C --> C2[”位置偏移自适应模块<br>Position Shift Adaptive Module”]\n        C --> C3[”数据增强与骨架对齐<br>Data Augmentation & Skeleton Alignment”]\n        D --> D1[”超越现有SOTA方法<br>Outperforms SOTA”]\n        D --> D2[”实现超1分钟动画<br>Exceeds 1-minute Animation”]"
    },
    {
      "title": "Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition",
      "authors": "Zeyu Liang, Hailun Xia, Naichuan Zheng",
      "institution": "Beijing University of Posts and Telecommunications",
      "link": "https://arxiv.org/pdf/2512.21916",
      "code": null,
      "tags": [
        "multimodal action recognition",
        "human-centric graph representation learning",
        "attention-based post calibration",
        "spatiotemporal graph",
        "multimodal fusion",
        "skeleton-guided sampling"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8ada463935a334d688b0ec740d70f3f4578fc9000b1c19263667ad6c53ea2b18_w640_q70.webp",
      "contributions": "1. Proposes PAN, the first human-centric graph representation learning framework for multimodal action recognition, which models RGB patches containing human joints as spatiotemporal graphs to align with skeleton data and suppress redundancy. 2. Introduces an attention-based post calibration module to reduce the framework's dependency on high-quality skeletal data with minimal performance cost. 3. Presents two model variants, PAN-Ensemble (dual-path with late fusion) and PAN-Unified (single-network unified learning), both achieving state-of-the-art performance on three benchmark datasets.",
      "summary": "This paper addresses the challenge of effectively fusing heterogeneous RGB and skeleton data for action recognition by proposing PAN, a human-centric graph learning framework. PAN represents RGB patches containing human joints as spatiotemporal graphs, enabling semantically coherent multimodal fusion and reducing RGB redundancy. The proposed method achieves state-of-the-art performance on three datasets through its two variants, PAN-Ensemble and PAN-Unified.",
      "mindmap": "graph TB\n        A[Patch as Node: Human-Centric Graph Representation Learning for Multimodal Action Recognition] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[RGB与骨架模态异构融合困难/RGB-Skeleton Heterogeneous Fusion Difficulty]\n        C --> C1[以人为中心的图表示学习/Human-Centric Graph Representation Learning]\n        C1 --> C2[基于注意力的后校准/Attention-Based Post Calibration]\n        C1 --> C3[双变体: PAN-Ensemble与PAN-Unified/Two Variants: PAN-Ensemble & PAN-Unified]\n        D --> D1[三个数据集上SOTA性能/SOTA Performance on Three Datasets]"
    },
    {
      "title": "Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning",
      "authors": "Tao Yang, Xiuying Wang, Hao Liu, Guanzhong Gong, Lian-Ming Wu, Yu-Ping Wang, Lisheng Wang",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.21924",
      "code": null,
      "tags": [
        "medical image analysis",
        "unsupervised anomaly detection",
        "disentangled representation",
        "pseudo-healthy image reconstruction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/785ff0154b41353c2fdfb9a61f66c2f97de1b49c0e9dbba451d335e3a8460e71_w640_q70.webp",
      "contributions": "1. Proposed a disentangled representation module to decouple brain MRI into imaging-invariant anatomical information and imaging-specific information, improving model generalizability across multi-modality and multi-center data. 2. Designed an edge-to-image restoration module that reconstructs high-quality pseudo-healthy images from edge information, suppressing the propagation of abnormal residuals. 3. Introduced brain anatomical priors and a differentiable one-hot encoding operator to constrain and stabilize the disentanglement learning process.",
      "summary": "This paper addresses the limitations of generalizability and performance in unsupervised anomaly detection for brain MRI. It proposes a new framework that disentangles anatomical from imaging information and reconstructs pseudo-healthy images from edges, achieving state-of-the-art results on multi-center datasets.",
      "mindmap": "graph TB\n        A[Unsupervised Anomaly Detection in Brain MRI via Disentangled Anatomy Learning] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[泛化性差与异常残留/Generalizability & Residuals]\n        C --> C1[解耦表示模块/Disentangled Representation Module]\n        C --> C2[边缘到图像恢复模块/Edge-to-Image Restoration Module]\n        D --> D1[性能超越17种SOTA方法/Outperforms 17 SOTA Methods]"
    },
    {
      "title": "AutoPP: Towards Automated Product Poster Generation and Optimization",
      "authors": "Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law",
      "institution": "JD.COM",
      "link": "https://arxiv.org/pdf/2512.21921",
      "code": "https://github.com/JD-GenX/AutoPP",
      "tags": [
        "image generation",
        "product poster generation",
        "click-through rate optimization",
        "isolated direct preference optimization",
        "AutoPP1M dataset",
        "unified design module"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f18700c508d46682b8040947d4af38f1b6c821d269a8518370be8bf9c574fe71_w640_q70.webp",
      "contributions": "1. An automated pipeline (AutoPP) for end-to-end product poster generation and optimization, requiring only basic product information as input. 2. A novel optimization method that uses systematic element replacement and Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to specific poster elements. 3. The creation and release of AutoPP1M, the largest dataset for product poster generation and optimization, containing one million posters and user feedback.",
      "summary": "The paper introduces AutoPP, an automated pipeline that generates product posters from basic product information and then optimizes them for higher Click-Through Rate (CTR) using online feedback and a novel Isolated Direct Preference Optimization technique. It is supported by a large-scale dataset, AutoPP1M. Experiments show that AutoPP achieves state-of-the-art performance in both offline and online evaluations.",
      "mindmap": "graph TB\n        A[AutoPP: Towards Automated Product Poster Generation and Optimization] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[人工制作与优化海报耗时耗力/Manual poster creation and optimization is laborious]\n        C --> C1[自动化生成与优化管道/Automated generation and optimization pipeline]\n        C1 --> C1_1[生成器: 统一设计模块与元素渲染/Generator: Unified design & element rendering]\n        C1 --> C1_2[优化器: 元素替换与IDPO/Optimizer: Element replacement & IDPO]\n        C --> C2[数据集: AutoPP1M/Dataset: AutoPP1M]\n        D --> D1[离线和在线SOTA结果/Offline and online SOTA results]\n        D --> D2[代码与数据集公开/Code & dataset released]"
    },
    {
      "title": "Data relativistic uncertainty framework for low-illumination anime scenery image enhancement",
      "authors": "Yiquan Gao, John See",
      "institution": "Heriot-Watt University",
      "link": "https://arxiv.org/pdf/2512.21944",
      "code": null,
      "tags": [
        "low-light image enhancement",
        "Data Relativistic Uncertainty",
        "Unsupervised Learning",
        "EnlightenGAN",
        "Anime Scenery",
        "Domain Gap"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp",
      "contributions": "1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.",
      "summary": "This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.",
      "mindmap": "graph TB\n        Root(”Data relativistic uncertainty framework for low-illumination anime scenery image enhancement”) --> Problem\n        Root --> Method\n        Root --> Results\n        Problem(”核心问题/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.”)\n        Method(”主要方法/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.”)\n        Results(”关键结果/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.”)"
    },
    {
      "title": "Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials",
      "authors": "Ali Lotfi, Adam Carter, Thuan Ha, Mohammad Meysami, Kwabena Nketia, Steve Shirtliffe",
      "institution": "University of Saskatchewan, New Mexico State University",
      "link": "https://arxiv.org/pdf/2512.21948",
      "code": null,
      "tags": [
        "remote sensing",
        "vegetation classification",
        "normalized difference polynomials",
        "spectral indices",
        "feature selection",
        "Sentinel-2",
        "illumination invariance"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/511f8067b02514f2cb415c41eb77aefa341affb7b0beba8fc78dcc37ebff9508_w640_q70.webp",
      "contributions": "1. Introduces an automated framework for generating a structured search space of spectral indices using pairwise normalized differences and polynomial combinations up to a fixed degree. 2. Employs feature selection methods (ANOVA filtering, recursive elimination, L1-regularized SVM) to select small, interpretable sets of indices that maintain high classification accuracy. 3. Demonstrates the effectiveness of the approach on a real-world task (Kochia detection), showing that simple degree-2 product indices achieve high accuracy and are deployable on platforms like Google Earth Engine.",
      "summary": "This paper introduces an automated method to discover simple spectral indices for vegetation classification by generating polynomial combinations of pairwise normalized differences and applying feature selection. The method was tested on detecting Kochia with Sentinel-2 imagery, where a single degree-2 index achieved over 96% accuracy. The results show that discriminative signals often come from spectral interactions, and the resulting indices are simple enough for direct deployment in cloud platforms.",
      "mindmap": "graph TB\n        A[Automated Discovery of Parsimonious Spectral Indices via Normalized Difference Polynomials] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[自动化发现用于植被分类的紧凑光谱指数/Automated discovery of compact spectral indices for vegetation classification]\n        C --> C1[生成归一化差异多项式候选特征/Generate candidate features via normalized difference polynomials]\n        C --> C2[使用特征选择方法挑选指数/Use feature selection methods to pick indices]\n        D --> D1[单个二阶指数达到96.26%准确率/Single degree-2 index achieves 96.26% accuracy]\n        D --> D2[指数简单，可直接部署于GEE/Indices are simple and deployable on GEE]"
    },
    {
      "title": "Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models",
      "authors": "Dunyuan XU, Xikai Yang, Yaoqian Li, Juzheng Miao, Jinpeng Li, Pheng-Ann Heng",
      "institution": "The Chinese University of Hong Kong (CUHK)",
      "link": "https://arxiv.org/pdf/2512.21964",
      "code": null,
      "tags": [
        "multi-modal robustness",
        "multi-modal large language model",
        "input perturbation",
        "training-free calibration",
        "denoising",
        "benchmark"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/252aad7806c9998b636d3335e0a4e90e437f57a51e2a942eb6d7b871a984ef2b_w640_q70.webp",
      "contributions": "1. A systematic analysis of the impact of various visual and textual perturbations on medical MLLMs, revealing their sensitivity. 2. A novel training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs' own capabilities for robustness enhancement. 3. The introduction of two specific methods within IMC: Perturbation-aware Denoising Calibration (PDC) for visual noise and a Self-instantiated Multi-agent System (SMS) for textual noise.",
      "summary": "This paper addresses the vulnerability of Medical Multi-modal Large Language Models (MLLMs) to input noise like imaging artifacts and text errors. It proposes a training-free framework called Inherent-enhanced Multi-modal Calibration (IMC) that uses the model's own vision encoder and self-assessment capabilities to denoise inputs. Experiments on a new benchmark show the method achieves state-of-the-art robustness, enhancing clinical applicability.",
      "mindmap": "graph TB\n        A[Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models] --> B(核心问题/Problem: MLLMs are sensitive to input noise, undermining clinical use)\n        A --> C(主要方法/Method: Training-free IMC framework with PDC for vision and SMS for text)\n        A --> D(关键结果/Results: SOTA performance on new multi-modal noise benchmark)"
    },
    {
      "title": "LVLM-Aided Alignment of Task-Specific Vision Models",
      "authors": "Alexander Koebler, Lukas Kuhn, Ingo Thon, Florian Buettner",
      "institution": "Goethe University Frankfurt, Siemens AG, German Cancer Research Center (DKFZ)",
      "link": "https://arxiv.org/pdf/2512.21985",
      "code": null,
      "tags": [
        "model alignment and interpretability",
        "LVLM-VA",
        "spurious correlations",
        "explainable AI (XAI)",
        "vision-language model",
        "human-in-the-loop"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7c85ee676094853ba26d7711ac1d50d0ab994498bc2f91f2aaab4f1104d3222_w640_q70.webp",
      "contributions": "1. Introduces LVLM-Aided Visual Alignment (LVLM-VA), a novel method for aligning small task-specific vision models with human domain knowledge using a Large Vision Language Model (LVLM). 2. Proposes a bidirectional interface that translates model behavior into natural language and maps human class-level specifications to image-level critiques, enabling efficient expert-model interaction. 3. Demonstrates that the method effectively reduces the model's dependence on spurious features and group-specific biases without requiring fine-grained, instance-level feedback.",
      "summary": "This paper addresses the problem of small task-specific vision models relying on spurious correlations, which leads to brittle real-world performance. The authors propose LVLM-Aided Visual Alignment (LVLM-VA), a method that uses a Large Vision Language Model to create a bidirectional interface between human domain knowledge and the model, translating explanations and critiques. The method is shown to significantly improve model alignment with human specifications and reduce dependence on spurious features across synthetic and real-world datasets.",
      "mindmap": "graph TB\n        A[LVLM-Aided Alignment of Task-Specific Vision Models] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[小规模任务专用视觉模型依赖虚假相关性/Small task-specific vision models rely on spurious correlations]\n        B --> B2[导致部署时行为脆弱/Leads to brittle behavior when deployed]\n        C --> C1[利用LVLM进行视觉对齐/Leverage LVLM for visual alignment]\n        C --> C2[双向接口: 行为转语言, 规范转评估/Bidirectional interface: behavior to language, specs to critiques]\n        D --> D1[模型行为与人类规范更好对齐/Better alignment of model behavior with human specifications]\n        D --> D2[减少对虚假特征和偏见的依赖/Reduced dependence on spurious features and biases]"
    },
    {
      "title": "A Lightweight Multi-Scale Attention Framework for Real-Time Spinal Endoscopic Instance Segmentation",
      "authors": "Qi Lai, JunYan Li, Qiang Cai, Lei Wang, Tao Yan, XiaoKun Liang",
      "institution": "The affiliations include IEEE members, suggesting an academic or research institution, but specific names are not fully listed in the provided text. Based on the GitHub URL pattern, a likely institution is not explicitly stated in the given content.",
      "link": "https://arxiv.org/pdf/2512.21984",
      "code": "https://github.com/hhwmortal/PELD-Instance-segmentation",
      "tags": [
        "instance segmentation",
        "re-parameterized convolution",
        "efficient multi-scale attention",
        "lightweight multi-task head"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ddb0a22c2e72d9e15e574413f3c78c60ca63b7152d6320583b76bc00a64110f_w640_q70.webp",
      "contributions": "1. Proposed LMSF-A, a lightweight multi-scale attention framework co-designed across backbone, neck, and head for real-time instance segmentation. 2. Introduced the C2f-Pro backbone module combining RepViT-style re-parameterized convolution with efficient multi-scale attention for multi-branch training and fast inference. 3. Released a new clinically reviewed PELD dataset for spinal endoscopic instance segmentation.",
      "summary": "This paper addresses the challenge of real-time instance segmentation in spinal endoscopy, where factors like a narrow field of view and hardware constraints make deployment difficult. The authors propose LMSF-A, a lightweight framework featuring a re-parameterized backbone, a feature fusion neck, and a shared head, which achieves competitive accuracy with only 1.8M parameters. The method is validated on a new clinical dataset and shows good generalization.",
      "mindmap": "graph TB\n        A[LMSF-A: Real-Time Spinal Endoscopic Instance Segmentation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[狭窄视野, 伪影, 硬件限制/Narrow FOV, Artifacts, Hardware Constraints]\n        C --> C1[轻量级多尺度注意力框架/Lightweight Multi-scale Attention Framework]\n        C1 --> C2[主干: C2f-Pro (重参数化卷积+EMA)/Backbone: C2f-Pro (Rep Conv+EMA)]\n        C1 --> C3[颈部: SSFF与TFE/Neck: SSFF and TFE]\n        C1 --> C4[头部: 轻量共享头 (LMSH)/Head: Lightweight Shared Head (LMSH)]\n        D --> D1[性能优越, 参数量少 (1.8M)/High Performance, Few Params (1.8M)]\n        D --> D2[发布PELD数据集/Release PELD Dataset]\n        D --> D3[良好泛化性/Good Generalization]"
    },
    {
      "title": "Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs",
      "authors": "Jiayu Hu, Beibei Li, Jiangwei Xia, Yanjun Qin, Bing Ji, Zhongshi He",
      "institution": "Chongqing University, Xinjiang University",
      "link": "https://arxiv.org/pdf/2512.21999",
      "code": "https://github.com/hujiayu1223/ALEAHallu",
      "tags": [
        "multi-modal training",
        "hallucination mitigation",
        "adversarial parametric editing",
        "parameter clustering",
        "visual-language models",
        "activation dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d0943d59c080a6d39ec6bf82dc20b417033bcda2fee9178d9a845e37b90a47c_w640_q70.webp",
      "contributions": "1. Proposes a novel adversarial parametric editing framework (ALEAHallu) following an Activate-Locate-Edit Adversarially paradigm for mitigating hallucinations in VLMs. 2. Introduces a method to construct an activation dataset with grounded and hallucinatory response pairs and identifies critical hallucination-prone parameter clusters via differential hidden state analysis. 3. Demonstrates the framework's effectiveness by fine-tuning identified parameter clusters with adversarially tuned prefixes to force the model to prioritize visual evidence over linguistic priors.",
      "summary": "This paper addresses the hallucination problem in Vision-Language Models (VLMs), where models generate content misaligned with visual inputs due to over-reliance on linguistic priors. The authors propose ALEAHallu, an adversarial parametric editing framework that identifies and fine-tunes hallucination-prone parameter clusters using adversarially optimized prompts to enhance visual grounding. Evaluations show the method significantly reduces hallucinations in both generative and discriminative VLM tasks.",
      "mindmap": "graph TB\n        A[Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: VLM幻觉问题/VLM Hallucination Issue]\n        C[主要方法/Method: ALEAHallu框架/ALEAHallu Framework]\n        D[关键结果/Results: 有效缓解幻觉/Effectively Mitigates Hallucinations]\n        C --> C1[激活数据集/Activation Dataset]\n        C --> C2[定位关键参数/Locate Critical Parameters]\n        C --> C3[对抗性编辑/Adversarial Editing]"
    },
    {
      "title": "iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception",
      "authors": "Sarthak Mehrotra, Sairam V C Rebbapragada, Mani Hemanth Reddy Bonthu, Vineeth N Balasubramanian",
      "institution": "Indian Institute of Technology, Bombay, Indian Institute of Technology, Hyderabad",
      "link": "https://arxiv.org/pdf/2512.22009",
      "code": null,
      "tags": [
        "agent system",
        "multimodal large language models (MLLMs)",
        "slow-fast inference",
        "adaptive perception",
        "visual grounding",
        "lightweight agent"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6067c294acdf08cb7927ad962f2d0d2c4f3efd938837b5cdb9ca1bdad4019422_w640_q70.webp",
      "contributions": "1. Introduces iSHIFT, a lightweight (2.5B parameter) agent framework that integrates implicit chain-of-thought reasoning with a perception control module. 2. Proposes a novel slow-fast hybrid inference mechanism, allowing the model to adaptively switch between a detailed, high-precision \"slow mode\" and an efficient \"fast mode\" based on task demands. 3. Employs special perception tokens to dynamically guide the model's visual attention to relevant screen regions, enabling precise visual grounding for GUI interaction.",
      "summary": "The paper addresses the challenge of building efficient yet precise GUI agents by proposing iSHIFT, a lightweight MLLM agent. iSHIFT uses an adaptive slow-fast inference mechanism and perception tokens to dynamically balance efficiency and accuracy for GUI tasks. Despite its small 2.5B size, it achieves state-of-the-art performance on multiple benchmarks.",
      "mindmap": "graph TB\n        A[iSHIFT: Lightweight Slow-Fast GUI Agent with Adaptive Perception] --> B[核心问题/Problem: Building efficient and precise GUI agents is challenging]\n        A --> C[主要方法/Method: Slow-fast hybrid inference with adaptive perception tokens]\n        A --> D[关键结果/Results: Matches SOTA performance with compact 2.5B size]"
    },
    {
      "title": "LongFly: Long-Horizon UAV Vision-and-Language Navigation with Spatiotemporal Context Integration",
      "authors": "Wen Jiang, Li Wang, Kangyao Huang, Wei Fan, Jinyuan Liu, Shaoyu Liu, Hongwei Duan, Bin Xu, Xiangyang Ji",
      "institution": "Beijing Institute of Technology, Tsinghua University, Dalian University of Technology, Xidian University",
      "link": "https://arxiv.org/pdf/2512.22010",
      "code": null,
      "tags": [
        "vision-and-language navigation",
        "spatiotemporal context modeling",
        "slot-based compression",
        "prompt-guided multimodal integration"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c00484796a9df425f1884ea8ae22314b0592466ebe305303cf7f1f0c467b528_w640_q70.webp",
      "contributions": "1. A slot-based historical image compression module to distill multi-view historical observations into fixed-length contextual representations. 2. A spatiotemporal trajectory encoding module to capture the temporal dynamics and spatial structure of UAV trajectories. 3. A prompt-guided multimodal integration module to fuse spatiotemporal context with current observations for robust waypoint prediction.",
      "summary": "This paper proposes LongFly, a framework for long-horizon UAV vision-and-language navigation that addresses the challenge of modeling spatiotemporal context. The method integrates a history-aware modeling strategy with modules for compressing past observations, encoding trajectories, and fusing multimodal information. Experimental results show it outperforms state-of-the-art baselines in navigation success metrics across seen and unseen environments.",
      "mindmap": "graph TB\n        A[LongFly: Long-Horizon UAV Vision-and-Language Navigation] --> B[核心问题/Problem: Current UAV VLN methods struggle with long-horizon spatiotemporal context, leading to inaccurate alignment and unstable planning.]\n        A --> C[主要方法/Method: History-aware spatiotemporal modeling with slot-based image compression, trajectory encoding, and prompt-guided multimodal integration.]\n        A --> D[关键结果/Results: Outperforms SOTA baselines by 7.89% in success rate and 6.33% in SPL.]"
    },
    {
      "title": "SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching",
      "authors": "Xiangwen Zhang, Xiaowei Dai, Runnan Chen, Xiaoming Chen, Zeke Zexi Hu",
      "institution": "Beijing Technology and Business University, The University of Sydney",
      "link": "https://arxiv.org/pdf/2512.22016",
      "code": null,
      "tags": [
        "Human-Computer Interaction (HCI)",
        "Gestural Interaction",
        "Physics Simulation",
        "Air-drawn Sketches",
        "VR Content Creation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79eefb65bc566df3d83196a3500892e4d09f26b4aa064a68193142a9ec59b0c0_w640_q70.webp",
      "contributions": "1. A novel VR interaction framework (SketchPlay) that combines air-drawn sketches and gestures to create dynamic, physically realistic scenes. 2. A method that uses sketches to capture object/scene structure and gestures to convey physical cues (velocity, force) for defining motion and behavior. 3. Enables the generation of complex physical phenomena (rigid body motion, elastic deformation, cloth dynamics) through an intuitive, controller-free creation process.",
      "summary": "The paper proposes SketchPlay, a VR framework that allows users to create physically realistic dynamic scenes by sketching objects in the air and using gestures to define their motion. This method combines structural and dynamic intent to simulate phenomena like rigid body and cloth dynamics. The approach is shown to be more expressive and offer a better user experience than text-driven methods, lowering the barrier for non-expert creators.",
      "mindmap": "graph TB\n        A[SketchPlay: Intuitive Creation of Physically Realistic VR Content with Gesture-Driven Sketching] --> B[核心问题/Problem: Creating physically realistic VR content is complex and requires expert tools, creating barriers for non-expert users.]\n        A --> C[主要方法/Method: A novel VR framework that transforms air-drawn sketches (for structure) and gestures (for physical cues like velocity/force) into dynamic, physically realistic scenes.]\n        A --> D[关键结果/Results: Offers significant advantages in expressiveness and user experience over traditional methods, lowering the entry barrier and showing potential for education, art, and storytelling.]"
    },
    {
      "title": "Patch-Discontinuity Mining for Generalized Deepfake Detection",
      "authors": "Huanhuan Yuan, Yang Ping, Zhengqin Xu, Junyi Cao, Shuai Jia, Chao Ma",
      "institution": "Shanghai Jiao Tong University, Chinese Academy of Military Science",
      "link": "https://arxiv.org/pdf/2512.22027",
      "code": "https://gendf.github.io/",
      "tags": [
        "deepfake detection",
        "patch-discontinuity",
        "feature space redistribution",
        "classification-invariant feature augmentation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f93245fe55a81a22e8997db22f045beb4494e864df08fd145bf36088517c580_w640_q70.webp",
      "contributions": "1. Proposed a deepfake-specific representation learning (DSRL) scheme to capture patch-discontinuity patterns in fake images and continuity in real ones. 2. Introduced a feature space redistribution (FSR) scheme to separately optimize the distributions of real and fake features, mitigating domain mismatch. 3. Designed a classification-invariant feature augmentation (CIFAug) strategy to enhance generalization by expanding feature scopes orthogonally to the classification direction without adding trainable parameters.",
      "summary": "This paper proposes GenDF, a generalized deepfake detection framework that transfers a large-scale vision model to detect fake faces by mining patch-discontinuity patterns. It introduces three key components—deepfake-specific representation learning, feature space redistribution, and a parameter-free feature augmentation strategy—to improve generalization to unseen forgery methods. The method achieves state-of-the-art cross-domain performance with only 0.28M trainable parameters, demonstrating high efficiency and effectiveness.",
      "mindmap": "graph TB\n        A[”Patch-Discontinuity Mining for Generalized Deepfake Detection”] --> B[”核心问题/Problem: Existing deepfake detectors generalize poorly to unseen forgery patterns.”]\n        A --> C[”主要方法/Method: Propose GenDF framework with DSRL, FSR, and CIFAug to learn generalizable features from a pre-trained vision model.”]\n        A --> D[”关键结果/Results: Achieves SOTA generalization with only 0.28M parameters.”]"
    },
    {
      "title": "Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models",
      "authors": "Zongmin Zhang, Zhen Sun, Yifan Liao, Wenhan Dong, Xinlei He, Xingshuo Han, Shengmin Xu, Xinyi Huang",
      "institution": "Hong Kong University of Science and Technology (Guangzhou), Nanjing University of Aeronautics and Astronautics, Fujian Normal University, Jinan University",
      "link": "https://arxiv.org/pdf/2512.22046",
      "code": null,
      "tags": [
        "backdoor attacks",
        "video segmentation foundation models",
        "backdoor attack",
        "two-stage training",
        "gradient analysis",
        "attention shift"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6a7e9ff0ffc63f2085d6ca65db8e87f14fed435be5d8a51fd3d1265b22b668b1_w640_q70.webp",
      "contributions": "1. Identifies the ineffectiveness of classic backdoor attacks on prompt-driven Video Segmentation Foundation Models (VSFMs) and provides analysis via gradient and attention maps. 2. Proposes BadVSFM, the first dedicated backdoor attack framework for VSFMs, using a novel two-stage training strategy to separate clean and triggered representations. 3. Demonstrates strong, controllable attack performance across multiple models and datasets while preserving clean-task accuracy, and shows the vulnerability persists against existing defenses.",
      "summary": "This paper identifies that standard backdoor attacks fail on prompt-driven Video Segmentation Foundation Models (VSFMs) like SAM2. To solve this, the authors propose BadVSFM, a two-stage backdoor attack framework that successfully implants a controllable backdoor by steering the encoder and decoder separately. Experiments show the attack is effective and evades current defenses, revealing a significant security vulnerability in VSFMs.",
      "mindmap": "graph TB\n        A[Backdoor Attacks on Prompt-Driven Video Segmentation Foundation Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Classic backdoor attacks fail on VSFMs (ASR<5%)]\n        C[主要方法/Method: BadVSFM - Two-stage training (steer encoder, train decoder)]\n        D[关键结果/Results: High ASR, preserves clean performance, defenses ineffective]"
    },
    {
      "title": "StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars",
      "authors": "Zhiyao Sun, Ziqiao Peng, Yifeng Ma, Yi Chen, Zhengguang Zhou, Zixiang Zhou, Guozhen Zhang, Youliang Zhang, Yuan Zhou, Qinglin Lu, Yong-Jin Liu",
      "institution": "Tsinghua University, Renmin University of China, Tencent Hunyuan, Nanjing University",
      "link": "https://arxiv.org/pdf/2512.22065",
      "code": "https://streamavatar.github.io",
      "tags": [
        "diffusion models",
        "autoregressive distillation",
        "adversarial refinement",
        "real-time streaming",
        "reference-anchored positional re-encoding",
        "consistency-aware discriminator"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfab379af0573f473d87dcf0d615682a378ca15f3e5158289e25ba256124414_w640_q70.webp",
      "contributions": "1. A two-stage autoregressive adaptation and acceleration framework (autoregressive distillation + adversarial refinement) to adapt a non-causal human video diffusion model for real-time, interactive streaming. 2. Three novel components to ensure long-term stability and consistency: a Reference Sink, a Reference-Anchored Positional Re-encoding (RAPR) strategy, and a Consistency-Aware Discriminator. 3. A one-shot, interactive human avatar model capable of generating both natural talking and listening behaviors with coherent full-body gestures, surpassing existing methods in quality, efficiency, and interaction naturalness.",
      "summary": "This paper addresses the challenge of making diffusion-based human avatar generation suitable for real-time, interactive streaming. It proposes StreamAvatar, a two-stage framework that adapts a high-fidelity human video diffusion model using autoregressive distillation and adversarial refinement, incorporating novel components for long-term consistency. The method achieves state-of-the-art performance in generating high-resolution, full-body interactive avatars with natural talking/listening behaviors in real-time.",
      "mindmap": "graph TB\n        A[StreamAvatar: Streaming Diffusion Models for Real-Time Interactive Human Avatars] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Non-causal, high-cost diffusion models unsuitable for real-time streaming; Limited to head-and-shoulder, lacking gestures.]\n        C[主要方法/Method<br>Two-stage autoregressive adaptation (distillation + refinement) with Reference Sink, RAPR, Consistency-Aware Discriminator.]\n        D[关键结果/Results<br>State-of-the-art real-time, interactive full-body avatar with natural talking/listening and gestures.]"
    },
    {
      "title": "MAI-UI Technical Report: Real-World Centric Foundation GUI Agents",
      "authors": "Hanzhang Zhou, Xu Zhang, Panrong Tong, Jianan Zhang, Liangyu Chen, Quyu Kong, Chenglin Cai, Chen Liu, Yue Wang, Jingren Zhou, Steven Hoi",
      "institution": "Tongyi Lab, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.22047",
      "code": "https://github.com/Tongyi-MAI/MAI-UI",
      "tags": [
        "agent system",
        "GUI agent",
        "device-cloud collaboration",
        "online reinforcement learning",
        "self-evolving data pipeline",
        "foundation model"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/819d67f31c44f0847dff819fbdf82773fea54c76d001f429e9e731b2ba253b85_w640_q70.webp",
      "contributions": "1. A self-evolving data pipeline that expands GUI navigation data to include user interaction and tool calls. 2. A native device-cloud collaboration system that routes task execution based on state to improve efficiency and privacy. 3. An online RL framework with optimizations for scaling parallel environments and context length.",
      "summary": "The paper presents MAI-UI, a family of foundation GUI agents designed to address challenges in real-world deployment, such as limited interaction and dynamic environments. Its unified methodology includes a self-evolving data pipeline, a device-cloud collaboration system, and an online RL framework. MAI-UI achieves state-of-the-art performance on multiple GUI grounding and mobile navigation benchmarks.",
      "mindmap": "graph TB\n        A[MAI-UI Technical Report: Real-World Centric Foundation GUI Agents] --> B1(核心问题/Problem)\n        A --> B2(主要方法/Method)\n        A --> B3(关键结果/Results)\n        B1 --> C1(缺乏原生人机交互/Lack of Native Agent-User Interaction)\n        B1 --> C2(仅UI操作的限制/Limits of UI-Only Operation)\n        B1 --> C3(缺乏实用部署架构/Absence of Practical Deployment Architecture)\n        B1 --> C4(动态环境脆弱性/Brittleness in Dynamic Environments)\n        B2 --> D1(自演进数据管道/Self-Evolving Data Pipeline)\n        B2 --> D2(原生设备-云协作系统/Native Device-Cloud Collaboration System)\n        B2 --> D3(在线RL框架/Online RL Framework)\n        D1 --> E1(包含用户交互/Includes User Interaction)\n        D1 --> E2(包含MCP工具调用/Includes MCP Tool Calls)\n        D3 --> E3(扩展并行环境/Scales Parallel Environments)\n        D3 --> E4(扩展上下文长度/Scales Context Length)\n        B3 --> F1(GUI Grounding SOTA/GUI Grounding SOTA)\n        B3 --> F2(移动导航SOTA/Mobile Navigation SOTA)\n        B3 --> F3(系统性能提升/System Performance Gains)\n        F1 --> G1(ScreenSpot-Pro: 73.5%/ScreenSpot-Pro: 73.5%)\n        F1 --> G2(MMBench GUI L2: 91.3%/MMBench GUI L2: 91.3%)\n        F2 --> G3(AndroidWorld: 76.7%/AndroidWorld: 76.7%)\n        F2 --> G4(MobileWorld: 41.7%/MobileWorld: 41.7%)\n        F3 --> G5(设备性能提升33%/On-Device Perf. +33%)\n        F3 --> G6(云调用减少40%/Cloud Calls -40%)"
    },
    {
      "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model",
      "authors": "Xiaofeng Mao, Zhen Li, Chuanhao Li, Xiaojie Xu, Kaining Ying, Tong He, Jiangmiao Pang, Yu Qiao, Kaipeng Zhang",
      "institution": "Shanghai AI Laboratory, Fudan University, Shanghai Innovation Institute",
      "link": "https://arxiv.org/pdf/2512.22096",
      "code": "https://github.com/stdstu12/YUME",
      "tags": [
        "diffusion models",
        "interactive world generation",
        "long-video generation",
        "attention distillation",
        "context compression",
        "text-controlled generation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f7ffd3f0a90ba67551ade4e28abf8e27d5d08c106e463f85f9447011008416b_w640_q70.webp",
      "contributions": "1. A long-video generation framework integrating unified context compression with linear attention. 2. A real-time streaming acceleration strategy using bidirectional attention distillation and an enhanced text embedding scheme. 3. A text-controlled method for generating world events.",
      "summary": "This paper proposes Yume-1.5, a framework to address challenges in generating interactive, explorable worlds using diffusion models, such as large model size and slow inference. The method introduces a novel architecture combining context compression, attention distillation, and text-based event control to enable real-time, keyboard-controlled world generation from text or images. The work concludes with a public codebase demonstrating the feasibility of text-controlled interactive world creation.",
      "mindmap": "graph TB\n        Root[Yume-1.5: A Text-Controlled Interactive World Generation Model] --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1[大模型参数与慢推理/Large Model & Slow Inference]\n        Problem --> P2[缺乏文本控制/Lack of Text Control]\n        Method --> M1[长视频生成框架/Long-Video Gen Framework]\n        Method --> M2[实时流加速策略/Real-time Streaming]\n        Method --> M3[文本控制事件生成/Text-Controlled Events]\n        M1 --> M1_Sub[统一上下文压缩与线性注意力/Unified Context Compression & Linear Attention]\n        M2 --> M2_Sub[双向注意力蒸馏与文本嵌入/Bidirectional Attention Distillation & Text Embedding]\n        Results --> R1[生成交互式世界/Generates Interactive Worlds]\n        Results --> R2[支持键盘探索/Supports Keyboard Exploration]\n        Results --> R3[公开代码库/Public Codebase]"
    },
    {
      "title": "Learning Association via Track-Detection Matching for Multi-Object Tracking",
      "authors": "Momir Adžemović",
      "institution": "University of Belgrade",
      "link": "https://arxiv.org/pdf/2512.22105",
      "code": "https://github.com/Robotmurlock/TDLP",
      "tags": [
        "multi-object tracking",
        "tracking-by-detection",
        "link prediction",
        "association learning"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d61776d860da3a903f769a1427363e91df9be50d56b83666c73ac36008099062_w640_q70.webp",
      "contributions": "1. Proposes Track-Detection Link Prediction (TDLP), a tracking-by-detection method that learns data association via link prediction between tracks and detections, eliminating handcrafted heuristics. 2. Demonstrates that TDLP achieves state-of-the-art performance across multiple benchmarks, outperforming both traditional tracking-by-detection and end-to-end methods. 3. Provides analysis showing link prediction is more effective than metric learning for association, especially when handling heterogeneous features like bounding boxes.",
      "summary": "The paper proposes TDLP, a tracking-by-detection method that learns to associate object detections across video frames by predicting links between existing tracks and new detections. This approach avoids handcrafted rules while remaining computationally efficient. Experiments show it outperforms state-of-the-art methods, and analysis indicates link prediction is superior to metric learning for this association task.",
      "mindmap": "graph TB\n        A[Learning Association via Track-Detection Matching for Multi-Object Tracking] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Tracking-by-detection methods rely on handcrafted heuristics, end-to-end methods are computationally complex.]\n        C[主要方法/Method<br>Propose TDLP: Track-Detection Link Prediction for learning association via link prediction.]\n        D[关键结果/Results<br>TDLP surpasses SOTA performance; link prediction is more effective than metric learning.]"
    },
    {
      "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
      "authors": "Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang",
      "institution": "Microsoft Research, Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.22120",
      "code": "https://github.com/zss02/BiPS",
      "tags": [
        "visual question answering",
        "perceptual shaping",
        "KL-consistency",
        "KL-separation",
        "evidence-preserving view",
        "evidence-ablated view"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74efcdb66ae5cc2a16fd7b59382dce3e78d24b03e998520275db9a96986fa158_w640_q70.webp",
      "contributions": "1. Proposes Bi-directional Perceptual Shaping (BiPS), a novel training paradigm that uses question-conditioned masked views to generate bidirectional \"where-to-look\" signals. 2. Introduces a dual KL constraint mechanism: a KL-consistency constraint to encourage complete visual evidence coverage and a KL-separation constraint to discourage text-only shortcuts and enforce fine-grained visual reliance. 3. Demonstrates strong performance improvements and out-of-domain generalization across multiple benchmarks without adding inference-time cost.",
      "summary": "The paper addresses the problem that large vision-language models often overlook fine-grained visual evidence and rely on text shortcuts. It proposes Bi-directional Perceptual Shaping (BiPS), a training method that uses KL constraints on evidence-preserving and evidence-ablated image views to shape the model's perception. The method significantly boosts the performance of a base VLM model and shows strong generalization to unseen data.",
      "mindmap": "graph TB\n        A[See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: VLMs overlook fine-grained visual evidence, generalize poorly, and have high inference cost]\n        C[主要方法/Method: BiPS uses bidirectional KL constraints (consistency & separation) on masked views to shape perception during training]\n        D[关键结果/Results: Boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization]"
    },
    {
      "title": "ProEdit: Inversion-based Editing From Prompts Done Right",
      "authors": "Zhi Ouyang, Dian Zheng, Xiao-Ming Wu, Jian-Jian Jiang, Kun-Yu Lin, Jingke Meng, Wei-Shi Zheng",
      "institution": "Sun Yat-sen University, CUHK MMLab, Nanyang Technological University, The University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.22118",
      "code": "https://isee-laboratory.github.io/ProEdit/",
      "tags": [
        "image editing",
        "inversion-based editing",
        "KV-mix",
        "Latents-Shift",
        "plug-and-play",
        "flow inversion"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/878121cf01366a9dcec34198113a6d864c4ea37e9b41c39e221a71b26e72039f_w640_q70.webp",
      "contributions": "1. Proposes KV-mix to mix source and target KV features in the attention mechanism, reducing source influence in edited regions while preserving background. 2. Introduces Latents-Shift to perturb the source latent in the edited region, eliminating the influence of the inverted latent during sampling. 3. Presents a plug-and-play design that can be integrated into existing inversion-based editing methods like RF-Solver, FireFlow, and UniEdit.",
      "summary": "This paper addresses the problem in inversion-based visual editing where over-reliance on source image information hinders accurate attribute changes. The proposed method, ProEdit, introduces two techniques: KV-mix in the attention aspect and Latents-Shift in the latent aspect, to mitigate source influence. It achieves state-of-the-art performance on image and video editing benchmarks and is designed as a plug-and-play module compatible with existing methods.",
      "mindmap": "graph TB\n        A[ProEdit: Inversion-based Editing From Prompts Done Right] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法过度注入源图像信息/Existing methods overly inject source info]\n        B1 --> B2[阻碍属性编辑如姿态、数量、颜色/Hinders editing attributes like pose, number, color]\n        C --> C1[注意力层面: KV-mix/Attention Aspect: KV-mix]\n        C1 --> C11[混合源与目标KV特征/Mix source & target KV features]\n        C --> C2[潜在层面: Latents-Shift/Latent Aspect: Latents-Shift]\n        C2 --> C21[扰动编辑区域的源潜在表示/Perturb source latent in edited region]\n        D --> D1[SOTA性能/SOTA performance]\n        D --> D2[即插即用设计/Plug-and-play design]"
    },
    {
      "title": "A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer with Region-Aware Attention for Gastrointestinal Disease Classification with Explainable AI",
      "authors": "Md Assaduzzaman, Nushrat Jahan Oyshi, Eram Mahamud",
      "institution": "Daffodil International University",
      "link": "https://arxiv.org/pdf/2512.21372",
      "code": null,
      "tags": [
        "medical image classification",
        "Knowledge Distillation",
        "Vision Transformer",
        "Swin Transformer",
        "Explainable AI",
        "Wireless Capsule Endoscopy"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5cd724a13af61d1181b015dc7b5fe86e10cc834beac172261c5bebe54e371bc3_w640_q70.webp",
      "contributions": "1. Proposed a hybrid dual-stream teacher model combining Swin Transformer for global context and Vision Transformer for local features. 2. Developed a compact Tiny-ViT student model via knowledge distillation to balance accuracy and efficiency for clinical deployment. 3. Validated the model's clinical relevance through extensive interpretability analysis using Grad-CAM, LIME, and Score-CAM.",
      "summary": "This paper addresses the challenge of classifying gastrointestinal diseases from endoscopic images by proposing a knowledge distillation framework. A high-capacity dual-stream teacher model guides a compact Tiny-ViT student, achieving near-perfect accuracy on WCE datasets while providing explainable predictions. The work offers an efficient and interpretable solution suitable for resource-constrained clinical environments.",
      "mindmap": "graph TB\n        A[论文标题 / Paper Title: A Graph-Augmented knowledge Distillation based Dual-Stream Vision Transformer for GI Disease Classification] --> B(核心问题 / Problem: 胃肠道疾病图像分类挑战 / GI Disease Image Classification Challenge)\n        A --> C(主要方法 / Method: 基于知识蒸馏的双流Vision Transformer / Knowledge Distillation based Dual-Stream Vision Transformer)\n        A --> D(关键结果 / Results: 高准确率与可解释性 / High Accuracy & Explainability)\n        B --> B1[数据量大, 类间差异小 / Large Data Volume, Subtle Inter-class Variation]\n        C --> C1[教师模型: Swin + ViT / Teacher: Swin + ViT]\n        C --> C2[学生模型: Tiny-ViT / Student: Tiny-ViT]\n        C --> C3[可解释性分析: Grad-CAM等 / XAI: Grad-CAM etc.]\n        D --> D1[准确率 > 0.99 / Accuracy > 0.99]\n        D --> D2[AUC = 1.0000]\n        D --> D3[适用于临床环境 / Suitable for Clinical Settings]"
    },
    {
      "title": "Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models",
      "authors": "Takuro Kutsuna",
      "institution": "Toyota Central R&D Labs., Inc.",
      "link": "https://arxiv.org/pdf/2512.21593",
      "code": null,
      "tags": [
        "diffusion models",
        "diffusion models",
        "generative modeling",
        "evidence lower bound",
        "residual learning",
        "two-stage framework"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/08078ea833fb6d000def6c1e69b08b734ff7e29a1c3014bf3ec3e8b313815438_w640_q70.webp",
      "contributions": "1. Proposes Residual Prior Diffusion (RPD), a two-stage probabilistic framework that integrates a coarse prior model with a diffusion model to capture large-scale structure and fine-scale details separately. 2. Formulates RPD with a tractable evidence lower bound, showing optimization reduces to familiar noise/velocity prediction objectives, and introduces auxiliary variables to leverage prior information and theoretically reduce prediction difficulty. 3. Demonstrates experimentally that RPD outperforms standard diffusion models on synthetic datasets with fine local structure and matches or exceeds baselines on natural image generation, maintaining performance with few inference steps.",
      "summary": "The paper identifies a problem where standard diffusion models struggle to simultaneously model global structure and fine local details. It proposes Residual Prior Diffusion (RPD), a two-stage framework that first learns a coarse prior and then a diffusion model for the residual. Experiments show RPD captures fine details better than standard models and maintains strong performance with fewer inference steps.",
      "mindmap": "graph TB\n        Root[”Residual Prior Diffusion (RPD) / 残差先验扩散模型”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”单一扩散模型难以同时捕捉全局结构和局部细节 / Single diffusion model struggles with global structure and local details”]\n        Method --> M1[”两阶段框架: 粗粒度先验 + 残差扩散模型 / Two-stage framework: coarse prior + residual diffusion model”]\n        Method --> M2[”概率模型与可处理ELBO / Probabilistic model with tractable ELBO”]\n        Results --> R1[”在合成数据上准确捕捉细节 / Accurately captures details on synthetic data”]\n        Results --> R2[”自然图像生成匹配或超越基线 / Natural image generation matches or exceeds baselines”]\n        Results --> R3[”少步推理保持性能 / Maintains performance with few inference steps”]"
    },
    {
      "title": "RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring",
      "authors": "Zhuoyu Wu, Wenhui Ou, Qiawei Zheng, Jiayan Yang, Quanjun Wang, Wenqi Fang, Zheng Wang, Yongkui Yang, Heshan Li",
      "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; Shenzhen Unifyware Co., Ltd.; Monash University; Hong Kong University of Science and Technology; Shenzhen Infynova Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.21975",
      "code": "https://github.com/ReaganWu/RT-Focuser",
      "tags": [
        "image deblurring",
        "lightweight network",
        "real-time inference",
        "edge deployment",
        "U-shaped architecture",
        "motion blur"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d65834719412d7909823764ddbf757cadad16b1b273fd3e67cf177cad638b9d_w640_q70.webp",
      "contributions": "1. Proposes a lightweight U-shaped network (RT-Focuser) with a Lightweight Deblurring Block (LD) for edge-aware feature extraction. 2. Introduces a Multi-Level Integrated Aggregation module (MLIA) for encoder feature integration. 3. Designs a Cross-source Fusion Block (X-Fuse) for progressive decoder refinement.",
      "summary": "This paper proposes RT-Focuser, a lightweight U-shaped neural network designed for real-time image deblurring on edge devices. It achieves a balance between speed and accuracy with only 5.85M parameters, running at over 140 FPS on both GPU and mobile platforms. The model demonstrates strong potential for deployment in real-time applications like autonomous driving and UAV perception.",
      "mindmap": "graph TB\n        A[RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[运动模糊降低图像质量 / Motion blur degrades image quality]\n        B --> B2[实时应用挑战 / Challenges for real-time applications]\n        C --> C1[轻量级U型网络 / Lightweight U-shaped network]\n        C --> C2[三个关键组件 / Three key components: LD, MLIA, X-Fuse]\n        D --> D1[30.67 dB PSNR / 30.67 dB PSNR]\n        D --> D2[5.85M参数, 15.76 GMACs / 5.85M params, 15.76 GMACs]\n        D --> D3[>140 FPS on GPU/mobile / >140 FPS on GPU/mobile]"
    },
    {
      "title": "The Color-Clinical Decoupling: Why Perceptual Calibration Fails Clinical Biomarkers in Smartphone Dermatology",
      "authors": "Sungwoo Kang",
      "institution": "Korea University",
      "link": "https://arxiv.org/pdf/2512.21988",
      "code": null,
      "tags": [
        "medical imaging",
        "colorimetric calibration",
        "clinical biomarkers",
        "Individual Typology Angle (ITA)",
        "Melanin Index",
        "intraclass correlation coefficient (ICC)"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7fbc8b90f041b184cdc9f22e1063fa065b2e1f8b098825058b11e381c89ffee7_w640_q70.webp",
      "contributions": "1. Identifies and defines the \"color-clinical decoupling\" phenomenon, where perceptual color accuracy does not guarantee reliability of clinical biomarkers. 2. Demonstrates that anatomical variance (facial region) is a dominant source of color variance, significantly outweighing device effects, challenging single-patch calibration. 3. Provides a mathematical explanation for the decoupling by showing the ITA biomarker's disproportionate sensitivity to noise in the b* color channel.",
      "summary": "This paper investigates the assumption that colorimetric calibration ensures reliable clinical biomarker extraction in smartphone dermatology. Using a large dataset and standard calibration, it finds that while color error is reduced, biomarker reliability remains poor, a phenomenon termed \"color-clinical decoupling,\" primarily due to anatomical variance and formula sensitivity. The conclusion is that current calibration standards are insufficient for clinical use and require region-aware protocols.",
      "mindmap": "graph TB\n        A[The Color-Clinical Decoupling<br>颜色-临床解耦] --> B[核心问题/Problem<br>Does color calibration ensure clinical reliability?<br>色彩校准能否确保临床可靠性？]\n        A --> C[主要方法/Method<br>Analyze 43,425 images across devices with CCM<br>使用CCM分析43,425张跨设备图像]\n        A --> D[关键结果/Results<br>Color-clinical decoupling: ∆E↓ but ICC(ITA) poor<br>颜色-临床解耦：∆E下降但ITA的ICC差]\n        B --> D\n        C --> D"
    },
    {
      "title": "MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation",
      "authors": "Chi-Hsiang Hsiao, Yi-Cheng Wang, Tzung-Sheng Lin, Yi-Ren Yeh, Chu-Song Chen",
      "institution": "National Taiwan University, E.SUN Financial Holding Co., Ltd., National Kaohsiung Normal University",
      "link": "https://arxiv.org/pdf/2512.20626",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "multimodal knowledge graph",
        "cross-modal reasoning",
        "visual document understanding",
        "retrieval-augmented generation",
        "entity-centric structure"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/425d6eb853edb40749e686474d27dc018d8a86017a4cd69160f9ac2081d36385_w640_q70.webp",
      "contributions": "1. Proposes a multimodal knowledge graph-based RAG framework that integrates visual cues into KG construction, retrieval, and answer generation for cross-modal reasoning. 2. Addresses the limitation of existing text-only KG-RAG methods by automatically building KGs that capture text-to-figure and figure-to-figure relationships. 3. Demonstrates superior performance over existing RAG approaches on both textual and multimodal question-answering tasks through comprehensive experiments.",
      "summary": "The paper introduces MegaRAG, a multimodal knowledge graph-based retrieval-augmented generation method designed to overcome the limitations of text-only RAG systems in understanding complex, long-form visual documents. It integrates visual information into the knowledge graph construction and retrieval process to enable better cross-modal reasoning. Experimental results show it consistently outperforms existing RAG methods on various question-answering tasks.",
      "mindmap": "graph LR\n        A[MegaRAG: 多模态知识图谱检索增强生成 / MegaRAG: Multimodal Knowledge Graph-Based Retrieval Augmented Generation] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有RAG方法在长文档、多模态内容上理解不足 / Existing RAG struggles with long-form, multimodal document understanding]\n        C --> C1[构建融合视觉线索的多模态知识图谱 / Construct multimodal KG incorporating visual cues]\n        C --> C2[在多模态检索与生成中利用图谱 / Utilize KG in multimodal retrieval & generation]\n        D --> D1[在全局与细粒度QA任务上超越现有方法 / Outperforms existing methods on global & fine-grained QA]"
    },
    {
      "title": "MaskOpt: A Large-Scale Mask Optimization Dataset to Advance AI in Integrated Circuit Manufacturing",
      "authors": "Yuting Hu, Lei Zhuang, Hua Xiang, Jinjun Xiong, Gi-Joon Nam",
      "institution": "University at Buffalo, IBM Research",
      "link": "https://arxiv.org/pdf/2512.20655",
      "code": null,
      "tags": [
        "others",
        "mask optimization",
        "optical proximity correction",
        "inverse lithography technique",
        "deep learning",
        "benchmark dataset"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce9bc1ac552258257c450a9bc4d4c4ae0264c958efae291f56fc44af367bf07a_w640_q70.webp",
      "contributions": "1. Introduces MaskOpt, a large-scale benchmark dataset for AI-driven mask optimization constructed from real 45nm IC designs, addressing limitations of synthetic data. 2. The dataset preserves standard-cell hierarchy and includes varying context window sizes to enable cell- and context-aware model training. 3. Provides comprehensive benchmarks by evaluating state-of-the-art deep learning models, revealing performance trade-offs and validating the importance of contextual and cell-aware inputs.",
      "summary": "The paper presents MaskOpt, a large-scale dataset built from real integrated circuit designs to advance deep learning for mask optimization in semiconductor manufacturing. It addresses the limitations of existing synthetic datasets by including cell hierarchy and surrounding context. Benchmarking results demonstrate the dataset's utility and highlight the critical role of context and cell information for accurate mask generation.",
      "mindmap": "graph LR\n    A[MaskOpt Dataset<br/>MaskOpt数据集] --> B[核心问题/Problem<br/>Existing datasets are synthetic, lack cell hierarchy & context<br/>现有数据集为合成数据，缺乏单元层次和上下文];\n    A --> C[主要方法/Method<br/>Build large-scale dataset from real 45nm IC designs with cell-aware tiles & context windows<br/>基于真实45nm设计构建大规模数据集，包含单元感知切片和上下文窗口];\n    A --> D[关键结果/Results<br/>Benchmarks show model trade-offs, context & cell info are crucial<br/>基准测试显示模型权衡，上下文和单元信息至关重要];"
    },
    {
      "title": "HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model",
      "authors": "Yuanhao Xi, Xiaohuan Bing, Ramin Yahyapour",
      "institution": "Liaoning Technical University, University of Göttingen, Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen",
      "link": "https://arxiv.org/pdf/2512.20674",
      "code": null,
      "tags": [
        "multi-modal training",
        "Low-Rank Adaptation (LoRA)",
        "parameter-efficient fine-tuning",
        "rank adaptation",
        "mobile vision language model",
        "dynamic scheduling"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9589d36616e78e4826e61d2dbafa4ccc718075f3659352d4d01c1b6f4795a02a_w640_q70.webp",
      "contributions": "1. Proposes HyDRA, a parameter-efficient fine-tuning framework for mobile VLMs that implements hierarchical and dynamic rank scheduling. 2. Introduces hierarchical optimization with coarse-grained (layer-level) and fine-grained (intra-layer) rank assignment. 3. Employs dynamic adjustment via an end-to-end automatic optimization using a lightweight performance model to determine ranks during fine-tuning.",
      "summary": "This paper introduces HyDRA, a parameter-efficient fine-tuning framework for mobile Vision Language Models (VLMs) that addresses the computational inefficiency of standard LoRA by implementing hierarchical and dynamic rank scheduling. The method uses a two-pronged optimization strategy and a lightweight performance model to adjust ranks automatically. Experiments show HyDRA outperforms baselines, achieving a 4.7% average improvement without extra parameters and sometimes surpassing full fine-tuning.",
      "mindmap": "graph LR\n    A[HyDRA: Hierarchical and Dynamic Rank Adaptation for Mobile Vision Language Model] --> B[核心问题/Problem: Standard LoRA with fixed rank is insufficient for training mobile VLMs]\n    A --> C[主要方法/Method: HyDRA framework with hierarchical & dynamic rank scheduling]\n    A --> D[关键结果/Results: Outperforms baseline by 4.7%, no extra parameters, sometimes beats full fine-tuning]"
    },
    {
      "title": "VL4Gaze: Unleashing Vision-Language Models for Gaze Following",
      "authors": "Shijing Wang, Chaoqun Cui, Yaping Huang, Hyung Jin Chang, Yihua Cheng",
      "institution": "Beijing Jiaotong University, Institute of Automation, Chinese Academy of Sciences, University of Birmingham",
      "link": "https://arxiv.org/pdf/2512.20735",
      "code": null,
      "tags": [
        "gaze following",
        "vision-language models",
        "gaze understanding",
        "visual question answering",
        "benchmark dataset",
        "multi-task learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48a1d18f4099c1ab4f26169d964e14f98c077ef87b0bc95c0bf4868b43217b89_w640_q70.webp",
      "contributions": "1. Introduces VL4Gaze, the first large-scale benchmark for evaluating and training vision-language models on gaze understanding. 2. Formulates gaze understanding as a unified VQA problem across four complementary tasks: gaze object description, gaze direction description, gaze point location, and ambiguous question recognition. 3. Demonstrates that targeted multi-task supervision on VL4Gaze substantially improves VLMs' gaze understanding capabilities, which do not reliably emerge from general pre-training.",
      "summary": "This paper addresses the lack of gaze understanding in vision-language models by introducing VL4Gaze, a large-scale benchmark dataset with 489K QA pairs. It evaluates VLMs on four gaze-related VQA tasks and finds that task-specific fine-tuning on this dataset is crucial for achieving reliable performance, as general-purpose VLMs struggle with gaze inference.",
      "mindmap": "graph LR\n        A[VL4Gaze: Unleashing Vision-Language Models for Gaze Following] --> B[核心问题/Problem: Gaze understanding is unexplored in VLMs, with no existing benchmark.]\n        A --> C[主要方法/Method: Introduce VL4Gaze benchmark with 489K QA pairs across four gaze VQA tasks.]\n        A --> D[关键结果/Results: VLMs struggle without supervision; fine-tuning on VL4Gaze brings substantial improvements.]"
    },
    {
      "title": "TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection",
      "authors": "Tony Tran, Bin Hu",
      "institution": "University of Houston",
      "link": "https://arxiv.org/pdf/2512.20746",
      "code": null,
      "tags": [
        "on-device ai",
        "neural architecture search",
        "hardware-aware search",
        "edge detection",
        "TinyML",
        "waste detection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dbb2f9a6c26aed837d78c4cfa78db76890d85a7fadbb49f8592bc1ae30894e1_w640_q70.webp",
      "contributions": "1. Proposes an iterative hardware-aware neural architecture search (NAS) framework that alternates between backbone and neck/head optimization for efficient object detection under TinyML constraints. 2. Introduces a population passthrough mechanism and an accuracy predictor to reduce search cost and improve the stability of the evolutionary search. 3. Delivers a family of deployment-ready detectors (TrashDets) that significantly improve efficiency (energy, latency, power) and accuracy on microcontroller hardware compared to existing TinyML baselines.",
      "summary": "This paper addresses efficient waste detection for edge/IoT devices under TinyML constraints. It proposes an iterative hardware-aware neural architecture search framework to generate a family of efficient detectors called TrashDets. The resulting models achieve higher accuracy with fewer parameters and significantly reduce energy consumption and latency on resource-constrained microcontrollers.",
      "mindmap": "graph LR\n        A[TrashDet] --> B[核心问题/Problem: 边缘设备垃圾检测<br>TinyML Constraints];\n        A --> C[主要方法/Method: 迭代硬件感知NAS<br>Iterative Hardware-aware NAS];\n        A --> D[关键结果/Results: 高效TrashDet家族<br>Efficient TrashDet Family];\n        B --> D;\n        C --> D;"
    },
    {
      "title": "OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective",
      "authors": "Markus Gross, Sai B. Matha, Aya Fahmy, Rui Song, Daniel Cremers, Henri Meess",
      "institution": "Fraunhofer IVI, TU Munich, MCML, UCLA",
      "link": "https://arxiv.org/pdf/2512.20770",
      "code": "https://github.com/markus-42/occufly",
      "tags": [
        "semantic scene completion",
        "semantic scene completion",
        "3D reconstruction",
        "aerial perspective",
        "benchmark dataset",
        "label transfer"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/292af942047832eaba7423020289566350434f49381259d1665a57faf1a20881_w640_q70.webp",
      "contributions": "1. Introduces OccuFly, the first real-world, camera-based aerial Semantic Scene Completion (SSC) benchmark dataset captured at different altitudes and seasons. 2. Proposes a LiDAR-free data generation framework that automates label transfer from 2D masks to 3D point clouds, minimizing manual annotation. 3. Benchmarks state-of-the-art methods on the new dataset and highlights unique challenges of aerial viewpoints.",
      "summary": "This paper addresses the lack of aerial datasets for Semantic Scene Completion (SSC) by introducing OccuFly, a camera-based benchmark captured from UAVs. The authors propose a framework that uses traditional 3D reconstruction and 2D mask lifting to automate 3D annotation. The resulting dataset enables benchmarking and reveals specific challenges for 3D perception from elevated viewpoints.",
      "mindmap": "graph LR\n    A[OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[缺乏空中SSC基准/Lack of Aerial SSC Benchmark]\n    B --> B2[LiDAR对UAV不友好/LiDAR Unsuitable for UAVs]\n    C --> C1[提出相机模态数据集/Propose Camera-based Dataset]\n    C --> C2[基于3D重建的标签迁移/Label Transfer via 3D Reconstruction]\n    D --> D1[覆盖多场景季节/Covers Scenarios & Seasons]\n    D --> D2[基准测试揭示挑战/Benchmark Reveals Challenges]"
    },
    {
      "title": "NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts",
      "authors": "Raja Mallina, Bryar Shareef",
      "institution": "University of Nevada, Las Vegas",
      "link": "https://arxiv.org/pdf/2512.20783",
      "code": null,
      "tags": [
        "medical image segmentation",
        "nullable prompts",
        "mixed-supervision",
        "vision-language models",
        "breast ultrasound segmentation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9c68929834a07c86ac43fe9856efa137aa11c30a231a02ea87272f2b689e4f7f_w640_q70.webp",
      "contributions": "1. Proposes NullBUS, a multimodal mixed-supervision framework for BUS segmentation that can learn from images both with and without prompts in a single model. 2. Introduces nullable prompts, implemented as learnable null embeddings with presence masks, to handle missing text metadata by enabling fallback to image-only evidence. 3. Demonstrates state-of-the-art performance on a unified pool of three public BUS datasets under mixed prompt availability.",
      "summary": "The paper addresses the problem that many public breast ultrasound datasets lack reliable text or spatial prompts, which limits the training of promptable segmentation models. It proposes NullBUS, a framework that uses nullable global-local prompts to learn from both prompted and prompt-free images. The method achieves state-of-the-art segmentation performance on a unified evaluation of three public datasets, showing robustness under mixed prompt availability.",
      "mindmap": "graph LR\n    A[NULLBUS] --> B[核心问题/Problem: BUS数据集缺乏可靠提示词]\n    A --> C[主要方法/Method: 可空全局-局部提示的混合监督框架]\n    A --> D[关键结果/Results: 在混合提示下达到SOTA性能]"
    },
    {
      "title": "Learning to Sense for Driving: Joint Optics-Sensor-Model Co-Design for Semantic Segmentation",
      "authors": "Reeshad Khan amd John Gauch",
      "institution": "University of Arkansas",
      "link": "https://arxiv.org/pdf/2512.20815",
      "code": null,
      "tags": [
        "semantic segmentation",
        "optics-sensor co-design",
        "RAW-to-task pipeline",
        "learnable color filter array",
        "differentiable simulation",
        "autonomous driving"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98485ccaafcb09f4e55fbe828874d818d5caf3b68c7d2d4f86250cb41081aebe_w640_q70.webp",
      "contributions": "1. A task-driven, end-to-end co-design framework that jointly optimizes optics (via lens models), sensor parameters (learnable color filter arrays, noise, quantization), and a lightweight semantic segmentation network for autonomous driving. 2. Demonstrates consistent performance improvements (mIoU) on KITTI-360, particularly for challenging classes, by adapting image acquisition directly to semantic structure rather than human-viewable imagery. 3. Shows that the robustness gains are achieved with a compact, efficient model (~1M parameters, ~28 FPS), proving the deployability of the full-stack co-optimization approach on edge devices.",
      "summary": "This paper proposes a joint optics-sensor-model co-design framework for autonomous driving perception. It unifies differentiable models of optics, sensor noise, and quantization with a lightweight segmentation network into an end-to-end RAW-to-task pipeline, optimized directly for semantic accuracy. The method outperforms traditional fixed pipelines, especially in challenging conditions, while maintaining high efficiency suitable for edge deployment.",
      "mindmap": "graph LR\n    A[Learning to Sense for Driving] --> B[核心问题/Problem: Traditional camera design is decoupled from perception, losing information and introducing artifacts.]\n    A --> C[主要方法/Method: End-to-end co-design of optics, sensor, and model via differentiable simulation.]\n    A --> D[关键结果/Results: Improved mIoU, robustness, and efficiency on KITTI-360.]"
    },
    {
      "title": "Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference",
      "authors": "Putu Indah Githa Cahyani, Komang David Dananjaya Suartana, Novanto Yudistira",
      "institution": "University of Brawijaya",
      "link": "https://arxiv.org/pdf/2512.20839",
      "code": "https://github.com/kmdavidds/mlfastlm",
      "tags": [
        "multi-modal inference",
        "adaptive preprocessing",
        "visual token reduction",
        "inference efficiency",
        "content-aware cropping",
        "FastVLM"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffbed91a603765fa692e3234db5637487cb3f57c201db5e6263fed6a6c03b2ca_w640_q70.webp",
      "contributions": "1. Proposed an adaptive visual preprocessing method that dynamically adjusts input resolution and spatial coverage based on image content to reduce redundancy. 2. Integrated the method with FastVLM without modifying its architecture or requiring retraining, maintaining model integrity. 3. Demonstrated significant efficiency improvements, including over 50% reduction in per-image inference time and over 55% reduction in visual token count on a DocVQA subset.",
      "summary": "The paper addresses the high inference latency and computational cost of Vision-Language Models (VLMs) when processing high-resolution images by proposing an adaptive visual preprocessing method. This method dynamically adjusts input resolution and cropping based on image content to reduce visual redundancy before encoding, integrated seamlessly with FastVLM. Experimental results show over 50% reduction in per-image inference time and over 55% reduction in visual token count, proving it as an effective lightweight strategy for deployment efficiency.",
      "mindmap": "graph LR\n    A[Input-Adaptive Visual Preprocessing] --> B[核心问题/Problem: High inference latency & cost in VLMs]\n    A --> C[主要方法/Method: Adaptive resolution & cropping based on content]\n    A --> D[关键结果/Results: >50% faster inference, >55% token reduction]"
    },
    {
      "title": "CHAMMI-75: pre-training multi-channel models with heterogeneous microscopy images",
      "authors": "Vidit Agrawal, John Peters, Tyler N. Thompson, Mohammad Vali Sanian, Chau Pham, Nikita Moshkov, Arshad Kazi, Aditya Pillai, Jack Freeman, Byunguk Kang, Samouil L. Farhi, Ernest Fraenkel, Ron Stewart, Lassi Paavolainen, Bryan A. Plummer, Juan C. Caicedo",
      "institution": "Morgridge Institute for Research, University of Wisconsin-Madison, Institute for Molecular Medicine Finland (FIMM), University of Helsinki, Boston University, Institute of Computational Biology (Helmholtz Munich), Massachusetts Institute of Technology, Broad Institute of MIT and Harvard",
      "link": "https://arxiv.org/pdf/2512.20833",
      "code": null,
      "tags": [
        "bioimage analysis",
        "multi-channel microscopy",
        "cellular morphology",
        "pre-training dataset",
        "channel-adaptive models",
        "heterogeneous data"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3554315333d73be844495c47c136c201cae29e405327a25f6e11fc1488c5f4f9_w640_q70.webp",
      "contributions": "1. Introduces CHAMMI-75, a novel open-access dataset of heterogeneous, multi-channel microscopy images curated from 75 diverse biological studies. 2. Proposes the use of this dataset to investigate and develop channel-adaptive models for cellular morphology that can process any microscopy image type. 3. Demonstrates experimentally that pre-training with the diverse CHAMMI-75 dataset improves performance on multi-channel bioimaging tasks.",
      "summary": "This paper addresses the problem that models for quantifying cell morphology are typically specialized to single microscopy types, limiting their reuse. It proposes CHAMMI-75, a diverse, multi-channel microscopy image dataset, and shows that pre-training with it improves model performance by enabling channel-adaptability and handling heterogeneous modalities. The work aims to pave the way for more generalizable cellular morphology models.",
      "mindmap": "graph LR\n        A[CHAMMI-75: Pre-training Multi-channel Models] --> B[核心问题/Problem: Specialized models cannot be reused across studies]\n        A --> C[主要方法/Method: Curate CHAMMI-75, a heterogeneous multi-channel dataset]\n        A --> D[关键结果/Results: Training with CHAMMI-75 improves performance via diversity]"
    },
    {
      "title": "ALIVE: An Avatar-Lecture Interactive Video Engine with Content-Aware Retrieval for Real-Time Interaction",
      "authors": "Md Zabirul Islam, Md Motaleb Hossen Manik, Ge Wang",
      "institution": "Rensselaer Polytechnic Institute",
      "link": "https://arxiv.org/pdf/2512.20858",
      "code": null,
      "tags": [
        "on-device ai",
        "content-aware retrieval",
        "FAISS",
        "neural talking-head synthesis",
        "local deployment",
        "multimodal interaction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/edf9b6fce27313fb183b0fbb9a8b32f719d83748e14517c97b10d89c253c6cac_w640_q70.webp",
      "contributions": "1. A fully local, privacy-preserving interactive video engine that integrates avatar synthesis, content retrieval, and multimodal interaction. 2. A content-aware retrieval mechanism combining semantic similarity with timestamp alignment to surface contextually relevant lecture segments. 3. A system design employing lightweight models, FAISS, and segmented synthesis with progressive preloading to achieve real-time responsiveness.",
      "summary": "The paper presents ALIVE, a system that transforms passive lecture videos into interactive experiences by using local ASR, LLMs, and neural avatars to generate lecture content, a content-aware retrieval mechanism to find relevant segments, and enabling real-time Q&A. It demonstrates that combining multimodal AI with local, content-aware retrieval can create engaging and pedagogically valuable interactive learning environments.",
      "mindmap": "graph LR\n        A[ALIVE: Avatar-Lecture Interactive Video Engine] --> B[核心问题/Problem: Passive lecture videos lack real-time clarification mechanisms]\n        A --> C[主要方法/Method: Local avatar synthesis, content-aware retrieval, real-time multimodal interaction]\n        A --> D[关键结果/Results: Provides accurate, engaging, and privacy-preserving real-time learning support]"
    },
    {
      "title": "NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder",
      "authors": "Daichi Arai, Kyohei Unno, Yasuko Sugito, Yuichi Kusakabe",
      "institution": "Science & Technology Research Laboratories, NHK",
      "link": "https://arxiv.org/pdf/2512.20871",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "360-degree video",
        "implicit neural representations",
        "viewport decoding",
        "spatial-temporal affine transform",
        "video compression"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77be849ee4606db0adeefbd5d6ebfff328fcdd9b300007aa2dc353aaea4af87e_w640_q70.webp",
      "contributions": "1. Proposes NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of the entire panoramic frame for 360-degree videos. 2. Introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. 3. Achieves significant reductions in memory consumption (7x) and increases in decoding speed (2.5x) compared to prior work HNeRV, while improving image quality.",
      "summary": "The paper addresses the high memory usage and slow decoding of applying implicit neural representations (NeRV) to high-resolution 360-degree videos. It proposes NeRV360, a framework that integrates viewport extraction directly into the decoding process using a conditional spatial-temporal affine transform module. Experiments show NeRV360 significantly reduces memory consumption and increases decoding speed while delivering better image quality compared to prior methods.",
      "mindmap": "graph LR\n    A[NeRV360] --> B[核心问题/Problem: High memory & slow decoding for 360° NeRV]\n    A --> C[主要方法/Method: Viewport decoder with spatial-temporal affine transform]\n    A --> D[关键结果/Results: 7x memory↓, 2.5x speed↑, better quality]"
    },
    {
      "title": "Lightweight framework for underground pipeline recognition and spatial localization based on multi-view 2D GPR images",
      "authors": "Haotian Lv, Chao Li, Jiangbo Dai, Yuhui Zhang, Zepeng Fan, Yiqiu Tan, Dawei Wang, Binglei Xie",
      "institution": "Harbin Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.20866",
      "code": null,
      "tags": [
        "object detection",
        "YOLOv11",
        "3D-DIoU",
        "multi-view fusion",
        "GPR",
        "FDTD"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c5ecaa60b98d3c92e7c85d9cd5e1f9894fba8806dbb24358189ef30201b7b93c_w640_q70.webp",
      "contributions": "1. Proposed a B/C/D-Scan three-view joint analysis strategy and a feature evaluation method validated by FDTD simulations and real data. 2. Developed the DCO-YOLO framework, integrating DySample, CGLU, and OutlookAttention into YOLOv11 to enhance small-scale pipeline feature extraction. 3. Introduced a 3D-DIoU spatial feature matching algorithm with 3D geometric constraints to automate multi-view annotation association and resolve single-view ambiguities.",
      "summary": "This paper proposes a lightweight framework for 3D underground pipeline detection using multi-view 2D GPR images. The method integrates an improved YOLO-based detection model (DCO-YOLO) with a novel 3D-DIoU spatial matching algorithm for multi-view fusion. Experiments on real urban data show the framework achieves high accuracy, recall, and mAP, outperforming the baseline and offering a reliable solution for pipeline recognition and localization.",
      "mindmap": "graph LR\n    A[Lightweight framework for underground pipeline recognition and spatial localization<br>基于多视图2D GPR图像的地下管道识别与空间定位轻量级框架] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[Weak multi-view feature correlation, low small-target accuracy<br>多视图特征关联弱，小目标识别精度低]\n    C --> C1[3D pipeline three-view feature evaluation<br>三维管道三视图特征评估]\n    C --> C2[DCO-YOLO framework<br>DCO-YOLO框架]\n    C --> C3[3D-DIoU spatial feature matching<br>3D-DIoU空间特征匹配]\n    D --> D1[Accuracy 96.2%, Recall 93.3%, mAP 96.7%<br>准确率96.2%，召回率93.3%，平均精度96.7%]"
    },
    {
      "title": "Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification",
      "authors": "Tingfeng Xian, Wenlve Zhou, Zhiheng Zhou, Zhelin Li",
      "institution": "South China University of Technology",
      "link": "https://arxiv.org/pdf/2512.20892",
      "code": "https://github.com/TingfengXian/DRI",
      "tags": [
        "cross-modal re-identification",
        "Parameter-Efficient Fine-Tuning",
        "Vision Foundation Models",
        "Domain Representation Injection",
        "feature-space adaptation",
        "cross-modality"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39fa98c450498929c15e9f360b859ef2bfff59f65e629afdd04c76cb67a0fe53_w640_q70.webp",
      "contributions": "1. Proposes a novel Parameter-Efficient Fine-Tuning (PEFT) strategy called Domain Representation Injection (DRI) that operates in the feature space instead of the weight space. 2. Introduces a lightweight Offset Encoder and a Modulator to extract and adapt domain-specific representations, which are then injected into a frozen Vision Foundation Model to bridge modality gaps. 3. Achieves state-of-the-art performance on cross-modal ship re-identification with minimal trainable parameters, demonstrating the effectiveness of feature-space adaptation for limited-capacity models.",
      "summary": "This paper addresses the challenge of cross-modal ship re-identification by proposing a feature-space adaptation method called Domain Representation Injection (DRI). Instead of fine-tuning model weights, DRI injects learned domain-specific features into a frozen Vision Foundation Model to bridge the modality gap. The method achieves state-of-the-art results with very few trainable parameters, showing its efficiency and effectiveness.",
      "mindmap": "graph LR\n    A[Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification] --> B[核心问题/Problem: 跨模态船舶重识别中的显著模态差异/Significant modality discrepancy in Cross-Modality Ship Re-ID]\n    A --> C[主要方法/Method: 提出特征空间领域表示注入(DRI)/Propose feature-space Domain Representation Injection (DRI)]\n    A --> D[关键结果/Results: 以极少的可训练参数实现SOTA性能/Achieve SOTA performance with minimal trainable parameters]"
    },
    {
      "title": "DGSAN: Dual-Graph Spatiotemporal Attention Network for Pulmonary Nodule Malignancy Prediction",
      "authors": "Xiao Yu, Zhaojie Fang, Guanyu Zhou, Yin Shen, Huoling Luo, Ye Li, Ahmed Elazab, Xiang Wan, Ruiquan Ge, Changmiao Wang",
      "institution": "Hangzhou Dianzi University",
      "link": "https://arxiv.org/pdf/2512.20898",
      "code": "https://github.com/lcbkmm/DGSAN",
      "tags": [
        "medical image analysis",
        "spatiotemporal attention",
        "graph neural network",
        "multimodal fusion",
        "pulmonary nodule classification",
        "feature encoder"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/422fb811704c808454d49662b57428a8e4f2132f4f9eb28d4def2caf51204b49_w640_q70.webp",
      "contributions": "1. Proposed a Dual-Graph Spatiotemporal Attention Network (DGSAN) for pulmonary nodule malignancy prediction. 2. Introduced a Dual-Graph Construction method and a Hierarchical Cross-Modal Graph Fusion Module for effective multimodal feature integration. 3. Compiled a novel multimodal dataset named NLST-cmst to support related research.",
      "summary": "The paper addresses the problem of inefficient multimodal fusion in pulmonary nodule malignancy prediction. It proposes a Dual-Graph Spatiotemporal Attention Network (DGSAN) that uses a Global-Local Feature Encoder and a hierarchical graph fusion module to integrate temporal and multimodal data. Experiments show DGSAN outperforms state-of-the-art methods with high computational efficiency.",
      "mindmap": "graph LR\n    A[DGSAN: Dual-Graph Spatiotemporal Attention Network] --> B(核心问题/Problem: Inefficient multimodal fusion for nodule prediction)\n    A --> C(主要方法/Method: Dual-Graph construction & Hierarchical Cross-Modal Fusion)\n    A --> D(关键结果/Results: Outperforms SOTA on NLST-cmst/CSTL datasets)"
    },
    {
      "title": "Benchmarking and Enhancing VLM for Compressed Image Understanding",
      "authors": "Zifu Zhang, Tongda Xu, Siqi Li, Shengxi Li, Yue Zhang, Mai Xu, Yan Wang",
      "institution": "Tsinghua University, Beihang University, Beijing University of Technology",
      "link": "https://arxiv.org/pdf/2512.20901",
      "code": null,
      "tags": [
        "vision-language models",
        "image compression",
        "benchmark",
        "vision-language models",
        "adaptor",
        "generalization gap"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39011e6665de56b73804a40551ab504f1a26f1c50146d14d5c84a2c6c5586f47_w640_q70.webp",
      "contributions": "1. Introduces the first comprehensive benchmark to evaluate Vision-Language Models (VLMs) on compressed images, covering multiple codecs and tasks with over a million images. 2. Analyzes the performance gap, categorizing it into information loss and VLM generalization failure, and identifies that only the generalization gap is mitigable. 3. Proposes a universal VLM adaptor that improves model performance across various codecs and bitrates by 10%-30%.",
      "summary": "This paper addresses the performance drop of Vision-Language Models (VLMs) when processing low-bitrate compressed images. It introduces a large-scale benchmark to evaluate this issue, analyzes the sources of the performance gap, and proposes a universal adaptor to enhance VLM robustness. The proposed adaptor successfully improves VLM performance on compressed images by 10%-30%, bridging the gap between VLMs and compressed data.",
      "mindmap": "graph LR\n        A[论文标题/Paper Title: Benchmarking and Enhancing VLM for Compressed Image Understanding] --> B(核心问题/Problem: VLM在低码率压缩图像上性能下降/VLM performance drop on low-bitrate compressed images)\n        A --> C(主要方法/Method: 提出基准测试与通用适配器/Propose benchmark & universal adaptor)\n        A --> D(关键结果/Results: 性能提升10%-30%/Performance improved by 10%-30%)"
    },
    {
      "title": "PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding",
      "authors": "Seongmin Jung, Seongho Choi, Gunwoo Jeon, Minsu Cho, Jongwoo Lim",
      "institution": "Seoul National University, Pohang University of Science and Technology (POSTECH)",
      "link": "https://arxiv.org/pdf/2512.20907",
      "code": "https://choiseongho-h.github.io/PanoGrounder",
      "tags": [
        "3D visual grounding",
        "panoramic rendering",
        "vision-language model (VLM)",
        "3D scene understanding",
        "multi-modal fusion",
        "viewpoint selection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb61d36c0dc9249907d446e1e54dd427e25dfcfa7af988fd4093a0199335a6e7_w640_q70.webp",
      "contributions": "1. Proposes a novel 3D visual grounding framework that uses panoramic scene renderings as an intermediate 2D-3D representation, enabling direct use of powerful pretrained 2D Vision-Language Models (VLMs) with minimal adaptation. 2. Introduces a three-stage pipeline that strategically places panoramic viewpoints, performs per-view grounding with a VLM, and fuses predictions into a final 3D bounding box. 3. Demonstrates state-of-the-art performance on standard benchmarks (ScanRefer, Nr3D) and superior generalization to unseen datasets and text paraphrases.",
      "summary": "This paper addresses the problem of limited generalization in 3D Visual Grounding (3DVG) by introducing PanoGrounder, a framework that bridges 2D and 3D using panoramic scene renderings. The method leverages pretrained 2D Vision-Language Models for reasoning on these panoramic views and fuses predictions into a 3D bounding box. It achieves state-of-the-art results and shows strong generalization capabilities.",
      "mindmap": "graph LR\n        A[PanoGrounder] --> B[核心问题/Problem: 3DVG泛化性差/3DVG Poor Generalization]\n        A --> C[主要方法/Method: 全景表示与VLM/Panoramic Rep. & VLM]\n        A --> D[关键结果/Results: SOTA与强泛化/SOTA & Strong Generalization]"
    },
    {
      "title": "Self-supervised Multiplex Consensus Mamba for General Image Fusion",
      "authors": "Yingying Wang, Rongjin Zhuang, Hui Zheng, Xuanhua He, Ke Cao, Xiaotong Tu, Xinghao Ding",
      "institution": "Xiamen University, The Hong Kong University of Science and Technology, University of Science and Technology of China",
      "link": "https://arxiv.org/pdf/2512.20921",
      "code": null,
      "tags": [
        "image fusion",
        "Mamba",
        "Self-supervised Learning",
        "Mixture of Experts",
        "Cross-modal Scanning",
        "Contrastive Learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91945f65768f96b0613d0702a5b5373c8b78673452a1717dc1624fdddc50aa4b_w640_q70.webp",
      "contributions": "1. Proposed the SMC-Mamba framework, featuring a Modality-Agnostic Feature Enhancement (MAFE) module for detail preservation and global representation enhancement. 2. Introduced the Multiplex Consensus Cross-modal Mamba (MCCM) module for dynamic expert collaboration and efficient cross-modal information integration. 3. Designed a Bi-level Self-supervised Contrastive Learning Loss (BSCL) to preserve high-frequency information and boost downstream task performance without extra computational cost.",
      "summary": "This paper proposes SMC-Mamba, a self-supervised Mamba-based framework for general image fusion. It introduces novel modules for feature enhancement and cross-modal consensus, along with a contrastive learning loss, to efficiently integrate information from various modalities. Experiments show it outperforms state-of-the-art methods across multiple fusion tasks and downstream vision applications.",
      "mindmap": "graph LR\n        A[Self-supervised Multiplex Consensus Mamba<br>自监督多重共识Mamba] --> B(核心问题/Problem: General Image Fusion<br>通用图像融合)\n        A --> C(主要方法/Method: SMC-Mamba Framework<br>SMC-Mamba框架)\n        A --> D(关键结果/Results: Outperforms SOTA<br>超越SOTA方法)\n        B --> B1(Integrate multi-modal info<br>融合多模态信息)\n        B --> B2(Enhance downstream tasks<br>增强下游任务)\n        C --> C1(MAFE Module<br>MAFE模块)\n        C --> C2(MCCM Module<br>MCCM模块)\n        C --> C3(BSCL Loss<br>BSCL损失)\n        D --> D1(IVIF, MDIF, MFIF, MEIF<br>红外-可见光、医学、多焦点、多曝光融合)\n        D --> D2(Better downstream performance<br>更好的下游性能)"
    },
    {
      "title": "Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting",
      "authors": "Yoonwoo Jeong, Cheng Sun, Frank Wang, Minsu Cho, Jaesung Choe",
      "institution": "NVIDIA, POSTECH",
      "link": "https://arxiv.org/pdf/2512.20927",
      "code": null,
      "tags": [
        "3D scene understanding",
        "3D Gaussian Splatting",
        "open-vocabulary segmentation",
        "quantile rendering",
        "feature rendering",
        "real-time rendering"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9eadb39e7ce78a2787efbad5a7c95d49fedbdfefe4e08690405521c36873eb3_w640_q70.webp",
      "contributions": "1. Introduces Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that sparsely samples only dominant Gaussians along a ray to efficiently handle high-dimensional features. 2. Proposes Gaussian Splatting Network (GS-Net), a generalizable 3D neural network that integrates Q-Render to predict Gaussian features. 3. Achieves state-of-the-art performance on benchmarks (ScanNet, LeRF) while enabling real-time rendering with a ~43.7x speedup for 512-D feature maps.",
      "summary": "This paper addresses the challenge of efficiently rendering high-dimensional features (e.g., 512-D CLIP features) for open-vocabulary segmentation in 3D Gaussian Splatting. The authors propose Quantile Rendering (Q-Render), which sparsely samples influential Gaussians, and integrate it into a generalizable network called GS-Net. The method outperforms existing approaches on standard datasets and achieves significant speedups, enabling real-time performance.",
      "mindmap": "graph LR\n    A[Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting] --> B(核心问题/Problem: 高维特征渲染效率低/High-dimensional feature rendering is inefficient)\n    A --> C(主要方法/Method: 分位数渲染与GS-Net/Quantile Rendering & GS-Net)\n    A --> D(关键结果/Results: 性能提升与实时渲染/Performance gain & real-time rendering)"
    },
    {
      "title": "Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning",
      "authors": "Shengguang Wu, Xiaohan Wang, Yuhui Zhang, Hao Zhu, Serena Yeung-Levy",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.20934",
      "code": "https://transductive-visualprogram.github.io/",
      "tags": [
        "visual reasoning",
        "visual programming",
        "spatial reasoning",
        "tool induction",
        "transductive learning",
        "3D scene understanding"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6374488a70a5d9147002f5652452c2f63ea3698c6660c54123c36fc9deef3991_w640_q70.webp",
      "contributions": "1. Proposes Transductive Visual Programming (TVP), a novel framework that builds new tools from experiential solutions rather than speculative induction., 2. Introduces a closed-loop system with an evolving Tool Library and an Example Library, enabling self-improvement through experience., 3. Demonstrates state-of-the-art performance on spatial reasoning benchmarks and shows that transductively learned tools are used more frequently and generalize better.",
      "summary": "The paper addresses the challenge of spatial reasoning in 3D scenes by proposing Transductive Visual Programming (TVP), a framework that learns reusable higher-level tools by abstracting patterns from its own successful solutions. This experience-driven approach outperforms existing methods and GPT-4o on benchmarks, showing more effective tool discovery and strong generalization to unseen tasks.",
      "mindmap": "graph LR\n        A[Transductive Visual Programming] --> B[核心问题/Problem<br>Spatial reasoning is challenging for VLMs]\n        A --> C[主要方法/Method<br>Build tools from experience, not speculation]\n        A --> D[关键结果/Results<br>SOTA performance, better tool reuse & generalization]"
    },
    {
      "title": "Reasoning-Driven Amodal Completion: Collaborative Agents and Perceptual Evaluation",
      "authors": "Hongxing Fan, Shuyu Zhao, Jiayang Ao, Lu Sheng",
      "institution": "Beihang University, The University of Melbourne",
      "link": "https://arxiv.org/pdf/2512.20936",
      "code": "https://fanhongxing.github.io/remac-page",
      "tags": [
        "amodal completion",
        "multi-agent reasoning",
        "semantic planning",
        "visual synthesis",
        "MAC-Score",
        "chain-of-thought"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/70d3435136a9939f15c6fefdbf2a3700e29be4992fac1f670aa29c0aa24f6e92_w640_q70.webp",
      "contributions": "1. A Collaborative Multi-Agent Reasoning Framework that decouples Semantic Planning from Visual Synthesis for stable, single-pass amodal completion. 2. Integration of a self-correcting Verification Agent and a Diverse Hypothesis Generator to improve reasoning and handle ambiguity. 3. Introduction of the MAC-Score, a novel human-aligned evaluation metric for assessing inferred invisible content.",
      "summary": "This paper addresses the challenges of inference instability and error accumulation in amodal completion by proposing a reasoning-driven framework. The method employs specialized agents for upfront semantic planning before visual synthesis, and introduces a new evaluation metric. Experiments show the framework outperforms state-of-the-art methods on multiple datasets.",
      "mindmap": "graph LR\n    A[Reasoning-Driven Amodal Completion<br>推理驱动的模态补全] --> B[核心问题/Problem<br>Inference Instability & Error Accumulation<br>推理不稳定与误差累积];\n    A --> C[主要方法/Method<br>Collaborative Multi-Agent Reasoning Framework<br>协作多智能体推理框架];\n    C --> D[Semantic Planning & Visual Synthesis<br>语义规划与视觉合成];\n    C --> E[Verification Agent & Diverse Hypothesis Generator<br>验证智能体与多样化假设生成器];\n    A --> F[关键结果/Results<br>Outperforms SOTA & Novel MAC-Score Metric<br>超越SOTA与新MAC-Score指标];"
    },
    {
      "title": "Beyond Artifacts: Real-Centric Envelope Modeling for Reliable AI-Generated Image Detection",
      "authors": "Ruiqi Liu, Yi Han, Zhengbo Zhang, Liwei Yao, Zhiyuan Yan, Jialiang Shen, ZhiJin Chen, Boyi Sun, Lubin Weng, Jing Dong, Yan Wang, Shu Wu",
      "institution": "Institute of Automation, Chinese Academy of Sciences; Tsinghua University; Peking University; The University of Sydney; Southwest University; Shanghai Second Polytechnic University",
      "link": "https://arxiv.org/pdf/2512.20937",
      "code": null,
      "tags": [
        "ai-generated image detection",
        "real-centric envelope modeling",
        "distribution modeling",
        "chain degradations",
        "benchmark",
        "generalization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e6b1c54aadd8c3872d06b909d6de02c5d6a8a1aafa2a19375b0c325e7c62c51_w640_q70.webp",
      "contributions": "1. Proposes a new paradigm called Real-centric Envelope Modeling (REM) that shifts the detection focus from learning generator-specific artifacts to modeling the robust distribution of real images. 2. Introduces a method that uses feature-level perturbations in self-reconstruction to generate near-real samples and an envelope estimator with cross-domain consistency to learn a boundary around the real image manifold. 3. Builds a comprehensive benchmark named RealChain that covers various generators and simulates real-world degradation chains to evaluate detector robustness.",
      "summary": "The paper addresses the problem that existing AI-generated image detectors overfit to generator artifacts and fail under real-world degradations. It proposes a new method called Real-centric Envelope Modeling (REM) that learns the robust distribution of real images instead, and introduces a new benchmark, RealChain, for evaluation. The results show REM achieves significant performance improvements and maintains strong generalization on degraded images.",
      "mindmap": "graph LR\n        A[Beyond Artifacts: Real-Centric Envelope Modeling] --> B[核心问题/Problem: 现有检测器过拟合生成器痕迹，对真实世界退化敏感]\n        A --> C[主要方法/Method: 提出真实中心包络建模(REM)，建模真实图像分布而非伪造痕迹]\n        A --> D[关键结果/Results: 在八个基准上平均提升7.5%，在RealChain基准上泛化性强]"
    },
    {
      "title": "Generalization of Diffusion Models Arises with a Balanced Representation Space",
      "authors": "Zekai Zhang, Xiao Li, Xiang Li, Lianghe Shi, Meng Wu, Molei Tao, Qing Qu",
      "institution": "University of Michigan, Georgia Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.20963",
      "code": "https://deepthink-umich.github.io",
      "tags": [
        "diffusion models",
        "representation learning",
        "denoising autoencoder",
        "memorization detection",
        "representation steering",
        "generalization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58c26c58c94d92dfa7924dfd1c702b8c3239c0a63a4e6ef6f0eaaf42d7dc410a_w640_q70.webp",
      "contributions": "1. Provides a theoretical analysis linking memorization and generalization in diffusion models to specific representation structures (\"spiky\" vs. \"balanced\") using a two-layer ReLU DAE model. 2. Validates the theoretical findings on real-world unconditional and text-to-image diffusion models, showing the practical emergence of these representation regimes. 3. Proposes a representation-based method for detecting memorization and a training-free editing technique for precise control via representation steering.",
      "summary": "This paper analyzes the distinction between memorization and generalization in diffusion models through representation learning. It theoretically proves that memorization leads to \"spiky\" representations while generalization yields \"balanced\" ones, and validates this on real models. Based on these insights, the authors propose methods for memorization detection and training-free image editing, highlighting the central role of good representations for meaningful generation.",
      "mindmap": "graph LR\n    A[Generalization of Diffusion Models Arises with a Balanced Representation Space] --> B(核心问题/Problem: Diffusion models risk memorizing training data)\n    A --> C(主要方法/Method: Analyze via representation learning using a two-layer ReLU DAE)\n    A --> D(关键结果/Results: Memorization yields spiky representations, generalization yields balanced ones; Enables detection and editing techniques)"
    },
    {
      "title": "XGrid-Mapping: Explicit Implicit Hybrid Grid Submaps for Efficient Incremental Neural LiDAR Mapping",
      "authors": "Zeqing Song, Zhongmiao Yan, Junyuan Deng, Songpengcheng Xia, Xiang Mu, Jingyi Xu, Qi Wu, Ling Pei",
      "institution": "Shanghai Jiao Tong University (inferred from author email domains, e.g., likely sjtu.edu.cn)",
      "link": "https://arxiv.org/pdf/2512.20976",
      "code": null,
      "tags": [
        "neural radiance fields",
        "hybrid grid",
        "VDB structure",
        "submap-based organization",
        "distillation-based alignment",
        "dynamic removal"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4f609d4aace0ae5130d408518726a0efd356768fd806198569bd7240de6d7ef_w640_q70.webp",
      "contributions": "1. Proposes XGrid-Mapping, a hybrid grid framework combining sparse explicit grids for geometric priors and implicit dense grids for rich scene representation to enable efficient neural LiDAR mapping. 2. Introduces a submap-based organization coupled with a VDB structure to reduce computational load and support large-scale incremental mapping. 3. Presents a distillation-based overlap alignment strategy and a dynamic removal module to ensure submap consistency and enhance robustness and sampling efficiency.",
      "summary": "The paper proposes XGrid-Mapping, a hybrid explicit-implicit grid framework for efficient incremental neural LiDAR mapping. It uses submaps with a VDB structure and a distillation-based alignment strategy to achieve real-time performance and superior mapping quality, outperforming existing state-of-the-art methods.",
      "mindmap": "graph LR\n    A[XGrid-Mapping] --> B[核心问题/Problem: 大规模增量神经LiDAR建图效率低/ Large-scale incremental neural LiDAR mapping is inefficient]\n    A --> C[主要方法/Method: 显式-隐式混合网格子图/ Explicit-Implicit Hybrid Grid Submaps]\n    A --> D[关键结果/Results: 实时性能与高质量建图/ Real-time performance & superior mapping quality]"
    },
    {
      "title": "SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking",
      "authors": "Yujin Noh, Inho Jake Park, Chigon Hwang",
      "institution": "Kwangwoon University, Gwangju Institute of Science and Technology (GIST)",
      "link": "https://arxiv.org/pdf/2512.20975",
      "code": null,
      "tags": [
        "multi-camera tracking",
        "spatial RAG",
        "beam search",
        "CARLA simulator"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28d25f4afe3b9ea5d43a1eab9aa50328a7e33a926ce2a3183f35335ab5710c43_w640_q70.webp",
      "contributions": "1. Proposes SPOT, a map-guided LLM agent for zero-shot vehicle tracking in multi-CCTV blind spots without prior training. 2. Introduces a method to structure road and CCTV data as spatial documents using chunking for real-time LLM querying. 3. Combines map data with vehicle dynamics to perform beam search at intersections for predicting the next CCTV location.",
      "summary": "This paper proposes SPOT, a method that uses a map-guided LLM agent to track vehicles across blind spots in multi-CCTV environments. It converts spatial map and camera data into a queryable format for the LLM and uses vehicle motion data with beam search to predict where a vehicle will reappear. Experiments in CARLA show it maintains continuous trajectories better than existing techniques.",
      "mindmap": "graph LR\n    A[SPOT!: Map-Guided LLM Agent<br>SPOT!: 地图引导的LLM智能体] --> B[核心问题/Problem<br>Multi-CCTV盲点导致轨迹中断<br>Multi-CCTV blind spots cause trajectory loss]\n    A --> C[主要方法/Method<br>地图引导LLM + 空间RAG + 波束搜索<br>Map-guided LLM + Spatial RAG + Beam Search]\n    A --> D[关键结果/Results<br>在CARLA中准确预测下一个CCTV<br>Accurately predicts next CCTV in CARLA]"
    },
    {
      "title": "X-ray Insights Unleashed: Pioneering the Enhancement of Multi-Label Long-Tail Data",
      "authors": "Xinquan Yang, Jinheng Xie, Yawen Huang, Yuexiang Li, Huimin Huang, Hao Zheng, Xian Wu, Yefeng Zheng, Linlin Shen",
      "institution": "Tencent, Shenzhen University, National University of Singapore, Westlake University",
      "link": "https://arxiv.org/pdf/2512.20980",
      "code": null,
      "tags": [
        "medical image synthesis",
        "diffusion model",
        "data augmentation",
        "long-tail learning",
        "chest X-ray",
        "inpainting"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0edf9e433c973b8803a686a6312cf73fb691cb44807918973120eb68c5308b2a_w640_q70.webp",
      "contributions": "1. A novel data synthesis pipeline that uses a diffusion model trained on normal X-rays to inpaint head-class lesions, thereby augmenting tail-class data. 2. The introduction of a Large Language Model Knowledge Guidance (LKG) module to aid the inpainting process. 3. The proposal of a Progressive Incremental Learning (PIL) strategy to stabilize the fine-tuning of the inpainting model.",
      "summary": "This paper addresses the challenge of diagnosing rare, long-tail pulmonary anomalies in chest X-rays by proposing a data augmentation pipeline. The method trains a diffusion model on normal X-rays and uses it to inpaint common lesions from diseased images, preserving rare lesions for training, and is guided by an LLM and a progressive learning strategy. Evaluations on MIMIC and CheXpert datasets show the method achieves state-of-the-art performance.",
      "mindmap": "graph LR\n    A[X-ray Insights Unleashed: Pioneering the Enhancement of Multi-Label Long-Tail Data] --> B[核心问题/Problem: 长尾肺异常诊断挑战/Long-tail pulmonary anomaly diagnosis challenge]\n    A --> C[主要方法/Method: 基于扩散模型的尾部数据增强/Diffusion-based tail-class data augmentation with LKG & PIL]\n    A --> D[关键结果/Results: 在公开数据集上达到新SOTA/Achieves new SOTA on public datasets]"
    },
    {
      "title": "PUFM++: Point Cloud Upsampling via Enhanced Flow Matching",
      "authors": "Zhi-Song Liu, Chenhang He, Roland Maier, Andreas Rupp",
      "institution": "Lappeenranta-Lahti University of Technology LUT, The Hong Kong Polytechnic University, Karlsruhe Institute of Technology (KIT), Saarland University",
      "link": "https://arxiv.org/pdf/2512.20988",
      "code": "https://github.com/Holmes-Alan/Enhanced_PUFM",
      "tags": [
        "point cloud upsampling",
        "flow matching",
        "two-stage strategy",
        "adaptive time scheduler",
        "on-manifold constraints",
        "recurrent interface network"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/76eef67ea87c98d8de86d38ef9deebb9dc41ba30f94748973adbad178bbaab09_w640_q70.webp",
      "contributions": "1. A two-stage flow-matching strategy for improved geometric fidelity and distribution approximation, 2. A data-driven adaptive time scheduler to accelerate and stabilize inference, 3. The incorporation of on-manifold constraints and a recurrent interface network (RIN) to enhance surface alignment and feature interaction.",
      "summary": "The paper presents PUFM++, an enhanced flow-matching framework for generating dense and accurate point clouds from sparse, noisy inputs. It introduces a two-stage flow strategy, an adaptive time scheduler, on-manifold constraints, and a recurrent network to improve fidelity, robustness, and efficiency. Experiments show it achieves state-of-the-art performance in point cloud upsampling.",
      "mindmap": "graph LR\n        A[PUFM++] --> B[核心问题/Problem<br>Sparse, Noisy, Partial Point Clouds]\n        A --> C[主要方法/Method<br>Enhanced Flow Matching]\n        C --> D[Two-Stage Flow]\n        C --> E[Adaptive Time Scheduler]\n        C --> F[On-Manifold Constraints]\n        C --> G[Recurrent Interface Network]\n        A --> H[关键结果/Results<br>State-of-the-Art Upsampling]"
    },
    {
      "title": "Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations",
      "authors": "Jinghan Li, Yang Jin, Hao Jiang, Yadong Mu, Yang Song, Kun Xu",
      "institution": "Peking University",
      "link": "https://arxiv.org/pdf/2512.21004",
      "code": "https://github.com/Singularity0104/NExT-Vid",
      "tags": [
        "video representation learning",
        "autoregressive pretraining",
        "next-frame prediction",
        "flow-matching",
        "context-isolated predictor",
        "generative pretraining"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/680ed2b04d0f3e4083f2df212eb836dbdb953f63f6593cd3dbdaba933611f46c_w640_q70.webp",
      "contributions": "1. Proposes NExT-Vid, a novel autoregressive visual generative pretraining framework using masked next-frame prediction to jointly model images and videos. 2. Introduces a context-isolated autoregressive predictor to decouple semantic representation learning from target decoding. 3. Employs a conditioned flow-matching decoder to enhance the quality and diversity of generated frames.",
      "summary": "This paper addresses the limitation of existing visual generative pretraining methods, which often neglect temporal information, by proposing NExT-Vid, an autoregressive framework for next-frame prediction. The method uses a context-isolated predictor and a flow-matching decoder to learn effective video representations. Experiments show it outperforms previous generative pretraining methods on downstream classification tasks.",
      "mindmap": "graph LR\n    A[Learning from Next-Frame Prediction<br>基于下一帧预测的学习] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[视觉生成预训练忽视时序信息<br>Visual Pretraining Ignores Temporal Info]\n    C --> C1[提出NExT-Vid框架<br>Propose NExT-Vid Framework]\n    C1 --> C2[上下文隔离自回归预测器<br>Context-Isolated AR Predictor]\n    C1 --> C3[条件流匹配解码器<br>Conditioned Flow-Matching Decoder]\n    D --> D1[下游分类表现更优<br>Better Downstream Classification]"
    },
    {
      "title": "Granular-ball Guided Masking: Structure-aware Data Augmentation",
      "authors": "Shuyin Xia, Fan Chen, Dawei Dai, Meng Yang, Junwei Han, Xinbo Gao, Guoyin Wang",
      "institution": "Chongqing University of Posts and Telecommunications",
      "link": "https://arxiv.org/pdf/2512.21011",
      "code": null,
      "tags": [
        "data augmentation",
        "Granular-ball Computing",
        "structure-aware masking",
        "information dropping",
        "hierarchical masking"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/20fc1c10cf1817c8407a18f117844ed1bf0dd4159dfb95e173f2060df3d75c18_w640_q70.webp",
      "contributions": "1. Proposes Granular-ball Guided Masking (GBGM), a novel structure-aware data augmentation method guided by Granular-ball Computing (GBC). 2. Introduces a coarse-to-fine hierarchical masking process that adaptively preserves semantically rich regions while suppressing redundant areas. 3. Demonstrates the method's model-agnostic nature and effectiveness through extensive experiments on multiple benchmarks, showing improvements in classification and reconstruction.",
      "summary": "This paper addresses the lack of structural awareness in existing mask-based data augmentation methods, which can discard essential semantics. The authors propose Granular-ball Guided Masking (GBGM), a strategy that uses Granular-ball Computing to guide a hierarchical masking process, preserving important regions. Experiments show GBGM improves model robustness and performance across various benchmarks and architectures.",
      "mindmap": "graph LR\n    A[Granular-ball Guided Masking] --> B[核心问题/Problem: 数据增强缺乏结构感知，可能丢弃关键语义/Data augmentation lacks structural awareness, may discard key semantics]\n    A --> C[主要方法/Method: 基于粒球计算的层次化掩码/GBGM: Granular-ball Computing guided hierarchical masking]\n    A --> D[关键结果/Results: 提升分类与重建性能，模型无关/Improves classification & reconstruction, model-agnostic]"
    },
    {
      "title": "FluencyVE: Marrying Temporal-Aware Mamba with Bypass Attention for Video Editing",
      "authors": "Mingshu Cai, Yixuan Li, Osamu Yoshie, Yuya Ieiri",
      "institution": "Waseda University, Southeast University",
      "link": "https://arxiv.org/pdf/2512.21015",
      "code": null,
      "tags": [
        "video editing",
        "Mamba",
        "Stable Diffusion",
        "temporal attention",
        "low-rank approximation",
        "one-shot editing"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3419f702725d02a1b18daa40499c9c2d89fb38a4a84ff67afde4aaf53c6e943_w640_q70.webp",
      "contributions": "1. Proposes FluencyVE, a one-shot video editing method that integrates the Mamba module to replace temporal attention layers for improved temporal consistency and reduced computation. 2. Employs low-rank approximation matrices for query and key weights and a weighted averaging technique during training to preserve generative power while lowering computational burden. 3. Demonstrates effective editing of various video attributes, subjects, and locations in real-world videos through experiments.",
      "summary": "The paper addresses the challenges of temporal inconsistency and high computational cost in text-driven video editing. It proposes FluencyVE, a method that integrates the Mamba module into a Stable Diffusion-based model to replace temporal attention, and uses low-rank approximation and weighted averaging to maintain performance while reducing overhead. Experiments show the method's effectiveness in diverse video editing tasks.",
      "mindmap": "graph LR\n        A[FluencyVE: Video Editing] --> B[核心问题/Problem: Temporal inconsistency & High computational cost in video editing]\n        A --> C[主要方法/Method: Integrate Mamba, Replace temporal attention, Use low-rank approximation & weighted averaging]\n        A --> D[关键结果/Results: Effective editing of attributes, subjects, locations; Reduced computational burden]"
    },
    {
      "title": "MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds",
      "authors": "Xiangzuo Wu, Chengwei Ren, Jun Zhou, Xiu Li, Yuan Liu",
      "institution": "Tsinghua University, Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.21003",
      "code": "https://maddog241.github.io/mvinverse-page/",
      "tags": [
        "inverse rendering",
        "multi-view consistency",
        "feed-forward network",
        "attention mechanism",
        "consistency-based finetuning",
        "intrinsic decomposition"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4a474ea3dff714bb84e38df3458cdbb2ec256329a9c182d4c1eeb5f20afad380_w640_q70.webp",
      "contributions": "1. A feed-forward multi-view inverse rendering framework that jointly predicts consistent geometry and material properties from RGB image sequences in a single forward pass. 2. A model architecture that uses alternating attention across views to capture both intra-view lighting and inter-view material consistency. 3. A consistency-based finetuning strategy using unlabeled real-world videos to improve generalization and robustness in real-world conditions.",
      "summary": "This paper introduces MVInverse, a fast feed-forward framework for multi-view inverse rendering that predicts consistent scene properties like albedo and normals from RGB images. It uses cross-view attention for coherent reasoning and a novel finetuning strategy on real videos to improve generalization. Experiments show it achieves state-of-the-art performance in consistency and quality while being much faster than optimization-based methods.",
      "mindmap": "graph LR\n        A[MVInverse: Feed-forward Multi-view Inverse Rendering in Seconds] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[单视图方法多视图不一致/Single-view methods cause multi-view inconsistency]\n        B --> B2[多视图优化方法速度慢/Multi-view optimization methods are slow]\n        B --> B3[合成数据泛化能力差/Synthetic data leads to poor real-world generalization]\n        C --> C1[前馈多视图网络/Feed-forward multi-view network]\n        C --> C2[跨视图注意力/Cross-view attention]\n        C --> C3[基于一致性的微调/Consistency-based finetuning]\n        D --> D1[多视图一致性高/High multi-view consistency]\n        D --> D2[材质与法线质量优/Superior material & normal quality]\n        D --> D3[泛化到真实图像/Generalizes to real-world imagery]"
    },
    {
      "title": "Efficient and Robust Video Defense Framework against 3D-field Personalized Talking Face",
      "authors": "Rui-qing Sun, Xingshan Yao, Tian Lan, Hui-Yang Zhao, Jia-Ling Shi, Chen-Hao Cui, Zhijing Wu, Chen Yang, Xian-Ling Mao",
      "institution": "Beijing Institute of Technology, Alibaba International Digital Commerce",
      "link": "https://arxiv.org/pdf/2512.21019",
      "code": "https://github.com/Richen7418/VDF",
      "tags": [
        "adversarial defense",
        "talking face generation",
        "3D neural field",
        "adversarial perturbation",
        "video defense",
        "spatial-frequency optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba8dbfa36170ca60f99422da322b8d87e90e6f7380199d6d5fb91ee653784372_w640_q70.webp",
      "contributions": "1. A novel video defense framework that perturbs the 3D information acquisition process of talking face generation models to protect portrait videos. 2. A similarity-guided parameter sharing mechanism to achieve high computational efficiency (47x acceleration). 3. A multi-scale dual-domain attention module to jointly optimize perturbations in both spatial and frequency domains for robustness and high visual fidelity.",
      "summary": "This paper addresses the privacy threat posed by 3D-field talking face generation models by proposing an efficient video defense framework. The method introduces a parameter-sharing mechanism for speed and a dual-domain attention module to perturb 3D information while preserving video quality. Experiments show it offers strong defense, is 47x faster than baselines, and is robust against common attacks and transformations.",
      "mindmap": "graph LR\n        A[Efficient and Robust Video Defense Framework<br>高效鲁棒的视频防御框架] --> B[Problem: 3D-field TFG models pose privacy risks<br>问题: 3D场TFG模型带来隐私风险]\n        A --> C[Method: Perturb 3D info acquisition with shared params & dual-domain attention<br>方法: 用共享参数和双域注意力扰动3D信息获取]\n        A --> D[Results: Strong defense, 47x faster, high fidelity & robust<br>结果: 强防御性, 47倍加速, 高保真且鲁棒]"
    },
    {
      "title": "Multi-Attribute guided Thermal Face Image Translation based on Latent Diffusion Model",
      "authors": "Mingshu Cai, Osamu Yoshie, Yuya Ieiri",
      "institution": "Waseda University",
      "link": "https://arxiv.org/pdf/2512.21032",
      "code": null,
      "tags": [
        "heterogeneous face recognition",
        "latent diffusion model",
        "multi-attribute classifier",
        "Self-attn Mamba"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fef179e889d0a26d06a1c9b52dedf4221a1040e9e221b0c19692c72d9430246_w640_q70.webp",
      "contributions": "1. Proposes a novel latent diffusion-based model for generating high-quality visible face images from thermal inputs while preserving identity features. 2. Incorporates a multi-attribute classifier to extract key facial attributes from visible images to mitigate feature loss during translation. 3. Introduces the Self-attn Mamba module to enhance global modeling of cross-modal features and improve inference speed.",
      "summary": "This paper addresses the problem of performance degradation in face recognition models when applied to thermal infrared images due to domain shift. It proposes a new method using a latent diffusion model guided by a multi-attribute classifier and a Self-attn Mamba module to translate thermal images into high-quality visible images that preserve identity. The approach achieves state-of-the-art performance in image quality and identity preservation on benchmark datasets.",
      "mindmap": "graph LR\n    A[Multi-Attribute guided Thermal Face Image Translation<br>基于潜在扩散模型的多属性引导热人脸图像翻译] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[域偏移导致红外识别性能下降<br>Domain shift degrades IR recognition]\n    B --> B2[现有方法存在特征失真与丢失<br>Existing methods cause feature distortion/loss]\n    C --> C1[潜在扩散模型生成可见光图像<br>Latent Diffusion Model for V image generation]\n    C --> C2[多属性分类器引导特征保留<br>Multi-attribute classifier guides feature preservation]\n    C --> C3[Self-attn Mamba模块提升建模与速度<br>Self-attn Mamba enhances modeling & speed]\n    D --> D1[图像质量高<br>High image quality]\n    D --> D2[身份特征保留好<br>Good identity preservation]\n    D --> D3[达到SOTA性能<br>Achieves SOTA performance]"
    },
    {
      "title": "Next-Scale Prediction: A Self-Supervised Approach for Real-World Image Denoising",
      "authors": "Yiwen Shan, Haiyu Zhao, Peng Hu, Xi Peng, Yuanbiao Gou",
      "institution": "Sichuan University",
      "link": "https://arxiv.org/pdf/2512.21038",
      "code": null,
      "tags": [
        "image denoising",
        "self-supervised learning",
        "blind-spot network",
        "pixel-shuffle downsampling",
        "real-world noise",
        "super-resolution"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8fd7469e2ff88e84d5b9f4c335e67c32a82564aabaef8eb11c7625010499218a_w640_q70.webp",
      "contributions": "1. Proposes Next-Scale Prediction (NSP), a novel self-supervised paradigm that decouples noise decorrelation from detail preservation by constructing cross-scale training pairs for blind-spot networks. 2. Introduces a method where the network takes low-resolution, fully decorrelated sub-images as input to predict high-resolution targets, alleviating the trade-off between noise removal and detail retention. 3. Demonstrates that the proposed NSP framework naturally supports super-resolution of noisy images as a by-product, without requiring retraining or architectural modification.",
      "summary": "This paper addresses the challenge in self-supervised real-world image denoising where existing methods struggle to balance noise decorrelation and detail preservation. It introduces Next-Scale Prediction (NSP), a method that uses cross-scale training pairs to decouple these two objectives, allowing a blind-spot network to learn from decorrelated low-resolution inputs to predict clean high-resolution outputs. Experiments show NSP achieves state-of-the-art performance and can also perform super-resolution on noisy images without additional training.",
      "mindmap": "graph LR\n    A[Next-Scale Prediction: A Self-Supervised Approach for Real-World Image Denoising] --> B[核心问题/Problem: 自监督去噪中噪声去相关与细节保留的权衡/Trade-off between noise decorrelation and detail preservation in self-supervised denoising]\n    A --> C[主要方法/Method: 下一尺度预测/Next-Scale Prediction: 构建跨尺度训练对/Constructing cross-scale training pairs]\n    A --> D[关键结果/Results: 实现SOTA性能，自然支持超分辨率/Achieves SOTA performance, naturally supports super-resolution]"
    },
    {
      "title": "A Large-Depth-Range Layer-Based Hologram Dataset for Machine Learning-Based 3D Computer-Generated Holography",
      "authors": "Jaehong Lee, You Chan No, YoungWoo Kim, Duksu Kim",
      "institution": "Korea University of Technology & Education (KOREATECH)",
      "link": "https://arxiv.org/pdf/2512.21040",
      "code": null,
      "tags": [
        "computer-generated holography",
        "hologram dataset",
        "amplitude projection",
        "angular spectrum method",
        "RGB-D images",
        "super-resolution"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52ea7153ddcddf48944d10c7358fc7dea7c5e544520bd72d9c3d2634cf167b35_w640_q70.webp",
      "contributions": "1. Introduces KOREATECH-CGH, a large-scale public dataset of 6,000 RGB-D and complex hologram pairs with wide resolution and depth range for ML-CGH. 2. Proposes amplitude projection, a post-processing technique to enhance hologram reconstruction fidelity at large depth ranges by replacing amplitude components while preserving phase. 3. Validates the dataset's utility by demonstrating its effectiveness for training state-of-the-art ML models on tasks like hologram generation and super-resolution.",
      "summary": "This paper addresses the lack of high-quality datasets for machine learning-based computer-generated holography (ML-CGH) by introducing KOREATECH-CGH, a large-scale dataset of RGB-D and hologram pairs. The authors also propose a novel amplitude projection technique to improve hologram quality at large depth ranges. The dataset and method are shown to enable effective training of ML models and achieve superior reconstruction quality compared to prior work.",
      "mindmap": "graph LR\n    A[KOREATECH-CGH Dataset Paper] --> B(核心问题/Problem: ML-CGH缺乏高质量大规模数据集)\n    A --> C(主要方法/Method: 构建KOREATECH-CGH数据集并提出振幅投影技术/Amplitude Projection)\n    A --> D(关键结果/Results: 重建质量提升，验证了数据集的实用性/Improved reconstruction, validated dataset utility)"
    },
    {
      "title": "Matrix Completion Via Reweighted Logarithmic Norm Minimization",
      "authors": "Zhijie Wang, Liangtian He, Qinghua Zhang, Jifei Miao, Liang-Jian Deng, Jun Liu",
      "institution": "The affiliations are not explicitly listed in the provided content. Based on the author names (Zhijie Wang, Liangtian He, Qinghua Zhang, Jifei Miao, Liang-Jian Deng, Jun Liu), it is not possible to reliably infer a single main institution without the full paper.",
      "link": "https://arxiv.org/pdf/2512.21050",
      "code": null,
      "tags": [
        "low-rank matrix completion",
        "reweighted logarithmic norm",
        "matrix completion",
        "ADMM",
        "nonconvex optimization",
        "image inpainting"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81808ff3d4d128d0df5f11ee05651c74a2691f1ac5943d560a7091bdb5c374c8_w640_q70.webp",
      "contributions": "1. Proposes a novel reweighted matrix logarithmic norm (RMLN) as a more accurate nonconvex surrogate for the rank function. 2. Develops an ADMM-based optimization algorithm that works for any p in (0,1], removing a restriction present in prior work. 3. Demonstrates superior performance in image inpainting experiments compared to state-of-the-art low-rank matrix completion methods.",
      "summary": "This paper addresses the suboptimal solutions from nuclear norm minimization in low-rank matrix completion by proposing a novel reweighted logarithmic norm as a better nonconvex surrogate. The resulting optimization problem is solved efficiently using ADMM. Experiments on image inpainting show the proposed method outperforms existing state-of-the-art approaches.",
      "mindmap": "graph LR\n    A[Matrix Completion Via Reweighted Logarithmic Norm Minimization] --> B(核心问题/Problem: 核范数导致次优解/Nuclear norm yields suboptimal solutions)\n    A --> C(主要方法/Method: 提出重加权对数范数/Propose reweighted logarithmic norm)\n    A --> D(关键结果/Results: 图像修复性能优越/Superior image inpainting performance)"
    },
    {
      "title": "Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera",
      "authors": "Zibin Liu, Banglei Guan, Yang Shang, Shunkun Liang, Zhenbao Yu, Qifeng Yu",
      "institution": "National University of Defense Technology, Wuhan University",
      "link": "https://arxiv.org/pdf/2512.21053",
      "code": null,
      "tags": [
        "object pose tracking",
        "event camera",
        "optical flow",
        "6DoF pose estimation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9212058f75e634f43650bad542c684ec2b99117ed801e190d2e670146ed2f351_w640_q70.webp",
      "contributions": "1. Proposes a novel optical flow-guided 6DoF object pose tracking method using an event camera. 2. Introduces a 2D-3D hybrid feature extraction strategy to detect corners and edges from events and object models. 3. Establishes correlation between features by searching for corner optical flow and minimizing distances between corners and edges for iterative pose optimization.",
      "summary": "This paper proposes a new method for 6DoF object pose tracking using an event camera. The method uses optical flow to guide the association between extracted 2D-3D hybrid features (corners and edges) and iteratively optimizes the object pose. Experiments on simulated and real event data show the method outperforms existing event-based approaches in accuracy and robustness.",
      "mindmap": "graph LR\n    A[论文标题: Optical Flow-Guided 6DoF Object Pose Tracking with an Event Camera] --> B(核心问题/Problem: 传统相机位姿跟踪面临运动模糊、光照变化等挑战)\n    A --> C(主要方法/Method: 使用光流引导的2D-3D混合特征提取与关联进行位姿优化)\n    A --> D(关键结果/Results: 在模拟和真实事件数据上优于现有事件相机方法)"
    },
    {
      "title": "DexAvatar: 3D Sign Language Reconstruction with Hand and Body Pose Priors",
      "authors": "Kaustubh Kundu, Hrishav Bakul Barua, Lucy Robertson-Bell, Zhixi Cai, Kalin Stefanov",
      "institution": "Monash University, TCS Research",
      "link": "https://arxiv.org/pdf/2512.21054",
      "code": "https://github.com/kaustesseract/DexAvatar",
      "tags": [
        "3D human pose estimation",
        "3D sign language reconstruction",
        "biomechanical accuracy",
        "hand and body pose priors",
        "monocular video",
        "SMPL-X"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/594bef871fe9a00d58a9f3f12c9a0b4bf4f66d3d738bd3f02dedcbad04bdcd25_w640_q70.webp",
      "contributions": "1. A novel framework (DexAvatar) for reconstructing biomechanically accurate 3D hand and body poses from monocular sign language videos. 2. The use of learned 3D hand and body pose priors to guide the reconstruction and overcome challenges like self-occlusion and motion blur. 3. Demonstrating strong performance on the SGNify benchmark, achieving a 35.11% improvement over the state-of-the-art.",
      "summary": "The paper introduces DexAvatar, a framework that uses learned 3D hand and body pose priors to reconstruct accurate 3D sign language poses from monocular videos. It addresses the limitations of existing 2D datasets and noisy 3D estimations. The method significantly outperforms prior work on the SGNify motion capture benchmark.",
      "mindmap": "graph LR\n        A[DexAvatar] --> B[核心问题/Problem: 手语视频缺乏准确3D数据，现有3D姿态估计质量差]\n        A --> C[主要方法/Method: 利用学习到的3D手部和身体姿态先验，从单目视频重建]\n        A --> D[关键结果/Results: 在SGNify数据集上性能提升35.11%]"
    },
    {
      "title": "Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition",
      "authors": "Hongsong Wang, Heng Fei, Bingxuan Dai, Jie Gui",
      "institution": "Southeast University, Purple Mountain Laboratories",
      "link": "https://arxiv.org/pdf/2512.21064",
      "code": null,
      "tags": [
        "skeleton-based action recognition",
        "multimodal fusion",
        "self-supervised learning",
        "decomposition and composition",
        "skeleton data",
        "representation learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c579dcce18ac587c13f2fd1178f66dd2922786e4c3434ff17ad5319761f7aa1_w640_q70.webp",
      "contributions": "1. Proposes a novel self-supervised multimodal skeleton-based action representation learning framework named \"Decomposition and Composition\". 2. Introduces a Decomposition strategy to decompose fused multimodal features into distinct unimodal features and align them with ground truth unimodal counterparts. 3. Introduces a Composition strategy that integrates multiple unimodal features as self-supervised guidance to enhance multimodal representation learning.",
      "summary": "This paper proposes a self-supervised framework called \"Decomposition and Composition\" for multimodal skeleton-based action recognition. It aims to effectively utilize the complementarity of different modalities while maintaining computational efficiency. Experiments on major datasets show the method achieves a strong balance between performance and cost.",
      "mindmap": "graph LR\n    A[Multimodal Skeleton-Based Action Representation Learning via Decomposition and Composition] --> B(核心问题/Problem: 多模态互补性与模型效率的平衡/Balancing multimodal complementarity and model efficiency)\n    A --> C(主要方法/Method: 分解与组合自监督框架/Decomposition and Composition self-supervised framework)\n    A --> D(关键结果/Results: 在效率与性能间取得优秀平衡/Achieves excellent balance between efficiency and performance)"
    },
    {
      "title": "Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation",
      "authors": "Zebin Jiang, Tianle Jin, Xiangtong Yao, Alois Knoll, Hu Cao",
      "institution": "Technical University of Munich",
      "link": "https://arxiv.org/pdf/2512.21065",
      "code": null,
      "tags": [
        "robotic grasping",
        "language-guided grasping",
        "cross-modal fusion",
        "coarse-to-fine learning",
        "CLIP embeddings",
        "dynamic convolution"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2da981b4739ec7852bb087293e335e475a0b336479e9ff3e69ca100703c7262_w640_q70.webp",
      "contributions": "1. Proposes a hierarchical cross-modal fusion pipeline that progressively injects linguistic cues into visual feature reconstruction for fine-grained visual-semantic alignment., 2. Introduces a language-conditioned dynamic convolution head (LDCH) that adaptively mixes convolution experts based on sentence-level features for instruction-adaptive predictions., 3. Presents a final refinement module to enhance grasp consistency and robustness in complex scenes, validated on real robotic platforms.",
      "summary": "This paper proposes LGGD, a language-guided grasp detection method using a coarse-to-fine learning paradigm with hierarchical cross-modal fusion and a language-conditioned dynamic convolution head. It achieves superior performance on benchmark datasets and demonstrates effective real-world robotic manipulation.",
      "mindmap": "graph LR\n    A[Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation] --> B[核心问题/Problem: Weak alignment between language instructions and visual grasp reasoning in cluttered scenes.]\n    A --> C[主要方法/Method: Coarse-to-fine learning with hierarchical cross-modal fusion and a language-conditioned dynamic convolution head (LDCH).]\n    A --> D[关键结果/Results: Outperforms existing methods, shows strong generalization, and is effective on a real robot.]"
    },
    {
      "title": "Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control",
      "authors": "Minghao Han, YiChen Liu, Yizhou Liu, Zizhi Chen, Jingqun Tang, Xuecheng Wu, Dingkang Yang, Lihua Zhang",
      "institution": "Fudan University, University of Science and Technology Beijing, Xi'an Jiaotong University, ByteDance, Fysics Intelligence Technologies Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.21058",
      "code": null,
      "tags": [
        "medical image generation",
        "diagnostic semantic tokens",
        "prototype control",
        "multi-stream control",
        "pathology MLLM",
        "Patho-FID"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54421c653fc67f5d29e7359f606ed3b8f53d435c83275a71b19858309287fe45_w640_q70.webp",
      "contributions": "1. Proposed UniPath, a semantics-driven pathology image generation framework with Multi-Stream Control (Raw-Text, High-Level Semantics, and Prototype streams) for fine-grained, diagnosis-aware control. 2. Curated a large-scale 2.65M image-text corpus and a high-quality 68K subset to address data scarcity in computational pathology. 3. Established a comprehensive four-tier evaluation hierarchy tailored for pathology and demonstrated state-of-the-art performance, including a 51% improvement in Patho-FID.",
      "summary": "This paper introduces UniPath, a framework for generating pathology images with fine-grained semantic control by leveraging diagnostic understanding through multi-stream conditioning and a prototype bank. It addresses key challenges like data scarcity and terminological heterogeneity by curating large datasets and using a frozen pathology MLLM to distill robust semantic tokens. Experiments show UniPath achieves state-of-the-art performance, significantly outperforming prior methods in both image quality and semantic fidelity.",
      "mindmap": "graph LR\n    A[Beyond Pixel Simulation: Pathology Image Generation via Diagnostic Semantic Tokens and Prototype Control] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[数据稀缺/Data Scarcity]\n    B --> B2[语义控制不足/Lack of Semantic Control]\n    B --> B3[术语异质性/Terminological Heterogeneity]\n    C --> C1[多流控制/Multi-Stream Control]\n    C1 --> C1a[原始文本流/Raw-Text Stream]\n    C1 --> C1b[高层语义流/High-Level Semantics Stream]\n    C1 --> C1c[原型流/Prototype Stream]\n    C --> C2[诊断语义令牌/Diagnostic Semantic Tokens]\n    C --> C3[原型库/Prototype Bank]\n    D --> D1[Patho-FID 80.9 / Patho-FID 80.9]\n    D --> D2[语义控制达98.7% / Semantic Control 98.7%]\n    D --> D3[开源数据与代码 / Open Data & Code]"
    },
    {
      "title": "UniPR-3D: Towards Universal Visual Place Recognition with Visual Geometry Grounded Transformer",
      "authors": "Tianchen Deng, Xun Chen, Ziming Li, Hongming Shen, Danwei Wang, Javier Civera, Hesheng Wang",
      "institution": "Shanghai Jiao Tong University, University of Zaragoza, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.21078",
      "code": "https://github.com/dtc111111/UniPR-3D",
      "tags": [
        "visual place recognition",
        "multi-view",
        "transformer",
        "3D representation",
        "feature aggregation",
        "geometry-grounded"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f608df655d14ba7ae116d2888176d7d1cb60fee1d08eaf9644d0c320c42e0fe6_w640_q70.webp",
      "contributions": "1. Introduces UniPR-3D, the first VPR architecture that effectively integrates information from multiple views. 2. Proposes a method that jointly leverages 2D and 3D tokens from a VGGT backbone and designs dedicated aggregation modules for each. 3. Incorporates both single- and multi-frame aggregation schemes with a variable-length sequence retrieval strategy to enhance generalization.",
      "summary": "This paper introduces UniPR-3D, a novel Visual Place Recognition (VPR) method that uses a Visual Geometry Grounded Transformer (VGGT) backbone to integrate multi-view information. It constructs descriptors by aggregating both 2D and 3D tokens with dedicated modules and supports variable-length sequence retrieval. Experiments show that UniPR-3D achieves state-of-the-art performance, outperforming existing single- and multi-view baselines.",
      "mindmap": "graph LR\n    A[UniPR-3D] --> B[核心问题/Problem: 传统单视图VPR泛化能力有限/Traditional single-view VPR has limited generalization]\n    A --> C[主要方法/Method: 基于VGGT的多视图2D/3D特征聚合/Multi-view 2D/3D feature aggregation with VGGT]\n    A --> D[关键结果/Results: 达到SOTA性能/Achieves state-of-the-art performance]"
    },
    {
      "title": "Hierarchical Modeling Approach to Fast and Accurate Table Recognition",
      "authors": "Takaya Kawakatsu",
      "institution": "Preferred Networks, Inc.",
      "link": "https://arxiv.org/pdf/2512.21083",
      "code": null,
      "tags": [
        "document analysis",
        "table recognition",
        "multi-task learning",
        "non-causal attention",
        "parallel inference",
        "hierarchical modeling"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd3e215f24ffc72ff43528e39e86c65289bbfb5de75955d48f0284f277f0089e_w640_q70.webp",
      "contributions": "1. A novel multi-task model utilizing non-causal attention to capture the entire table structure. 2. A parallel inference algorithm for cell content recognition that significantly speeds up inference. 3. A hierarchical modeling approach that extends the performance of multi-task models while addressing their speed and explainability limitations.",
      "summary": "This paper addresses the slow inference speed and lack of explainability in existing table recognition models. It proposes a new multi-task model with non-causal attention for global structure understanding and a parallel inference algorithm to decode cell contents simultaneously, achieving both superior accuracy and a 10x+ speedup on large public datasets.",
      "mindmap": "graph LR\n    A[Hierarchical Modeling Approach to Fast and Accurate Table Recognition] --> B(核心问题/Problem: 现有表格识别模型推理慢且有效性未充分解释/Existing models are slow and their effectiveness is not fully explained)\n    A --> C(主要方法/Method: 使用非因果注意力的多任务模型与并行推理算法/Novel multi-task model with non-causal attention & parallel inference algorithm)\n    A --> D(关键结果/Results: 在大型公开数据集上实现视觉与统计上的优越性/Superiority demonstrated visually and statistically on two large public datasets)"
    },
    {
      "title": "UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters",
      "authors": "Yongkun Du, Zhineng Chen, Yazhen Xie, Weikang Baiand Hao Feng, Wei Shi, Yuchen Su, Can Huang, Yu-Gang Jiang",
      "institution": "Fudan University, ByteDance",
      "link": "https://arxiv.org/pdf/2512.21095",
      "code": "https://github.com/Topdu/OpenOCR",
      "tags": [
        "document parsing",
        "unified recognition",
        "hierarchical supervision",
        "semantic-decoupled tokenizer",
        "lightweight model",
        "multi-level recognition"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1d562a464fcb112a02846c5c1d5056088c1b9a37f7ece91f77cfeea28939b63b_w640_q70.webp",
      "contributions": "1. Proposed UniRec-0.1B, a lightweight unified model with only 0.1B parameters for multi-level text and formula recognition. 2. Introduced a hierarchical supervision training strategy to address structural variability across different recognition levels. 3. Developed a semantic-decoupled tokenizer to separate text and formula representations, mitigating semantic entanglement.",
      "summary": "This paper proposes UniRec-0.1B, a lightweight vision-language model for unified text and formula recognition across multiple document levels. To address challenges like structural variability and semantic entanglement, the method introduces hierarchical supervision training and a semantic-decoupled tokenizer, trained on a new 40M-sample dataset. Experiments show it outperforms existing general and expert models while being 2-9x faster.",
      "mindmap": "graph LR\n    A[UniRec-0.1B] --> B[核心问题/Problem: Large VLMs are computationally demanding for unified text/formula recognition]\n    A --> C[主要方法/Method: Lightweight model with hierarchical supervision & semantic-decoupled tokenizer]\n    A --> D[关键结果/Results: Outperforms SOTA models with 2-9x speedup]"
    },
    {
      "title": "T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation",
      "authors": "Zhe Cao, Tao Wang, Jiaming Wang, Yanghai Wang, Yuanxing Zhang, Jialu Chen, Miao Deng, Jiahao Wang, Yubin Guo, Chenxi Liao, Yize Zhang, Zhaoxiang Zhang, Jiaheng Liu",
      "institution": "Nanjing University, Kuaishou Technology, Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.21094",
      "code": "https://github.com/NJU-LINK/T2AV-Compass",
      "tags": [
        "multimodal generation evaluation",
        "text-to-audio-video",
        "cross-modal alignment",
        "MLLM-as-a-Judge",
        "benchmark",
        "evaluation framework"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d57ae1315c90b252f611ac78bcd0cff28f02c7dc2648194faa7e1614fe031d1_w640_q70.webp",
      "contributions": "1. Introduces T2AV-Compass, a unified benchmark with 500 diverse and complex prompts for evaluating Text-to-Audio-Video generation systems. 2. Proposes a dual-level evaluation framework combining objective signal-level metrics with a subjective MLLM-as-a-Judge protocol. 3. Provides extensive evaluation of 11 T2AV systems, revealing significant gaps in realism and cross-modal consistency, establishing a diagnostic testbed for future research.",
      "summary": "The paper addresses the fragmented evaluation of Text-to-Audio-Video (T2AV) generation by proposing T2AV-Compass, a unified benchmark and evaluation framework. It combines objective metrics for quality and alignment with a subjective MLLM-as-a-Judge protocol. The evaluation of 11 systems shows they still fall short of human-level realism and synchronization, highlighting the benchmark's value for advancing the field.",
      "mindmap": "graph LR\n    A[T2AV-Compass: Towards Unified Evaluation for Text-to-Audio-Video Generation] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[评估碎片化/Fragmented Evaluation]\n    B --> B2[缺乏跨模态对齐/Lacks Cross-modal Alignment]\n    C --> C1[统一基准/Unified Benchmark]\n    C --> C2[双层级评估/Dual-level Evaluation]\n    C1 --> C1a[500个提示/500 Prompts]\n    C2 --> C2a[客观指标/Objective Metrics]\n    C2 --> C2b[主观MLLM评判/Subjective MLLM-as-a-Judge]\n    D --> D1[模型表现不足/Models Fall Short]\n    D --> D2[显著的改进空间/Significant Improvement Room]"
    },
    {
      "title": "TexAvatars : Hybrid Texel-3D Representations for Stable Rigging of Photorealistic Gaussian Head Avatars",
      "authors": "Jaeseong Lee, Junyeong Ahn, Taewoong Kang, Jaegul Choo",
      "institution": "KAIST, Hanyang University",
      "link": "https://arxiv.org/pdf/2512.21099",
      "code": null,
      "tags": [
        "3D avatar generation",
        "3D Gaussian Splatting",
        "analytic rigging",
        "texel-space deformation",
        "hybrid representation",
        "head reenactment"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8320fd6b1a6f131ad704b82b65143269040ab86b9b67005a1edf15fca8097f6_w640_q70.webp",
      "contributions": "1. A hybrid avatar representation (TexAvatars) that combines analytic rigging for geometric grounding with texel-space neural regression for spatial continuity. 2. A method that predicts Gaussian attributes in UV space via CNNs but drives 3D deformation using mesh-aware Jacobians, enabling smooth transitions across mesh boundaries. 3. The model demonstrates improved generalization, stability, and capture of fine-grained expression details (e.g., wrinkles, mouth cavity) under extreme poses and expressions.",
      "summary": "This paper introduces TexAvatars, a method for creating drivable 3D head avatars by hybridizing analytic rigging with texel-space neural regression to improve generalization to unseen expressions. It predicts local attributes in UV space but uses mesh-aware Jacobians for 3D deformation, separating semantic modeling from geometric control. The approach achieves state-of-the-art performance in challenging reenactment scenarios, capturing fine details with high fidelity.",
      "mindmap": "graph LR\n        A[TexAvatars] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方法泛化性差/Existing methods generalize poorly]\n        B --> B2[难以处理极端表情与姿态/Struggle with extreme expressions & poses]\n        C --> C1[混合表示/Hybrid Representation]\n        C --> C2[UV空间预测，3D网格驱动/UV-space prediction, 3D mesh-driven deformation]\n        D --> D1[泛化能力提升/Improved generalization]\n        D --> D2[高保真细节/High-fidelity details]\n        D --> D3[状态领先性能/State-of-the-art performance]"
    },
    {
      "title": "FreeInpaint: Tuning-free Prompt Alignment and Visual Rationality Enhancement in Image Inpainting",
      "authors": "Chao Gong, Dong Li, Yingwei Pan, Jingjing Chen, Ting Yao, Tao Mei",
      "institution": "Fudan University, HiDream.ai Inc.",
      "link": "https://arxiv.org/pdf/2512.21104",
      "code": "https://github.com/CharlesGong12/FreeInpaint",
      "tags": [
        "image inpainting",
        "diffusion models",
        "latent optimization",
        "prompt alignment",
        "visual rationality",
        "plug-and-play"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4367afc5cacaf5baa9675d465f870aa3305d6715ef1ced7b9a233ea1cdf470b_w640_q70.webp",
      "contributions": "1. Proposes a plug-and-play, tuning-free approach (FreeInpaint) that optimizes diffusion latents during inference to enhance image faithfulness., 2. Introduces a prior-guided noise optimization method to steer model attention towards valid inpainting regions by optimizing the initial noise., 3. Designs a composite guidance objective tailored for inpainting to direct the denoising process, improving both prompt alignment and visual rationality by optimizing intermediate latents.",
      "summary": "The paper addresses the challenge in text-guided image inpainting of aligning generated content with user prompts while maintaining visual fidelity. It proposes FreeInpaint, a tuning-free method that optimizes diffusion latents during inference using a prior-guided noise optimization and a composite guidance objective. Extensive experiments demonstrate its effectiveness in enhancing both prompt alignment and visual rationality compared to existing methods.",
      "mindmap": "graph LR\n    A[FreeInpaint] --> B[核心问题/Problem: 文本引导图像修复中提示对齐与视觉合理性的平衡问题/Balancing prompt alignment and visual rationality in text-guided inpainting]\n    A --> C[主要方法/Method: 无需微调的潜在优化与复合引导目标/Tuning-free latent optimization & composite guidance objective]\n    A --> D[关键结果/Results: 提升了提示对齐与视觉合理性，验证了方法的有效性与鲁棒性/Improved prompt alignment & visual rationality, validated effectiveness & robustness]"
    },
    {
      "title": "STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting",
      "authors": "Shi Quan Foo, Chi-Ho Wong, Zhihan Gao, Dit-Yan Yeung, Ka-Hing Wong, Wai-Kin Wong",
      "institution": "The Hong Kong University of Science and Technology, Hong Kong Observatory",
      "link": "https://arxiv.org/pdf/2512.21118",
      "code": "https://github.com/sqfoo/stldm_official",
      "tags": [
        "diffusion models",
        "precipitation nowcasting",
        "latent diffusion model",
        "spatio-temporal prediction",
        "variational autoencoder",
        "conditioning network"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e3f04a94a2a07676690c2f108f7a602a6b052c1463701d217a319cd81e05ecce_w640_q70.webp",
      "contributions": "1. Proposes STLDM, a novel two-stage diffusion-based architecture for precipitation nowcasting that combines deterministic forecasting with generative enhancement. 2. Introduces an end-to-end learning framework that jointly trains a Variational Autoencoder, a conditioning network, and a latent diffusion model. 3. Demonstrates superior performance and improved inference efficiency compared to state-of-the-art methods on multiple radar datasets.",
      "summary": "The paper addresses the challenge of precipitation nowcasting, where deterministic models produce blurry predictions and generative models suffer from poor accuracy. It proposes STLDM, a spatio-temporal latent diffusion model that decomposes the task into a deterministic forecasting stage and a generative enhancement stage. Experiments show STLDM outperforms state-of-the-art methods while being more efficient.",
      "mindmap": "graph LR\n    A[STLDM: 降水临近预报模型] --> B[核心问题/Problem: 确定性模型模糊，生成模型精度差]\n    A --> C[主要方法/Method: 两阶段潜扩散模型]\n    A --> D[关键结果/Results: 性能优越，推理高效]"
    },
    {
      "title": "MarineEval: Assessing the Marine Intelligence of Vision-Language Models",
      "authors": "YuK-Kwan Wong, Tuan-An To, Jipeng Zhang, Ziqiang Zheng, Sai-Kit Yeung",
      "institution": "Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.21126",
      "code": "https://marineeval.hkustvgd.com",
      "tags": [
        "vision-language models",
        "marine intelligence",
        "domain-specific evaluation",
        "benchmark dataset"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/98ea3a0e94d1e540398b18f5612700e51ce9e190b312a741182b20da5ecfacac_w640_q70.webp",
      "contributions": "1. Introduces MarineEval, the first large-scale marine domain-specific dataset and benchmark for evaluating Vision-Language Models (VLMs). 2. Constructs a diverse dataset with 2,000 image-based QA pairs, covering 7 task dimensions and 20 capacity dimensions, validated by marine domain experts. 3. Provides a comprehensive benchmark evaluation of 17 existing VLMs, revealing their significant limitations in handling domain-specific marine questions and highlighting areas for future improvement.",
      "summary": "This paper investigates whether existing general-purpose Vision-Language Models (VLMs) can act as domain experts for marine science. To evaluate this, the authors construct MarineEval, a large-scale, expert-verified benchmark dataset of 2,000 marine image-question-answer pairs. The benchmark reveals that current VLMs perform poorly on this domain-specific task, indicating a significant gap and need for future research in specialized VLM capabilities.",
      "mindmap": "graph LR\n    A[MarineEval: Assessing the Marine Intelligence of Vision-Language Models] --> B(核心问题/Problem: Can VLMs act as marine domain experts?);\n    A --> C(主要方法/Method: Construct MarineEval benchmark with 2000 expert-verified marine image-QA pairs);\n    A --> D(关键结果/Results: Existing VLMs perform poorly, highlighting a large room for improvement);"
    },
    {
      "title": "ORCA: Object Recognition and Comprehension for Archiving Marine Species",
      "authors": "Yuk-Kwan Wong, Haixin Liang, Zeyu Ma, Yiwei Chen, Ziqiang Zheng, Rinaldi Gotama, Pascal Sebastian, Lauren D. Sparks, Sai-Kit Yeung",
      "institution": "Hong Kong University of Science and Technology, University of Electronic Science and Technology of China, Indo Ocean Foundation",
      "link": "https://arxiv.org/pdf/2512.21150",
      "code": "https://orca.hkustvgd.com",
      "tags": [
        "multi-modal benchmark",
        "marine visual understanding",
        "object detection",
        "instance captioning",
        "visual grounding",
        "open-vocabulary"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cfd1cc706ce3b8fe57cf7dcd49a501ba304d0ea7e587a81bddd8288ab2c69be7_w640_q70.webp",
      "contributions": "1. Introduces ORCA, a comprehensive multi-modal benchmark dataset for marine species with 14,647 images, 42,217 bounding boxes, and 22,321 expert-verified instance captions. 2. Formulates and evaluates three core computer vision tasks (object detection, instance captioning, visual grounding) on the benchmark to align domain challenges with well-defined tasks. 3. Provides an extensive evaluation of 18 state-of-the-art models, highlighting key challenges like species diversity and morphological overlap, establishing a baseline for future research.",
      "summary": "This paper presents ORCA, a multi-modal benchmark dataset for marine visual understanding, addressing the lack of systematic data and task formulation in the domain. It evaluates 18 state-of-the-art models on tasks like object detection and captioning, revealing significant challenges due to species diversity and domain-specific demands. The work establishes a comprehensive benchmark to advance research in automated marine ecosystem monitoring.",
      "mindmap": "graph LR\n        A[ORCA: Object Recognition and Comprehension for Archiving Marine Species] --> B[核心问题/Problem: Limited marine training data & lack of systematic task formulation]\n        A --> C[主要方法/Method: Introduce multi-modal benchmark with images, bboxes, captions & evaluate models on 3 tasks]\n        A --> D[关键结果/Results: Highlights challenges (diversity, overlap) & establishes comprehensive benchmark]"
    },
    {
      "title": "TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation",
      "authors": "Gaoren Lin, Huangxuan Zhao, Yuan Xiong, Lefei Zhang, Bo Du, Wentao Zhu",
      "institution": "Wuhan University (Inferred from authors Gaoren Lin, Lefei Zhang, Bo Du, Wentao Zhu, who are known to be affiliated with Wuhan University. Huangxuan Zhao and Yuan Xiong are likely from the same group.)",
      "link": "https://arxiv.org/pdf/2512.21135",
      "code": null,
      "tags": [
        "medical image segmentation",
        "CLIP",
        "multimodal fusion",
        "parameter-efficient fine-tuning",
        "vision-language alignment",
        "anatomical structure"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b11d6061b6a08f8b03b2395e16fc0847c1b68ab4e388d93a886ee875211fcf44_w640_q70.webp",
      "contributions": "1. Proposes a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch to preserve fine-grained anatomical structures. 2. Introduces a Domain-Augmented Text Encoder (DATE) that injects medical knowledge from large language models to better model complex clinical descriptions. 3. Designs a Vision-Language Calibration Module (VLCM) to refine cross-modal correspondence in a unified feature space, addressing domain-specific semantic misalignment.",
      "summary": "The paper proposes TGC-Net, a parameter-efficient CLIP-based framework for text-guided medical image segmentation. It addresses CLIP's limitations in medical imaging by introducing modules for structural refinement, medical knowledge injection, and cross-modal calibration. Experiments on five datasets show state-of-the-art performance with fewer trainable parameters.",
      "mindmap": "graph LR\n        A[TGC-Net: Text-Guided Medical Image Segmentation<br>文本引导的医学图像分割] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n    \n        B --> B1[CLIP在医学领域应用受限<br>CLIP Limitations in Medical Domain]\n        B1 --> B2[结构细节丢失<br>Loss of Fine-grained Structure]\n        B1 --> B3[复杂文本建模不足<br>Inadequate Text Modeling]\n        B1 --> B4[领域语义未对齐<br>Domain Semantic Misalignment]\n    \n        C --> C1[语义-结构协同编码器 SSE<br>Semantic-Structural Synergy Encoder]\n        C --> C2[领域增强文本编码器 DATE<br>Domain-Augmented Text Encoder]\n        C --> C3[视觉-语言校准模块 VLCM<br>Vision-Language Calibration Module]\n    \n        D --> D1[在5个数据集上SOTA<br>SOTA on 5 Datasets]\n        D --> D2[参数高效<br>Parameter-Efficient]\n        D --> D3[Dice分数显著提升<br>Notable Dice Gains]"
    },
    {
      "title": "Towards Arbitrary Motion Completing via Hierarchical Continuous Representation",
      "authors": "Chenghao Xu, Guangtao Lyu, Qi Liu, Jiexi Yan, Muli Yang, Cheng Deng",
      "institution": "Hohai University, Xidian University, Institute for Infocomm Research (I2R), A*STAR",
      "link": "https://arxiv.org/pdf/2512.21183",
      "code": null,
      "tags": [
        "motion representation and generation",
        "Implicit Neural Representations",
        "hierarchical temporal encoding",
        "parametric activation",
        "continuous representation",
        "motion interpolation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d8371d82733eca85c31c35a7b6beb75b440d219ddb44ee03521161e6e3f48a6_w640_q70.webp",
      "contributions": "1. Proposes a novel hierarchical continuous representation framework (PA-HiRes) for human motion sequences based on Implicit Neural Representations (INRs). 2. Introduces a hierarchical temporal encoding mechanism to capture intricate motion patterns at multiple temporal scales. 3. Integrates a custom parametric activation function, powered by Fourier transformations, into the MLP decoder to enhance the expressiveness and accuracy of the continuous motion model.",
      "summary": "This paper addresses the limitation of fixed-frame-rate human motion data by proposing a continuous representation model called PA-HiRes. The method uses a hierarchical implicit neural representation with a novel parametric activation function to enable interpolation and extrapolation of motion at arbitrary frame rates. Extensive evaluations show the approach is effective and robust for representing complex motion behaviors.",
      "mindmap": "graph LR\n    A[Towards Arbitrary Motion Completing via Hierarchical Continuous Representation] --> B(核心问题/Problem: Fixed-frame-rate motion sequences compromise fidelity and smoothness)\n    A --> C(主要方法/Method: PA-HiRes framework with hierarchical temporal encoding and parametric Fourier activation)\n    A --> D(关键结果/Results: Enables arbitrary frame rate interpolation/extrapolation; Demonstrates effectiveness and robustness)"
    },
    {
      "title": "UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement",
      "authors": "Tanghui Jia, Dongyu Yan, Dehao Hao, Yang Li, Kaiyi Zhang, Xianyi He, Lanjiong Li, Jinnan Chen, Lutao Jiang, Qishen Yin, Long Quan, Ying-Cong Chen, Li Yuan",
      "institution": "Peking University, The Hong Kong University of Science and Technology, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.21185",
      "code": null,
      "tags": [
        "3D shape generation",
        "diffusion models",
        "geometric refinement",
        "watertight processing",
        "voxel-based refinement",
        "RoPE"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73c8cc72abe4a497351656b82b6735dbe1550a6d18e5c93d7aa031162f07f81e_w640_q70.webp",
      "contributions": "1. A comprehensive data processing pipeline for 3D datasets, featuring a novel watertight processing method and high-quality filtering to improve geometric quality. 2. A two-stage 3D diffusion framework that decouples spatial localization from geometric detail synthesis, using voxel-based refinement with RoPE-encoded positional anchors. 3. Training and evaluation demonstrating competitive performance with existing open-source methods using only publicly available datasets and limited resources.",
      "summary": "This paper introduces UltraShape 1.0, a two-stage diffusion framework for generating high-fidelity 3D shapes. It first synthesizes a coarse structure and then refines it using a novel method that decouples spatial localization from detail synthesis via voxel queries and RoPE encoding. The approach, supported by an improved data processing pipeline, achieves competitive geometry generation quality using public datasets.",
      "mindmap": "graph LR\n    A[UltraShape 1.0] --> B(核心问题/Problem: High-fidelity 3D shape generation 高保真3D形状生成)\n    A --> C(主要方法/Method: Two-stage diffusion with geometric refinement 两阶段扩散与几何细化)\n    C --> C1(Stage 1: Coarse structure synthesis 粗结构合成)\n    C --> C2(Stage 2: Voxel-based detail refinement 基于体素的细节细化)\n    A --> D(关键结果/Results: Competitive generation quality 具有竞争力的生成质量)"
    },
    {
      "title": "A Turn Toward Better Alignment: Few-Shot Generative Adaptation with Equivariant Feature Rotation",
      "authors": "Chenghao Xu, Qi Liu, Jiexi Yan, Muli Yang, Cheng Deng",
      "institution": "Hohai University, Xidian University, Institute for Infocomm Research (I2R), A*STAR",
      "link": "https://arxiv.org/pdf/2512.21174",
      "code": null,
      "tags": [
        "few-shot image generation",
        "domain adaptation",
        "feature rotation",
        "Lie Group",
        "equivariant space",
        "generative adversarial networks"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b06e02ddfdad8176f73c53632a928df39be42b51a926b7d00530f341c4b6945e_w640_q70.webp",
      "contributions": "1. Proposes Equivariant Feature Rotation (EFR), a novel adaptation strategy that aligns source and target domains within a self-rotated proxy feature space to bridge the domain gap. 2. Introduces adaptive rotations within a parameterized Lie Group to transform features into an equivariant space, preserving intra-domain structural information without distortion. 3. Demonstrates through comprehensive experiments that the method significantly enhances generative performance in the target domain compared to existing approaches.",
      "summary": "This paper addresses the challenge of few-shot image generation, where existing methods struggle to align source and target domains effectively due to strict or relaxed constraints. The authors propose Equivariant Feature Rotation (EFR), a method that uses learnable rotations in a Lie Group to map features into an equivariant proxy space for better alignment. Experiments show that EFR significantly improves generative performance in the target domain.",
      "mindmap": "graph LR\n    A[A Turn Toward Better Alignment: Few-Shot Generative Adaptation with Equivariant Feature Rotation] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[现有对齐方法效果不佳 / Existing alignment methods are ineffective]\n    C --> C1[提出等变特征旋转 / Propose Equivariant Feature Rotation]\n    C1 --> C2[在参数化李群中进行自适应旋转 / Adaptive rotations in parameterized Lie Group]\n    C2 --> C3[在代理空间中对齐 / Align in proxy space]\n    D --> D1[生成性能显著提升 / Generative performance significantly improved]"
    },
    {
      "title": "VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs",
      "authors": "Brigitta Malagurski Törtei, Yasser Dahou, Ngoc Dung Huynh, Wamiq Reyaz Para, Phúc H. Lê Khac, Ankit Singh, Sofian Chaybouti, Sanath Narayan",
      "institution": "Technology Innovation Institute, Tuebingen AI Center/University of Tuebingen",
      "link": "https://arxiv.org/pdf/2512.21194",
      "code": null,
      "tags": [
        "visual reasoning",
        "vision-language models",
        "benchmark",
        "perceptual reasoning",
        "compositional reasoning",
        "abstraction"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da4a0d93325d1e003056f13f48074769efe0198717d9c263282ebe51f3848352_w640_q70.webp",
      "contributions": "1. Introduces VisRes Bench, a new benchmark for evaluating visual reasoning in naturalistic settings without language supervision. 2. Proposes a structured evaluation across three levels of complexity (perceptual, single-attribute, multi-attribute) to isolate distinct reasoning abilities. 3. Reveals that state-of-the-art VLMs perform near random under subtle perceptual perturbations, exposing their limited abstraction beyond pattern recognition.",
      "summary": "The paper introduces VisRes Bench, a benchmark designed to evaluate the visual reasoning capabilities of Vision-Language Models (VLMs) without relying on linguistic cues. It tests models across three levels of increasing complexity—perceptual completion, rule-based inference, and compositional reasoning—using over 19,000 controlled images. The main finding is that current VLMs perform poorly, near random chance, under perceptual perturbations, indicating a lack of genuine abstract visual reasoning.",
      "mindmap": "graph LR\n        A[VisRes Bench] --> B{核心问题/Problem};\n        A --> C{主要方法/Method};\n        A --> D{关键结果/Results};\n        B --> B1[VLMs依赖语言先验/Linguistic Priors];\n        B --> B2[视觉推理能力不明/Unclear Visual Reasoning];\n        C --> C1[三级评估框架/3-Level Framework];\n        C1 --> C1_1[L1: 感知补全/Perceptual Completion];\n        C1 --> C1_2[L2: 单属性推理/Single-Attribute];\n        C1 --> C1_3[L3: 组合推理/Compositional];\n        D --> D1[性能接近随机/Near Random Performance];\n        D --> D2[抽象能力有限/Limited Abstraction];"
    },
    {
      "title": "Schrödinger's Navigator: Imagining an Ensemble of Futures for Zero-Shot Object Navigation",
      "authors": "Yu He, Da Huang, Zhenyang Liu, Zixiao Gu, Qiang Sun, Guangnan Ye, Yanwei Fu",
      "institution": "Fudan University, Shanghai Jiao Tong University, Shanghai University of International Business and Economics, Shanghai Innovation Institute",
      "link": "https://arxiv.org/pdf/2512.21201",
      "code": "https://heyu322.github.io/Schrodinger-Navigator.github.io/",
      "tags": [
        "robot navigation",
        "zero-shot object navigation",
        "trajectory-conditioned 3D imagination",
        "occlusion-aware planning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/34e56028ea40f6f3b4a9150683288695e8b7fd2724c676c4f7f56f07367b4fb3_w640_q70.webp",
      "contributions": "1. Proposed Schrödinger's Navigator, a novel navigation framework that models unobserved space as an ensemble of plausible future worlds to handle uncertainty. 2. Introduced a trajectory-conditioned 3D world model that imagines future observations along candidate paths to see beyond occlusions and anticipate risks. 3. Developed a method to fuse imagined 3D observations into a navigation map to update a value map, guiding the policy toward safer, less-occluded routes for better object tracking.",
      "summary": "The paper addresses the challenge of zero-shot object navigation in cluttered environments with occlusions and moving targets. It proposes Schrödinger's Navigator, a framework that samples candidate trajectories and uses a 3D imagination model to predict future observations, enabling the robot to plan safer paths and locate hidden objects. Experiments on a quadruped robot show the method outperforms baselines in success rate and localization in occlusion-heavy settings.",
      "mindmap": "graph LR\n        A[Schrödinger's Navigator] --> B[核心问题/Problem: ZSON struggles with occlusions & uncertainty]\n        A --> C[主要方法/Method: Trajectory-conditioned 3D imagination of futures]\n        A --> D[关键结果/Results: Outperforms baselines on real robot]"
    },
    {
      "title": "Latent Implicit Visual Reasoning",
      "authors": "Kelvin Li, Chuyi Shang, Leonid Karlinsky, Rogerio Feris, Trevor Darrell, Roei Herzig",
      "institution": "University of California, Berkeley, Xero, MIT-IBM Watson AI Lab",
      "link": "https://arxiv.org/pdf/2512.21218",
      "code": null,
      "tags": [
        "multimodal learning",
        "visual reasoning tokens",
        "task-agnostic",
        "implicit supervision"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42f15cd73a3268dae7d708e796f38593c4f7159611e5b226fceb4458a983b525_w640_q70.webp",
      "contributions": "1. Proposes a task-agnostic mechanism for training Large Multimodal Models (LMMs) to discover and use visual reasoning tokens without explicit supervision. 2. Enables models to adaptively re-encode images for a task, extracting relevant visual information without hand-crafted intermediate steps. 3. Achieves state-of-the-art results on diverse vision-centric tasks and generalizes well to multi-task instruction tuning.",
      "summary": "The paper addresses the limitation of text-centric Large Multimodal Models (LMMs) in handling vision-heavy reasoning tasks. It proposes a method that allows LMMs to learn and use visual reasoning tokens implicitly, without needing supervised intermediate visual steps. This approach outperforms direct fine-tuning and achieves state-of-the-art performance on a range of challenging visual reasoning tasks.",
      "mindmap": "graph LR\n    A[Latent Implicit Visual Reasoning] --> B[核心问题/Problem: LMMs are text-centric, struggle with visual reasoning tasks]\n    A --> C[主要方法/Method: Train LMMs to discover task-adaptive visual reasoning tokens without explicit supervision]\n    A --> D[关键结果/Results: Outperforms fine-tuning, SOTA on diverse vision tasks, generalizes well]"
    },
    {
      "title": "RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic",
      "authors": "Le Wang, Zonghao Ying, Xiao Yang, Quanchen Zou, Zhenfei Yin, Tianlin Li, Jian Yang, Yaodong Yang, Aishan Liu, Xianglong Liu",
      "institution": "Beihang University, Beijing University of Posts and Telecommunications, 360 AI Security Lab, The University of Sydney, Nanyang Technological University, Peking University",
      "link": "https://arxiv.org/pdf/2512.21220",
      "code": null,
      "tags": [
        "agent system",
        "runtime safety guardrail",
        "executable safety logic",
        "hybrid reasoning",
        "temporal safety predicate",
        "context-aware safety predicate"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adb68be6c6803ce76bf0036925bc1f629db0f2d1ae9be491b5ab0a450b37d1e5_w640_q70.webp",
      "contributions": "1. Proposes RoboSafe, a hybrid reasoning runtime safeguard for embodied agents using executable predicate-based safety logic. 2. Introduces a Backward Reflective Reasoning module to infer temporal safety predicates from recent trajectories and trigger replanning. 3. Introduces a Forward Predictive Reasoning module to anticipate risks by generating context-aware safety predicates from long-term memory and observations.",
      "summary": "The paper addresses the vulnerability of vision-language model-driven embodied agents to hazardous instructions in dynamic environments. It proposes RoboSafe, a runtime safety system that uses hybrid reasoning with backward reflection and forward prediction to generate executable safety logic. Experiments show RoboSafe significantly reduces hazardous actions while maintaining task performance, and its practicality is validated on physical robots.",
      "mindmap": "graph LR\n        A[RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic] --> B[核心问题/Problem: Embodied agents vulnerable to hazardous instructions in dynamic environments]\n        A --> C[主要方法/Method: Hybrid reasoning runtime safeguard with Backward Reflective & Forward Predictive modules]\n        A --> D[关键结果/Results: Reduces hazardous actions (-36.8%), maintains task performance, validated on physical robots]"
    },
    {
      "title": "Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval",
      "authors": "Dao Sy Duy Minh, Huynh Trung Kiet, Nguyen Lam Phu Quy, Phu-Hoa Pham, Tran Chi Nguyen",
      "institution": "University of Science - VNUHCM",
      "link": "https://arxiv.org/pdf/2512.21221",
      "code": "https://github.com/PhamPhuHoa-23/Event-Based-Image-Retrieval",
      "tags": [
        "image-text retrieval",
        "event-centric entity extraction",
        "BM25",
        "BEiT-3",
        "two-stage retrieval"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3fccaf1dfde41ddfe9c023d8a1f41a9f87c4a0899f25d8a475702d3f368a129_w640_q70.webp",
      "contributions": "1. Proposes a lightweight two-stage retrieval pipeline for event-based image retrieval. 2. Leverages event-centric entity extraction to incorporate temporal and contextual signals for efficient candidate filtering. 3. Combines BM25-based filtering with BEiT-3 reranking to achieve high accuracy on the OpenEvents benchmark.",
      "summary": "This paper addresses the challenge of retrieving images from natural language descriptions in complex, real-world scenarios. It proposes a two-stage method that first filters candidates using BM25 on extracted event entities, then reranks them with a BEiT-3 model. The approach significantly outperforms prior baselines on the OpenEvents benchmark, demonstrating the effectiveness of combining lightweight entity guidance with deep multimodal modeling.",
      "mindmap": "graph LR\n    A[Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval] --> B[核心问题/Problem: Real-world image-text retrieval is challenging due to vague queries and scalability needs.]\n    A --> C[主要方法/Method: Two-stage pipeline with event-centric entity extraction, BM25 filtering, and BEiT-3 reranking.]\n    A --> D[关键结果/Results: Achieves high mean average precision (0.559) on OpenEvents v1, outperforming baselines.]"
    },
    {
      "title": "Human Motion Estimation with Everyday Wearables",
      "authors": "Siqi Zhu, Yixuan Li, Junfu Li, Qi Wu, Zan Wang, Haozhe Ma, Wei Liang",
      "institution": "Beijing Institute of Technology, Yangtze Delta Region Academy of Beijing Institute of Technology, Shenzhen MSU-BIT University",
      "link": "https://arxiv.org/pdf/2512.21209",
      "code": "https://pie-lab.cn/EveryWear/",
      "tags": [
        "human motion capture",
        "multimodal learning",
        "teacher-student framework",
        "egocentric vision",
        "inertial measurement units (IMUs)",
        "sim-to-real gap"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ced793b8564fa6f3ddfc9f17a175ca19ac32fba9b281a74c4be9046c4484d995_w640_q70.webp",
      "contributions": "1. Proposes EveryWear, a lightweight and practical full-body motion capture system using only common consumer wearables (smartphone, smartwatch, earbuds, smart glasses) without requiring explicit calibration. 2. Introduces Ego-Elec, a large-scale, real-world dataset with 9 hours of data covering 56 daily activities in 17 environments, providing ground-truth 3D motion annotations. 3. Presents a multimodal teacher-student framework that fuses visual cues from egocentric cameras with inertial signals, trained directly on real-world data to eliminate the sim-to-real gap.",
      "summary": "The paper proposes EveryWear, a human motion estimation method that uses everyday wearables like smartphones and smartwatches in a multimodal teacher-student framework. It introduces a real-world dataset, Ego-Elec, for training and benchmarking. Experiments show the method outperforms baselines, offering a practical solution for full-body motion capture.",
      "mindmap": "graph LR\n    A[Human Motion Estimation with Everyday Wearables] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[现有方法穿戴性差、硬件昂贵、校准繁琐 / Poor wearability, expensive hardware, cumbersome calibration]\n    C --> C1[EveryWear: 基于日常可穿戴设备 / Everyday Wearables-based]\n    C --> C2[多模态师生框架 / Multimodal Teacher-Student Framework]\n    C --> C3[Ego-Elec 真实世界数据集 / Ego-Elec Real-World Dataset]\n    D --> D1[消除仿真到现实的差距 / Eliminates Sim-to-Real Gap]\n    D --> D2[性能超越基线模型 / Outperforms Baseline Models]"
    },
    {
      "title": "SegMo: Segment-aligned Text to 3D Human Motion Generation",
      "authors": "Bowen Dang, Lin Wu, Xiaohang Yang, Zheng Yuan, Zhixiang Chen",
      "institution": "University of Sheffield, University of Glasgow, Queen Mary University of London",
      "link": "https://arxiv.org/pdf/2512.21237",
      "code": null,
      "tags": [
        "human motion generation",
        "segment alignment",
        "contrastive learning",
        "text-to-motion",
        "fine-grained correspondence",
        "shared embedding space"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39d0fd9df6a0e35165dba62659aeda0ea42d69345d39516d1c7d95fc5895ac0a_w640_q70.webp",
      "contributions": "1. Proposes a novel segment-aligned framework (SegMo) for text-to-3D-motion generation, decomposing both text and motion into temporally ordered segments for fine-grained alignment. 2. Introduces a method for fine-grained text-motion alignment using contrastive learning to create a shared embedding space for text and motion segments. 3. Demonstrates improved performance on standard benchmarks and shows the framework's applicability to retrieval tasks like motion grounding and motion-to-text retrieval.",
      "summary": "This paper addresses the problem of coarse alignment in text-to-3D human motion generation by proposing SegMo, a framework that decomposes text and motion into segments and aligns them using contrastive learning. The method achieves improved performance on the HumanML3D dataset and enables retrieval-style applications. The results show that fine-grained segment alignment enhances the accuracy and realism of generated motions.",
      "mindmap": "graph LR\n    A[SegMo: Segment-aligned Text to 3D Human Motion Generation] --> B(核心问题/Problem: Coarse sequence-level text-motion alignment)\n    A --> C(主要方法/Method: Segment decomposition & fine-grained alignment via contrastive learning)\n    A --> D(关键结果/Results: Improved TOP1 score on HumanML3D; Enables retrieval tasks)"
    },
    {
      "title": "Improving the Convergence Rate of Ray Search Optimization for Query-Efficient Hard-Label Attacks",
      "authors": "Xinjie Xu, Shuyu Cheng, Dongwei Xu, Qi Xuan, Chen Ma",
      "institution": "Zhejiang University of Technology, Binjiang Institute of Artificial Intelligence, ZJUT, JQ Investments",
      "link": "https://arxiv.org/pdf/2512.21241",
      "code": "https://github.com/machanic/hard_label_attacks",
      "tags": [
        "adversarial attacks",
        "hard-label black-box attacks",
        "query efficiency",
        "ray search optimization",
        "Nesterov's Accelerated Gradient",
        "momentum-based optimization"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/302b574932ba227c13854e5c9c2cab3d7cf8f1ed29ee145fbc73062632304cd4_w640_q70.webp",
      "contributions": "1. Proposed ARS-OPT, a momentum-based algorithm inspired by Nesterov's Accelerated Gradient to accelerate the convergence of ray search in hard-label attacks. 2. Introduced PARS-OPT, which further enhances ARS-OPT by incorporating surrogate-model priors into the gradient estimation. 3. Provided theoretical convergence guarantees for the proposed methods and demonstrated superior query efficiency over 13 state-of-the-art approaches on ImageNet and CIFAR-10.",
      "summary": "This paper addresses the high query cost of hard-label black-box adversarial attacks by optimizing ray search. The authors propose ARS-OPT, a momentum-based algorithm, and its enhanced version PARS-OPT, which uses surrogate-model priors, to accelerate convergence. Experiments show the methods outperform 13 existing approaches in query efficiency on standard datasets.",
      "mindmap": "graph LR\n    A[Improving the Convergence Rate of Ray Search Optimization<br>改进射线搜索优化的收敛率] --> B[核心问题/Problem<br>Hard-label攻击查询成本高<br>High query cost in hard-label attacks]\n    A --> C[主要方法/Method<br>提出ARS-OPT & PARS-OPT<br>Propose ARS-OPT & PARS-OPT]\n    A --> D[关键结果/Results<br>超越13种SOTA方法<br>Outperforms 13 SOTA methods]"
    },
    {
      "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation",
      "authors": "Jiawei Liu, Junqiao Li, Jiangfan Deng, Gen Li, Siyu Zhou, Zetao Fang, Shanshan Lao, Zengde Deng, Jianing Zhu, Tingting Ma, Jiayi Li, Yunqiu Wang, Qian He, Xinglong Wu",
      "institution": "ByteDance",
      "link": "https://arxiv.org/pdf/2512.21252",
      "code": "https://dreamontage.github.io/DreaMontage",
      "tags": [
        "video generation",
        "Diffusion Transformers (DiT)",
        "intermediate-conditioning",
        "Segment-wise Auto-Regressive (SAR)",
        "Direct Preference Optimization (DPO)",
        "one-shot video"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b17202b644882c3a0dbe4c4001d4778edc5131e61435a474ca0e440fd66482a_w640_q70.webp",
      "contributions": "1. Introduced a lightweight intermediate-conditioning mechanism into the DiT architecture with an Adaptive Tuning strategy for robust arbitrary-frame control. 2. Developed a Visual Expression SFT stage and a Tailored DPO scheme to enhance visual fidelity, motion rationality, and transition smoothness. 3. Designed a memory-efficient Segment-wise Auto-Regressive (SAR) inference strategy for generating long-duration videos.",
      "summary": "This paper introduces DreaMontage, a framework for generating seamless, long-duration \"one-shot\" videos guided by arbitrary user-provided frames. It achieves this by integrating an intermediate-conditioning mechanism into a Diffusion Transformer, employing specialized fine-tuning and preference optimization, and using a segment-wise auto-regressive inference strategy. The method produces visually coherent and expressive one-shot videos efficiently.",
      "mindmap": "graph LR\n        A[DreaMontage] --> B(核心问题/Problem: 传统一镜到底成本高，现有视频生成方法拼接不连贯)\n        A --> C(主要方法/Method: 中间帧条件控制、视觉表达SFT与DPO、分段自回归推理)\n        A --> D(关键结果/Results: 生成视觉震撼、无缝连贯的长一镜到底视频)"
    },
    {
      "title": "ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision",
      "authors": "Weiqi Li, Zehao Zhang, Liang Lin, Guangrun Wang",
      "institution": "Sun Yat-sen University, Yonsei University",
      "link": "https://arxiv.org/pdf/2512.21268",
      "code": null,
      "tags": [
        "video generation",
        "attention supervision",
        "conditional control",
        "video diffusion",
        "layout controlnet",
        "3D-aware layout"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb3dd27adaccdaa06447db98898dc4dfc553a4184249c20523f8f6b4642036c3_w640_q70.webp",
      "contributions": "1. Proposes ACD (Attention-Conditional Diffusion), a novel framework for direct conditional control in video diffusion models via attention supervision. 2. Introduces a sparse 3D-aware object layout as an efficient conditioning signal and a dedicated Layout ControlNet. 3. Presents an automated annotation pipeline for scalable layout integration.",
      "summary": "The paper addresses the limited controllability in existing video diffusion models by proposing ACD, a framework that directly aligns the model's attention maps with external control signals. This method uses a novel sparse 3D-aware layout and a Layout ControlNet to achieve superior alignment with conditioning inputs while maintaining video quality. Experiments show ACD establishes an effective paradigm for conditional video synthesis.",
      "mindmap": "graph LR\n    A[ACD: Direct Conditional Control for Video Diffusion Models via Attention Supervision] --> B[核心问题/Problem: Limited Controllability in Video Synthesis]\n    A --> C[主要方法/Method: Attention Supervision & Sparse 3D-Aware Layout]\n    A --> D[关键结果/Results: Superior Alignment & Preserved Fidelity]"
    },
    {
      "title": "AnyAD: Unified Any-Modality Anomaly Detection in Incomplete Multi-Sequence MRI",
      "authors": "Changwei Wu, Yifei Chen, Yuxin Du, Mingxuan Liu, Jinying Zong, Beining Wu, Jie Dong, Feiwei Qin, Yunkang Cao, Qiyuan Tian",
      "institution": "Hangzhou Dianzi University, Tsinghua University, Hunan University",
      "link": "https://arxiv.org/pdf/2512.21264",
      "code": "https://github.com/wuchangw/AnyAD",
      "tags": [
        "medical image analysis",
        "anomaly detection",
        "missing modality",
        "feature alignment",
        "prototype learning",
        "multi-sequence MRI"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d759b100d62183be434837a07a310b265daec4b80f872064c9f08fd1e67db57_w640_q70.webp",
      "contributions": "1. A unified any-modality anomaly detection framework for incomplete multi-sequence MRI. 2. A dual-pathway encoder with feature distribution alignment to handle severe modality dropout. 3. An Intrinsic Normal Prototypes (INPs) extractor and guided decoder to reconstruct normal patterns and amplify anomalies.",
      "summary": "This paper proposes AnyAD, a unified framework for anomaly detection in brain MRI that works robustly with any available combination of imaging modalities, even when some are missing. The method uses a feature alignment mechanism and normal prototype-guided reconstruction to adapt to incomplete data without retraining. Experiments show it outperforms state-of-the-art methods across multiple datasets and modality combinations, establishing a scalable approach for real-world clinical use.",
      "mindmap": "graph LR\n    A[AnyAD: Unified Any-Modality Anomaly Detection] --> B[核心问题/Problem: 临床MRI模态缺失与异常样本稀缺]\n    A --> C[主要方法/Method: 特征分布对齐与正常原型引导重建]\n    A --> D[关键结果/Results: 在多种模态组合上超越SOTA方法]"
    },
    {
      "title": "GriDiT: Factorized Grid-Based Diffusion for Efficient Long Image Sequence Generation",
      "authors": "Snehal Singh Tomar, Alexandros Graikos, Arjun Krishna, Dimitris Samaras, Klaus Mueller",
      "institution": "Stony Brook University",
      "link": "https://arxiv.org/pdf/2512.21276",
      "code": null,
      "tags": [
        "video generation",
        "diffusion transformer",
        "factorized generation",
        "grid-based diffusion",
        "super-resolution",
        "long sequence"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b10d59f334dda58c40390950d7473806d1453d042361c2e6ae7ef6e5e1a680df_w640_q70.webp",
      "contributions": "1. Proposes a factorized generation approach for image sequences, separating coarse sequence generation at low resolution from individual frame refinement at high resolution. 2. Introduces a method to train a model on grid images of subsampled frames, effectively extending a 2D image generator to a 3D sequence generator without architectural changes. 3. Demonstrates superior synthesis quality, improved sequence coherence, high-fidelity generation of arbitrary-length sequences, and increased efficiency (at least 2x faster inference) across diverse datasets.",
      "summary": "This paper addresses inefficiencies in generating long image sequences by proposing GriDiT, a method that factorizes the process. It first generates a coarse, low-resolution sequence using a grid-based Diffusion Transformer, then super-resolves each frame individually. This approach achieves higher quality, better coherence, and at least twice the inference speed compared to state-of-the-art methods.",
      "mindmap": "graph LR\n        A[GriDiT: Factorized Grid-Based Diffusion] --> B[核心问题/Problem: Inefficient modeling of image sequences as large tensors]\n        A --> C[主要方法/Method: Factorized generation: low-res grid sequence + per-frame super-resolution]\n        A --> D[关键结果/Results: Superior quality, coherence, >2x faster inference]"
    },
    {
      "title": "Surgical Scene Segmentation using a Spike-Driven Video Transformer with Real-Time Potential",
      "authors": "Shihao Zou, Jingjing Li, Wei Ji, Jincai Huang, Kai Wang, Guo Dan, Weixin Si, Yi Pan",
      "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences; University of Alberta; Yale University; Southern University of Science and Technology; Nanfang Hospital Southern Medical University; Shenzhen University",
      "link": "https://arxiv.org/pdf/2512.21284",
      "code": null,
      "tags": [
        "surgical scene segmentation",
        "spiking neural networks",
        "video transformer",
        "masked autoencoding",
        "real-time inference",
        "surgical video"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67a3926b4197163c38fdd24a63b06867e674a7f4f039c4976e75df11c771f6b6_w640_q70.webp",
      "contributions": "1. Proposes SpikeSurgSeg, the first spike-driven video Transformer framework for surgical scene segmentation with real-time potential on non-GPU platforms. 2. Introduces a surgical-scene masked autoencoding pretraining strategy for SNNs using layer-wise tube masking to learn robust spatiotemporal representations from limited labeled data. 3. Designs a lightweight spike-driven segmentation head that maintains temporal consistency and the low-latency characteristics of SNNs.",
      "summary": "This paper addresses the challenge of real-time surgical scene segmentation in resource-constrained environments by proposing SpikeSurgSeg, a spike-driven video Transformer based on Spiking Neural Networks (SNNs). The method uses a masked autoencoding pretraining strategy and a lightweight segmentation head to achieve efficient inference. Experiments show it matches the accuracy of state-of-the-art ANN models while reducing latency by at least 8x and achieving over 20x acceleration compared to foundation models, demonstrating its potential for time-critical surgical applications.",
      "mindmap": "graph LR\n    A[SpikeSurgSeg<br>论文标题/Paper Title] --> B[问题: 手术场景分割需要高精度与低延迟，但现有模型计算开销大<br>Problem: Surgical scene segmentation requires high accuracy & low latency, but existing models are computationally expensive]\n    A --> C[方法: 提出首个基于脉冲神经网络的视频Transformer，使用掩码自编码预训练<br>Method: Proposes first spike-driven video Transformer with masked autoencoding pretraining]\n    A --> D[结果: 精度媲美SOTA ANN模型，推理延迟降低8倍以上<br>Results: Accuracy comparable to SOTA ANN models, inference latency reduced by >8x]"
    },
    {
      "title": "Post-Processing Mask-Based Table Segmentation for Structural Coordinate Extraction",
      "authors": "Suren Bandara",
      "institution": "University of Moratuwa",
      "link": "https://arxiv.org/pdf/2512.21287",
      "code": null,
      "tags": [
        "document image analysis",
        "table structure extraction",
        "mask-based segmentation",
        "Gaussian convolution",
        "signal processing",
        "Cell-Aware Segmentation Accuracy (CASA)"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4ec531bbcda4dfdb5e4b7b3bbe3d643d9a1e7bf829357622807bc23e628f0b58_w640_q70.webp",
      "contributions": "1. Proposes a novel multi-scale signal-processing method that models row/column transitions as 1D signals and uses Gaussian convolution with increasing variances for robust edge detection from imperfect table masks. 2. Introduces a statistical thresholding technique applied after convolution to suppress noise while preserving stable structural edges, improving boundary localization. 3. Demonstrates robustness to resolution variations through zero-padding and scaling strategies, significantly boosting layout-aware accuracy (CASA) on a standard benchmark.",
      "summary": "This paper addresses the challenge of accurately extracting table row and column boundaries from noisy or low-resolution document images. It proposes a post-processing method that converts table masks into 1D signals, applies multi-scale Gaussian convolution and statistical thresholding to detect edges, and maps peaks back to image coordinates. The approach improves Cell-Aware Segmentation Accuracy from 67% to 76% on PubLayNet-1M, showing robustness to image degradation and producing structured outputs suitable for downstream analysis.",
      "mindmap": "graph LR\n    A[Post-Processing Mask-Based Table Segmentation<br>基于掩码的后处理表格分割] --> B(核心问题/Problem)\n    A --> C(主要方法/Method)\n    A --> D(关键结果/Results)\n    B --> B1[表格边界检测不准确<br>Inaccurate table boundary detection]\n    B --> B2[噪声与低分辨率图像<br>Noisy & low-res images]\n    C --> C1[将掩码转为1D信号<br>Convert mask to 1D signal]\n    C --> C2[多尺度高斯卷积<br>Multi-scale Gaussian convolution]\n    C --> C3[统计阈值处理<br>Statistical thresholding]\n    D --> D1[CASA从67%提升至76%<br>CASA improved from 67% to 76%]\n    D --> D2[对分辨率变化鲁棒<br>Robust to resolution variations]"
    },
    {
      "title": "AndroidLens: Long-latency Evaluation with Nested Sub-targets for Android GUI Agents",
      "authors": "Yue Cao, Yingyao Wang, Pi Bu, Jingxuan Xing, Wei Jiang, Zekun Zhu, Junpeng Ma, Sashuai Zhou, Tong Lu, Jun Song, Yu Cheng, Yuning Jiang, Bo Zheng",
      "institution": "Alibaba Group, Nanjing University, Fudan University, Zhejiang University",
      "link": "https://arxiv.org/pdf/2512.21302",
      "code": "https://github.com/alibaba/AndroidLens",
      "tags": [
        "agent system",
        "GUI agents",
        "mobile automation",
        "long-latency tasks",
        "evaluation benchmark",
        "Average Task Progress (ATP)"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/662a645c6b32441afe19665c8ae67f9217e2983c70a42ab2e9430700bb7e5305_w640_q70.webp",
      "contributions": "1. Introduces AndroidLens, a challenging benchmark with 571 long-latency tasks across 38 real-world domains in Chinese and English environments. 2. Proposes a static evaluation that preserves real-world anomalies and allows multiple valid paths to reduce bias. 3. Designs a dynamic evaluation with a milestone-based scheme for fine-grained progress measurement via Average Task Progress (ATP).",
      "summary": "This paper introduces AndroidLens, a new evaluation framework designed to assess mobile GUI agents on complex, long-latency tasks derived from real-world scenarios. The framework features both static and dynamic evaluation methods, including a novel milestone-based progress metric. The evaluation results show that even state-of-the-art models perform poorly, achieving only a 12.7% success rate, highlighting significant challenges in real-world mobile automation.",
      "mindmap": "graph LR\n    A[AndroidLens] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[现有基准受限 / Existing Benchmarks Limited]\n    B1 --> B2[应用、任务、指标简单 / Simple Apps, Tasks, Metrics]\n    C --> C1[静态与动态评估 / Static & Dynamic Evaluation]\n    C1 --> C2[真实长延迟任务 / Real-world Long-latency Tasks]\n    C1 --> C3[里程碑与ATP / Milestone & ATP]\n    D --> D1[成功率低 / Low Success Rate (12.7%)]\n    D --> D2[ATP为50.47% / ATP is 50.47%]"
    },
    {
      "title": "Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks",
      "authors": "Roy Turgeman, Tom Tirer",
      "institution": "Bar-Ilan University",
      "link": "https://arxiv.org/pdf/2512.21315",
      "code": null,
      "tags": [
        "information theory",
        "statistical learning",
        "data processing inequality",
        "Bayes classifier",
        "low-level processing",
        "classification accuracy",
        "finite sample analysis"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59be928ca6eaa4e86b8cd3fb9b4f9e66ff3aed7a604e05c3f63b00a3ca0c56bd_w640_q70.webp",
      "contributions": "1. A theoretical proof that for any finite number of training samples, there exists a pre-classification processing that improves classification accuracy, even for a classifier converging to the optimal Bayes classifier. 2. An analysis of how class separation, training set size, and class balance affect the relative gain from low-level pre-processing. 3. Empirical validation of the theory on a synthetic setup and on practical deep classifiers with denoising/encoding tasks on benchmark datasets, showing consistent trends.",
      "summary": "This paper investigates the practical utility of performing low-level signal processing tasks (like denoising) before classification, which seemingly contradicts the information-theoretic Data Processing Inequality. Through a theoretical binary classification analysis and empirical studies, it demonstrates that for finite training samples, such pre-processing can improve accuracy, with the benefit influenced by dataset characteristics like size and noise level.",
      "mindmap": "graph LR\n    A[Does the Data Processing Inequality Reflect Practice?<br/>数据加工不等式反映实践吗?] --> B(核心问题/Problem: Low-level processing is common in practice but seems to contradict the Data Processing Inequality.<br/>实践中的低级处理似乎与数据加工不等式矛盾)\n    A --> C(主要方法/Method: Theoretical study of a binary classifier + empirical validation on deep networks.<br/>二元分类器的理论研究+深度网络的实证验证)\n    A --> D(关键结果/Results: For finite samples, pre-processing can improve accuracy; gain depends on dataset properties.<br/>对于有限样本，预处理可提高精度；收益取决于数据集属性)"
    },
    {
      "title": "TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning",
      "authors": "Varun Belagali, Saarthak Kapse, Pierre Marza, Srijan Das, Zilinghan Li, Sofiène Boutaj, Pushpak Pati, Srikar Yellapragada, Tarak Nath Nandi, Ravi K Madduri, Joel Saltz, Prateek Prasanna, Stergios Christodoulidis Maria Vakalopoulou, Dimitris Samaras",
      "institution": "Stony Brook University, MICS/CentraleSupélec/Université Paris-Saclay, UNC Charlotte, Argonne National Laboratory",
      "link": "https://arxiv.org/pdf/2512.21331",
      "code": "https://cvlab-stonybrook.github.io/TICON/",
      "tags": [
        "computational pathology",
        "tile contextualization",
        "transformer",
        "masked modeling",
        "whole slide image",
        "foundation model"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd2cc5f1b4233e238b13cc9ecf555914fae34743ffd9e8646b2f8300433d9479_w640_q70.webp",
      "contributions": "1. Introduces TICON, a unified transformer-based model to contextualize tile embeddings from any tile-level foundation model for computational pathology. 2. Uses a masked modeling objective during pretraining to simultaneously unify and enrich representations from diverse tile encoders. 3. Demonstrates that TICON-contextualized embeddings achieve state-of-the-art results on multiple tile-level and slide-level benchmarks and enables a highly data-efficient slide-level foundation model.",
      "summary": "The paper addresses the problem of interpreting small image tiles in computational pathology without their larger slide-level context. It proposes TICON, a transformer-based model that contextualizes embeddings from any tile-level foundation model using a masked modeling pretraining objective. The results show that TICON significantly improves performance across various tasks and enables a slide-level foundation model that outperforms prior work while using far less data.",
      "mindmap": "graph LR\n    A[TICON: Slide-Level Tile Contextualizer] --> B[核心问题/Problem: Tile embeddings lack slide-level context];\n    A --> C[主要方法/Method: Transformer-based contextualizer with masked modeling];\n    A --> D[关键结果/Results: SOTA on benchmarks, efficient slide-level foundation model];"
    },
    {
      "title": "Streaming Video Instruction Tuning",
      "authors": "Jiaer Xia, Peixian Chen, Mengdan Zhang, Xing Sun, Kaiyang Zhou",
      "institution": "Hong Kong Baptist University, Tencent Youtu Lab",
      "link": "https://arxiv.org/pdf/2512.21334",
      "code": "https://github.com/maifoundations/Streamo",
      "tags": [
        "video understanding",
        "streaming video",
        "instruction tuning",
        "real-time assistant",
        "temporal reasoning",
        "multimodal LLM"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/683a0c8f318f6ed415484df41515d10ec5ce6b48a1d38eba2ff127258aac07cd_w640_q70.webp",
      "contributions": "1. Proposes Streamo, a real-time streaming video LLM for general-purpose interactive assistance across diverse tasks. 2. Constructs Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. 3. Demonstrates strong performance in temporal reasoning and generalization across streaming benchmarks, bridging offline models and real-time assistants.",
      "summary": "This paper introduces Streamo, a real-time streaming video LLM trained on a large-scale instruction dataset (Streamo-Instruct-465K) to perform diverse tasks like narration and question answering on continuous video. The model shows strong temporal reasoning and generalization, bridging the gap between offline video models and real-time multimodal assistants.",
      "mindmap": "graph LR\n    A[Streaming Video Instruction Tuning] --> B[核心问题/Problem: Offline video models struggle with real-time, continuous video streams]\n    A --> C[主要方法/Method: Train Streamo LLM on Streamo-Instruct-465K dataset for unified streaming tasks]\n    A --> D[关键结果/Results: Streamo achieves strong temporal reasoning and bridges offline/real-time gap]"
    },
    {
      "title": "Fast SAM2 with Text-Driven Token Pruning",
      "authors": "Avilasha Mandal, Chaoning Zhang, Fachrina Dewi Puspitasari, Xudong Wang, Jiaquan Zhang, Caiyan Qin, Guoqing Wang, Yang Yang, Heng Tao Shen",
      "institution": "University of Electronic Science and Technology of China, Indian Institute of Technology Delhi, Harbin Institute of Technology (Shenzhen)",
      "link": "https://arxiv.org/pdf/2512.21333",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "token pruning",
        "video object segmentation",
        "vision transformer",
        "inference efficiency",
        "text guidance"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/135ec6d87d21a48705862639dd6d546443e4868af716ccfb3932e1ca4a9ba7d9_w640_q70.webp",
      "contributions": "1. A text-guided token pruning framework that reduces token density before temporal propagation in SAM2 without modifying the core architecture. 2. A lightweight routing mechanism for token ranking that integrates local visual context, semantic relevance from text, and uncertainty cues. 3. Demonstrates significant improvements in inference speed (up to 42.50% faster) and memory usage (37.41% lower) while maintaining competitive segmentation performance on video benchmarks.",
      "summary": "This paper addresses the high computational cost of the Segment Anything Model 2 (SAM2) for video segmentation by proposing a text-driven token pruning method. The method selectively removes less informative visual tokens before temporal processing using a routing mechanism that considers visual, textual, and uncertainty information. This achieves substantially faster inference and lower memory usage with minimal impact on segmentation accuracy.",
      "mindmap": "graph LR\n        A[Fast SAM2 with Text-Driven Token Pruning] --> B(核心问题/Problem: SAM2计算和内存成本高/High computational & memory cost)\n        A --> C(主要方法/Method: 文本引导的令牌剪枝/Text-guided token pruning)\n        A --> D(关键结果/Results: 推理更快，内存更低/Faster inference & lower memory)"
    },
    {
      "title": "Flow Gym",
      "authors": "Francesco Banelli, Antonio Terpin, Alan Bonomi, Raffaello D'Andrea",
      "institution": "ETH Zürich",
      "link": "https://arxiv.org/pdf/2512.20642",
      "code": "https://github.com/antonioterpin/flowgym",
      "tags": [
        "optical flow / particle image velocimetry",
        "flow-field quantification",
        "synthetic data generation",
        "JAX",
        "reinforcement learning environment",
        "benchmarking toolkit"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e5173f537ce94701f33cf868d525b0c8117477e440d08e96c43c778b59915a4_w640_q70.webp",
      "contributions": "1. Introduces Flow Gym, a unified toolkit for research and deployment of flow-field quantification methods, inspired by OpenAI Gym. 2. Provides a modular, stateless interface for testing, training, and deploying both learning-based and classical algorithms using a synthetic image generation engine (SynthPix). 3. Offers stable JAX re-implementations and integrations of existing algorithms for standardized benchmarking.",
      "summary": "The paper presents Flow Gym, a toolkit designed to standardize the development and evaluation of algorithms for quantifying flow fields from particle images. It provides a unified, modular interface inspired by reinforcement learning environments, enabling easy testing and training of methods using synthetic data. The main outcome is a framework that facilitates reproducible research and benchmarking in flow-field quantification.",
      "mindmap": "graph LR\n    A[Flow Gym] --> B[核心问题/Problem: 流场量化算法缺乏标准化测试框架/Lack of standardized framework for flow-field quantification algorithms];\n    A --> C[主要方法/Method: 提供受RL启发的统一接口与合成数据引擎/Provides RL-inspired unified interface & synthetic data engine];\n    A --> D[关键结果/Results: 用于算法开发与基准测试的JAX兼容工具包/JAX-compatible toolkit for algorithm dev & benchmarking];"
    },
    {
      "title": "Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models",
      "authors": "Li-Zhong Szu-Tu, Ting-Lin Wu, Chia-Jui Chang, He Syu, Yu-Lun Liu",
      "institution": "National Yang Ming Chiao Tung University",
      "link": "https://arxiv.org/pdf/2512.21337",
      "code": "https://sytwu.github.io/BeyondMemo/",
      "tags": [
        "vision-language models",
        "popularity bias",
        "ordinal regression",
        "multi-modal benchmark"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6c6407a1e5e9ce1a54aebfb3d9bee052114a5e56feb5e9df1910a7031677d3c6_w640_q70.webp",
      "contributions": "1. Introduces YearGuessr, the largest open benchmark dataset for building age estimation with multi-modal attributes (55,546 images, construction years, GPS, page-view counts). 2. Frames construction year prediction as an ordinal regression task and proposes popularity-aware interval accuracy metrics to quantify bias. 3. Benchmarks 30+ models, including YearCLIP, exposing significant popularity bias in VLMs where accuracy is up to 34% higher on famous vs. ordinary buildings.",
      "summary": "This paper exposes a popularity bias in vision-language models (VLMs), where models perform much better on famous buildings than ordinary ones, indicating reliance on memorization rather than generalizable understanding. To study this, the authors introduce YearGuessr, a large multi-modal benchmark dataset for building age prediction framed as ordinal regression, and propose metrics to measure bias. Their evaluation confirms VLMs excel on popular items but struggle with unrecognized subjects, revealing a critical flaw in reasoning.",
      "mindmap": "graph LR\n    A[Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models] --> B[核心问题/Problem: VLMs show popularity bias, relying on memorization over understanding]\n    A --> C[主要方法/Method: Introduce YearGuessr dataset and popularity-aware ordinal regression metrics]\n    A --> D[关键结果/Results: VLMs perform up to 34% better on famous buildings, exposing reasoning flaws]"
    },
    {
      "title": "HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming",
      "authors": "Haonan Qiu, Shikun Liu, Zijian Zhou, Zhaochong An, Weiming Ren, Zhiheng Liu, Jonas Schult, Sen He, Shoufa Chen, Yuren Cong, Tao Xiang, Ziwei Liu, Juan-Manuel Perez-Rua",
      "institution": "Meta AI, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.21338",
      "code": "http://haonanqiu.com/projects/HiStream.html",
      "tags": [
        "diffusion models",
        "video generation",
        "inference acceleration",
        "redundancy elimination",
        "autoregressive framework",
        "feature caching"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fdfd3105f76a9173c10ae0b43fda34a955ef34708de3f700fa0fc7c67d1f3ab3_w640_q70.webp",
      "contributions": "1. Introduces a three-axis redundancy elimination framework (spatial, temporal, timestep) for efficient high-resolution video generation. 2. Proposes a dual-resolution caching mechanism for spatial compression and a chunk-by-chunk strategy with a fixed-size anchor cache for temporal compression. 3. Demonstrates state-of-the-art visual quality with up to 107.5x faster denoising compared to baselines, making 1080p video generation practical.",
      "summary": "The paper addresses the computational bottleneck of high-resolution video generation by proposing HiStream, an autoregressive framework that eliminates redundancy across spatial, temporal, and timestep dimensions. The method achieves significant speedups (up to 107.5x) with negligible quality loss, making high-fidelity 1080p video generation scalable and practical.",
      "mindmap": "graph LR\n    A[HiStream: Efficient High-Resolution Video Generation] --> B[核心问题/Problem: Quadratic complexity makes high-res video generation infeasible]\n    A --> C[主要方法/Method: Redundancy elimination via Spatial, Temporal, Timestep Compression]\n    A --> D[关键结果/Results: Up to 107.5x faster denoising with SOTA quality]"
    },
    {
      "title": "Equivariant Multiscale Learned Invertible Reconstruction for Cone Beam CT: From Simulated to Real Data",
      "authors": "Nikita Moriakov, Efstratios Gavves, Jonathan H. Mason, Carmen Seller-Oria, Jonas Teuwen, Jan-Jakob Sonke",
      "institution": "Netherlands Cancer Institute, University of Amsterdam, Elekta Limited",
      "link": "https://arxiv.org/pdf/2512.21180",
      "code": null,
      "tags": [
        "medical image reconstruction",
        "rotational equivariance",
        "multiscale reconstruction",
        "learned invertible primal-dual",
        "quasi-Monte Carlo simulation",
        "cone beam CT"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80fc637f4006372b13c2de48295a289562b18e569dfcb797d3ad0d9157751aa8_w640_q70.webp",
      "contributions": "1. Proposed LIRE++, an end-to-end rotationally-equivariant multiscale learned invertible primal-dual scheme for fast and memory-efficient CBCT reconstruction. 2. Developed a fast quasi-Monte Carlo CBCT projection simulator to generate training data. 3. Demonstrated improved performance on both synthetic and real clinical data compared to deep learning baselines and a state-of-the-art proprietary hybrid method.",
      "summary": "This paper addresses the challenge of low image quality in Cone Beam CT (CBCT) by proposing LIRE++, a deep learning-based reconstruction method. LIRE++ incorporates rotational equivariance and multiscale processing for efficiency and was trained using a custom fast simulator. It shows improved reconstruction quality over existing methods on both simulated and real clinical data.",
      "mindmap": "graph LR\n    A[Equivariant Multiscale Learned Invertible Reconstruction for CBCT] --> B(核心问题/Problem: CBCT图像质量低, 缺乏真实数据, 内存限制/CBCT has lower image quality, lacks ground truth, has memory constraints)\n    A --> C(主要方法/Method: LIRE++: 旋转等变多尺度可逆对偶学习方案, 使用快速模拟器训练/LIRE++: rotationally-equivariant multiscale learned invertible scheme, trained with fast simulator)\n    A --> D(关键结果/Results: 合成数据PSNR提升1dB, 真实数据MAE降低10HU/1 dB PSNR gain on synthetic data, 10 HU MAE reduction on real data)"
    },
    {
      "title": "PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility",
      "authors": "Md Nahid Hasan Shuvo, Moinul Hossain",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19711",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d516fcc6c3de6b0e65c078e5e3f3dda23bfd77fbb9c5f4abfe2c509c2cb6dfe_w640_q70.webp",
      "contributions": "",
      "summary": "PHANTOM: PHysical ANamorphic Threats Obstructing Connected Vehicle Mobility",
      "mindmap": ""
    },
    {
      "title": "Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems",
      "authors": "Xiangzhong Luo, Weichen Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19731",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10d82314834148af959284b60a6d6f0fa9e36928106648626f8c43d4f07b79_w640_q70.webp",
      "contributions": "",
      "summary": "Exploring Deep-to-Shallow Transformable Neural Networks for Intelligent Embedded Systems",
      "mindmap": ""
    },
    {
      "title": "Learning to Refocus with Video Diffusion Models",
      "authors": "SaiKiran Tedla, Zhoutong Zhang, Xuaner Zhang, Shumian Xin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19823",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d67b80f0c15466284688038780928e07f307c7269e0df0ded0424d3e770fcd6_w640_q70.webp",
      "contributions": "",
      "summary": "Learning to Refocus with Video Diffusion Models",
      "mindmap": ""
    },
    {
      "title": "RANSAC Scoring Functions: Analysis and Reality Check",
      "authors": "A. Shekhovtsov",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19850",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dd50e25656a023b3c2995d13c741fc721d03038e35f7503eb116497b3cb8637d_w640_q70.webp",
      "contributions": "",
      "summary": "RANSAC Scoring Functions: Analysis and Reality Check",
      "mindmap": ""
    },
    {
      "title": "Generating the Past, Present and Future from a Motion-Blurred Image",
      "authors": "SaiKiran Tedla, Kelly Zhu, Trevor Canham, Felix Taubner, Michael S. Brown, Kiriakos N. Kutulakos, David B. Lindell",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19817",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d34df9bd29ac26646aab435f8fb3761d7ba6f54b4c9919286b04b5436dba1d0_w640_q70.webp",
      "contributions": "",
      "summary": "Generating the Past, Present and Future from a Motion-Blurred Image",
      "mindmap": ""
    },
    {
      "title": "HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction",
      "authors": "Jong Wook Kim, Wonseok Roh, Ha Dam Baek, Pilhyeon Lee, Jonghyun Choi, Sangpil Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19871",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a4679f5f60c2af6c77a6712d0c27272bc4719706cb56235956b7caf4c1bac0f_w640_q70.webp",
      "contributions": "",
      "summary": "HyGE-Occ: Hybrid View-Transformation with 3D Gaussian and Edge Priors for 3D Panoptic Occupancy Prediction",
      "mindmap": ""
    },
    {
      "title": "Unified Brain Surface and Volume Registration",
      "authors": "S. Mazdak Abulnaga, Andrew Hoopes, Malte Hoffmann, Robin Magnet, Maks Ovsjanikov, Lilla Zöllei, John Guttag, Bruce Fischl, Adrian Dalca",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19928",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52472b7c53844ab7af247ff699ee1c659d2b3ff27e2e21ee8f187677824a5d8d_w640_q70.webp",
      "contributions": "",
      "summary": "Unified Brain Surface and Volume Registration",
      "mindmap": ""
    },
    {
      "title": "Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs",
      "authors": "Houston H. Zhang, Tao Zhang, Baoze Lin, Yuanqi Xue, Yincheng Zhu, Huan Liu, Li Gu, Linfeng Ye, Ziqiang Wang, Xinxin Zuo, Yang Wang, Yuanhao Yu, Zhixiang Chi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19918",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/afcd12646a75911cc37486f5af7f55491eaf6f54718182f47a36695579c55660_w640_q70.webp",
      "contributions": "",
      "summary": "Widget2Code: From Visual Widgets to UI Code via Multimodal LLMs",
      "mindmap": ""
    },
    {
      "title": "Vehicle-centric Perception via Multimodal Structured Pre-training",
      "authors": "Wentao Wu, Xiao Wang, Chenglong Li, Jin Tang, Bin Luo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19934",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25393280cf5e09ecc25c375a88de26bef6b6904fd90a6775cf7591ed8a615fe1_w640_q70.webp",
      "contributions": "",
      "summary": "Vehicle-centric Perception via Multimodal Structured Pre-training",
      "mindmap": ""
    },
    {
      "title": "Block-Recurrent Dynamics in Vision Transformers",
      "authors": "Mozes Jacobs, Thomas Fel, Richard Hakim, Alessandra Brondetta, Demba Ba, T. Andy Keller",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19941",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e10f9b4ab6d210902b2c5092ebfe1fcc82dd7a3d2032bfc97fbfadd16867c2a_w640_q70.webp",
      "contributions": "",
      "summary": "Block-Recurrent Dynamics in Vision Transformers",
      "mindmap": ""
    },
    {
      "title": "SE360: Semantic Edit in 360$^$ Panoramas via Hierarchical Data Construction",
      "authors": "Haoyi Zhong, Fang-Lue Zhang, Andrew Chalmers, Taehyun Rhee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19943",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5acefee064893ec36480803c8a614ffcd70f5e22f389b691a22dff4da224abf7_w640_q70.webp",
      "contributions": "",
      "summary": "SE360: Semantic Edit in 360$^\\circ$ Panoramas via Hierarchical Data Construction",
      "mindmap": ""
    },
    {
      "title": "HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes",
      "authors": "Yuechen Yang, Junlin Guo, Yanfan Zhu, Jialin Yue, Junchao Zhu, Yu Wang, Shilin Zhao, Haichun Yang, Xingyi Guo, Jovan Tanevski, Laura Barisoni, Avi Z. Rosenberg, Yuankai Huo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19954",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebf1e5c1a2595dc81a69957123729d9aa0fa024a71bdfda164ce0ae6bf95d110_w640_q70.webp",
      "contributions": "",
      "summary": "HistoWAS: A Pathomics Framework for Large-Scale Feature-Wide Association Studies of Tissue Topology and Patient Outcomes",
      "mindmap": ""
    },
    {
      "title": "How Much 3D Do Video Foundation Models Encode?",
      "authors": "Zixuan Huang, Xiang Li, Zhaoyang Lv, James M. Rehg",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19949",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b54fab192e555a908a0f7daf8d7992c85cd3241844d6a17c19d685c99c93dc5e_w640_q70.webp",
      "contributions": "",
      "summary": "How Much 3D Do Video Foundation Models Encode?",
      "mindmap": ""
    },
    {
      "title": "A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping",
      "authors": "Peng Gao, Ke Li, Di Wang, Yongshan Zhu, Yiming Zhang, Xuemei Luo, Yifeng Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19990",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1691a202602f3bcf2d0e97787c25b8551ee2510f815f2c351ebacb3b3a37953_w640_q70.webp",
      "contributions": "",
      "summary": "A Dual-Branch Local-Global Framework for Cross-Resolution Land Cover Mapping",
      "mindmap": ""
    },
    {
      "title": "WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification",
      "authors": "Le Feng, Li Xiao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19982",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fecc5e0134d3112ae90b5c8ac61ae68bcd52b0697a6c32184a7fc10d72e0640_w640_q70.webp",
      "contributions": "",
      "summary": "WSD-MIL: Window Scale Decay Multiple Instance Learning for Whole Slide Image Classification",
      "mindmap": ""
    },
    {
      "title": "A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection",
      "authors": "Tamim Ahasan Rijon, Yeasin Arafath",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19989",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e7415dab1086d45b2bb6f7a49ab33be753b3da047de23310de552ead99dc2448_w640_q70.webp",
      "contributions": "",
      "summary": "A Novel CNN Gradient Boosting Ensemble for Guava Disease Detection",
      "mindmap": ""
    },
    {
      "title": "Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models",
      "authors": "Zhenhao Li, Shaohan Yi, Zheng Liu, Leonartinus Gao, Minh Ngoc Le, Ambrose Ling, Zhuoran Wang, Md Amirul Islam, Zhixiang Chi, Yuanhao Yu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20000",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7f2d07a108f93995c65cf13d740ca889c8e7dd494a7ef3c9222caba5e6ccf3c3_w640_q70.webp",
      "contributions": "",
      "summary": "Few-Shot-Based Modular Image-to-Video Adapter for Diffusion Models",
      "mindmap": ""
    },
    {
      "title": "PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification",
      "authors": "Blessing Agyei Kyem, Joshua Kofi Asamoah, Anthony Dontoh, Andrews Danyo, Eugene Denteh, Armstrong Aboah",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20011",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff4db8075e196efc52322813cedc23193239565c930612aec052f3acbb95eab5_w640_q70.webp",
      "contributions": "",
      "summary": "PaveSync: A Unified and Comprehensive Dataset for Pavement Distress Analysis and Classification",
      "mindmap": ""
    },
    {
      "title": "SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images",
      "authors": "Zepeng Xin, Kaiyu Li, Luodi Chen, Wanchen Li, Yuchen Xiao, Hui Qiao, Weizhan Zhang, Deyu Meng, Xiangyong Cao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20013",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24d12d8c96876ce1d14987d2507601d9688f72e9f86e06abfd01bc397241cbd6_w640_q70.webp",
      "contributions": "",
      "summary": "SegEarth-R2: Towards Comprehensive Language-guided Segmentation for Remote Sensing Images",
      "mindmap": ""
    },
    {
      "title": "VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement",
      "authors": "Chang Sun, Dongliang Xie, Bo Qin, Hong Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20032",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/800e8f7c8c22b74ebd9c804082b6c7f5a5c4998365cdbe511167991351910a98_w640_q70.webp",
      "contributions": "",
      "summary": "VALLR-Pin: Dual-Decoding Visual Speech Recognition for Mandarin with Pinyin-Guided LLM Refinement",
      "mindmap": ""
    },
    {
      "title": "A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments",
      "authors": "Anthony Dontoh, Stephanie Ivey, Armstrong Aboah",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20025",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8e8d7557e6f332097ab10b8c45895c9a3f2b19aa186bcd6c3071ac6e02ee8e7_w640_q70.webp",
      "contributions": "",
      "summary": "A Contextual Analysis of Driver-Facing and Dual-View Video Inputs for Distraction Detection in Naturalistic Driving Environments",
      "mindmap": ""
    },
    {
      "title": "$H^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning",
      "authors": "Lin Li, Jiahui Li, Jiaming Lei, Jun Xiao, Feifei Shao, Long Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20029",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1aa2e8e4d0ae946b7e977e841084f034f6f83372776d08e5a2bf43a04e81003_w640_q70.webp",
      "contributions": "",
      "summary": "$\\text\\{H\\}^2$em: Learning Hierarchical Hyperbolic Embeddings for Compositional Zero-Shot Learning",
      "mindmap": ""
    },
    {
      "title": "MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis",
      "authors": "Ziwei Qin, Xuhui Song, Deqing Huang, Na Qin, Jun Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20026",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7dc5f8098baa15ab99d6ec49af6ea9b561b6b787a33ac1ec6e365ad910668048_w640_q70.webp",
      "contributions": "",
      "summary": "MAPI-GNN: Multi-Activation Plane Interaction Graph Neural Network for Multimodal Medical Diagnosis",
      "mindmap": ""
    },
    {
      "title": "Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva",
      "authors": "Nguyen Lam Phu Quy, Pham Phu Hoa, Tran Chi Nguyen, Dao Sy Duy Minh, Nguyen Hoang Minh Ngoc, Huynh Trung Kiet",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20042",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fb136c3c44d734ccd03ee7608dfc92d3612b466bfffc82e1d2efdfd79e5f161_w640_q70.webp",
      "contributions": "",
      "summary": "Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieva",
      "mindmap": ""
    },
    {
      "title": "FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs",
      "authors": "Andreas Zinonos, Michał Stypułkowski, Antoni Bigata, Stavros Petridis, Maja Pantic, Nikita Drobyshev",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20033",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24426a84ed65236012992da21d9b902f793ee45db54516610ca6203c50494625_w640_q70.webp",
      "contributions": "",
      "summary": "FlashLips: 100-FPS Mask-Free Latent Lip-Sync using Reconstruction Instead of Diffusion or GANs",
      "mindmap": ""
    },
    {
      "title": "Progressive Learned Image Compression for Machine Perception",
      "authors": "Jungwoo Kim, Jun-Hyuk Kim, Jong-Seok Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20070",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24993bfda343219d771658c8c9767048aec44f5b6784dcfb07bb075ba4304dd1_w640_q70.webp",
      "contributions": "",
      "summary": "Progressive Learned Image Compression for Machine Perception",
      "mindmap": ""
    },
    {
      "title": "Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach",
      "authors": "Hao Li, Fabian Deuser, Wenping Yin, Steffen Knoblauch, Wufan Zhao, Filip Biljecki, Yong Xue, Wei Huang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20056",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c37110ad3319d1434c17c04ae94a4dcfa94a278faf51360e359e1a063ce32e7_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Generative Location Awareness for Disaster Response: A Probabilistic Cross-view Geolocalization Approach",
      "mindmap": ""
    },
    {
      "title": "LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs",
      "authors": "Haiyun Wei, Fan Lu, Yunwei Zhu, Zehan Zheng, Weiyi Xue, Lin Shao, Xudong Zhang, Ya Wu, Rong Fu, Guang Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20105",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a2dee7bb51f34affab3884be56b1bfb46b76c6a66e40400ed09830c5aabd0b0_w640_q70.webp",
      "contributions": "",
      "summary": "LiDARDraft: Generating LiDAR Point Cloud from Versatile Inputs",
      "mindmap": ""
    },
    {
      "title": "Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models",
      "authors": "Subrata Kumer Paula, Dewan Nafiul Islam Noora, Rakhi Rani Paula, Md. Ekramul Hamidb, Fahmid Al Faridc, Hezerul Abdul Karimd, Md. Maruf Al Hossain Princee, Abu Saleh Musa Miahb",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20104",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e8b67a33af7843f8ee6ec84eb87891b7d12b57e4af5fa1f77bce5e76a6b92b0d_w640_q70.webp",
      "contributions": "",
      "summary": "Effect of Activation Function and Model Optimizer on the Performance of Human Activity Recognition System Using Various Deep Learning Models",
      "mindmap": ""
    },
    {
      "title": "Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts",
      "authors": "Jinyoung Choi, Youngchae Kwon, Injung Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20088",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1946d7c3329c37db07556d963544aad7dbfeda194c515e4debdc6e3754d8920_w640_q70.webp",
      "contributions": "",
      "summary": "Item Region-based Style Classification Network (IRSN): A Fashion Style Classifier Based on Domain Knowledge of Fashion Experts",
      "mindmap": ""
    },
    {
      "title": "Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection",
      "authors": "Alireza Moayedikia, Sattar Dorafshan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20113",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/00b50254f658c253763f9df9bd57707873bdb0982863020f26c263961aa323d8_w640_q70.webp",
      "contributions": "",
      "summary": "Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection",
      "mindmap": ""
    },
    {
      "title": "milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion",
      "authors": "Niraj Prakash Kini, Shiau-Rung Tsai, Guan-Hsun Lin, Wen-Hsiao Peng, Ching-Wen Ma, Jenq-Neng Hwang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20128",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b40cd83392a8ac698c9e282aef2bd70809bcd4957965aa0c36506eb68c77ec30_w640_q70.webp",
      "contributions": "",
      "summary": "milliMamba: Specular-Aware Human Pose Estimation via Dual mmWave Radar with Multi-Frame Mamba Fusion",
      "mindmap": ""
    },
    {
      "title": "HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer",
      "authors": "Mohammad Helal Uddin, Liam Seymour, Sabur Baidya",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20120",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f965376a2586790dd27548936a060ee2437b3c5085b58331952dfddc1265b29f_w640_q70.webp",
      "contributions": "",
      "summary": "HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer",
      "mindmap": ""
    },
    {
      "title": "Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs",
      "authors": "Cyrus Vachha, Yixiao Kang, Zach Dive, Ashwat Chidambaram, Anik Gupta, Eunice Jun, Bjoern Hartmann",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20129",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8400f1033a93635b0a5b706c568585f557f04cf8533a2b77ce1c55f08e14bef6_w640_q70.webp",
      "contributions": "",
      "summary": "Dreamcrafter: Immersive Editing of 3D Radiance Fields Through Flexible, Generative Inputs and Outputs",
      "mindmap": ""
    },
    {
      "title": "DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation",
      "authors": "Jingqi Tian, Yiheng Du, Haoji Zhang, Yuji Wang, Isaac Ning Lee, Xulong Bai, Tianrui Zhu, Jingxuan Niu, Yansong Tang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20117",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a290a5343ed508cf106c79abdb63608b4b4b69f2e0d42fc218565b45b8eb64f_w640_q70.webp",
      "contributions": "",
      "summary": "DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation",
      "mindmap": ""
    },
    {
      "title": "UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis",
      "authors": "Thanh-Tung Le, Tuan Pham, Tung Nguyen, Deying Kong, Xiaohui Xie, Stephan Mandt",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20107",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/668ef8c168df89e0d5190bbd1ac0a2d6de0affc034e4a50d8e050242fd138dd8_w640_q70.webp",
      "contributions": "",
      "summary": "UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis",
      "mindmap": ""
    },
    {
      "title": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models",
      "authors": "Xiang Chen, Yixin Ou, Quan Feng, Lei Li, Piji Li, Haibo Ye, Sheng-Jun Huang, Shuofei Qiao, Shumin Deng, Huajun Chen, Ningyu Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20145",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028ab8e7171d7f497cbdc48e136d419e8642bf876111786382aee29b15d0992_w640_q70.webp",
      "contributions": "",
      "summary": "Retrieval-augmented Prompt Learning for Pre-trained Foundation Models",
      "mindmap": ""
    },
    {
      "title": "Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)",
      "authors": "Robert van de Ven, Trim Bresilla, Bram Nelissen, Ard Nieuwenhuizen, Eldert J. van Henten, Gert Kootstra",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20148",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0476ec59995b5f3259a9b2ab1456d67ddbbb750902c5acd3d684600b97c47598_w640_q70.webp",
      "contributions": "",
      "summary": "Enhancing annotations for 5D apple pose estimation through 3D Gaussian Splatting (3DGS)",
      "mindmap": ""
    },
    {
      "title": "CoDi -- an exemplar-conditioned diffusion model for low-shot counting",
      "authors": "Grega Šuštar, Jer Pelhan, Alan Lukežič, Matej Kristan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20153",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/19ee3eb6d09129b7fef5f1398fe3a279f203f7ff1a53a416574b63094eeefdf5_w640_q70.webp",
      "contributions": "",
      "summary": "CoDi -- an exemplar-conditioned diffusion model for low-shot counting",
      "mindmap": ""
    },
    {
      "title": "AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model",
      "authors": "Sofian Chaybouti, Sanath Narayan, Yasser Dahou, Phúc H. Lê Khac, Ankit Singh, Ngoc Dung Huynh, Wamiq Reyaz Para, Hilde Kuehne, Hakim Hacid",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20157",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/61612d4352aae9f7b2e5272f0843ecd4da94205a817c26dac3655aa0b24e73b2_w640_q70.webp",
      "contributions": "",
      "summary": "AMoE: Agglomerative Mixture-of-Experts Vision Foundation Model",
      "mindmap": ""
    },
    {
      "title": "Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark",
      "authors": "Hao Guo, Xugong Qin, Jun Jie Ou Yang, Peng Zhang, Gangyan Zeng, Yubo Li, Hailun Lin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20174",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a74295652803d99c4b0631ef38e9684d12032ddd9797f217033f356497ff647_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark",
      "mindmap": ""
    },
    {
      "title": "Generative Latent Coding for Ultra-Low Bitrate Image Compression",
      "authors": "Zhaoyang Jia, Jiahao Li, Bin Li, Houqiang Li, Yan Lu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20194",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e95ca206803e3acc79be8a04fea7c616d1d6d5dc840a97528ce68341f589c143_w640_q70.webp",
      "contributions": "",
      "summary": "Generative Latent Coding for Ultra-Low Bitrate Image Compression",
      "mindmap": ""
    },
    {
      "title": "LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation",
      "authors": "Xiangxuan Ren, Zhongdao Wang, Pin Tang, Guoqing Wang, Jilai Zheng, Chao Ma",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20217",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8799680440a14b4dded747c223ca374b82c834b527822c9fff101d2247c1d65d_w640_q70.webp",
      "contributions": "",
      "summary": "LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation",
      "mindmap": ""
    },
    {
      "title": "JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement",
      "authors": "Tao Ye, Hongbin Ren, Chongbing Zhang, Haoran Chen, Xiaosong Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20213",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c290a68b3d97be1a375bfedfbc89e4ad87f03c4cfd59b51b99adeb151186d5ed_w640_q70.webp",
      "contributions": "",
      "summary": "JDPNet: A Network Based on Joint Degradation Processing for Underwater Image Enhancement",
      "mindmap": ""
    },
    {
      "title": "How I Met Your Bias: Investigating Bias Amplification in Diffusion Models",
      "authors": "Nathan Roos, Ekaterina Iakovleva, Ani Gjergji, Vito Paolo Pastore, Enzo Tartaglione",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20233",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f3c43be0c18a78b4304c027f45372c35a663531d4f9536a15c2b4ca0dbb155b2_w640_q70.webp",
      "contributions": "",
      "summary": "How I Met Your Bias: Investigating Bias Amplification in Diffusion Models",
      "mindmap": ""
    },
    {
      "title": "Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion",
      "authors": "Xuanyu Hu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20249",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1c6a732f9a0082b695f01f79b2bcc47f909daa0f9d50a6af4d86a633213b328_w640_q70.webp",
      "contributions": "",
      "summary": "Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion",
      "mindmap": ""
    },
    {
      "title": "IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing",
      "authors": "Oikantik Nath, Sahithi Kukkala, Mitesh Khapra, Ravi Kiran Sarvadevabhatla",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20236",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/027b9426177f33cb782197baba41e405dda8dcde86fe03cd16387612d6f69294_w640_q70.webp",
      "contributions": "",
      "summary": "IndicDLP: A Foundational Dataset for Multi-Lingual and Multi-Domain Document Layout Parsing",
      "mindmap": ""
    },
    {
      "title": "BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation",
      "authors": "Jinghao Shi, Jianing Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20255",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2b3920e5370f3c041a4e84894da06ffa2a4f371a30e7ead753e72b679159943_w640_q70.webp",
      "contributions": "",
      "summary": "BiCoR-Seg: Bidirectional Co-Refinement Framework for High-Resolution Remote Sensing Image Segmentation",
      "mindmap": ""
    },
    {
      "title": "LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation",
      "authors": "Daniele Cardullo, Simone Teglia, Irene Amerini",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20257",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77044e7e1eb0cbde388f554efc56df1d6faee9e6b4ec8be9ca9c2664befbf208_w640_q70.webp",
      "contributions": "",
      "summary": "LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation",
      "mindmap": ""
    },
    {
      "title": "Degradation-Aware Metric Prompting for Hyperspectral Image Restoration",
      "authors": "Binfeng Wang, Di Wang, Haonan Guo, Ying Fu, Jing Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20251",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4f8150acede0f77c8c88a8f6399592fa53d83830252a9609aa77972be050f28_w640_q70.webp",
      "contributions": "",
      "summary": "Degradation-Aware Metric Prompting for Hyperspectral Image Restoration",
      "mindmap": ""
    },
    {
      "title": "$\\{D\\}^\\{3\\}$\\{ETOR\\}: $\\{D\\}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive $\\{D\\}$ebiasing for Weakly-Supervised Camouflaged Object $\\{D\\}$etection with Scribble Annotations",
      "authors": "Jiawei Ge, Jiuxin Cao, Xinyi Li, Xuelin Zhu, Chang Liu, Bo Liu, Chen Feng, Ioannis Patras",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20260",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ee0606d3fe6e6ae30bbf9417baa7948dab878cfcc6b70e36031424c107aafb81_w640_q70.webp",
      "contributions": "",
      "summary": "$\\{D\\}^\\{3\\}$\\{ETOR\\}: $\\{D\\}$ebate-Enhanced Pseudo Labeling and Frequency-Aware Progressive $\\{D\\}$ebiasing for Weakly-Supervised Camouflaged Object $\\{D\\}$etection with Scribble Annotations",
      "mindmap": ""
    },
    {
      "title": "UbiQVision: Quantifying Uncertainty in XAI for Image Recognition",
      "authors": "Akshat Dubey, Aleksandar Anžel, Bahar İlgen, Georges Hattab",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20288",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6106b87e48429449f0b11c2765bdedb47797d7822e4b38ad3a9fe7f94b92dacb_w640_q70.webp",
      "contributions": "",
      "summary": "UbiQVision: Quantifying Uncertainty in XAI for Image Recognition",
      "mindmap": ""
    },
    {
      "title": "TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation",
      "authors": "Ji-Hoon Kim, Junseok Ahn, Doyeop Kwak, Joon Son Chung, Shinji Watanabe",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20296",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/217e6d95bfc0e38be8f5a7356d3e869c3b3a5b9d4daf05b6435c3c756df247c6_w640_q70.webp",
      "contributions": "",
      "summary": "TAVID: Text-Driven Audio-Visual Interactive Dialogue Generation",
      "mindmap": ""
    },
    {
      "title": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System",
      "authors": "Zhongyu Xia, Wenhao Chen, Yongtao Wang, Ming-Hsuan Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20299",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0961ae49fbc925ad5eacb6602aeec6cfdfabae07b252a044cd181bdb5a47746_w640_q70.webp",
      "contributions": "",
      "summary": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System",
      "mindmap": ""
    },
    {
      "title": "The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection",
      "authors": "Qingdong He, Xueqin Chen, Yanjie Pan, Peng Tang, Pengcheng Xu, Zhenye Gan, Chengjie Wang, Xiaobin Hu, Jiangning Zhang, Yabiao Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20340",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce852f1d570d91487912f5d0abf0a93c49ad00eeecfdd840d9dbb9025ff8fd3a_w640_q70.webp",
      "contributions": "",
      "summary": "The devil is in the details: Enhancing Video Virtual Try-On via Keyframe-Driven Details Injection",
      "mindmap": ""
    },
    {
      "title": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation",
      "authors": "V. Kovalev, A. Kuvshinov, A. Buzovkin, D. Pokidov, D. Timonin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20362",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9180090a8d8d701e63ea635b11cc6a39fed9f252167568980aefb8786a66ef43_w640_q70.webp",
      "contributions": "",
      "summary": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation",
      "mindmap": ""
    },
    {
      "title": "Field-Space Attention for Structure-Preserving Earth System Transformers",
      "authors": "Maximilian Witte, Johannes Meuer, Étienne Plésiat, Christopher Kadow",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20350",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8623c14b77fd771294e1b936a58143cff6a715df1b4a5026fe2d59708383e6e2_w640_q70.webp",
      "contributions": "",
      "summary": "Field-Space Attention for Structure-Preserving Earth System Transformers",
      "mindmap": ""
    },
    {
      "title": "Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge",
      "authors": "Marta Moscati, Ahmed Abdullah, Muhammad Saad Saeed, Shah Nawaz, Rohan Kumar Das, Muhammad Zaigham Zaheer, Junaid Mir, Muhammad Haroon Yousaf, Khalid Mahmood Malik, Markus Schedl",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20376",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/483debde320d1a401b91478496a4b53ca3f74d52718353845af8598a16bf6167_w640_q70.webp",
      "contributions": "",
      "summary": "Linking Faces and Voices Across Languages: Insights from the FAME 2026 Challenge",
      "mindmap": ""
    },
    {
      "title": "SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images",
      "authors": "Linfei Li, Lin Zhang, Zhong Wang, Ying Shen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20377",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06dc77546a31f168b83f87c8efbfe002590b29ab252385b482db477a74d615ac_w640_q70.webp",
      "contributions": "",
      "summary": "SmartSplat: Feature-Smart Gaussians for Scalable Compression of Ultra-High-Resolution Images",
      "mindmap": ""
    },
    {
      "title": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems",
      "authors": "YuChe Hsu, AnJui Wang, TsaiChing Ni, YuanFu Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20387",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13d9aa5293bf4ca8e839b122277f1484ffb43b6211d54741d7eb7f58312f5509_w640_q70.webp",
      "contributions": "",
      "summary": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems",
      "mindmap": ""
    },
    {
      "title": "Chain-of-Anomaly Thoughts with Large Vision-Language Models",
      "authors": "Pedro Domingos, João Pereira, Vasco Lopes, João Neves, David Semedo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20417",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e26e3aba92227ff2a24c227033ffa15e8e191cf437a0a97c819748f4dfb7c94_w640_q70.webp",
      "contributions": "",
      "summary": "Chain-of-Anomaly Thoughts with Large Vision-Language Models",
      "mindmap": ""
    },
    {
      "title": "Simplifying Multi-Task Architectures Through Task-Specific Normalization",
      "authors": "Mihai Suteu, Ovidiu Serban",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20420",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8d24e6af533166dd44855079b97d7233c65878aa61e02267e65f4e306467d04b_w640_q70.webp",
      "contributions": "",
      "summary": "Simplifying Multi-Task Architectures Through Task-Specific Normalization",
      "mindmap": ""
    },
    {
      "title": "DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning",
      "authors": "Junho Yoon, Jaemo Jung, Hyunju Kim, Dongman Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20409",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d4c944d1aa99e38d69c83c388503646f62271c8bb649aafcafb57ad0ba7c7d0_w640_q70.webp",
      "contributions": "",
      "summary": "DETACH : Decomposed Spatio-Temporal Alignment for Exocentric Video and Ambient Sensors with Staged Learning",
      "mindmap": ""
    },
    {
      "title": "Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks",
      "authors": "Abdullah Al Shafi, Abdul Muntakim, Pintu Chandra Shill, Rowzatul Zannat, Abdullah Al-Amin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20431",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/905e502b77110f2fdb7bcfd863c906242998a2915d794e39c3da2377a2743392_w640_q70.webp",
      "contributions": "",
      "summary": "Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks",
      "mindmap": ""
    },
    {
      "title": "High Dimensional Data Decomposition for Anomaly Detection of Textured Images",
      "authors": "Ji Song, Xing Wang, Jianguo Wu, Xiaowei Yue",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20432",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1df48b800de53f9fff1712283209aa73147125a9725200492a8bcc4cd35e7182_w640_q70.webp",
      "contributions": "",
      "summary": "High Dimensional Data Decomposition for Anomaly Detection of Textured Images",
      "mindmap": ""
    },
    {
      "title": "Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding",
      "authors": "Anh Dao, Manh Tran, Yufei Zhang, Xiaoming Liu, Zijun Cui",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20451",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab892d1698a54ca5bea9ec4d990ec556b33cc4b339ac925ef5d18dba7b5dbae_w640_q70.webp",
      "contributions": "",
      "summary": "Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding",
      "mindmap": ""
    },
    {
      "title": "Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems",
      "authors": "James E. Gallagher, Edward J. Oughton, Jana Kosecka",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20487",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09ac139242f7c2d01b6739f6aa8ae3ee3439e687ec6c7b420e17bccf33e9de40_w640_q70.webp",
      "contributions": "",
      "summary": "Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems",
      "mindmap": ""
    },
    {
      "title": "SirenPose: Dynamic Scene Reconstruction via Geometric Supervision",
      "authors": "Kaitong Cai, Jensen Zhang, Jing Yang, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20531",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbf03f3fbe032434f49cece08cea9e779ea7ea57fa26c8f00eeed2e7c9080766_w640_q70.webp",
      "contributions": "",
      "summary": "SirenPose: Dynamic Scene Reconstruction via Geometric Supervision",
      "mindmap": ""
    },
    {
      "title": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
      "authors": "Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20557",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e481c21d437a92edaec37fae6af73e02495508b9597f95d16d784b230f3165c_w640_q70.webp",
      "contributions": "",
      "summary": "Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models",
      "mindmap": ""
    },
    {
      "title": "Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition",
      "authors": "Gorjan Radevski",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20501",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e4fc93af34bdcb98669d1a3c900d03900cb8c1359b89368e444c1c70848cdd4_w640_q70.webp",
      "contributions": "",
      "summary": "Bridging Modalities and Transferring Knowledge: Enhanced Multimodal Understanding and Recognition",
      "mindmap": ""
    },
    {
      "title": "UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images",
      "authors": "Yiming Zhao, Yuanpeng Gao, Yuxuan Luo, Jiwei Duan, Shisong Lin, Longfei Xiong, Zhouhui Lian",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20479",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4093d97a2621866450d2ab0e47bbdaa622a6fe81e981843cdd087e11e4301902_w640_q70.webp",
      "contributions": "",
      "summary": "UTDesign: A Unified Framework for Stylized Text Editing and Generation in Graphic Design Images",
      "mindmap": ""
    },
    {
      "title": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving",
      "authors": "Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20563",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5f62c50da026de5eec286e01930af394650fb8b9c309ff48cd8f733ee9ca220b_w640_q70.webp",
      "contributions": "",
      "summary": "LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving",
      "mindmap": ""
    },
    {
      "title": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models",
      "authors": "Kaitong Cai, Jusheng Zhang, Jing Yang, Yijia Fan, Pengtao Xie, Jian Wang, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20561",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/72707d221b008970aee7befd580ee702951e338a32b995af62d9c893fe16e7e1_w640_q70.webp",
      "contributions": "",
      "summary": "FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models",
      "mindmap": ""
    },
    {
      "title": "AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment",
      "authors": "Anna Šárová Mikeštíková, Médéric Fourmy, Martin Cífka, Josef Sivic, Vladimir Petrik",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20538",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6f6450c33e31646a32cf9129dad4fbbcc84ead81c36e23e134b1b0f9930b9db8_w640_q70.webp",
      "contributions": "",
      "summary": "AlignPose: Generalizable 6D Pose Estimation via Multi-view Feature-metric Alignment",
      "mindmap": ""
    },
    {
      "title": "Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios",
      "authors": "Mingwei Tang, Jiahao Nie, Guang Yang, Ziqing Cui, Jie Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20556",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7eb3ac652519f5f42206d482e71edb24ef458336500aad066aa021c53b988cca_w640_q70.webp",
      "contributions": "",
      "summary": "Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios",
      "mindmap": ""
    },
    {
      "title": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs",
      "authors": "Dhruv Anand, Ehsan Shareghi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20595",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb95f031dbfb3f7bfc348a6d3e77d5c3ae7e2408f06dd57529b77764de0ce0b7_w640_q70.webp",
      "contributions": "",
      "summary": "Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs",
      "mindmap": ""
    },
    {
      "title": "FedPOD: the deployable units of training for federated learning",
      "authors": "Daewoon Kim, Si Young Yie, Jae Sung Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20610",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e8e1f095bc0447c82283e16e3fb09acd3ae3773107b9096b3a06a1659a978e0_w640_q70.webp",
      "contributions": "",
      "summary": "FedPOD: the deployable units of training for federated learning",
      "mindmap": ""
    },
    {
      "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
      "authors": "Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20618",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e38fba460d45fac945020bc323269f04211978c3e2e544d86072d5b062f40958_w640_q70.webp",
      "contributions": "",
      "summary": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
      "mindmap": ""
    },
    {
      "title": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
      "authors": "Xuanhua He, Tianyu Yang, Ke Cao, Ruiqi Wu, Cheng Meng, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Qifeng Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20615",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/507ac9f976cb24dc0dc72fb35cca194ceb4a25829fd305c6f45493a8a9531c76_w640_q70.webp",
      "contributions": "",
      "summary": "Active Intelligence in Video Avatars via Closed-loop World Modeling",
      "mindmap": ""
    },
    {
      "title": "Repurposing Video Diffusion Transformers for Robust Point Tracking",
      "authors": "Soowon Son, Honggyu An, Chaehyun Kim, Hyunah Ko, Jisu Nam, Dahyun Chung, Siyoon Jin, Jung Yi, Jaewon Min, Junhwa Hur, Seungryong Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20606",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/415d9c5c4a502667ccd87f6d4f24a9276a6e9f5e7f729b850e01ea33b4b6681a_w640_q70.webp",
      "contributions": "",
      "summary": "Repurposing Video Diffusion Transformers for Robust Point Tracking",
      "mindmap": ""
    },
    {
      "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
      "authors": "Yuxi Xiao, Longfei Li, Shen Yan, Xinhang Liu, Sida Peng, Yunchao Wei, Xiaowei Zhou, Bingyi Kang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20617",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c18a98475f6303ed360b86b5c0ec7ec6b4ab088ad0e70f925606628c827b46d_w640_q70.webp",
      "contributions": "",
      "summary": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
      "mindmap": ""
    },
    {
      "title": "SemanticGen: Video Generation in Semantic Space",
      "authors": "Jianhong Bai, Xiaoshi Wu, Xintao Wang, Fu Xiao, Yuanxing Zhang, Qinghe Wang, Xiaoyu Shi, Menghan Xia, Zuozhu Liu, Haoji Hu, Pengfei Wan, Kun Gai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20619",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77e0d53ce289ecacefa061dc5d124f7b98c80f5b9417f83e06a4ccaafb10e8a8_w640_q70.webp",
      "contributions": "",
      "summary": "SemanticGen: Video Generation in Semantic Space",
      "mindmap": ""
    },
    {
      "title": "SAM Audio: Segment Anything in Audio",
      "authors": "Bowen Shi, Andros Tjandra, John Hoffman, Helin Wang, Yi-Chiao Wu, Luya Gao, Julius Richter, Matt Le, Apoorv Vyas, Sanyuan Chen, Christoph Feichtenhofer, Piotr Dollár, Wei-Ning Hsu, Ann Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18099",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4568ea657a5ec8dc7d7944a03205d941e2bc9d054f9586938d2caa09e0b272e6_w640_q70.webp",
      "contributions": "",
      "summary": "SAM Audio: Segment Anything in Audio",
      "mindmap": ""
    },
    {
      "title": "Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI",
      "authors": "Muhammad Usman, Azka Rehman, Muhammad Mutti Ur Rehman, Abd Ur Rehman, Muhammad Umar Farooq",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20436",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efdfcdd8483dd2617be0701188cb1e86708fc680b1516caff77f5075ed2baf49_w640_q70.webp",
      "contributions": "",
      "summary": "Dual-Encoder Transformer-Based Multimodal Learning for Ischemic Stroke Lesion Segmentation Using Diffusion MRI",
      "mindmap": ""
    },
    {
      "title": "Snapshot 3D image projection using a diffractive decoder",
      "authors": "Cagatay Isil, Alexander Chen, Yuhang Li, F. Onuralp Ardic, Shiqi Chen, Che-Yung Shen, Aydogan Ozcan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20464",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2f05e75cf0d45d4f4a6b73b6c248e120e8372e756e8b025e6f78b8ce6098b183_w640_q70.webp",
      "contributions": "",
      "summary": "Snapshot 3D image projection using a diffractive decoder",
      "mindmap": ""
    },
    {
      "title": "CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images",
      "authors": "Yujia Fu, Zhiyu Dong, Tianwen Qian, Chenye Zheng, Danian Ji, Linhai Zhuo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20374",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b125441b1639c4b3867045b3843d2e46f162ef46f6b24b77dee1c01e02ade5a8_w640_q70.webp",
      "contributions": "",
      "summary": "CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images",
      "mindmap": ""
    },
    {
      "title": "A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes",
      "authors": "Yuncheng Lu, Yucen Shi, Aobo Li, Zehao Li, Junying Li, Bo Wang, Tony Tae-Hyoung Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17939",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/962df2fe1f424af4979b55e1a459771077ddad5484459248d7cbdb7f3d246823_w640_q70.webp",
      "contributions": "",
      "summary": "A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes",
      "mindmap": ""
    },
    {
      "title": "NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction",
      "authors": "Karthik Prabhakar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17943",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a630b7d9810838c6059574b3dae2d2f3fcc9c79699030e8640202f2dbcd2d4b0_w640_q70.webp",
      "contributions": "",
      "summary": "NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction",
      "mindmap": ""
    },
    {
      "title": "SuperFlow: Training Flow Matching Models with RL on the Fly",
      "authors": "Kaijie Chen, Zhiyang Xu, Ying Shen, Zihao Lin, Yuguang Yao, Lifu Huang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17951",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/948003a9b36edb891b1eea74cedf1aaab2c086c912b2c476bae0259abbca38e3_w640_q70.webp",
      "contributions": "",
      "summary": "SuperFlow: Training Flow Matching Models with RL on the Fly",
      "mindmap": ""
    },
    {
      "title": "SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries",
      "authors": "Bin Wang, Fadi Dornaika",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17954",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bf3a9a797cfd9bf487a467cfbf96c2912048bb9780e4ea911d8505de4a3fd09_w640_q70.webp",
      "contributions": "",
      "summary": "SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries",
      "mindmap": ""
    },
    {
      "title": "Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition",
      "authors": "Ellie Zhou, Jihoon Chung, Olga Russakovsky",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17953",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8fc5229281981de8f8ce6b4446b9b416a3bf03a5b7dfe015f7327ed14da6c6c_w640_q70.webp",
      "contributions": "",
      "summary": "Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition",
      "mindmap": ""
    },
    {
      "title": "A Modular Framework for Single-View 3D Reconstruction of Indoor Environments",
      "authors": "Yuxiao Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17955",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/984f7066ad7f63ea4f496d0d617a7a83be0b5004c759374d9228f95333891ba4_w640_q70.webp",
      "contributions": "",
      "summary": "A Modular Framework for Single-View 3D Reconstruction of Indoor Environments",
      "mindmap": ""
    },
    {
      "title": "Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models",
      "authors": "Shubham Kumar Nigam, Parjanya Aditya Shukla, Noel Shallum, Arnab Bhattacharya",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18004",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2ffa85f9a5ddd1bd5f673a211deae55a7bad9087838680e7b143e6daac52df8_w640_q70.webp",
      "contributions": "",
      "summary": "Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models",
      "mindmap": ""
    },
    {
      "title": "Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization",
      "authors": "Omar Faruq Shikdar, Fahad Ahammed, B. M. Shahria Alam, Golam Kibria, Tawhidur Rahman, Nishat Tasnim Niloy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17987",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e0e026b2918baadec02fb27d5fb57e2257968a98d72147f8159070f8c9fbb7d1_w640_q70.webp",
      "contributions": "",
      "summary": "Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization",
      "mindmap": ""
    },
    {
      "title": "Robotic VLA Benefits from Joint Learning with Motion Image Diffusion",
      "authors": "Yu Fang, Kanchana Ranasinghe, Le Xue, Honglu Zhou, Juntao Tan, Ran Xu, Shelby Heinecke, Caiming Xiong, Silvio Savarese, Daniel Szafir, Mingyu Ding, Michael S. Ryoo, Juan Carlos Niebles",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18007",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b59654686bbf5b51e6cb8f755af962b88b4d3f2a65f9c991f5f10b64604828_w640_q70.webp",
      "contributions": "",
      "summary": "Robotic VLA Benefits from Joint Learning with Motion Image Diffusion",
      "mindmap": ""
    },
    {
      "title": "Name That Part: 3D Part Segmentation and Naming",
      "authors": "Soumava Paul, Prakhar Kaushik, Ankit Vaidya, Anand Bhattad, Alan Yuille",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18003",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7489c4c88b9369f0e55ac4eb9e395d18b5597618ea2266650f451ea4d5f3eec_w640_q70.webp",
      "contributions": "",
      "summary": "Name That Part: 3D Part Segmentation and Naming",
      "mindmap": ""
    },
    {
      "title": "Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation",
      "authors": "Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18028",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/30065ffa9a2b8c3aa56f4dd49b67477e394ef33e769980a2e7968c85edfa3346_w640_q70.webp",
      "contributions": "",
      "summary": "Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation",
      "mindmap": ""
    },
    {
      "title": "NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging",
      "authors": "Fakrul Islam Tushar, Ehsan Samei, Cynthia Rudin, Joseph Y. Lo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18038",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7823e2e22739636a655f10a5159ed96eedc679b68020943f440020f0658dad87_w640_q70.webp",
      "contributions": "",
      "summary": "NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging",
      "mindmap": ""
    },
    {
      "title": "FOODER: Real-time Facial Authentication and Expression Recognition",
      "authors": "Sabri Mustafa Kahya, Muhammet Sami Yavuz, Boran Hamdi Sivrikaya, Eckehard Steinbach",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18057",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17048a3dede1c165a0f52aab61de33bef5559518cf2996ad61858f9a9d680457_w640_q70.webp",
      "contributions": "",
      "summary": "FOODER: Real-time Facial Authentication and Expression Recognition",
      "mindmap": ""
    },
    {
      "title": "YolovN-CBi: A Lightweight and Efficient Architecture for Real-Time Detection of Small UAVs",
      "authors": "Ami Pandat, Punna Rajasekhar, Gopika Vinod, Rohit Shukla",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18046",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74e5d6d01e4d7b6683268a6dfd04d4c061677c1530a7ce95eaaf2ce9e55a0600_w640_q70.webp",
      "contributions": "",
      "summary": "YolovN-CBi: A Lightweight and Efficient Architecture for Real-Time Detection of Small UAVs",
      "mindmap": ""
    },
    {
      "title": "FPBench: A Comprehensive Benchmark of Multimodal Large Language Models for Fingerprint Analysis",
      "authors": "Ekta Balkrishna Gavas, Sudipta Banerjee, Chinmay Hegde, Nasir Memon",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18073",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c6335692d3a738711a3704dedcaa3eaa60dbe8b762e225fb2857789d12a428e0_w640_q70.webp",
      "contributions": "",
      "summary": "FPBench: A Comprehensive Benchmark of Multimodal Large Language Models for Fingerprint Analysis",
      "mindmap": ""
    },
    {
      "title": "Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation",
      "authors": "Shreshth Rajan, Raymond Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18082",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6dd5ca9b561b8ad179709036acde1f313672cfa0c00a271b9235f8ab0e640d0a_w640_q70.webp",
      "contributions": "",
      "summary": "Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation",
      "mindmap": ""
    },
    {
      "title": "Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown",
      "authors": "Changxu Duan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18115",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13ec88807faf2814dad94d8489b36894b8aa8c290f9026ca2108a6c17ac22ca6_w640_q70.webp",
      "contributions": "",
      "summary": "Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown",
      "mindmap": ""
    },
    {
      "title": "SERA-H: Beyond Native Sentinel Spatial Limits for High-Resolution Canopy Height Mapping",
      "authors": "Thomas Boudras, Martin Schwartz, Rasmus Fensholt, Martin Brandt, Ibrahim Fayad, Jean-Pierre Wigneron, Gabriel Belouze, Fajwel Fogel, Philippe Ciais",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18128",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40cedf819eb6eb6cd85f9bb6838fd612711be404b26144f5bf2363883a2eac28_w640_q70.webp",
      "contributions": "",
      "summary": "SERA-H: Beyond Native Sentinel Spatial Limits for High-Resolution Canopy Height Mapping",
      "mindmap": ""
    },
    {
      "title": "EndoStreamDepth: Temporally Consistent Monocular Depth Estimation for Endoscopic Video Streams",
      "authors": "Hao Li, Daiwei Lu, Jiacheng Wang, Robert J. Webster III, Ipek Oguz",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18159",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c6bfdccbbe4e580860cba1ef6a6436d8fc986b7e46d121db741672f2f3a233_w640_q70.webp",
      "contributions": "",
      "summary": "EndoStreamDepth: Temporally Consistent Monocular Depth Estimation for Endoscopic Video Streams",
      "mindmap": ""
    },
    {
      "title": "NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI",
      "authors": "Midhat Urooj, Ayan Banerjee, Sandeep Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18177",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7ef165e056ae631599bd024eb4341c57c1705258598a82662ae1166302f2947_w640_q70.webp",
      "contributions": "",
      "summary": "NEURO-GUARD: Neuro-Symbolic Generalization and Unbiased Adaptive Routing for Diagnostics -- Explainable Medical AI",
      "mindmap": ""
    },
    {
      "title": "Local Patches Meet Global Context: Scalable 3D Diffusion Priors for Computed Tomography Reconstruction",
      "authors": "Taewon Yang, Jason Hu, Jeffrey A. Fessler, Liyue Shen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18161",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6f2d7690da449f60ab28b9f1c8644a2829a779454fd531b2852837f43afe4bb_w640_q70.webp",
      "contributions": "",
      "summary": "Local Patches Meet Global Context: Scalable 3D Diffusion Priors for Computed Tomography Reconstruction",
      "mindmap": ""
    },
    {
      "title": "Atlas is Your Perfect Context: One-Shot Customization for Generalizable Foundational Medical Image Segmentation",
      "authors": "Ziyu Zhang, Yi Yu, Simeng Zhu, Ahmed Aly, Yunhe Gao, Ning Gu, Yuan Xue",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18176",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/011e5fe0a450b1348324ad1d5a24fbfca2367916d6e12d05481fe9aeccee423d_w640_q70.webp",
      "contributions": "",
      "summary": "Atlas is Your Perfect Context: One-Shot Customization for Generalizable Foundational Medical Image Segmentation",
      "mindmap": ""
    },
    {
      "title": "Is There a Better Source Distribution than Gaussian? Exploring Source Distributions for Image Flow Matching",
      "authors": "Junho Lee, Kwanseok Kim, Joonseok Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18184",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4568908c3f989abda31d63ece857e924f12b9a87e165bc3d982eeccfbbad2aa2_w640_q70.webp",
      "contributions": "",
      "summary": "Is There a Better Source Distribution than Gaussian? Exploring Source Distributions for Image Flow Matching",
      "mindmap": ""
    },
    {
      "title": "ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection",
      "authors": "Janghyun Baek, Mincheol Chang, Seokha Moon, Seung Joon Lee, Jinkyu Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18187",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1281826184eb88510283760e8d18262afdfcf1143f3ba9a7c22b684fb4f4489_w640_q70.webp",
      "contributions": "",
      "summary": "ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection",
      "mindmap": ""
    },
    {
      "title": "MACE-Dance: Motion-Appearance Cascaded Experts for Music-Driven Dance Video Generation",
      "authors": "Kaixing Yang, Jiashu Zhu, Xulong Tang, Ziqiao Peng, Xiangyue Zhang, Puwei Wang, Jiahong Wu, Xiangxiang Chu, Hongyan Liu, Jun He",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18181",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f91052deb821e034255a2a5fe3873c583d9fcbf1ff48249b8457b8bc7b44e261_w640_q70.webp",
      "contributions": "",
      "summary": "MACE-Dance: Motion-Appearance Cascaded Experts for Music-Driven Dance Video Generation",
      "mindmap": ""
    },
    {
      "title": "Multi-Part Object Representations via Graph Structures and Co-Part Discovery",
      "authors": "Alex Foo, Wynne Hsu, Mong Li Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18192",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/691a99ce976b35cbbbe1a4ae36131ee76e0a4332b25068ac660d1ddb59eb7132_w640_q70.webp",
      "contributions": "",
      "summary": "Multi-Part Object Representations via Graph Structures and Co-Part Discovery",
      "mindmap": ""
    },
    {
      "title": "Unsupervised Anomaly Detection with an Enhanced Teacher for Student-Teacher Feature Pyramid Matching",
      "authors": "Mohammad Zolfaghari, Hedieh Sajedi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18219",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/49c3ce25aa0d46d7576b119913f2eb1d74bf6b2d970566f7b3c4a55bb19063d9_w640_q70.webp",
      "contributions": "",
      "summary": "Unsupervised Anomaly Detection with an Enhanced Teacher for Student-Teacher Feature Pyramid Matching",
      "mindmap": ""
    },
    {
      "title": "Stable and Efficient Single-Rollout RL for Multimodal Reasoning",
      "authors": "Rui Liu, Dian Yu, Lei Ke, Haolin Liu, Yujun Zhou, Zhenwen Liang, Haitao Mi, Pratap Tokekar, Dong Yu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18215",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/81e83852cf5de78a73c53dc039a80d4328420c326e71217ed656de7348457003_w640_q70.webp",
      "contributions": "",
      "summary": "Stable and Efficient Single-Rollout RL for Multimodal Reasoning",
      "mindmap": ""
    },
    {
      "title": "Investigating Spatial Attention Bias in Vision-Language Models",
      "authors": "Aryan Chaudhary, Sanchit Goyal, Pratik Narang, Dhruv Kumar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18231",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be5cd89ea4d004fbb18410473c51ae236387bc088e028d1ddd84dbce2d703bfc_w640_q70.webp",
      "contributions": "",
      "summary": "Investigating Spatial Attention Bias in Vision-Language Models",
      "mindmap": ""
    },
    {
      "title": "Multifaceted Exploration of Spatial Openness in Rental Housing: A Big Data Analysis in Tokyo's 23 Wards",
      "authors": "Takuya OKi, Yuan Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18226",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa20e5bd064a3600a6a9399f88b0f303ee4cc42c7edd5c8236408c1ec3052eb8_w640_q70.webp",
      "contributions": "",
      "summary": "Multifaceted Exploration of Spatial Openness in Rental Housing: A Big Data Analysis in Tokyo's 23 Wards",
      "mindmap": ""
    },
    {
      "title": "SG-RIFE: Semantic-Guided Real-Time Intermediate Flow Estimation with Diffusion-Competitive Perceptual Quality",
      "authors": "Pan Ben Wong, Chengli Wu, Hanyue Lu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18241",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/973299de069f19e1fee6c5ddbbadac21151ac4addddd917990024e02cf59b042_w640_q70.webp",
      "contributions": "",
      "summary": "SG-RIFE: Semantic-Guided Real-Time Intermediate Flow Estimation with Diffusion-Competitive Perceptual Quality",
      "mindmap": ""
    },
    {
      "title": "Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction",
      "authors": "Shahram Najam Syed, Yitian Hu, Yuchao Yao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18237",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5b04b692d1acf5e8524839243e2fe0778331620858ff93eb02166948c9684c4_w640_q70.webp",
      "contributions": "",
      "summary": "Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction",
      "mindmap": ""
    },
    {
      "title": "Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image",
      "authors": "Xiao He, Chang Tang, Xinwang Liu, Wei Zhang, Zhimin Gao, Chuankun Li, Shaohua Qiu, Jiangfeng Xu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18245",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8028cfc83b63f51bf798c2014f1dc8e1f4c786134910334880eb4c0cf340a5eb_w640_q70.webp",
      "contributions": "",
      "summary": "Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image",
      "mindmap": ""
    },
    {
      "title": "Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks",
      "authors": "Yucheng Fan, Jiawei Chen, Yu Tian, Zhaoxia Yin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18264",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6181175b477e5c50e93e9a1a3a4690fb1673471073933b770754e208c7b0e776_w640_q70.webp",
      "contributions": "",
      "summary": "Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks",
      "mindmap": ""
    },
    {
      "title": "Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model",
      "authors": "Rui Xing, Runmin Cong, Yingying Wu, Can Wang, Zhongming Tang, Fen Wang, Hao Wu, Sam Kwong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18247",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d341345aee67211dc7e3cea021a11051fd66231eb649e4da442fda6828319f0d_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model",
      "mindmap": ""
    },
    {
      "title": "Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System",
      "authors": "Se-Young Jang, Su-Yeon Yoon, Jae-Woong Jung, Dong-Hun Lee, Seong-Hun Choi, Soo-Kyung Jun, Yu-Bin Kim, Young-Seon Ju, Kyounggon Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18269",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/367924eac7293dd60fe2ea42c0f8cc5f30a08d5958f71c4e9e1a77afcf26f521_w640_q70.webp",
      "contributions": "",
      "summary": "Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System",
      "mindmap": ""
    },
    {
      "title": "Loom: Diffusion-Transformer for Interleaved Generation",
      "authors": "Mingcheng Ye, Jiaming Liu, Yiren Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18254",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2702848bca515645a4d065b0a7036ee7ddebe3d80478578da0afc6637e8987e1_w640_q70.webp",
      "contributions": "",
      "summary": "Loom: Diffusion-Transformer for Interleaved Generation",
      "mindmap": ""
    },
    {
      "title": "UniMPR: A Unified Framework for Multimodal Place Recognition with Arbitrary Sensor Configurations",
      "authors": "Zhangshuo Qi, Jingyi Xu, Luqi Cheng, Shichen Wen, Yiming Ma, Guangming Xiong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18279",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d97551a04523bb912a1f4f7a4089cc0954eacc215acb16ece3401441f9ae898e_w640_q70.webp",
      "contributions": "",
      "summary": "UniMPR: A Unified Framework for Multimodal Place Recognition with Arbitrary Sensor Configurations",
      "mindmap": ""
    },
    {
      "title": "Pyramidal Adaptive Cross-Gating for Multimodal Detection",
      "authors": "Zidong Gu, Shoufu Tian",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18291",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/686814403fba53ef7884ea37a0cf31a6148bda927776fcfca1291870ba0502e3_w640_q70.webp",
      "contributions": "",
      "summary": "Pyramidal Adaptive Cross-Gating for Multimodal Detection",
      "mindmap": ""
    },
    {
      "title": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
      "authors": "Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18318",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp",
      "contributions": "",
      "summary": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
      "mindmap": ""
    },
    {
      "title": "MatE: Material Extraction from Single-Image via Geometric Prior",
      "authors": "Zeyu Zhang, Wei Zhai, Jian Yang, Yang Cao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18312",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb74dfaa1e58347f80eafe1b8d6c9ccb8621df2345727b43170380c7a9ec7111_w640_q70.webp",
      "contributions": "",
      "summary": "MatE: Material Extraction from Single-Image via Geometric Prior",
      "mindmap": ""
    },
    {
      "title": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry",
      "authors": "Philipp Langsteiner, Jan-Niklas Dihlmann, Hendrik P.A. Lensch",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18314",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1bc201c384a37d43b2bd10a9944a6df93bc871b921e058d53b3a80eb95aa3784_w640_q70.webp",
      "contributions": "",
      "summary": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry",
      "mindmap": ""
    },
    {
      "title": "A two-stream network with global-local feature fusion for bone age assessment",
      "authors": "Qiong Lou, Han Yang, Fang Lu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18331",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5d26c3d7fded2a10e84ebad25bc6bb1810428c48d125c9ad4b3a401fe14ca66d_w640_q70.webp",
      "contributions": "",
      "summary": "A two-stream network with global-local feature fusion for bone age assessment",
      "mindmap": ""
    },
    {
      "title": "MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation",
      "authors": "Zhiheng Zhang, Jiajun Yang, Hong Sun, Dong Wang, Honghua Jiang, Yaru Chen, Tangyuan Ning",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18344",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f19d6aa9f347693da897453773133b3a6f0b66ee06e5f6bdf421bb24507e5f56_w640_q70.webp",
      "contributions": "",
      "summary": "MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation",
      "mindmap": ""
    },
    {
      "title": "Enhancing 3D Semantic Scene Completion with a Refinement Module",
      "authors": "Dunxing Zhang, Jiachen Lu, Han Yang, Lei Bao, Bo Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18363",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87b47e037d36c9a89056d6636a00bc4c1b54b6f312aac970b439024a3aa71b85_w640_q70.webp",
      "contributions": "",
      "summary": "Enhancing 3D Semantic Scene Completion with a Refinement Module",
      "mindmap": ""
    },
    {
      "title": "RecurGS: Interactive Scene Modeling via Discrete-State Recurrent Gaussian Fusion",
      "authors": "Wenhao Hu, Haonan Zhou, Zesheng Li, Liu Liu, Jiacheng Dong, Zhizhong Su, Gaoang Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18386",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e36e78691b7bf84af271eac170b5599e6d8dffe940baf4afb324df4c1c26545_w640_q70.webp",
      "contributions": "",
      "summary": "RecurGS: Interactive Scene Modeling via Discrete-State Recurrent Gaussian Fusion",
      "mindmap": ""
    },
    {
      "title": "Automated Mosaic Tesserae Segmentation via Deep Learning Techniques",
      "authors": "Charilaos Kapelonis, Marios Antonakakis, Konstantinos Politof, Aristomenis Antoniadis, Michalis Zervakis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18406",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6893130bacdfd82f19e38a48fe6c1a981e8a6e139face8bc5ca4379fb98ab0c8_w640_q70.webp",
      "contributions": "",
      "summary": "Automated Mosaic Tesserae Segmentation via Deep Learning Techniques",
      "mindmap": ""
    },
    {
      "title": "AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning",
      "authors": "Fei Song, Yi Li, Jiangmeng Li, Rui Wang, Changwen Zheng, Fanjiang Xu, Hui Xiong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18411",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c480d382f1a840aaa7b8f0ed1cc380b9ece2b0a04c01a7bfeaf7551356ef5a0c_w640_q70.webp",
      "contributions": "",
      "summary": "AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning",
      "mindmap": ""
    },
    {
      "title": "Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance",
      "authors": "Badr Moufad, Navid Bagheri Shouraki, Alain Oliviero Durmus, Thomas Hirtz, Eric Moulines, Jimmy Olsson, Yazid Janati",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18365",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb10f5246f2791c1a5c5d83727bb85643a1c89da44e041633e646e724bfa1d_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance",
      "mindmap": ""
    },
    {
      "title": "Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval",
      "authors": "Dimitrios Georgoulopoulos, Nikolaos Chaidos, Angeliki Dimitriou, Giorgos Stamou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18407",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8250115733311080fc732003a6de9ee573531db16fb388825dac28ed45b0904f_w640_q70.webp",
      "contributions": "",
      "summary": "Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval",
      "mindmap": ""
    },
    {
      "title": "E-RGB-D: Real-Time Event-Based Perception with Structured Light",
      "authors": "Seyed Ehsan Marjani Bajestani, Giovanni Beltrame",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18429",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/adf7afb7ba214958716e0fa581dd6ddbddce2e0c6851dde58c99a8a067def71d_w640_q70.webp",
      "contributions": "",
      "summary": "E-RGB-D: Real-Time Event-Based Perception with Structured Light",
      "mindmap": ""
    },
    {
      "title": "Object-Centric Framework for Video Moment Retrieval",
      "authors": "Zongyao Li, Yongkang Wong, Satoshi Yamazaki, Jianquan Liu, Mohan Kankanhalli",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18448",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aeaf50384c57c0a0975048f565d619000d31089ae116153752950a50d948d161_w640_q70.webp",
      "contributions": "",
      "summary": "Object-Centric Framework for Video Moment Retrieval",
      "mindmap": ""
    },
    {
      "title": "Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System",
      "authors": "Xavier Rafael-Palou, Jose Munuera, Ana Jimenez-Pastor, Richard Osuala, Karim Lekadir, Oliver Diaz",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18450",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24a54a0b377f2d61cdb058b9dd088fe0948517999354f7c502389a3f1149e8a3_w640_q70.webp",
      "contributions": "",
      "summary": "Agent-Based Output Drift Detection for Breast Cancer Response Prediction in a Multisite Clinical Decision Support System",
      "mindmap": ""
    },
    {
      "title": "MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading",
      "authors": "Shurui Xu, Siqi Yang, Jiapin Ren, Zhong Cao, Hongwei Yang, Mengzhen Fan, Yuyu Sun, Shuyan Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18437",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82297f34fc5f7dfab35d4ab984e7ba607ad5ac2c849bffbec01d38cbdcf42e3d_w640_q70.webp",
      "contributions": "",
      "summary": "MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading",
      "mindmap": ""
    },
    {
      "title": "NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic",
      "authors": "Jayant Lohia",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18453",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e4686d2a1c44f6a0f1d346c2e5401709f3d974dfd9062781f00cc8eb5d71952_w640_q70.webp",
      "contributions": "",
      "summary": "NOVA: Discovering Well-Conditioned Winograd Transforms through Numerical Optimization of Vandermonde Arithmetic",
      "mindmap": ""
    },
    {
      "title": "Plasticine: A Traceable Diffusion Model for Medical Image Translation",
      "authors": "Tianyang Zhanng, Xinxing Cheng, Jun Cheng, Shaoming Zheng, He Zhao, Huazhu Fu, Alejandro F Frangi, Jiang Liu, Jinming Duan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18455",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2148612b9f3ccd68ab3abdcc94c84b8f1b0d8d4ed68811c6de5496dd1c2352d_w640_q70.webp",
      "contributions": "",
      "summary": "Plasticine: A Traceable Diffusion Model for Medical Image Translation",
      "mindmap": ""
    },
    {
      "title": "PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs",
      "authors": "Santwana Sagnika, Manav Malhotra, Ishtaj Kaur Deol, Soumyajit Roy, Swarnav Kumar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18500",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3d3024bb6cf729a0d0827d13bba185e065be8eb34bd4dffa40d58782bc4e5ab_w640_q70.webp",
      "contributions": "",
      "summary": "PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs",
      "mindmap": ""
    },
    {
      "title": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
      "authors": "Wenjun Lin, Jensen Zhang, Kaitong Cai, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18477",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/79949cb474434bd024983169a211bb0a029e6412a974a6dd6f4ee7a51cf05349_w640_q70.webp",
      "contributions": "",
      "summary": "STORM: Search-Guided Generative World Models for Robotic Manipulation",
      "mindmap": ""
    },
    {
      "title": "Adaptive-VoCo: Complexity-Aware Visual Token Compression for Vision-Language Models",
      "authors": "Xiaoyang Guo, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18496",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff175aad1ff6f6b7f47faa5f2c01c3d0f7b27451e06bf33f2c763508f1ff6f0e_w640_q70.webp",
      "contributions": "",
      "summary": "Adaptive-VoCo: Complexity-Aware Visual Token Compression for Vision-Language Models",
      "mindmap": ""
    },
    {
      "title": "NASTaR: NovaSAR Automated Ship Target Recognition Dataset",
      "authors": "Benyamin Hosseiny, Kamirul Kamirul, Odysseas Pappas, Alin Achim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18503",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4ac29e833bc5eaed0a9881e81dd877d55f600f6bd8722c27c1466b58afc1378_w640_q70.webp",
      "contributions": "",
      "summary": "NASTaR: NovaSAR Automated Ship Target Recognition Dataset",
      "mindmap": ""
    },
    {
      "title": "GTMA: Dynamic Representation Optimization for OOD Vision-Language Models",
      "authors": "Jensen Zhang, Ningyuan Liu, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18504",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ec81480aabd317eb3778c36f956b818e8aed383ab7f9a13fe2c867243fcec025_w640_q70.webp",
      "contributions": "",
      "summary": "GTMA: Dynamic Representation Optimization for OOD Vision-Language Models",
      "mindmap": ""
    },
    {
      "title": "WoundNet-Ensemble: A Novel IoMT System Integrating Self-Supervised Deep Learning and Multi-Model Fusion for Automated, High-Accuracy Wound Classification and Healing Progression Monitoring",
      "authors": "Moses Kiprono",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18528",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c30b62937b70850aaf6278e7cee47b3850562dc10f4c505a5d923c08c6e1e20c_w640_q70.webp",
      "contributions": "",
      "summary": "WoundNet-Ensemble: A Novel IoMT System Integrating Self-Supervised Deep Learning and Multi-Model Fusion for Automated, High-Accuracy Wound Classification and Healing Progression Monitoring",
      "mindmap": ""
    },
    {
      "title": "Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism",
      "authors": "Rahul Yumlembam, Biju Issac, Nauman Aslam, Eaby Kollonoor Babu, Josh Collyer, Fraser Kennedy",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18527",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1520c61491b8f395b60f64432e37eff57c8608eba74cd9029c6109d32db7554_w640_q70.webp",
      "contributions": "",
      "summary": "Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism",
      "mindmap": ""
    },
    {
      "title": "Enhancing Medical Large Vision-Language Models via Alignment Distillation",
      "authors": "Aofei Chang, Ting Wang, Fenglong Ma",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18554",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65cfdaf6c50eb13e1535902993b0e58d2ccb2d1d8a89304254b7eb116f2e3bec_w640_q70.webp",
      "contributions": "",
      "summary": "Enhancing Medical Large Vision-Language Models via Alignment Distillation",
      "mindmap": ""
    },
    {
      "title": "Hierarchical Bayesian Framework for Multisource Domain Adaptation",
      "authors": "Alexander M. Glandon, Khan M. Iftekharuddin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18553",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8faca3b573bb5e3938ff1e6d161ec550a7c61bbaaa60d1a247d4dd139982ad93_w640_q70.webp",
      "contributions": "",
      "summary": "Hierarchical Bayesian Framework for Multisource Domain Adaptation",
      "mindmap": ""
    },
    {
      "title": "Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model",
      "authors": "Sumaiya Ali, Areej Alhothali, Ohoud Alzamzami, Sameera Albasri, Ahmed Abduljabbar, Muhammad Alwazzan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18573",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/387c39b67caa5e84daf0327dfb4c7a948d6d72c292a3404e6c7e8a2bcd6db7df_w640_q70.webp",
      "contributions": "",
      "summary": "Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model",
      "mindmap": ""
    },
    {
      "title": "OpenView: Empowering MLLMs with Out-of-view VQA",
      "authors": "Qixiang Chen, Cheng Zhang, Chi-Wing Fu, Jingwen Ye, Jianfei Cai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18563",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd24136fa7bb319394718494b72886c4e4dd61a8ff3664079b5014b3c8e9d20_w640_q70.webp",
      "contributions": "",
      "summary": "OpenView: Empowering MLLMs with Out-of-view VQA",
      "mindmap": ""
    },
    {
      "title": "ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning",
      "authors": "Weijie Zhou, Xuangtang Xiong, Ye Tian, Lijun Yue, Xinyu Wu, Wei Li, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang, Zhengyou Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18571",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e48075d8fabfbd12f97c2605f729d021ebb805999e01bbf511f08a872cf4cbae_w640_q70.webp",
      "contributions": "",
      "summary": "ESearch-R1: Learning Cost-Aware MLLM Agents for Interactive Embodied Search via Reinforcement Learning",
      "mindmap": ""
    },
    {
      "title": "Commercial Vehicle Braking Optimization: A Robust SIFT-Trajectory Approach",
      "authors": "Zhe Li, Kun Cheng, Hanyue Mo, Jintao Lu, Ziwen Kuang, Jianwen Ye, Lixu Xu, Xinya Meng, Jiahui Zhao, Shengda Ji, Shuyuan Liu, Mengyu Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18597",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e804718e966934e64cc22d02e796b1123866e2dd8d037801ef21cbcbee4c0537_w640_q70.webp",
      "contributions": "",
      "summary": "Commercial Vehicle Braking Optimization: A Robust SIFT-Trajectory Approach",
      "mindmap": ""
    },
    {
      "title": "SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback",
      "authors": "Jianglin Lu, Yuanwei Wu, Ziyi Zhao, Hongcheng Wang, Felix Jimenez, Abrar Majeedi, Yun Fu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18599",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/196c51c23a0210e0d5f61fa9aa109b4c1b71673440779c2559e8762b7020daf5_w640_q70.webp",
      "contributions": "",
      "summary": "SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback",
      "mindmap": ""
    },
    {
      "title": "Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments",
      "authors": "Saeideh Yousefzadeh, Hamidreza Pourreza",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18613",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/15155edf4c23c5fa3645a1383be98a1728e9025130895c36fb1d8c7a536e2335_w640_q70.webp",
      "contributions": "",
      "summary": "Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments",
      "mindmap": ""
    },
    {
      "title": "PTTA: A Pure Text-to-Animation Framework for High-Quality Creation",
      "authors": "Ruiqi Chen, Kaitong Cai, Yijia Fan, Keze Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18614",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95e6d5eb29d8f200e9b4d751e2105c378ddc6c8efc5742b330b0ff8118931fb1_w640_q70.webp",
      "contributions": "",
      "summary": "PTTA: A Pure Text-to-Animation Framework for High-Quality Creation",
      "mindmap": ""
    },
    {
      "title": "Adversarial Robustness in Zero-Shot Learning:An Empirical Study on Class and Concept-Level Vulnerabilities",
      "authors": "Zhiyuan Peng, Zihan Ye, Shreyank N Gowda, Yuping Yan, Haotian Xu, Ling Shao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18651",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/723c55bcd003bd92c1a8106676da980d908b4363b91caeab957599d3002e301d_w640_q70.webp",
      "contributions": "",
      "summary": "Adversarial Robustness in Zero-Shot Learning:An Empirical Study on Class and Concept-Level Vulnerabilities",
      "mindmap": ""
    },
    {
      "title": "PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval",
      "authors": "Pengxiang Ouyang, Qing Ma, Zheng Wang, Cong Bai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18660",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06fe8e37dc2d1d1d6a5d48298128004f86b1be4d6ecbeca32f5c72786b1a535c_w640_q70.webp",
      "contributions": "",
      "summary": "PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval",
      "mindmap": ""
    },
    {
      "title": "SplatBright: Generalizable Low-Light Scene Reconstruction from Sparse Views via Physically-Guided Gaussian Enhancement",
      "authors": "Yue Wen, Liang Song, Hesheng Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18655",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e36abe6a038f84b030a1de9a4b07a7b5e542151e56041f8a4107ffcc75a8c4fa_w640_q70.webp",
      "contributions": "",
      "summary": "SplatBright: Generalizable Low-Light Scene Reconstruction from Sparse Views via Physically-Guided Gaussian Enhancement",
      "mindmap": ""
    },
    {
      "title": "Offline Reinforcement Learning for End-to-End Autonomous Driving",
      "authors": "Chihiro Noguchi, Takaki Yamamoto",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18662",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48070c89f8318c57738214d175fd04c9e38d7e43e2838b599453ae0e31d8c27f_w640_q70.webp",
      "contributions": "",
      "summary": "Offline Reinforcement Learning for End-to-End Autonomous Driving",
      "mindmap": ""
    },
    {
      "title": "Geometric-Photometric Event-based 3D Gaussian Ray Tracing",
      "authors": "Kai Kohyama, Yoshimitsu Aoki, Guillermo Gallego, Shintaro Shiba",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18640",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/85632c631e489e2f789f1b5319b05b69e8e883a12a7b626de475c98e6066ef45_w640_q70.webp",
      "contributions": "",
      "summary": "Geometric-Photometric Event-based 3D Gaussian Ray Tracing",
      "mindmap": ""
    },
    {
      "title": "SmartSight: Mitigating Hallucination in Video-LLMs Without Compromising Video Understanding via Temporal Attention Collapse",
      "authors": "Yiming Sun, Mi Zhang, Feifei Li, Geng Hong, Min Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18671",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aba3b21e95cf3e99fe497ad62634d3e70efc41858e08b88889ea760e9fc41f24_w640_q70.webp",
      "contributions": "",
      "summary": "SmartSight: Mitigating Hallucination in Video-LLMs Without Compromising Video Understanding via Temporal Attention Collapse",
      "mindmap": ""
    },
    {
      "title": "Uni-Neur2Img: Unified Neural Signal-Guided Image Generation, Editing, and Stylization via Diffusion Transformers",
      "authors": "Xiyue Bai, Ronghao Yu, Jia Xiu, Pengfei Zhou, Jie Xia, Peng Ji",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18635",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c6acb4dae57f2c9f4c60c94122091feee1beca126d52be260f7006e1f25433b_w640_q70.webp",
      "contributions": "",
      "summary": "Uni-Neur2Img: Unified Neural Signal-Guided Image Generation, Editing, and Stylization via Diffusion Transformers",
      "mindmap": ""
    },
    {
      "title": "brat: Aligned Multi-View Embeddings for Brain MRI Analysis",
      "authors": "Maxime Kayser, Maksim Gridnev, Wanting Wang, Max Bain, Aneesh Rangnekar, Avijit Chatterjee, Aleksandr Petrov, Harini Veeraraghavan, Nathaniel C. Swinburne",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18679",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e2d9cbeed3e332a02f5cc9adf5abc0ec370a32b059de24cd550cc930eb84a82_w640_q70.webp",
      "contributions": "",
      "summary": "brat: Aligned Multi-View Embeddings for Brain MRI Analysis",
      "mindmap": ""
    },
    {
      "title": "AsyncDiff: Asynchronous Timestep Conditioning for Enhanced Text-to-Image Diffusion Inference",
      "authors": "Longhuan Xu, Feng Yin, Cunjian Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18675",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b77064b6897ecbdf7c4ad30fb422607730ad3fdb70df7e888f14b1c35318262c_w640_q70.webp",
      "contributions": "",
      "summary": "AsyncDiff: Asynchronous Timestep Conditioning for Enhanced Text-to-Image Diffusion Inference",
      "mindmap": ""
    },
    {
      "title": "A Study of Finetuning Video Transformers for Multi-view Geometry Tasks",
      "authors": "Huimin Wu, Kwang-Ting Cheng, Stephen Lin, Zhirong Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18684",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbb0d58e96608a91e501c2ec636326573d0b8bbd34db61dfabc4e1544edbaaef_w640_q70.webp",
      "contributions": "",
      "summary": "A Study of Finetuning Video Transformers for Multi-view Geometry Tasks",
      "mindmap": ""
    },
    {
      "title": "EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images",
      "authors": "Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, Munchurl Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18692",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbc186e81fb4bc0dba691524b3e057afe1a2f63cefe632fd49cd141b1ddd38ac_w640_q70.webp",
      "contributions": "",
      "summary": "EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images",
      "mindmap": ""
    },
    {
      "title": "Rectification Reimagined: A Unified Mamba Model for Image Correction and Rectangling with Prompts",
      "authors": "Linwei Qiu, Gongzhe Li, Xiaozhe Zhang, Qinlin Sun, Fengying Xie",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18718",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1fb34acfd0681b1e7d5e9dd5a9b2ead7a096ce7f062456abd5c1d666d63c9f6d_w640_q70.webp",
      "contributions": "",
      "summary": "Rectification Reimagined: A Unified Mamba Model for Image Correction and Rectangling with Prompts",
      "mindmap": ""
    },
    {
      "title": "Breast Cancer Recurrence Risk Prediction Based on Multiple Instance Learning",
      "authors": "Jinqiu Chen, Huyan Xu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18734",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/703d5b9807b107a0861def2e3d8e792e6ba0cafc487dd572e6b8472d4694e38c_w640_q70.webp",
      "contributions": "",
      "summary": "Breast Cancer Recurrence Risk Prediction Based on Multiple Instance Learning",
      "mindmap": ""
    },
    {
      "title": "$M^3-Verse$: A \"Spot the Difference\" Challenge for Large Multimodal Models",
      "authors": "Kewei Wei, Bocheng Hu, Jie Cao, Xiaohan Chen, Zhengxi Lu, Wubing Xia, Weili Xu, Jiaao Wu, Junchen He, Mingyu Jia, Ciyun Zhao, Ye Sun, Yizhi Li, Zhonghan Zhao, Jian Zhang, Gaoang Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18735",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52a20f9bfaa32af67c363579cc4ad37ddbcaa7484f53c2ad3e6f6287dffcb22d_w640_q70.webp",
      "contributions": "",
      "summary": "$M^3-Verse$: A \"Spot the Difference\" Challenge for Large Multimodal Models",
      "mindmap": ""
    },
    {
      "title": "AMLID: An Adaptive Multispectral Landmine Identification Dataset for Drone-Based Detection",
      "authors": "James E. Gallagher, Edward J. Oughton",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18738",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bd46cd4988eda7be0f3fc725e20d0cd6d84d2d855b2fe6ca9ada6da8ce6533a1_w640_q70.webp",
      "contributions": "",
      "summary": "AMLID: An Adaptive Multispectral Landmine Identification Dataset for Drone-Based Detection",
      "mindmap": ""
    },
    {
      "title": "Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation",
      "authors": "Tianrui Zhu, Shiyi Zhang, Zhirui Sun, Jingqi Tian, Yansong Tang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18741",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17ec1d3f6ff19284c38fae7fffb7c89e33c210bb632c6e29bab17704791956af_w640_q70.webp",
      "contributions": "",
      "summary": "Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation",
      "mindmap": ""
    },
    {
      "title": "Context-Aware Network Based on Multi-scale Spatio-temporal Attention for Action Recognition in Videos",
      "authors": "Xiaoyang Li, Wenzhu Yang, Kanglin Wang, Tiebiao Wang, Qingsong Fei",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18750",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d25c33c0793bb9ac5e532b11ef64839818a8ed3877ba9c30ed6683982c402d3b_w640_q70.webp",
      "contributions": "",
      "summary": "Context-Aware Network Based on Multi-scale Spatio-temporal Attention for Action Recognition in Videos",
      "mindmap": ""
    },
    {
      "title": "IPCV: Information-Preserving Compression for MLLM Visual Encoders",
      "authors": "Yuan Chen, Zichen Wen, Yuzhou Wu, Xuyang Liu, Shuang Chen, Junpeng Ma, Weijia Li, Conghui He, Linfeng Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18747",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/23f93076434c2a627bcdaf65dfd58999db9105798eceb360e343a3f4018cc020_w640_q70.webp",
      "contributions": "",
      "summary": "IPCV: Information-Preserving Compression for MLLM Visual Encoders",
      "mindmap": ""
    },
    {
      "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
      "authors": "Kaican Li, Lewei Yao, Jiannan Wu, Tiezheng Yu, Jierun Chen, Haoli Bai, Lu Hou, Lanqing Hong, Wei Zhang, Nevin L. Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18745",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87556fdc09b55b65c0d472d205a384cc42453256afeb9e7a28db98178fe1d145_w640_q70.webp",
      "contributions": "",
      "summary": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
      "mindmap": ""
    },
    {
      "title": "In-Context Audio Control of Video Diffusion Transformers",
      "authors": "Wenze Liu, Weicai Ye, Minghong Cai, Quande Liu, Xintao Wang, Xiangyu Yue",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18772",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2f6222ebfd1fddbc353b3e719e28fb0ea12ffea379c74f8f8539de1c07ea2e3_w640_q70.webp",
      "contributions": "",
      "summary": "In-Context Audio Control of Video Diffusion Transformers",
      "mindmap": ""
    },
    {
      "title": "MaskFocus: Focusing Policy Optimization on Critical Steps for Masked Image Generation",
      "authors": "Guohui Zhang, Hu Yu, Xiaoxiao Ma, Yaning Pan, Hang Xu, Feng Zhao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18766",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e30d5e631331f72f5d3daa88862f5f4de5bcba185997003c4eb4c2df4ba695e_w640_q70.webp",
      "contributions": "",
      "summary": "MaskFocus: Focusing Policy Optimization on Critical Steps for Masked Image Generation",
      "mindmap": ""
    },
    {
      "title": "Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers",
      "authors": "Fanis Mathioulakis, Gorjan Radevski, Tinne Tuytelaars",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18784",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe0254e0e9393c18241de33ec503d4cc8d75622e94c6a0d08ec4ee16da3e3b6b_w640_q70.webp",
      "contributions": "",
      "summary": "Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers",
      "mindmap": ""
    },
    {
      "title": "Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation",
      "authors": "Guangtao Lyu, Chenghao Xu, Qi Liu, Jiexi Yan, Muli Yang, Fen Fang, Cheng Deng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18804",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab2ab51309551324dcfdba087c2e5ec1e6dfdae10633047423e7233f30fdf84_w640_q70.webp",
      "contributions": "",
      "summary": "Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation",
      "mindmap": ""
    },
    {
      "title": "FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation",
      "authors": "Ziyuan Tao, Chuanzhi Xu, Sandaru Jayawardana, Wei Bao, Kanchana Thilakarathna, Teng Joon Lim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18809",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/396672768b4960b9fefe0f861b938a8b78842c8181043ded9d12c7e8f28dbdfe_w640_q70.webp",
      "contributions": "",
      "summary": "FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation",
      "mindmap": ""
    },
    {
      "title": "Revealing Perception and Generation Dynamics in LVLMs: Mitigating Hallucinations via Validated Dominance Correction",
      "authors": "Guangtao Lyu, Xinyi Cheng, Chenghao Xu, Qi Liu, Muli Yang, Fen Fang, Huilin Chen, Jiexi Yan, Xu Yang, Cheng Deng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18813",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c99ecc3b83a3bbbf2f3d70f79eb80fb5574e88c49c36b11bed230f795915fb03_w640_q70.webp",
      "contributions": "",
      "summary": "Revealing Perception and Generation Dynamics in LVLMs: Mitigating Hallucinations via Validated Dominance Correction",
      "mindmap": ""
    },
    {
      "title": "EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer",
      "authors": "Yuxiao Yang, Hualian Sheng, Sijia Cai, Jing Lin, Jiahao Wang, Bing Deng, Junzhe Lu, Haoqian Wang, Jieping Ye",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18814",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a2ba0db72977cbda98db1501df497120d084239fa62014678ae702cb513369e_w640_q70.webp",
      "contributions": "",
      "summary": "EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer",
      "mindmap": ""
    },
    {
      "title": "Brain-Gen: Towards Interpreting Neural Signals for Stimulus Reconstruction Using Transformers and Latent Diffusion Models",
      "authors": "Hasib Aslam, Muhammad Talal Faiz, Muhammad Imran Malik",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18843",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c10b7d7d79ed94bbb6547ab2dcefe6d392d199280eab76c815bf57db3927efb_w640_q70.webp",
      "contributions": "",
      "summary": "Brain-Gen: Towards Interpreting Neural Signals for Stimulus Reconstruction Using Transformers and Latent Diffusion Models",
      "mindmap": ""
    },
    {
      "title": "VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference",
      "authors": "Sicheng Song, Yanjie Zhang, Zixin Chen, Huamin Qu, Changbo Wang, Chenhui Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18853",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6df051754b8f594a7ef88a46e4855d2eb43c4c662b1afbca9997caf2b4fbad53_w640_q70.webp",
      "contributions": "",
      "summary": "VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference",
      "mindmap": ""
    },
    {
      "title": "Application of deep learning approaches for medieval historical documents transcription",
      "authors": "Maksym Voloshchuk, Bohdana Zarembovska, Mykola Kozlenko",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18865",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bc697efc3bfd30e66b187f56c365b6a8add07756fb768bc77ba4586f1ab7d205_w640_q70.webp",
      "contributions": "",
      "summary": "Application of deep learning approaches for medieval historical documents transcription",
      "mindmap": ""
    },
    {
      "title": "Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification",
      "authors": "Alina Elena Baia, Andrea Cavallaro",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18864",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54def5a8025f36fedd1db00274e2f13348c15e8cd933c948a5286d52b31223ae_w640_q70.webp",
      "contributions": "",
      "summary": "Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification",
      "mindmap": ""
    },
    {
      "title": "Localising Shortcut Learning in Pixel Space via Ordinal Scoring Correlations for Attribution Representations (OSCAR)",
      "authors": "Akshit Achara, Peter Triantafillou, Esther Puyol-Antón, Alexander Hammers, Andrew P. King",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18888",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2c85ecde5b204bf7cf4f7ce66885bc81ef00aee9a5651c7332b2bd485f6e54e2_w640_q70.webp",
      "contributions": "",
      "summary": "Localising Shortcut Learning in Pixel Space via Ordinal Scoring Correlations for Attribution Representations (OSCAR)",
      "mindmap": ""
    },
    {
      "title": "CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis",
      "authors": "Kaidi Liang, Ke Li, Xianbiao Hu, Ruwen Qin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18878",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/40cc111afbcdc76e8f9c40d867f3e3d92fefb4f06215bb877943f16f5fc7f761_w640_q70.webp",
      "contributions": "",
      "summary": "CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis",
      "mindmap": ""
    },
    {
      "title": "Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs",
      "authors": "Dmitry Demidov, Zaigham Zaheer, Zongyan Han, Omkar Thawakar, Rao Anwer",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18897",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b8993f53f872c636102637faace6bbf220931bba2b5c07be198751d05e23fa52_w640_q70.webp",
      "contributions": "",
      "summary": "Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs",
      "mindmap": ""
    },
    {
      "title": "Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models",
      "authors": "Mohamad Zamini, Diksha Shukla",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18910",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82dc1b903fe39645cfdac67e793b8aaf7d0b36f43bd3088781f3ec68c702dafb_w640_q70.webp",
      "contributions": "",
      "summary": "Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models",
      "mindmap": ""
    },
    {
      "title": "Point What You Mean: Visually Grounded Instruction Policy",
      "authors": "Hang Yu, Juntu Zhao, Yufeng Liu, Kaiyu Li, Cheng Ma, Di Zhang, Yingdong Hu, Guang Chen, Junyuan Xie, Junliang Guo, Junqiao Zhao, Yang Gao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18933",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d0cc4da48f9bff61ef73842c7f9922cf58cd10179d0b3d6cb1241fbc052909cc_w640_q70.webp",
      "contributions": "",
      "summary": "Point What You Mean: Visually Grounded Instruction Policy",
      "mindmap": ""
    },
    {
      "title": "LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer",
      "authors": "Raina Panda, Daniel Fein, Arpita Singhal, Mark Fiore, Maneesh Agrawala, Matyas Bohacek",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18930",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/17d9e52f35a5e302d613cba6423f95b6e9bb58c2b559fbd5a209c0516f8e2326_w640_q70.webp",
      "contributions": "",
      "summary": "LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer",
      "mindmap": ""
    },
    {
      "title": "Symmetrization of 3D Generative Models",
      "authors": "Nicolas Caytuiro, Ivan Sipiran",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18953",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67504b808f29cea86fc99619a21f55d7a71b7b925241cf4a90f7273b07bddf83_w640_q70.webp",
      "contributions": "",
      "summary": "Symmetrization of 3D Generative Models",
      "mindmap": ""
    },
    {
      "title": "DVI: Disentangling Semantic and Visual Identity for Training-Free Personalized Generation",
      "authors": "Guandong Li, Yijun Ding",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18964",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5f5e482fec6d1911329e83df08fb77c38f618acd901b6be5084bffe87124bfb_w640_q70.webp",
      "contributions": "",
      "summary": "DVI: Disentangling Semantic and Visual Identity for Training-Free Personalized Generation",
      "mindmap": ""
    },
    {
      "title": "Total Curvature Regularization and its_Minimization for Surface and Image Smoothing",
      "authors": "Tianle Lu, Ke Chen, Yuping Duan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18968",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3e49d05c1cc8ec54ae283a3da000454b18542cf8f4a1408bd3980e43be6a00e3_w640_q70.webp",
      "contributions": "",
      "summary": "Total Curvature Regularization and its_Minimization for Surface and Image Smoothing",
      "mindmap": ""
    },
    {
      "title": "Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning",
      "authors": "Cheng-Hong Chang, Pei-Hsuan Tsai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18969",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05062b9ac64115654a255df578e1f3f61c6740d8e9db0b67ceca3387185661df_w640_q70.webp",
      "contributions": "",
      "summary": "Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning",
      "mindmap": ""
    },
    {
      "title": "VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion",
      "authors": "Zaidao Han, Risa Higashita, Jiang Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18954",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a277490a1036cca55f864049064f469e9f72d6c27becedf02e6e0afb0212e89_w640_q70.webp",
      "contributions": "",
      "summary": "VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion",
      "mindmap": ""
    },
    {
      "title": "Towards AI-Guided Open-World Ecological Taxonomic Classification",
      "authors": "Cheng Yaw Low, Heejoon Koo, Jaewoo Park, Kaleb Mesfin Asfaw, Meeyoung Cha",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18994",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/780199ccb79605e70d0938bc92678f9a79a328a23356a7c9a76644dcf9ab4dfe_w640_q70.webp",
      "contributions": "",
      "summary": "Towards AI-Guided Open-World Ecological Taxonomic Classification",
      "mindmap": ""
    },
    {
      "title": "Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation",
      "authors": "Ryosuke Korekata, Quanting Xie, Yonatan Bisk, Komei Sugiura",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18987",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/456f5beecc1c2c45f598578f8491d702dc76e9f500bae44e01454f783dc10b05_w640_q70.webp",
      "contributions": "",
      "summary": "Affordance RAG: Hierarchical Multimodal Retrieval with Affordance-Aware Embodied Memory for Mobile Manipulation",
      "mindmap": ""
    },
    {
      "title": "ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation",
      "authors": "Gyeongrok Oh, Youngdong Jang, Jonghyun Choi, Suk-Ju Kang, Guang Lin, Sangpil Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18991",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca47fc4083c13826225296ae5380ec0a994e6b8b0a936b8582bb3acb510605f6_w640_q70.webp",
      "contributions": "",
      "summary": "ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation",
      "mindmap": ""
    },
    {
      "title": "Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection",
      "authors": "Haoze Li, Jie Zhang, Guoying Zhao, Stephen Lin, Shiguang Shan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19022",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/462a93cd76a387b34292a443a8359c9699b91dba8c90d7da0799846e9545ee6a_w640_q70.webp",
      "contributions": "",
      "summary": "Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection",
      "mindmap": ""
    },
    {
      "title": "VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation",
      "authors": "Sihao Lin, Zerui Li, Xunyi Zhao, Gengze Zhou, Liuyi Wang, Rong Wei, Rui Tang, Juncheng Li, Hanqing Wang, Jiangmiao Pang, Anton van den Hengel, Jiajun Liu, Qi Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19021",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/73e25aea53ce484be801e13db8703f7ac91dcd519e03aa8f1f11869324466841_w640_q70.webp",
      "contributions": "",
      "summary": "VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation",
      "mindmap": ""
    },
    {
      "title": "Automatic Neuronal Activity Segmentation in Fast Four Dimensional Spatio-Temporal Fluorescence Imaging using Bayesian Approach",
      "authors": "Ran Li, Pan Xiao, Kaushik Dutta, Youdong Guo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19032",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/caef29a5648e5f1039a8eef029aa3d268223d6542939af5d70efe56c4350e067_w640_q70.webp",
      "contributions": "",
      "summary": "Automatic Neuronal Activity Segmentation in Fast Four Dimensional Spatio-Temporal Fluorescence Imaging using Bayesian Approach",
      "mindmap": ""
    },
    {
      "title": "CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization",
      "authors": "Zelin Zhao, Xinyu Gong, Bangya Liu, Ziyang Song, Jun Zhang, Suhui Wu, Yongxin Chen, Hao Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19020",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7bd5a2abb18589060985877478ba075c83351c0fe7a3b61f7db28dccf9b3d5b_w640_q70.webp",
      "contributions": "",
      "summary": "CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization",
      "mindmap": ""
    },
    {
      "title": "Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation",
      "authors": "Connor Kilrain, David Carlyn, Julia Chae, Sara Beery, Wei-Lun Chao, Jianyang Gu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19026",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f9212089c44919f3c00bd82b13c0b39b407491c5d9de5c2c72ed790b2f0a0a2f_w640_q70.webp",
      "contributions": "",
      "summary": "Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation",
      "mindmap": ""
    },
    {
      "title": "Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition",
      "authors": "Xiaoyang Li, Mingming Lu, Ruiqi Wang, Hao Li, Zewei Le",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19036",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3cd50ba346b66d56f1f55a399d0c82cbfbd81d4c166ba855757c74c8346c7c2_w640_q70.webp",
      "contributions": "",
      "summary": "Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition",
      "mindmap": ""
    },
    {
      "title": "WaTeRFlow: Watermark Temporal Robustness via Flow Consistency",
      "authors": "Utae Jeong, Sumin In, Hyunju Ryu, Jaewan Choi, Feng Yang, Jongheon Jeong, Seungryong Kim, Sangpil Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19048",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8f8776ce3ae3dfd5e014702d94d69af26362743bfccc9cf4f4e1fc818a95a539_w640_q70.webp",
      "contributions": "",
      "summary": "WaTeRFlow: Watermark Temporal Robustness via Flow Consistency",
      "mindmap": ""
    },
    {
      "title": "6DAttack: Backdoor Attacks in the 6DoF Pose Estimation",
      "authors": "Jihui Guo, Zongmin Zhang, Zhen Sun, Yuhao Yang, Jinlin Wu, Fu Zhang, Xinlei He",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19058",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4d7cd1a6d20a70ace7bef5d2b8b091492883b5588753a885d31218a86b7cb53e_w640_q70.webp",
      "contributions": "",
      "summary": "6DAttack: Backdoor Attacks in the 6DoF Pose Estimation",
      "mindmap": ""
    },
    {
      "title": "Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding",
      "authors": "Ruiqi Ma, Yu Yan, Chunhong Zhang, Minghao Yin, XinChao Liu, Zhihong Jin, Zheng Hu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19070",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d56f2f61fcc600eaa02837139337fdefe7e31f2b7f624524c972734f069b6c0f_w640_q70.webp",
      "contributions": "",
      "summary": "Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding",
      "mindmap": ""
    },
    {
      "title": "Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation",
      "authors": "Khanh Nguyen, Dasith de Silva Edirimuni, Ghulam Mubashar Hassan, Ajmal Mian",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19088",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d400fa28859b94a82c7a1ca0fe5b0ee2133535554ef479d696c7d0032c8503d3_w640_q70.webp",
      "contributions": "",
      "summary": "Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation",
      "mindmap": ""
    },
    {
      "title": "Decoupled Generative Modeling for Human-Object Interaction Synthesis",
      "authors": "Hwanhee Jung, Seunggwan Lee, Jeongyoon Yoon, SeungHyeon Kim, Giljoo Nam, Qixing Huang, Sangpil Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19049",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/efa5fbcac724949235f9de6d44ddfad055396c38cff93f03bbf6611e7504dfe4_w640_q70.webp",
      "contributions": "",
      "summary": "Decoupled Generative Modeling for Human-Object Interaction Synthesis",
      "mindmap": ""
    },
    {
      "title": "Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges",
      "authors": "Ariel Lubonja, Pedro R. A. S. Bassi, Wenxuan Li, Hualin Qiao, Randal Burns, Alan L. Yuille, Zongwei Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19091",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8b75e4c6b7e493a0c6d3b57468d0ae3edcb9efaeaaf9076c00744a6a04c7c6a_w640_q70.webp",
      "contributions": "",
      "summary": "Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges",
      "mindmap": ""
    },
    {
      "title": "Mamba-Based Modality Disentanglement Network for Multi-Contrast MRI Reconstruction",
      "authors": "Weiyi Lyu, Xinming Fang, Jun Wang, Jun Shi, Guixu Zhang, Juncheng Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19095",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/660b7ce425295dabbd643561507450b5b391330b19646c8c466147c1770bdf63_w640_q70.webp",
      "contributions": "",
      "summary": "Mamba-Based Modality Disentanglement Network for Multi-Contrast MRI Reconstruction",
      "mindmap": ""
    },
    {
      "title": "Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction",
      "authors": "Tao Li, Zhenbao Yu, Banglei Guan, Jianli Han, Weimin Lv, Friedrich Fraundorfer",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19110",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59e59cb599a47e950626af160c385ffc182ff74568bbebdcb198397b16efd9d1_w640_q70.webp",
      "contributions": "",
      "summary": "Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction",
      "mindmap": ""
    },
    {
      "title": "Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?",
      "authors": "Hengyi Feng, Zeang Sheng, Meiyi Qiang, Wentao Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19115",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1ccbf3745011850a9bf45220f261e89ca40bd9c98025e6d92083fb946fbf5c6_w640_q70.webp",
      "contributions": "",
      "summary": "Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?",
      "mindmap": ""
    },
    {
      "title": "GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting",
      "authors": "Tiantian Li, Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Jun Zhang, Yan Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19108",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a01ced772b8d2640ac7da9b4aa353acacbc0a7a10ccf022639e513769de09272_w640_q70.webp",
      "contributions": "",
      "summary": "GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting",
      "mindmap": ""
    },
    {
      "title": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving",
      "authors": "Pengxuan Yang, Ben Lu, Zhongpu Xia, Chao Han, Yinfeng Gao, Teng Zhang, Kun Zhan, XianPeng Lang, Yupeng Zheng, Qichao Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19133",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/95928df29dd1d6db8d9cc4946cd472f8722d19286d4bfa47dd919093172f5948_w640_q70.webp",
      "contributions": "",
      "summary": "WorldRFT: Latent World Model Planning with Reinforcement Fine-Tuning for Autonomous Driving",
      "mindmap": ""
    },
    {
      "title": "AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction",
      "authors": "Ruikai Li, Xinrun Li, Mengwei Xie, Hao Shan, Shoumeng Qiu, Xinyuan Chang, Yizhe Fan, Feng Xiong, Han Jiang, Yilong Ren, Haiyang Yu, Mu Xu, Yang Long, Varun Ojha, Zhiyong Cui",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19150",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aaaf65665b46ca412dac28df472cffd8faf9ff9cf5ae8945c3d13bbd791e3c88_w640_q70.webp",
      "contributions": "",
      "summary": "AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction",
      "mindmap": ""
    },
    {
      "title": "OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions",
      "authors": "Wendong Bu, Kaihang Pan, Yuze Lin, Jiacheng Li, Kai Shen, Wenqiao Zhang, Juncheng Li, Jun Xiao, Siliang Tang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19159",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5b0a157ededdb6863038e7005281e71ab57fdcaf96eba9b30702558285b1ed2e_w640_q70.webp",
      "contributions": "",
      "summary": "OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions",
      "mindmap": ""
    },
    {
      "title": "CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation",
      "authors": "Dazhen Deng, Sen Yang, Yuchen He, Yuan Tian, Yingcai Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19173",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4924c981ea9ee58543e98b565a1e8fca0e8ce65bad7464a4041f4c5ffc756918_w640_q70.webp",
      "contributions": "",
      "summary": "CycleChart: A Unified Consistency-Based Learning Framework for Bidirectional Chart Understanding and Generation",
      "mindmap": ""
    },
    {
      "title": "PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements",
      "authors": "Marios Thoma, Zenonas Theodosiou, Harris Partaourides, Vassilis Vassiliades, Loizos Michael, Andreas Lanitis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19190",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a96c05d7294228300f6794f5bbda416f7d11e2b7c58157c93485f6f6ba933ade_w640_q70.webp",
      "contributions": "",
      "summary": "PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements",
      "mindmap": ""
    },
    {
      "title": "InvCoSS: Inversion-driven Continual Self-supervised Learning in Medical Multi-modal Image Pre-training",
      "authors": "Zihao Luo, Shaohao Rui, Zhenyu Tang, Guotai Wang, Xiaosong Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19213",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c90f4c95929734535ee76b711f8c5c23d3fb093f6e99a2ba0fea1407c036c313_w640_q70.webp",
      "contributions": "",
      "summary": "InvCoSS: Inversion-driven Continual Self-supervised Learning in Medical Multi-modal Image Pre-training",
      "mindmap": ""
    },
    {
      "title": "HippMetric: A skeletal-representation-based framework for cross-sectional and longitudinal hippocampal substructural morphometry",
      "authors": "Na Gao, Chenfei Ye, Yanwu Yang, Anqi Li, Zhengbo He, Li Liang, Zhiyuan Liu, Xingyu Hao, Ting Ma, Tengfei Guo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19214",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e03fac6363ee3c142ec7100157cae8cca39cfd14894e066b219b9ace6425131_w640_q70.webp",
      "contributions": "",
      "summary": "HippMetric: A skeletal-representation-based framework for cross-sectional and longitudinal hippocampal substructural morphometry",
      "mindmap": ""
    },
    {
      "title": "Towards Minimal Fine-Tuning of VLMs",
      "authors": "Tiange Luo, Lajanugen Logeswaran, Jaekyeom Kim, Justin Johnson, Honglak Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19219",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/420c22fc5b580697b05c90c8fbf0115c2cea8a82f971a19f125c0456c3405309_w640_q70.webp",
      "contributions": "",
      "summary": "Towards Minimal Fine-Tuning of VLMs",
      "mindmap": ""
    },
    {
      "title": "From Pixels to Predicates Structuring urban perception with scene graphs",
      "authors": "Yunlong Liu, Shuyang Li, Pengyuan Liu, Yu Zhang, Rudi Stouffs",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19221",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/937a8aa60cbd8f9c8016e6a36a9941d4f6c5a8af94206f4d30a8f70eac213a2a_w640_q70.webp",
      "contributions": "",
      "summary": "From Pixels to Predicates Structuring urban perception with scene graphs",
      "mindmap": ""
    },
    {
      "title": "VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis",
      "authors": "Meng Chu, Senqiao Yang, Haoxuan Che, Suiyun Zhang, Xichen Zhang, Shaozuo Yu, Haokun Gui, Zhefan Rao, Dandan Tu, Rui Liu, Jiaya Jia",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19243",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/189a642ba98e396da7852dfdc680536e07570eee6c947c570c81130b8f827924_w640_q70.webp",
      "contributions": "",
      "summary": "VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis",
      "mindmap": ""
    },
    {
      "title": "Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study",
      "authors": "Carla Crivoi, Radu Tudor Ionescu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19253",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d70a17b56ecda8937a9bd488550e13c9d9833f836ea7a0e16e024c8b5e950c5b_w640_q70.webp",
      "contributions": "",
      "summary": "Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study",
      "mindmap": ""
    },
    {
      "title": "3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory",
      "authors": "Xinyang Song, Libin Wang, Weining Wang, Zhiwei Li, Jianxin Sun, Dandan Zheng, Jingdong Chen, Qi Li, Zhenan Sun",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19271",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/be02858b75b1adbf6d43c4ad543ed82d07789eb9f6bc5adee21b6c3bc2806e5d_w640_q70.webp",
      "contributions": "",
      "summary": "3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory",
      "mindmap": ""
    },
    {
      "title": "Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context",
      "authors": "Kyungwon Cho, Hanbyul Joo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19283",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eeffc3f64bbeee78f48fdd58e03a511d7a87a0d92465f1e6a49a7e082ffac166_w640_q70.webp",
      "contributions": "",
      "summary": "Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context",
      "mindmap": ""
    },
    {
      "title": "Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation",
      "authors": "Ivan DeAndres-Tame, Chengwei Ye, Ruben Tolosana, Ruben Vera-Rodriguez, Shiqi Yu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19275",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d12748fd351c66664603a2df32d39e3ed4e526013cc4fc7f328dc001dea6e6a2_w640_q70.webp",
      "contributions": "",
      "summary": "Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation",
      "mindmap": ""
    },
    {
      "title": "RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning",
      "authors": "Jun Li, Zikun Chen, Haibo Chen, Shuo Chen, Jian Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19300",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4417f7bedbf213cebde42bcbbe520cb32e68e60eec8444be79b07344e3b47020_w640_q70.webp",
      "contributions": "",
      "summary": "RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning",
      "mindmap": ""
    },
    {
      "title": "Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing",
      "authors": "Xu Zhang, Junyao Ge, Yang Zheng, Kaitai Guo, Jimin Liang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19302",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05bd6cba707b6fa3a38c669135a7bae69d4a297f430743568b2ec5258f28c554_w640_q70.webp",
      "contributions": "",
      "summary": "Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing",
      "mindmap": ""
    },
    {
      "title": "MAGIC: Achieving Superior Model Merging via Magnitude Calibration",
      "authors": "Yayuan Li, Jian Zhang, Jintao Guo, Zihan Cheng, Lei Qi, Yinghuan Shi, Yang Gao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19320",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7204250ada52cfa8e70ee24634b9a58aab0085ac9d5854e5c672a585fb92a0a6_w640_q70.webp",
      "contributions": "",
      "summary": "MAGIC: Achieving Superior Model Merging via Magnitude Calibration",
      "mindmap": ""
    },
    {
      "title": "Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations",
      "authors": "Marica Muffoletto, Uxio Hermida, Charlène Mauger, Avan Suinesiaputra, Yiyang Xu, Richard Burns, Lisa Pankewitz, Andrew D McCulloch, Steffen E Petersen, Daniel Rueckert, Alistair A Young",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19316",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2169c1336bc2f9517102921ae45f8684aaa37f9e051cbaf98dee4fa4fd0d1da4_w640_q70.webp",
      "contributions": "",
      "summary": "Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations",
      "mindmap": ""
    },
    {
      "title": "MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture",
      "authors": "Hui Li, Jiayue Lyu, Fu-Yun Wang, Kaihui Cheng, Siyu Zhu, Jingdong Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19311",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/333652b938a5ecac737d2dfcea2bae93a2f072e15ac1c354ce9b2da1931714cc_w640_q70.webp",
      "contributions": "",
      "summary": "MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture",
      "mindmap": ""
    },
    {
      "title": "Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome",
      "authors": "Moamal Fadhil Abdul, Jonas Bruun Hubrechts, Thomas Martini Jørgensen, Emil Hovad",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19327",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7996df4a8c51ad78b9cf6e38d68e28803f1e0e8374ec48e5258b82ca6726f085_w640_q70.webp",
      "contributions": "",
      "summary": "Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome",
      "mindmap": ""
    },
    {
      "title": "DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis",
      "authors": "Yueting Zhu, Yuehao Song, Shuai Zhang, Wenyu Liu, Xinggang Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19331",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09dc308a5637ec12a3c4efc8fea104a4feed83c0622643ceb02afbe80f82e58c_w640_q70.webp",
      "contributions": "",
      "summary": "DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis",
      "mindmap": ""
    },
    {
      "title": "GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis",
      "authors": "Siyuan Mei, Yan Xia, Fuxin Fan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19336",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da8f1615b6e017ea5a7845dd78d938dfd6175022b2518294c8d72a1f24b2befa_w640_q70.webp",
      "contributions": "",
      "summary": "GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis",
      "mindmap": ""
    },
    {
      "title": "ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining",
      "authors": "Zhenyang Huang, Xiao Yu, Yi Zhang, Decheng Wang, Hang Ruan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19354",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fe8395ab00c36c5c60a6a3cce591a67d23872c3d730ee745491ee87f86bf9993_w640_q70.webp",
      "contributions": "",
      "summary": "ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining",
      "mindmap": ""
    },
    {
      "title": "Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization",
      "authors": "Zhongwei Chen, Hai-Jun Rong, Zhao-Xu Yang, Guoqi Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19365",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e5753a79bc7c6daf06150580604d523fe5ee37173b2bd7e625292862bfda957a_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization",
      "mindmap": ""
    },
    {
      "title": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation",
      "authors": "Hongwei Fan, Hang Dai, Jiyao Zhang, Jinzhou Li, Qiyang Yan, Yujie Zhao, Mingju Gao, Jinghang Wu, Hao Tang, Hao Dong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19390",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dbf1e9d4003951dba668a306d606416b006b75689695b9667afa58a3ba76ea89_w640_q70.webp",
      "contributions": "",
      "summary": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation",
      "mindmap": ""
    },
    {
      "title": "DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition",
      "authors": "Yueyao Chen, Kai-Ni Wang, Dario Tayupo, Arnaud Huaulm'e, Krystel Nyangoh Timoh, Pierre Jannin, Qi Dou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19387",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d09473d7af5f8db0e2099dcd5a18592d023dddc412cada060f4f05596921ff9_w640_q70.webp",
      "contributions": "",
      "summary": "DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition",
      "mindmap": ""
    },
    {
      "title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
      "authors": "Yujie Zhao, Hongwei Fan, Di Chen, Shengcong Chen, Liliang Chen, Xiaoqi Li, Guanghui Ren, Hao Dong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19402",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d3585a78a8a01def680be454d4e0abbbbcf24961bd331f0f18d30d4fa2409128_w640_q70.webp",
      "contributions": "",
      "summary": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface",
      "mindmap": ""
    },
    {
      "title": "Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis",
      "authors": "Xiaoming Zhang, Chunli Li, Jiacheng Hao, Yuan Gao, Danyang Tu, Jianyi Qiao, Xiaoli Yin, Le Lu, Ling Zhang, Ke Yan, Yang Hou, Yu Shi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19415",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d823776fb42e2083e867607e00670f7745cc6a59e773b0a990924b705cbb63ce_w640_q70.webp",
      "contributions": "",
      "summary": "Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis",
      "mindmap": ""
    },
    {
      "title": "MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation",
      "authors": "Fei Ge, Ying Huang, Jie Liu, Guixuan Zhang, Zhi Zeng, Shuwu Zhang, Hu Guan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19438",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b0c5f1cc5f1508ff7f3dac04d0e138fe9c30202bb132ef0c4725622de8caf6c5_w640_q70.webp",
      "contributions": "",
      "summary": "MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation",
      "mindmap": ""
    },
    {
      "title": "dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models",
      "authors": "Yi Xin, Siqi Luo, Qi Qin, Haoxing Chen, Kaiwen Zhu, Zhiwei Zhang, Yangfan He, Rongchao Zhang, Jinbin Bai, Shuo Cao, Bin Fu, Junjun He, Yihao Liu, Yuewen Cao, Xiaohong Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19433",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e45156e486a1bc9ed274f1c77ca361208ca741b5f093796d4e95c9d983ba1b84_w640_q70.webp",
      "contributions": "",
      "summary": "dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models",
      "mindmap": ""
    },
    {
      "title": "D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning",
      "authors": "Evelyn Zhang, Fufu Yu, Aoqi Wu, Zichen Wen, Ke Yan, Shouhong Ding, Biqing Qi, Linfeng Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19443",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b276a091b11e8296a53fd51f1160b3e57aa375c543150f9b00e1f03505b59fa1_w640_q70.webp",
      "contributions": "",
      "summary": "D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning",
      "mindmap": ""
    },
    {
      "title": "Sign Language Recognition using Parallel Bidirectional Reservoir Computing",
      "authors": "Nitin Kumar Singh, Arie Rachmad Syulistyo, Yuichiro Tanaka, Hakaru Tamukoh",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19451",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/554347db8f25765f90ed0de383600422b9f8c928bbd223207e7df0e299098ce4_w640_q70.webp",
      "contributions": "",
      "summary": "Sign Language Recognition using Parallel Bidirectional Reservoir Computing",
      "mindmap": ""
    },
    {
      "title": "Emotion-Director: Bridging Affective Shortcut in Emotion-Oriented Image Generation",
      "authors": "Guoli Jia, Junyao Hu, Xinwei Long, Kai Tian, Kaiyan Zhang, KaiKai Zhao, Ning Ding, Bowen Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19479",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc7b54a9b2b78f7914a0108337ac26045e762e0d69fe4d901c3f358805a1715b_w640_q70.webp",
      "contributions": "",
      "summary": "Emotion-Director: Bridging Affective Shortcut in Emotion-Oriented Image Generation",
      "mindmap": ""
    },
    {
      "title": "Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration",
      "authors": "Shaochen Bi, Yuting He, Weiming Wang, Hao Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19486",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb129fee202e72128dcf5cd341766c1e138b9f80593cb33ba80612c8b99c355a_w640_q70.webp",
      "contributions": "",
      "summary": "Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration",
      "mindmap": ""
    },
    {
      "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
      "authors": "Ziyang Song, Zelin Zang, Zuyao Chen, Xusheng Liang, Dong Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19512",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d14f0ebe1771704c1788aacd2e3db88e7c3b990ef8113a061936422f9bb95889_w640_q70.webp",
      "contributions": "",
      "summary": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
      "mindmap": ""
    },
    {
      "title": "FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors",
      "authors": "Georgios Voulgaris",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19504",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b44fc34cd61f2154ab40ee342bff48a158e28f41d4da554eeb8a5175ecd28b9d_w640_q70.webp",
      "contributions": "",
      "summary": "FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors",
      "mindmap": ""
    },
    {
      "title": "A Convolutional Neural Deferred Shader for Physics Based Rendering",
      "authors": "Zhuo He, Yingdong Ru, Qianying Liu, Paul Henderson, Nicolas Pugeault",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19522",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cedf1c27957e3514f227025357a6a77d6e8cc183d8b8443ff380a0198a2b27e_w640_q70.webp",
      "contributions": "",
      "summary": "A Convolutional Neural Deferred Shader for Physics Based Rendering",
      "mindmap": ""
    },
    {
      "title": "SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates",
      "authors": "Chi Zhang, Braedon Gunn, Andrew M. Read-Fuller",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19534",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2dda742639cf4aa80038c7f5a4810b7a39a34acaa57f71b1646ebafce80e27e8_w640_q70.webp",
      "contributions": "",
      "summary": "SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates",
      "mindmap": ""
    },
    {
      "title": "Multi-Modal Soccer Scene Analysis with Masked Pre-Training",
      "authors": "Marc Peral, Guillem Capellera, Luis Ferraz, Antonio Rubio, Antonio Agudo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19528",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ca38cc1fee364a3f5a5912d1d267f89ff10d2261c0295e00eeca8f554a12e3d5_w640_q70.webp",
      "contributions": "",
      "summary": "Multi-Modal Soccer Scene Analysis with Masked Pre-Training",
      "mindmap": ""
    },
    {
      "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
      "authors": "Moritz Böhle, Amélie Royer, Juliette Marrie, Edouard Grave, Patrick Pérez",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19535",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b20ae57f683974a6959785befd45010cb87dde73d9ccc345f16e11af116951e_w640_q70.webp",
      "contributions": "",
      "summary": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
      "mindmap": ""
    },
    {
      "title": "StoryMem: Multi-shot Long Video Storytelling with Memory",
      "authors": "Kaiwen Zhang, Liming Jiang, Angtian Wang, Jacob Zhiyuan Fang, Tiancheng Zhi, Qing Yan, Hao Kang, Xin Lu, Xingang Pan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19539",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a7a12b30bf051906255b90290fece33407e3691deaf21452976c48e1d52f546_w640_q70.webp",
      "contributions": "",
      "summary": "StoryMem: Multi-shot Long Video Storytelling with Memory",
      "mindmap": ""
    },
    {
      "title": "ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars",
      "authors": "Ziqiao Peng, Yi Chen, Yifeng Ma, Guozhen Zhang, Zhiyao Sun, Zixiang Zhou, Youliang Zhang, Zhengguang Zhou, Zhaoxin Fan, Hongyan Liu, Yuan Zhou, Qinglin Lu, Jun He",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19546",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a48fa101dd9fce1c329f7df974385f6ec3d96f2ce0111f84aa53a212233bd01_w640_q70.webp",
      "contributions": "",
      "summary": "ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars",
      "mindmap": ""
    },
    {
      "title": "BabyFlow: 3D modeling of realistic and expressive infant faces",
      "authors": "Antonia Alomar, Mireia Masias, Marius George Linguraru, Federico M. Sukno, Gemma Piella",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19560",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f4bdcd1bc9dd6e1141b9b283f24968fbb4a40dc22257a719c5e16fbac178220f_w640_q70.webp",
      "contributions": "",
      "summary": "BabyFlow: 3D modeling of realistic and expressive infant faces",
      "mindmap": ""
    },
    {
      "title": "No Data? No Problem: Robust Vision-Tabular Learning with Missing Values",
      "authors": "Marta Hasny, Laura Daza, Keno Bressem, Maxime Di Folco, Julia Schnabel",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19602",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/750e85c1bc605ca0702dd33fe3c21b8f7a30131f1bd4c08b119b6cd868aa6e98_w640_q70.webp",
      "contributions": "",
      "summary": "No Data? No Problem: Robust Vision-Tabular Learning with Missing Values",
      "mindmap": ""
    },
    {
      "title": "KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning",
      "authors": "Eric Zimmermann, Harley Wiltzer, Justin Szeto, David Alvarez-Melis, Lester Mackey",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19605",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8876f353bd3b6bc215381979aff16556177f0d7ca7e5b9cf3aa366d3fbd3c21d_w640_q70.webp",
      "contributions": "",
      "summary": "KerJEPA: Kernel Discrepancies for Euclidean Self-Supervised Learning",
      "mindmap": ""
    },
    {
      "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
      "authors": "Jiaqi Peng, Wenzhe Cai, Yuqiang Yang, Tai Wang, Yuan Shen, Jiangmiao Pang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19629",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c789e71ef34ed4c917c96dfb4b01a72b2ebbbaf1b31aedd33c19890927a051c_w640_q70.webp",
      "contributions": "",
      "summary": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry",
      "mindmap": ""
    },
    {
      "title": "MapTrace: Scalable Data Generation for Route Tracing on Maps",
      "authors": "Artemis Panagopoulou, Aveek Purohit, Achin Kulshrestha, Soroosh Yazdani, Mohit Goyal",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19609",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d7cc350a173ade821ef309bec6e6155158ae8332042895a8d6f45396c8b70c06_w640_q70.webp",
      "contributions": "",
      "summary": "MapTrace: Scalable Data Generation for Route Tracing on Maps",
      "mindmap": ""
    },
    {
      "title": "Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment",
      "authors": "Da Tan, Michael Beck, Christopher P. Bidinosti, Robert H. Gulden, Christopher J. Henry",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19632",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2f66f3db49949bf517b4441efbd4d2391e38803e0ee9434c98db47da1306caf_w640_q70.webp",
      "contributions": "",
      "summary": "Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment",
      "mindmap": ""
    },
    {
      "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
      "authors": "Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi, Tushar Mungle, Pabitra Mitra",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19663",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cd268a98c5baac8b164ba733c255159614de6b969e8c6dd3f943fa74d98e5a1b_w640_q70.webp",
      "contributions": "",
      "summary": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
      "mindmap": ""
    },
    {
      "title": "4D Gaussian Splatting as a Learned Dynamical System",
      "authors": "Arnold Caleb Asiimwe, Carl Vondrick",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19648",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a72a5f6b1ca2d0116e20e704886e46a58e929e4b87b6d69b18361c69528e49d_w640_q70.webp",
      "contributions": "",
      "summary": "4D Gaussian Splatting as a Learned Dynamical System",
      "mindmap": ""
    },
    {
      "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
      "authors": "Hanyang Kong, Xingyi Yang, Xiaoxu Zheng, Xinchao Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19678",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4f113f5d19fa4ca45dd649c8c074bdcd96d439b1640d1340c9f10070d3b5a62_w640_q70.webp",
      "contributions": "",
      "summary": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
      "mindmap": ""
    },
    {
      "title": "Over++: Generative Video Compositing for Layer Interaction Effects",
      "authors": "Luchao Qi, Jiaye Wu, Jun Myeong Choi, Cary Phillips, Roni Sengupta, Dan B Goldman",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19661",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e4b3add32e80abbfca0b3aa657bd70035f1f8f7db466fff302f7edb85698647_w640_q70.webp",
      "contributions": "",
      "summary": "Over++: Generative Video Compositing for Layer Interaction Effects",
      "mindmap": ""
    },
    {
      "title": "Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning",
      "authors": "Mojtaba Safari, Shansong Wang, Vanessa L Wildman, Mingzhe Hu, Zach Eidex, Chih-Wei Chang, Erik H Middlebrooks, Richard L.J Qiu, Pretesh Patel, Ashesh B. Jania, Hui Mao, Zhen Tian, Xiaofeng Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19676",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/342e0f596d7de60763df762d3410c257d5603f80f4a044d27f87795d68b9c1dd_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning",
      "mindmap": ""
    },
    {
      "title": "Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models",
      "authors": "Zixuan Ye, Quande Liu, Cong Wei, Yuanxing Zhang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhan Luo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19686",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/843c501d9e520248d2e1a28597703cd3b54542449f629d50d194d54e77fa641e_w640_q70.webp",
      "contributions": "",
      "summary": "Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models",
      "mindmap": ""
    },
    {
      "title": "VA-$π$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
      "authors": "Xinyao Liao, Qiyuan He, Kai Xu, Xiaoye Qu, Yicong Li, Wei Wei, Angela Yao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19680",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce1823fb4399aafb9a993608f6bc9e7070ee1def4476bd9868d19ab0d1633442_w640_q70.webp",
      "contributions": "",
      "summary": "VA-$π$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation",
      "mindmap": ""
    },
    {
      "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
      "authors": "Weichen Fan, Haiwen Diao, Quan Wang, Dahua Lin, Ziwei Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19693",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/546a4d6b5b3d2bcc5adcfd25b27a121f46760fd0f9e85b7b4c89467641629c03_w640_q70.webp",
      "contributions": "",
      "summary": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
      "mindmap": ""
    },
    {
      "title": "Zero-shot Reconstruction of In-Scene Object Manipulation from Video",
      "authors": "Dixuan Lin, Tianyou Wang, Zhuoyang Pan, Yufu Wang, Lingjie Liu, Kostas Daniilidis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19684",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/967b97ff200c76a7e13dbcc9b4b2be1132807200b91f7312ad2eb4984f48bb88_w640_q70.webp",
      "contributions": "",
      "summary": "Zero-shot Reconstruction of In-Scene Object Manipulation from Video",
      "mindmap": ""
    },
    {
      "title": "A curated UK rain radar data set for training and benchmarking nowcasting models",
      "authors": "Viv Atureta, Rifki Priansyah Jasin, Stefan Siegert",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17924",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/71fd337b26c3bfd8670295386b9f2b4df184043caa1e2c1b8a39442f6cdb1c15_w640_q70.webp",
      "contributions": "",
      "summary": "A curated UK rain radar data set for training and benchmarking nowcasting models",
      "mindmap": ""
    },
    {
      "title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
      "authors": "Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao, Julius Richter, Sanyuan Chen, Matt Le, Piotr Dollár, Christoph Feichtenhofer, Ann Lee, Wei-Ning Hsu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19687",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de3d411e351c730f5a4f6bcbb2b3a9ec59b568b77417127cf865aadf04270b18_w640_q70.webp",
      "contributions": "",
      "summary": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
      "mindmap": ""
    },
    {
      "title": "Disentangled representations via score-based variational autoencoders",
      "authors": "Benjamin S. H. Lyo, Eero P. Simoncelli, Cristina Savin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17127",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c4fff8035bd7179107df39681970450e10d3f516b687330e2caedbac602c68b_w640_q70.webp",
      "contributions": "",
      "summary": "Disentangled representations via score-based variational autoencoders",
      "mindmap": ""
    },
    {
      "title": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
      "authors": "Mingrui Wu, Zhaozhi Wang, Fangjinhua Wang, Jiaolong Yang, Marc Pollefeys, Tong Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19683",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7452b3f045d2d058f3a16f8690c6b4f168b374381f31024557431e4ca6b30cf_w640_q70.webp",
      "contributions": "",
      "summary": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs",
      "mindmap": ""
    },
    {
      "title": "Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models",
      "authors": "Pablo Ruiz-Ponce, Sergio Escalera, José García-Rodríguez, Jiankang Deng, Rolandos Alexandros Potamias",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19692",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c358cce0c6057dff9e9db7532908f886d71d12899237ce5cdd1ce4b783108cd6_w640_q70.webp",
      "contributions": "",
      "summary": "Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models",
      "mindmap": ""
    },
    {
      "title": "CytoDINO: Risk-Aware and Biologically-Informed Adaptation of DINOv3 for Bone Marrow Cytomorphology",
      "authors": "Aziz Muminov, Anne Pham",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17930",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7391864933852f8b3d970d31b51ddd7af845e9cf2ef6c035d939e6f6cca13967_w640_q70.webp",
      "contributions": "",
      "summary": "CytoDINO: Risk-Aware and Biologically-Informed Adaptation of DINOv3 for Bone Marrow Cytomorphology",
      "mindmap": ""
    },
    {
      "title": "Standardized Evaluation of Automatic Methods for Perivascular Spaces Segmentation in MRI -- MICCAI 2024 Challenge Results",
      "authors": "Yilei Wu, Yichi Zhang, Zijian Dong, Fang Ji, An Sen Tan, Gifford Tan, Sizhao Tang, Huijuan Chen, Zijiao Chen, Eric Kwun Kei Ng, Jose Bernal, Hang Min, Ying Xia, Ines Vati, Liz Cooper, Xiaoyu Hu, Yuchen Pei, Yutao Ma, Victor Nozais, Ami Tsuchida, Pierre-Yves Hervé, Philippe Boutinaud, Marc Joliot, Junghwa Kang, Wooseung Kim, Dayeon Bak, Rachika E. Hamadache, Valeriia Abramova, Xavier Lladó, Yuntao Zhu, Zhenyu Gong, Xin Chen, John McFadden, Pek Lan Khong, Roberto Duarte Coello, Hongwei Bran Li, Woon Puay Koh, Christopher Chen, Joanna M. Wardlaw, Maria del C. Valdés Hernández, Juan Helen Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18197",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/010f615acca3968f6c9a01a9f1e16da3691c03b0a86b7fb021c3877735493e22_w640_q70.webp",
      "contributions": "",
      "summary": "Standardized Evaluation of Automatic Methods for Perivascular Spaces Segmentation in MRI -- MICCAI 2024 Challenge Results",
      "mindmap": ""
    },
    {
      "title": "SLIM: Semantic-based Low-bitrate Image compression for Machines by leveraging diffusion",
      "authors": "Hyeonjin Lee, Jun-Hyuk Kim, Jong-Seok Lee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18200",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9c567930d6d4ee43e33b9cf92ff8599d4af08021295eca7c47d016de7557640_w640_q70.webp",
      "contributions": "",
      "summary": "SLIM: Semantic-based Low-bitrate Image compression for Machines by leveraging diffusion",
      "mindmap": ""
    },
    {
      "title": "Selective Phase-Aware Training of nnU-Net for Robust Breast Cancer Segmentation in Multi-Center DCE-MRI",
      "authors": "Beyza Zayim, Aissiou Ikram, Boukhiar Naima",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19225",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ddf6be7e15b5d6602d6e9aa40624abefc0e83cbee9e90302c334bd468ba1ea9_w640_q70.webp",
      "contributions": "",
      "summary": "Selective Phase-Aware Training of nnU-Net for Robust Breast Cancer Segmentation in Multi-Center DCE-MRI",
      "mindmap": ""
    },
    {
      "title": "Deep Learning for Primordial $B$-mode Extraction",
      "authors": "Eric Guzman, Joel Meyers",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19577",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9bbecd5a2eae70e7ea81960a62c36c37f9e7805f682cebee98cb7ab81bd67249_w640_q70.webp",
      "contributions": "",
      "summary": "Deep Learning for Primordial $B$-mode Extraction",
      "mindmap": ""
    },
    {
      "title": "Rethinking Coupled Tensor Analysis for Hyperspectral Super-Resolution: Recoverable Modeling Under Endmember Variability",
      "authors": "Meng Ding, Xiao Fu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19489",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c4dd05c6080abf0291c4f46eccdfa18752df5e2fee1aad694dbdd4d50e61cd3_w640_q70.webp",
      "contributions": "",
      "summary": "Rethinking Coupled Tensor Analysis for Hyperspectral Super-Resolution: Recoverable Modeling Under Endmember Variability",
      "mindmap": ""
    },
    {
      "title": "Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior",
      "authors": "Ziqian Huang, Boxiao Yu, Siqi Li, Savas Ozdemir, Sangjin Bae, Jae Sung Lee, Guobao Wang, Kuang Gong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19584",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/35afd74d755844e3f7dd7c04116e75ba37dbf5e148c533c0e847939d30f5c6fc_w640_q70.webp",
      "contributions": "",
      "summary": "Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior",
      "mindmap": ""
    },
    {
      "title": "Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)",
      "authors": "Niclas Griesshaber, Jochen Streb",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19675",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d49754318d6e9803642e66f7555bb2551be305239532d52cb9f0b2b55048ad6b_w640_q70.webp",
      "contributions": "",
      "summary": "Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)",
      "mindmap": ""
    },
    {
      "title": "AVM: Towards Structure-Preserving Neural Response Modeling in the Visual Cortex Across Stimuli and Individuals",
      "authors": "Qi Xu, Shuai Gong, Xuming Ran, Haihua Luo, Yangfan Hu",
      "institution": "Dalian University of Technology, National University of Singapore, University of Jyvaskyla, Zhejiang University of Finance and Economics",
      "link": "https://arxiv.org/pdf/2512.16948",
      "code": null,
      "tags": [
        "computational neuroscience",
        "Vision Transformer",
        "modular subnetworks",
        "condition-aware adaptation",
        "structure-preserving framework",
        "neural response modeling"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces the Adaptive Visual Model (AVM), a structure-preserving framework that uses a frozen Vision Transformer encoder to capture stable visual features and separate modulation paths to adapt to stimulus and individual variations. It demonstrates improved generalization and interpretability in modeling mouse V1 neural responses, outperforming prior models in predictive correlation and explained variance across different experimental conditions.",
      "mindmap": ""
    },
    {
      "title": "Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories",
      "authors": "Chayan Jain, Rishant Sharma, Archit Garg, Ishan Bhanuka, Pratik Narang, Dhruv Kumar",
      "institution": "BITS Pilani",
      "link": "https://arxiv.org/pdf/2512.16954",
      "code": null,
      "tags": [
        "multi-modal inference",
        "visual anchoring",
        "asset-first mechanism",
        "temporal bridge",
        "diffusion models",
        "large language model (LLM)",
        "text-to-video (T2V)",
        "character consistency",
        "multi-stage pipeline"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a multi-stage pipeline for generating long, character-consistent video stories. It uses an LLM to create a script, a text-to-image model to design consistent character visuals as anchors, and a video generation model to synthesize scenes individually, with a temporal bridge linking them. The method's necessity is validated by showing that removing visual anchoring causes a catastrophic drop in character consistency, and cultural biases in current models are also analyzed.",
      "mindmap": ""
    },
    {
      "title": "Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections",
      "authors": "Adrian Straker, Paul Magdon, Marco Zullich, Maximilian Freudenberg, Christoph Kleinn, Johannes Breidenbach, Stefano Puliti, Nils Nölke",
      "institution": "University of Applied Sciences and Art (HAWK), University of Groningen, University of Göttingen, Norwegian Institute of Bioeconomy Research (NIBIO)",
      "link": "https://arxiv.org/pdf/2512.16950",
      "code": null,
      "tags": [
        "computer vision",
        "YOLOv8",
        "Finer-CAM",
        "saliency maps",
        "cross-validation",
        "TLS point cloud projections"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a novel method that links Finer-CAM explanations to structural segments in TLS point cloud projections to evaluate which features drive tree species classification using YOLOv8 models. The analysis of saliency maps reveals that models primarily rely on crown features for classification, with stem features being more important for certain species, and that the models' perception of species similarity aligns with human expert judgment. The results underscore the need for explainable AI to understand model decision processes and build confidence in predictions.",
      "mindmap": ""
    },
    {
      "title": "Comparison of deep learning models: CNN and VGG-16 in identifying pornographic content",
      "authors": "Reza Chandra, Adang Suhendra, Lintang Yuniar Banowosari, Prihandoko",
      "institution": "Gunadarma University",
      "link": "https://arxiv.org/pdf/2512.16947",
      "code": null,
      "tags": [
        "others",
        "convolutional neural network",
        "VGG-16",
        "deep learning",
        "image classification"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f77d0c91314abfde67968e7d31c4f0423abf8cdee43f2eaa3b2d640c1e5984a3_w640_q70.webp",
      "contributions": "",
      "summary": "This paper compares two deep learning models, a custom CNN and the VGG-16 architecture, for the task of identifying pornographic image content. The study found that a CNN model with specific hyperparameters (50 epochs, learning rate 0.001) achieved a higher accuracy of 94.87% compared to the VGG-16 model, concluding that the CNN was more effective for fast and accurate detection.",
      "mindmap": ""
    },
    {
      "title": "V-Agent: An Interactive Video Search System Using Vision-Language Models",
      "authors": "SunYoung Park, Jong-Hyeon Lee, Youngjune Kim, Daegyu Sung, Younghyun Yu, Young-rok Cha, Jeongho Ju",
      "institution": "NC AI, Kakao, Korea Advanced Institute of Science and Technology (KAIST)",
      "link": "https://arxiv.org/pdf/2512.16925",
      "code": null,
      "tags": [
        "multi-modal inference",
        "vision-language model",
        "fine-tuning",
        "retrieval vector",
        "re-ranking",
        "multi-agent system"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e827044257e239766415ac9500a06f7ddb67bfc47c517c041d42cc61ac33ad18_w640_q70.webp",
      "contributions": "",
      "summary": "V-Agent is a multi-agent video search system that fine-tunes a vision-language model with a small video preference dataset and enhances it with a retrieval vector to embed video frames and audio transcriptions into a shared multimodal space. It uses three agents—routing, search, and chat—to refine searches and interact with users, achieving state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark.",
      "mindmap": ""
    },
    {
      "title": "InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression",
      "authors": "Haotian Ye, Qiyuan He, Jiaqi Han, Puheng Li, Jiaojiao Fan, Zekun Hao, Fitsum Reda, Yogesh Balaji, Huayu Chen, Sheng Liu, Angela Yao, James Zou, Stefano Ermon, Haoxiang Wang, Ming-Yu Liu",
      "institution": "NVIDIA, Stanford University, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.16975",
      "code": null,
      "tags": [
        "multi-modal training",
        "discrete video tokenization",
        "transformer-based adaptive compressor",
        "evidence lower bound (ELBO)",
        "information-theoretic compression",
        "adaptive tokenization"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da83017a41e69234160f2c416b8a94507a537a66fd8a745a6bafc49c06817395_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces InfoTok, a principled framework for adaptive discrete video tokenization based on information theory, using a novel ELBO-based algorithm and a transformer-based adaptive compressor. It achieves state-of-the-art compression by allocating tokens according to informational richness, saving 20% of tokens without performance loss and outperforming prior heuristic approaches.",
      "mindmap": ""
    },
    {
      "title": "A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos",
      "authors": "Mohammed Irfan Kurpath, Jaseel Muhammad Kaithakkodan, Jinxing Zhou, Sahal Shaji Mullappilly, Mohammad Almansoori, Noor Ahsan, Beknur Kalmakhanbet, Sambal Shikhar, Rishabh Lalla, Jean Lahoud, Mariette Awad, Fahad Shahbaz Khan, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal",
      "institution": "Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), American University of Beirut, Linköping University",
      "link": "https://arxiv.org/pdf/2512.16978",
      "code": null,
      "tags": [
        "multi-modal inference",
        "LongShOTBench",
        "LongShOTAgent",
        "agentic tool use",
        "omni-modal reasoning",
        "benchmark",
        "open-ended questions",
        "graded rubric",
        "iterative refinement"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces LongShOTBench, a diagnostic benchmark for long-form video understanding that features open-ended questions and tasks requiring multimodal reasoning and agentic tool use. It also presents LongShOTAgent, an agentic system that analyzes videos through preprocessing, search, and iterative refinement. The results show a significant performance gap, with state-of-the-art models achieving only up to 52.95%, highlighting the difficulty of real-world long-form video understanding.",
      "mindmap": ""
    },
    {
      "title": "Endo-SemiS: Towards Robust Semi-Supervised Image Segmentation for Endoscopic Video",
      "authors": "Hao Li, Daiwei Lu, Xing Yao, Nicholas Kavoussi, Ipek Oguz",
      "institution": "Vanderbilt University, Vanderbilt University Medical Center",
      "link": "https://arxiv.org/pdf/2512.16977",
      "code": null,
      "tags": [
        "medical image segmentation",
        "semi-supervised learning",
        "cross-supervision",
        "uncertainty-guided pseudo-label",
        "joint pseudo-label supervision",
        "mutual learning",
        "spatiotemporal corrective network"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1c2b434a364e1db7fb30807eeda54f751647c7d8a35266a66353a42fbc7d8ed6_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces Endo-SemiS, a semi-supervised framework for endoscopic video segmentation that employs cross-supervision, uncertainty-guided pseudo-labeling, joint supervision, and mutual learning between two networks, along with a separate spatiotemporal corrective network. It demonstrates superior performance over state-of-the-art methods on kidney stone and polyp segmentation tasks when labeled data is limited.",
      "mindmap": ""
    },
    {
      "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation",
      "authors": "Chiao-An Yang, Ryo Hachiuma, Sifei Liu, Subhashree Radhakrishnan, Raymond A. Yeh, Yu-Chiang Frank Wang, Min-Hung Chen",
      "institution": "NVIDIA, Purdue University",
      "link": "https://arxiv.org/pdf/2512.17012",
      "code": null,
      "tags": [
        "multi-modal training",
        "4D-RGPT",
        "Perceptual 4D Distillation (P4D)",
        "R4D-Bench",
        "region-level prompting",
        "4D Video Question Answering",
        "multimodal LLM"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cc967334b04824c1dceb3e282d12da869887b61ee193194bf210064648df8377_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces 4D-RGPT, a multimodal LLM enhanced via a Perceptual 4D Distillation (P4D) framework to improve 4D (3D spatial + temporal) understanding from video inputs. It also proposes the R4D-Bench benchmark for evaluating region-level 4D reasoning. The method shows notable performance improvements on both existing and the new benchmark.",
      "mindmap": ""
    },
    {
      "title": "FORMSpoT: A Decade of Tree-Level, Country-Scale Forest Monitoring",
      "authors": "Martin Schwartz, Fajwel Fogel, Nikola Besic, Damien Robert, Louis Geist, Jean-Pierre Renaud, Jean-Matthieu Monnet, Clemens Mosig, Cédric Vega, Alexandre d'Aspremont, Loic Landrieu, Philippe Ciais",
      "institution": "Laboratoire des Sciences du Climat et de l’Environnement (LSCE), École Normale Supérieure – PSL, Université Gustave Eiffel, Université de Lorraine, University of Zurich, École des Ponts, Office National des Forêts, Université Grenoble Alpes, Institute of Earth System Science and Remote Sensing, Leipzig",
      "link": "https://arxiv.org/pdf/2512.17021",
      "code": null,
      "tags": [
        "others",
        "hierarchical transformer model (PVTv2)",
        "airborne laser scanning (ALS)",
        "spatio-temporal total variation denoising",
        "co-registration",
        "SPOT-6/7 composites"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces FORMSpoT, a method that uses a hierarchical transformer model trained on ALS data to derive high-resolution forest canopy height maps from SPOT satellite time series, combined with a post-processing pipeline for robust change detection. It demonstrates that this approach significantly outperforms existing products in detecting small-scale forest disturbances, enabling tree-level monitoring at a national scale. The results highlight the importance of very-high-resolution satellite data for accurate forest carbon loss quantification and monitoring under climate change.",
      "mindmap": ""
    },
    {
      "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation",
      "authors": "Min-Jung Kim, Jeongho Kim, Hoiyeong Jin, Junha Hyung, Jaegul Choo",
      "institution": "KAIST",
      "link": "https://arxiv.org/pdf/2512.17040",
      "code": null,
      "tags": [
        "computer vision",
        "infinite homography warping",
        "video diffusion model",
        "data augmentation",
        "camera-controlled video generation"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/984284ddd81cfd062ac9ad33ace38b4ec632b0ff9087f0906272c5b5d34e4175_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces InfCam, a framework that uses infinite homography warping to encode 3D camera rotations in a 2D latent space of a video diffusion model, avoiding depth estimation errors. It also employs a data augmentation pipeline to create diverse camera trajectories for training. The method demonstrates improved camera-pose accuracy and visual fidelity compared to existing approaches.",
      "mindmap": ""
    },
    {
      "title": "Interpretable Similarity of Synthetic Image Utility",
      "authors": "Panagiota Gatoula, George Dimas, Dimitris K. Iakovidis",
      "institution": "University of Thessaly",
      "link": "https://arxiv.org/pdf/2512.17080",
      "code": null,
      "tags": [
        "medical imaging",
        "Interpretable Utility Similarity (IUS)",
        "generalized neural additive models",
        "synthetic data evaluation",
        "deep learning",
        "clinical decision support"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes an interpretable measure called Interpretable Utility Similarity (IUS) to assess the similarity between synthetic and real medical images for deep learning-based clinical decision support. IUS uses generalized neural additive models to explain utility based on clinically relevant image features. Experiments show that selecting synthetic images with high IUS can improve classification performance by up to 54.6% across various medical imaging modalities.",
      "mindmap": ""
    },
    {
      "title": "DGH: Dynamic Gaussian Hair",
      "authors": "Junying Wang, Yuanlu Xu, Edith Tretschk, Ziyan Wang, Anastasia Ianina, Aljaz Bozic, Ulrich Neumann, Tony Tung",
      "institution": "University of Southern California, Meta Reality Labs Research",
      "link": "https://arxiv.org/pdf/2512.17094",
      "code": null,
      "tags": [
        "computer vision",
        "graphics",
        "dynamic Gaussian hair",
        "coarse-to-fine model",
        "strand-guided optimization",
        "differentiable rendering",
        "3D Gaussian splatting"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9785766bd69495591006a354948b34ae7d717dcbb9e1e10f4bd690b1e6c2569_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces Dynamic Gaussian Hair (DGH), a data-driven framework that learns hair dynamics and appearance using a coarse-to-fine motion model and a strand-guided 3D Gaussian representation. It enables photorealistic novel-view synthesis of hair under head motion without manual physics tuning. DGH provides a scalable, generalizable alternative to traditional simulation-based hair modeling.",
      "mindmap": ""
    },
    {
      "title": "Predictive Modeling of Maritime Radar Data Using Transformer Architecture",
      "authors": "Bjorna Qesaraku, Jan Steckel",
      "institution": "Not explicitly provided in the given text.",
      "link": "https://arxiv.org/pdf/2512.17098",
      "code": null,
      "tags": [
        "spatiotemporal forecasting",
        "transformer architecture",
        "predictive modeling",
        "radar frame prediction",
        "spatiotemporal sequence forecasting"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/694341ad0b22f753337bbaead36e9cd3e845d080e2995764ba4a45a74113f8d0_w640_q70.webp",
      "contributions": "",
      "summary": "This survey paper reviews predictive modeling approaches for maritime radar data, with a specific focus on the application of transformer architectures for spatiotemporal sequence forecasting. It concludes that while transformers have been used for AIS trajectory and sonar frame prediction, their use for maritime radar frame prediction remains an unexplored research gap, identifying a clear direction for future work.",
      "mindmap": ""
    },
    {
      "title": "PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics",
      "authors": "Nan Zhou, Huandong Wang, Jiahao Li, Yang Li, Xiao-Ping Zhang, Yong Li, Xinlei Chen",
      "institution": "- **link:** https://arxiv.org/pdf/2512.17152",
      "link": "https://arxiv.org/pdf/2512.17152",
      "code": null,
      "tags": [],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4068b512da42fb271d02b97ac83087d5dbffa83b4904b964cddb5a1ffcc6de68_w640_q70.webp",
      "contributions": "",
      "summary": "",
      "mindmap": ""
    },
    {
      "title": "SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction",
      "authors": "Puyang Wang, Pengfei Guo, Keyi Chai, Jinyuan Zhou, Daguang Xu, Shanshan Jiang",
      "institution": "Johns Hopkins University, NVIDIA",
      "link": "https://arxiv.org/pdf/2512.17137",
      "code": null,
      "tags": [
        "others",
        "deep unrolled model",
        "Restormer",
        "learned coil sensitivity map estimator",
        "sampling-aware weighted data consistency",
        "universal conditioning",
        "progressive cascade expansion training"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13a0f6637b571493e14f364092f2d39a108d211bbddd588a88df25d1db4a8e1c_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces SDUM, a scalable deep unrolled model for universal MRI reconstruction that integrates a Restormer-based reconstructor, learned coil sensitivity estimation, and sampling-aware data consistency. It demonstrates predictable performance scaling with model depth and achieves state-of-the-art results across diverse clinical MRI protocols without task-specific fine-tuning. The work establishes a practical framework for a single, universally applicable reconstruction model in clinical environments.",
      "mindmap": ""
    },
    {
      "title": "Pro-Pose: Unpaired Full-Body Portrait Synthesis via Canonical UV Maps",
      "authors": "Sandeep Mishra, Yasamin Jafarian, Andreas Lugmayr, Yingwei Li, Varsha Ramakrishnan, Srivatsan Varadharajan, Alan C. Bovik, Ira Kemelmacher-Shlizerman",
      "institution": "The University of Texas at Austin, Google",
      "link": "https://arxiv.org/pdf/2512.17143",
      "code": null,
      "tags": [
        "computer vision",
        "canonical UV maps",
        "SMPL-X poses",
        "novel view synthesis",
        "multi image finetuning",
        "unpaired dataset"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba0fc26561d25ebdc31e5c5b7c54d8ebaf35995e25f7717c3b45565ea557166_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes Pro-Pose, a method for synthesizing professional full-body portraits from a single casual photo by transforming the person into a canonical UV map space and leveraging SMPL-X poses for reposing. This approach uses unpaired datasets and personalizes results through multi-image fine-tuning. It concludes that the method effectively preserves identity while generating high-quality, reposed avatars in varied poses and lighting.",
      "mindmap": ""
    },
    {
      "title": "Text-Conditioned Background Generation for Editable Multi-Layer Documents",
      "authors": "Taewon Kang, Joseph K J, Chris Tensmeyer, Jihyung Kil, Wanrong Zhu, Ming C. Lin, Vlad I. Morariu",
      "institution": "University of Maryland at College Park, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.17151",
      "code": null,
      "tags": [
        "diffusion inference",
        "latent masking",
        "Automated Readability Optimization (ARO)",
        "summarization-and-instruction process",
        "multi-layer composition",
        "WCAG 2.2"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b3923a8122c7fd309cbab0c07d054d2af0f590d1779870be58f212682ecfafbb_w640_q70.webp",
      "contributions": "",
      "summary": "This paper presents a training-free framework for generating and editing document backgrounds using a diffusion model, ensuring text readability through latent masking and Automated Readability Optimization (ARO). It maintains multi-page thematic consistency via a summarization-and-instruction process and treats documents as editable multi-layer compositions. The method produces visually coherent, readable, and thematically aligned documents, bridging generative AI with practical design workflows.",
      "mindmap": ""
    },
    {
      "title": "Can Synthetic Images Serve as Effective and Efficient Class Prototypes?",
      "authors": "Dianxing Shi, Dingjie Fu, Yuqiao Liu, Jun Wang",
      "institution": "Beijing Research Institute of Uranium Geology, Huazhong University of Science and Technology, The Hong Kong University of Science and Technology (Guangzhou), Great Bay University",
      "link": "https://arxiv.org/pdf/2512.17160",
      "code": null,
      "tags": [
        "multi-modal inference",
        "CLIP",
        "large language model",
        "diffusion model",
        "zero-shot classification",
        "visual prototypes",
        "prompt generation"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6907c7ddddf9ad9be39d226d2f021022e7cd01e6d45d58e12d3f8d9a6af7d1df_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes LGCLIP, a framework that uses an LLM to generate class-specific prompts for a diffusion model to create synthetic images as visual prototypes, enabling zero-shot classification with only a visual encoder. It eliminates the need for annotated image-text pairs and reduces model complexity. Experiments show LGCLIP is effective and efficient, establishing a new lightweight paradigm for classification.",
      "mindmap": ""
    },
    {
      "title": "ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching",
      "authors": "Qi Zhang, Yuxu Chen, Lei Deng, Lili Shen",
      "institution": "Sichuan University",
      "link": "https://arxiv.org/pdf/2512.17178",
      "code": null,
      "tags": [
        "multi-modal inference",
        "CLIP",
        "semantic refinement mechanism",
        "local token-patch alignment",
        "attribute-object binding",
        "compositional image-text matching"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea9a6c4b5b64bc9b1ed96212005ee492c3fd2ae615fc413250d7a6ef7c3bc712_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes ABE-CLIP, a training-free method to enhance attribute-object binding in CLIP models. It uses a semantic refinement mechanism to improve text token embeddings and a local token-patch alignment strategy to compute image-text similarity. Experiments show the method significantly improves compositional matching performance, even surpassing some trained approaches.",
      "mindmap": ""
    },
    {
      "title": "It is not always greener on the other side: Greenery perception across demographics and personalities in multiple cities",
      "authors": "Matias Quintana, Fangqi Liu, Jussi Torkko, Youlong Gu, Xiucheng Liang, Yujun Hou, Koichi Ito, Yihan Zhu, Mahmoud Abdelrahman, Tuuli Toivonen, Yi Lu, Filip Biljecki",
      "institution": "Singapore-ETH Centre, City University of Hong Kong, University of Helsinki, National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.17186",
      "code": null,
      "tags": [
        "urban perception analysis",
        "Green View Index (GVI)",
        "street view imagery",
        "pairwise ratings",
        "perception survey"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper analyzes discrepancies between objective greenery measurements (like Green View Index from street view imagery) and subjective human perceptions collected via surveys across five countries. The main finding is that demographic and personality factors do not significantly influence perception, but the location where people live is a key factor, suggesting cultural and environmental experiences shape how urban greenery is observed.",
      "mindmap": ""
    },
    {
      "title": "Globally Optimal Solution to the Generalized Relative Pose Estimation Problem using Affine Correspondences",
      "authors": "Zhenbao Yu, Banglei Guan, Shunkun Liang, Zibin Liu, Yang Shang, Qifeng Yu",
      "institution": "National University of Defense Technology, Wuhan University",
      "link": "https://arxiv.org/pdf/2512.17188",
      "code": null,
      "tags": [
        "computer vision",
        "affine correspondences",
        "global optimization",
        "polynomial eigenvalue solver",
        "generalized essential matrix",
        "generalized camera model"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2a684e0cd1c8feea699186681976c3ac00a45866e1c3b726129daedbf69d23e_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes a globally optimal solver for estimating the generalized relative pose of multi-camera systems using affine correspondences and a known vertical direction. The method decouples rotation and translation, formulates a cost function, and solves it via polynomial eigenvalue techniques. Experimental results on synthetic and real data show the method outperforms state-of-the-art approaches in accuracy.",
      "mindmap": ""
    },
    {
      "title": "Anatomical Region-Guided Contrastive Decoding: A Plug-and-Play Strategy for Mitigating Hallucinations in Medical VLMs",
      "authors": "Xiao Liang, Chenxi Liu, Zhi Ma, Di Wang, Bin Jing, Quan Wang, Yuanyuan Shi",
      "institution": "Xidian University, Capital Medical University, the Ninth Medical Center of the Chinese PLA General Hospital",
      "link": "https://arxiv.org/pdf/2512.17189",
      "code": null,
      "tags": [
        "multi-modal inference",
        "contrastive decoding",
        "anatomical mask",
        "token re-weighting",
        "attention re-weighting",
        "logits re-weighting"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9ca8b7c91013f45d10742514e49b941b478dcfd8c7f8216c3572f801dc9f1f9_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces Anatomical Region-Guided Contrastive Decoding (ARCD), a plug-and-play method that uses an anatomical mask to guide a three-tiered contrastive decoding process at the token, attention, and logits levels to reduce hallucinations in Medical Vision-Language Models. Experiments across multiple medical imaging datasets show that ARCD effectively improves regional understanding, reduces factually incorrect outputs, and enhances diagnostic accuracy.",
      "mindmap": ""
    },
    {
      "title": "Fose: Fusion of One-Step Diffusion and End-to-End Network for Pansharpening",
      "authors": "Kai Liu, Zeli Lin, Weibo Wang, Linghe Kong, Yulun Zhang",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.17202",
      "code": null,
      "tags": [
        "diffusion inference",
        "one-step distillation",
        "lightweight ensemble blocks",
        "four-stage training",
        "pansharpening",
        "diffusion model",
        "end-to-end network"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes Fose, a lightweight network for pansharpening that fuses a one-step diffusion model and an end-to-end network using a novel four-stage training strategy. It uses one-step distillation to compress a diffusion model's inference from 50 steps to 1 and integrates it with an E2E model via lightweight ensemble blocks. The method achieves better performance than state-of-the-art approaches and a 7.42x speedup compared to the baseline diffusion model.",
      "mindmap": ""
    },
    {
      "title": "Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs",
      "authors": "Rujiao Long, Yang Li, Xingyao Zhang, Weixun Wang, Tianqianjin Lin, Xi Zhao, Yuchi Xu, Wenbo Su, Junchi Yan, Bo Zheng",
      "institution": "Alibaba Group, Shanghai Jiao Tong University, Zhejiang University",
      "link": "https://arxiv.org/pdf/2512.17206",
      "code": null,
      "tags": [
        "post-training",
        "latent modulation",
        "variational autoencoder",
        "reinforcement learning",
        "supervised fine-tuning",
        "reasoning strategies",
        "controllable exploration"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7bfcb7d02fac2a6f117f80403719551786b6b7722d6b86bf1338ef9e18ddda6b_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces Reasoning Palette, a framework that uses a latent variable from a VAE to modulate a model's reasoning trajectory via prepended token prefixes, enabling diverse strategic exploration. It shows that this approach improves exploration efficiency in RL training and leads to consistent performance gains over standard methods on reasoning benchmarks.",
      "mindmap": ""
    },
    {
      "title": "CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency",
      "authors": "Xiao Liang, Yuxuan An, Di Wang, Jiawei Hu, Zhicheng Jiao, Bin Jing, Quan Wang",
      "institution": "Xidian University, Brown University, Capital Medical University",
      "link": "https://arxiv.org/pdf/2512.17213",
      "code": null,
      "tags": [
        "post-training",
        "reinforcement learning",
        "group relative policy optimization",
        "knowledge graph consistency",
        "entity-relation matching",
        "process supervision",
        "hard-example mining"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/75c86c0e9aa8fe8870d86c5f63baf1ccd89856e7434ece25e03ba384660733fc_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes CheXPO-v2, an alignment framework that uses a Knowledge Graph Consistency Reward for process supervision to reduce hallucinations in medical VLMs. It parses reasoning into structured triplets to penalize incoherent logic, outperforming prior methods on benchmarks with high data efficiency. The approach achieves state-of-the-art accuracy on MIMIC-CXR-VQA using only 5k samples.",
      "mindmap": ""
    },
    {
      "title": "Robust Scene Coordinate Regression via Geometrically-Consistent Global Descriptors",
      "authors": "Son Tung Nguyen, Tobias Fischer, Alejandro Fontan, Michael Milford",
      "institution": "Queensland University of Technology",
      "link": "https://arxiv.org/pdf/2512.17226",
      "code": null,
      "tags": [
        "computer vision",
        "scene coordinate regression",
        "global descriptors",
        "covisibility graphs",
        "contrastive loss",
        "batch-mining"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a method that learns global descriptors for visual localization by combining geometric structure and visual similarity, using a batch-mining strategy and modified contrastive loss to train without manual labels. This approach corrects errors from unreliable geometric constraints and improves disambiguation in large-scale environments. Experiments show significant gains in localization accuracy while maintaining computational and memory efficiency.",
      "mindmap": ""
    },
    {
      "title": "DAVE: A VLM Vision Encoder for Document Understanding and Web Agents",
      "authors": "Brandon Huang, Hang Hua, Zhuoran Yu, Trevor Darrell, Rogerio Feris, Roei Herzig",
      "institution": "MIT-IBM Watson AI Lab, UC Berkeley, University of Wisconsin–Madison",
      "link": "https://arxiv.org/pdf/2512.17221",
      "code": null,
      "tags": [
        "multi-modal training",
        "vision encoder",
        "self-supervised pretraining",
        "autoregressive pretraining",
        "model merging",
        "ensemble training",
        "SigLIP2"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/344c53941f8829675f0d24c7d720447ce983b23a243af49de527a0a89a83b47b_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces DAVE, a vision encoder for VLMs designed for document understanding and web agents. Its training pipeline uses self-supervised pretraining on unlabeled data followed by supervised autoregressive pretraining, enhanced by model-merging and ensemble techniques to improve compatibility and performance. Experiments show DAVE is an effective vision encoder for document and web applications.",
      "mindmap": ""
    },
    {
      "title": "Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning",
      "authors": "Siqi Yang, Zilve Gao, Haibo Qiu, Fanfan Liu, Peng Shi, Zhixiong Zeng, Qingmin Liao, Lin Ma",
      "institution": "Meituan, Tsinghua University",
      "link": "https://arxiv.org/pdf/2512.17227",
      "code": null,
      "tags": [
        "multi-modal training",
        "curriculum learning",
        "supervised fine-tuning",
        "reinforcement learning",
        "perception-grounded chain-of-thought",
        "pivotal perception reward"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68b925dc42674866f1a5d50b8321678a682bbb13a5906e16c5013dcca31b9aea_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes a two-stage curriculum framework to address visual forgetting in multimodal reasoning. It first builds an abstract reasoning backbone using text-only data and then uses reinforcement learning with a novel reward to teach the model a strategic policy for when to perceive visual information. This approach transforms the model into a more strategic and visually grounded reasoner.",
      "mindmap": ""
    },
    {
      "title": "Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing",
      "authors": "Xuyang Li, Chenyu Li, Danfeng Hong",
      "institution": "Southeast University, Aerospace Information Research Institute, Chinese Academy of Sciences, University of Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.17224",
      "code": null,
      "tags": [
        "remote sensing foundation model",
        "spectrum-independent tokenizer",
        "multi-scale adaptive patch embedding",
        "channel-wise self-supervised masking and reconstruction",
        "multi-scale semantic alignment"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed61cab861efd49f74d6151efc271fc265c8ca3f3dbd380d3539ee4626e62ea6_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes Any-Optical-Model (AOM), a universal foundation model for optical remote sensing designed to handle arbitrary band compositions and spatial resolutions. Its core innovations include a spectrum-independent tokenizer, multi-scale adaptive patch embedding, and a channel-wise self-supervised pretraining strategy. Experiments show AOM achieves state-of-the-art performance in challenging scenarios like band missing and cross-sensor/cross-resolution settings.",
      "mindmap": ""
    },
    {
      "title": "Video Detective: Seek Critical Clues Recurrently to Answer Question from Long Videos",
      "authors": "Henghui Du, Chang Zhou, Chunjie Zhang, Xi Chen, Di Hu",
      "institution": "Renmin University of China, Tencent PCG",
      "link": "https://arxiv.org/pdf/2512.17229",
      "code": null,
      "tags": [
        "multi-modal inference",
        "question-aware memory mechanism",
        "recurrent processing",
        "token compression",
        "memory tokens",
        "long video question-answering"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c8fa371c579b373deada655507be872a0843b939c4fd62131127a51ed2f82f2_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes VideoDetective, a method that uses a question-aware memory mechanism to recurrently process long videos by compressing sub-segments into special memory tokens, enabling efficient question-answering. This approach allows models with limited context length to handle long videos with reduced memory and time. Experimental results show it effectively seeks critical clues from massive video information.",
      "mindmap": ""
    },
    {
      "title": "Mitty: Diffusion-based Human-to-Robot Video Generation",
      "authors": "Yiren Song, Cheng Liu, Weijia Mao, Mike Zheng Shou",
      "institution": "National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.17253",
      "code": null,
      "tags": [
        "robotics and computer vision",
        "diffusion transformer",
        "in-context learning",
        "video generation",
        "human-to-robot translation",
        "bidirectional attention"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ece085e6946a833f7d094099001b4940be20339b6e48e0def90236df73a83e6d_w640_q70.webp",
      "contributions": "",
      "summary": "Mitty is a Diffusion Transformer model that uses in-context learning to directly convert human demonstration videos into robot-execution videos without intermediate representations. It leverages a pretrained video diffusion model and an automatic synthesis pipeline to address data scarcity. The method achieves state-of-the-art performance and strong generalization in robot learning from human observations.",
      "mindmap": ""
    },
    {
      "title": "AnyCXR: Human Anatomy Segmentation of Chest X-ray at Any Acquisition Position using Multi-stage Domain Randomized Synthetic Data with Imperfect Annotations and Conditional Joint Annotation Regularization Learning",
      "authors": "Dong Zifei, Wu Wenjie, Hao Jinkui, Chen Tianqi, Weng Ziqiao, Zhou Bo",
      "institution": "Northwestern University, Vanderbilt University, Shanxi Medical University, University of Sydney",
      "link": "https://arxiv.org/pdf/2512.17263",
      "code": null,
      "tags": [
        "medical imaging segmentation",
        "Multi-stage Domain Randomization (MSDR)",
        "Conditional Joint Annotation Regularization (CAR)",
        "synthetic data generation",
        "zero-shot generalization",
        "anatomical consistency"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes AnyCXR, a framework for chest X-ray segmentation that uses synthetic data generated via Multi-stage Domain Randomization and a Conditional Joint Annotation Regularization learning strategy to handle imperfect labels. It demonstrates strong zero-shot generalization to real-world X-rays across different views, enabling accurate multi-organ segmentation and improving downstream clinical tasks.",
      "mindmap": ""
    },
    {
      "title": "WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images",
      "authors": "Guoping Cai, Houjin Chen, Yanfeng Li, Jia Sun, Ziwei Chen, Qingzi Geng",
      "institution": "Beijing Jiaotong University",
      "link": "https://arxiv.org/pdf/2512.17278",
      "code": null,
      "tags": [
        "medical image segmentation",
        "wavelet-guided enhancement",
        "dual-attention feature fusion",
        "U-shaped Mamba architecture",
        "Wavelet-denoised High-Frequency-guided Feature (WHF)",
        "Dual Attention Feature Fusion (DAFF)"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes WDFFU-Mamba, a novel network for breast ultrasound image segmentation that integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. It demonstrates superior segmentation accuracy and robustness on public datasets, making it a promising tool for clinical applications.",
      "mindmap": ""
    },
    {
      "title": "Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge",
      "authors": "Zehui Lin, Luyi Han, Xin Wang, Ying Zhou, Yanming Zhang, Tianyu Zhang, Lingyun Bao, Shandong Wu, Dong Xu, Tao Tan, UUSIC25 Challenge Consortium",
      "institution": "Macao Polytechnic University, Netherlands Cancer Institute, Zhejiang Cancer Hospital, The First People’s Hospital of Hangzhou, University of Pittsburgh",
      "link": "https://arxiv.org/pdf/2512.17279",
      "code": null,
      "tags": [
        "multi-modal inference",
        "deep learning",
        "multi-task learning",
        "domain generalization",
        "segmentation",
        "classification",
        "Dice Similarity Coefficient",
        "Area Under the Curve"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper evaluates general-purpose deep learning models for multi-organ classification and segmentation in ultrasound imaging through the UUSIC25 challenge. The top model demonstrated high accuracy and efficiency across tasks using a single architecture. However, performance degraded on data from unseen institutions, highlighting the critical need for improved domain generalization before clinical deployment.",
      "mindmap": ""
    },
    {
      "title": "Vision-Language Model Guided Image Restoration",
      "authors": "Cuixin Yang, Rongkang Dong, Kin-Man Lam",
      "institution": "The Hong Kong Polytechnic University",
      "link": "https://arxiv.org/pdf/2512.17292",
      "code": null,
      "tags": [
        "multi-modal inference",
        "vision-language model",
        "CLIP",
        "diffusion model",
        "cross-attention",
        "LoRA fine-tuning",
        "degradation predictor"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes the VLMIR framework, which uses a vision-language model (like CLIP) to extract aligned visual and textual features from images and integrates them via cross-attention into a diffusion model for restoration. It concludes that this approach, leveraging linguistic priors for semantic coherence, achieves superior performance in universal and degradation-specific image restoration tasks.",
      "mindmap": ""
    },
    {
      "title": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration",
      "authors": "Fanpu Cao, Yaofo Chen, Zeng You, Wei Luo, Cen Chen",
      "institution": "South China University of Technology, South China Agricultural University, Pazhou Laboratory",
      "link": "https://arxiv.org/pdf/2512.17298",
      "code": null,
      "tags": [
        "diffusion inference",
        "feature caching",
        "selective computation",
        "constraint-aware scheduling",
        "temporal redundancy",
        "activation schedule"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/af7870aa86d8ec0f40bf8ee51c36b05d76a3b23aeb26f3f88d6835a77ed1a314_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes ProCache, a training-free framework to accelerate Diffusion Transformers (DiTs) by using dynamic feature caching. It introduces a constraint-aware caching pattern search to create non-uniform activation schedules and a selective computation module to mitigate error accumulation. Experiments show ProCache achieves significant speedups with minimal quality loss, outperforming prior caching methods.",
      "mindmap": ""
    },
    {
      "title": "Towards Pixel-Wise Anomaly Location for High-Resolution PCBA \\\\ via Self-Supervised Image Reconstruction",
      "authors": "Wuyi Liu, Le Jin, Junxian Yang, Yuanchao Yu, Zishuo Peng, Jinfeng Xu, Xianzhi Li, Jun Zhou",
      "institution": "Huazhong University of Science and Technology, Siemens AG, University of Electronic Science and Technology of China, Peking University",
      "link": "https://arxiv.org/pdf/2512.17296",
      "code": null,
      "tags": [
        "computer vision",
        "self-supervised learning",
        "image reconstruction",
        "anomaly detection",
        "SIR-Gate",
        "ROPS"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2d229cd3ccc08641839e524f99e7548770f7ccd524f2ac8dd0882a9248c5c358_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces HiSIR-Net, a self-supervised image reconstruction framework for pixel-wise anomaly localization on high-resolution PCBA images. It uses two novel modules—SIR-Gate to reduce reconstruction noise and ROPS for coherent patch selection—to achieve accurate defect detection with low false positives. The method demonstrates superior performance on a new dataset (SIPCBA-500) and public benchmarks while maintaining practical inference speed.",
      "mindmap": ""
    },
    {
      "title": "EMAG: Self-Rectifying Diffusion Sampling with Exponential Moving Average Guidance",
      "authors": "Ankit Yadav, Ta Duc Huy, Lingqiao Liu",
      "institution": "Australian Institute for Machine Learning, The University of Adelaide",
      "link": "https://arxiv.org/pdf/2512.17303",
      "code": null,
      "tags": [
        "diffusion models",
        "classifier-free guidance",
        "attention modification",
        "exponential moving average",
        "adaptive layer selection",
        "diffusion transformers"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7a085ffcd94c7d87433d4ff44b17cee0378a874440ce80df0ee2bca3a5e2171d_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes Exponential Moving Average Guidance (EMAG), a training-free inference method for diffusion transformers that adaptively modifies attention maps to generate challenging negative samples. This allows the model to correct fine-grained artifacts, improving image quality and human preference scores over standard guidance. The method is shown to be compatible with other advanced guidance techniques for further gains.",
      "mindmap": ""
    },
    {
      "title": "Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images",
      "authors": "Wenhao Yang, Yu Xia, Jinlong Huang, Shiyin Lu, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Yuanyu Wan, Lijun Zhang",
      "institution": "Nanjing University, Alibaba Group, Shanghai Jiao Tong University, Zhejiang University",
      "link": "https://arxiv.org/pdf/2512.17306",
      "code": null,
      "tags": [
        "multi-modal inference",
        "chain-of-thought",
        "multi-turn reasoning",
        "self-reflection",
        "redundancy-penalized policy optimization",
        "supervised fine-tuning",
        "reinforcement learning"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/689b25012cac991ff522614c103b42d1e5450440f1eed392ffaf92e6a3ff7c51_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes DRIM, a model that enhances multi-turn reasoning in vision-language models by integrating a self-reflective chain-of-thought with tool invocation. It uses a three-stage pipeline of data construction, supervised fine-tuning, and redundancy-penalized reinforcement learning to encourage reliable exploration and self-correction. Experiments show DRIM achieves superior performance on visual understanding benchmarks.",
      "mindmap": ""
    },
    {
      "title": "CodeDance: A Dynamic Tool-integrated MLLM for Executable Visual Reasoning",
      "authors": "Qi Song, Honglin Li, Yingchen Yu, Haoyi Zhou, Lin Yang, Song Bai, Qi She, Zilong Huang, Yunqing Zhao",
      "institution": "Beihang University, Westlake University, ByteDance",
      "link": "https://arxiv.org/pdf/2512.17312",
      "code": null,
      "tags": [
        "multi-modal inference",
        "executable code",
        "tool orchestration",
        "reinforcement learning",
        "visual reasoning",
        "reward shaping"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/51476eaff011f154f477005e3f785b652c69da471d13c3f8a556338b397999d8_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces CodeDance, a multimodal large language model that uses executable code to dynamically orchestrate multiple tools for visual reasoning. It employs a reinforcement learning reward to balance tool use, leading to adaptive and efficient reasoning. The method outperforms schema-driven and text-only baselines, and even surpasses advanced closed models like GPT-4o on various benchmarks.",
      "mindmap": ""
    },
    {
      "title": "MatLat: Material Latent Space for PBR Texture Generation",
      "authors": "Kyeongmin Yeo, Yunhong Min, Jaihoon Kim, Minhyuk Sung",
      "institution": "KAIST",
      "link": "https://arxiv.org/pdf/2512.17302",
      "code": null,
      "tags": [
        "diffusion training",
        "latent diffusion",
        "VAE fine-tuning",
        "material latent space",
        "patch-based regularization",
        "correspondence-aware attention"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/706f201e005d8752d9c73c8781f7323062cdfffadcfa51a8faf2f6cc621472c2_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes MatLat, a generative framework that fine-tunes a pretrained VAE to create a material latent space for generating high-quality PBR textures, addressing dataset scarcity and distribution shift issues. It introduces a patch-based regularization during VAE fine-tuning to preserve spatial locality between latent codes and image pixels, which is crucial for cross-view consistency. The method demonstrates improved PBR texture fidelity and achieves state-of-the-art performance, with each component being essential.",
      "mindmap": ""
    },
    {
      "title": "Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model",
      "authors": "SuBeen Lee, GilHan Park, WonJun Moon, Hyun Seok Seong, Jae-Pil Heo",
      "institution": "Sungkyunkwan University",
      "link": "https://arxiv.org/pdf/2512.17313",
      "code": null,
      "tags": [
        "vision-language models",
        "few-shot adaptation",
        "parameter-efficient fine-tuning",
        "auxiliary descriptive knowledge",
        "large language model",
        "non-parametric attention",
        "compositional knowledge",
        "instance-specific knowledge"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b2fe4006b163d54a1cba24642885d5db064fb85d1d01ab783d4dee1ada2fbc75_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces Auxiliary Descriptive Knowledge (ADK), a framework that enhances few-shot adaptation of vision-language models by using an LLM to generate offline descriptive prompts for each class. ADK enriches text representations through averaged compositional knowledge and a lightweight attention mechanism for instance-specific knowledge, improving classification without added inference cost. It consistently boosts existing parameter-efficient fine-tuning methods, achieving state-of-the-art performance across various scenarios.",
      "mindmap": ""
    },
    {
      "title": "A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs",
      "authors": "Yunkai Dang, Meiyi Zhu, Donghao Wang, Yizhuo Zhang, Jiacheng Yang, Qi Fan, Yuekun Yang, Wenbin Li, Feng Miao, Yang Gao",
      "institution": "Nanjing University",
      "link": "https://arxiv.org/pdf/2512.17319",
      "code": null,
      "tags": [
        "remote sensing",
        "multimodal evaluation",
        "RSHR-Bench",
        "adversarial filtering",
        "high-resolution imagery",
        "multimodal large language models",
        "visual question answering",
        "image captioning"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces RSHR-Bench, a super-high-resolution benchmark for evaluating multimodal large language models (MLLMs) in remote sensing. The benchmark uses adversarial filtering with strong LLMs and human verification to create tasks that reduce reliance on language priors and require genuine visual understanding. The evaluation reveals that existing models, including RS-specific ones, show significant performance gaps when handling ultra-high-resolution remote sensing imagery.",
      "mindmap": ""
    },
    {
      "title": "EMMA: Concept Erasure Benchmark with Comprehensive Semantic Metrics and Diverse Categories",
      "authors": "Lu Wei, Yuta Nakashima, Noa Garcia",
      "institution": "Osaka University",
      "link": "https://arxiv.org/pdf/2512.17320",
      "code": null,
      "tags": [
        "diffusion inference",
        "concept erasure",
        "text-to-image generation",
        "benchmark evaluation",
        "semantic metrics",
        "robustness testing",
        "bias analysis"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/203b001da1851854025aaa0d66f1fc1bc1e32821cd9e1295f8360da31919f60c_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces EMMA, a comprehensive benchmark for evaluating concept erasure techniques in text-to-image diffusion models. It tests five key dimensions across 12 metrics, including robustness to indirect prompts and bias. The main conclusion is that existing erasure methods struggle with implicit descriptions and visually similar concepts, and some can amplify gender and ethnicity bias.",
      "mindmap": ""
    },
    {
      "title": "Democratizing Pathology Co-Pilots: An Open Pipeline and Dataset for Whole-Slide Vision-Language Modelling",
      "authors": "Sander Moonemans, Sebastiaan Ram, Frédérique Meeuwsen, Carlijn Lems, Jeroen van der Laak, Geert Litjens, Francesco Ciompi",
      "institution": "Radboud University Medical Center",
      "link": "https://arxiv.org/pdf/2512.17326",
      "code": null,
      "tags": [
        "multi-modal training",
        "vision-language model",
        "instruction tuning",
        "visual question answering",
        "whole-slide images",
        "synthetic instruction generation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces Polysome, a tool for generating synthetic instructions, and uses it to create HISTAI-Instruct, a large instruction-tuning dataset from whole-slide images. It then trains a vision-language model called ANTONI-α on this dataset, which is shown to outperform MedGemma on tasks like tissue identification and differential diagnosis.",
      "mindmap": ""
    },
    {
      "title": "Rotterdam artery-vein segmentation (RAV) dataset",
      "authors": "Jose Vargas Quiros, Bart Liefers, Karin van Garderen, Jeroen Vermeulen, Eyened Reading Center, Caroline Klaver",
      "institution": "Erasmus University Medical Center, Radboud University Medical Center, Institute of Molecular and Clinical Ophthalmology, University of Basel",
      "link": "https://arxiv.org/pdf/2512.17322",
      "code": null,
      "tags": [
        "medical imaging",
        "color fundus images",
        "artery-vein segmentation",
        "machine learning algorithms",
        "connectivity validation",
        "custom annotation interface"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces the Rotterdam Artery-Vein segmentation dataset, created by sampling and annotating color fundus images from the Rotterdam Study using a custom interface with connectivity validation tools. The dataset includes diverse, high-quality artery-vein segmentation masks and varied image modalities to support the development of robust machine learning models for retinal vascular analysis. The main conclusion is that this heterogeneous dataset enables benchmarking and training of clinically applicable algorithms under real-world variability in image quality and acquisition.",
      "mindmap": ""
    },
    {
      "title": "DESSERT: Diffusion-based Event-driven Single-frame Synthesis via Residual Training",
      "authors": "Jiyun Kong, Jun-Hyuk Kim, Jong-Seok Lee",
      "institution": "Yonsei University, Chung-Ang University",
      "link": "https://arxiv.org/pdf/2512.17323",
      "code": null,
      "tags": [
        "diffusion training",
        "diffusion model",
        "residual training",
        "variational autoencoder",
        "event camera",
        "frame synthesis"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c9efc7d352cf45ecba0837aaa2807f3af645c0b85f2e4138960afaf786d151d9_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes DESSERT, a method for event-driven video frame synthesis using a diffusion model trained on inter-frame residuals and conditioned on event data. It employs a two-stage pipeline with an Event-to-Residual Alignment VAE and a diffusion model, enhanced by Diverse-Length Temporal augmentation. The method outperforms existing approaches in producing sharper and more temporally consistent frames.",
      "mindmap": ""
    },
    {
      "title": "Multi-level distortion-aware deformable network for omnidirectional image super-resolution",
      "authors": "Cuixin Yang, Rongkang Dong, Kin-Man Lam, Yuhang Zhang, Guoping Qiu",
      "institution": "The Hong Kong Polytechnic University, Guangzhou University, University of Nottingham",
      "link": "https://arxiv.org/pdf/2512.17343",
      "code": null,
      "tags": [
        "computer vision",
        "deformable attention",
        "dilated deformable convolution",
        "low-rank decomposition",
        "multi-level feature fusion"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/07c76dd9c20ef0f6f5224210965ff34e6934829551ddedeec3fc607a5c362844_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes a Multi-level Distortion-aware Deformable Network (MDDN) for omnidirectional image super-resolution, which uses parallel branches of deformable attention and dilated deformable convolutions to capture geometric distortions. It also employs a low-rank decomposition to reduce computational cost. Experiments show that MDDN outperforms existing state-of-the-art methods.",
      "mindmap": ""
    },
    {
      "title": "SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation",
      "authors": "Shihang Li, Zhiqiang Gong, Minming Ye, Yue Gao, Wen Yao",
      "institution": "Shanghai Jiao Tong University, Defense Innovation Institute Academy of Military Science, Intelligent Game and Decision Laboratory",
      "link": "https://arxiv.org/pdf/2512.17331",
      "code": null,
      "tags": [
        "computer vision",
        "attention-guided cooperative warping",
        "3D dense optical flow",
        "cross-attention",
        "3D keypoints",
        "confidence-guided fusion"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5df9fe07b541e5d7185f0fbd4b4122b096d1ab08ec897c73021adba9ad3835b5_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes SynergyWarpNet, a three-stage framework for neural portrait animation that combines explicit warping using 3D optical flow with reference-augmented correction via cross-attention and a confidence-guided fusion module. It aims to address the limitations of traditional warping and attention-based methods by integrating geometric alignment with semantic completion. The model achieves state-of-the-art performance on benchmark datasets for high-fidelity talking head synthesis.",
      "mindmap": ""
    },
    {
      "title": "Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection",
      "authors": "Chenming Zhou, Jiaan Wang, Yu Li, Lei Li, Juan Cao, Sheng Tang",
      "institution": "Institute of Computing Technology, Chinese Academy of Sciences, University of Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.17350",
      "code": null,
      "tags": [
        "computer vision",
        "pixel-level mapping",
        "high-frequency traces",
        "semantic bias",
        "cross-generator generalization"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/12f826d61dc8f5ba342ef7b8b646a6c2d30576e4692242121c7c5e8cc16f5f7d_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces a pixel-level mapping pre-processing step to disrupt pixel value distributions and break semantic shortcuts, forcing detectors to focus on generalizable high-frequency traces from image generation. This method significantly improves the cross-generator performance of state-of-the-art AI-generated image detectors, verifying that disrupting semantic cues is key to generalization.",
      "mindmap": ""
    },
    {
      "title": "Towards Deeper Emotional Reflection: Crafting Affective Image Filters with Generative Priors",
      "authors": "Peixuan Zhang, Shuchen Weng, Jiajun Tang, Si Li, Boxin Shi",
      "institution": "Beijing University of Posts and Telecommunications, Beijing Academy of Artificial Intelligence, Peking University",
      "link": "https://arxiv.org/pdf/2512.17376",
      "code": null,
      "tags": [
        "multi-modal inference",
        "affective image filter",
        "multi-modal transformer",
        "diffusion models",
        "emotional fidelity",
        "content consistency"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d7baeb5807dd11748bebc6c7cf63706b9492484a047cbe2a12cba4c75d153a1_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes AIF-D, an Affective Image Filter model that extends a multi-modal transformer baseline by leveraging generative priors from pre-trained large-scale diffusion models to reflect emotions from text into images. It demonstrates superior performance in content consistency and emotional fidelity compared to state-of-the-art methods and is more effective at evoking specific emotions according to user studies.",
      "mindmap": ""
    },
    {
      "title": "Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?",
      "authors": "Zabir Al Nazi, G M Shahariar, Abrar Hossain, Wei Peng",
      "institution": "University of California, Riverside, University of Dhaka, Stanford University",
      "link": "https://arxiv.org/pdf/2512.17394",
      "code": null,
      "tags": [
        "vision-language models",
        "CulturalToM-VQA",
        "visual question answering",
        "chain-of-thought prompting",
        "compositional chain-of-thought prompting",
        "false belief reasoning",
        "social desirability bias"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces CulturalToM-VQA, a benchmark dataset built via a VLM-assisted human-in-the-loop pipeline to evaluate cross-cultural Theory of Mind reasoning in Vision-Language Models. It finds that while newer VLMs show strong performance on explicit tasks, they systematically struggle with false belief reasoning, and their results may be inflated by social desirability bias rather than genuine visual understanding.",
      "mindmap": ""
    },
    {
      "title": "RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering",
      "authors": "Léo Butsanets, Charles Corbière, Julien Khlaut, Pierre Manceron, Corentin Dancette",
      "institution": "Raidium, Université de Paris Cité, Hôpital Européen Georges Pompidou, AP-HP, INSERM",
      "link": "https://arxiv.org/pdf/2512.17396",
      "code": null,
      "tags": [
        "multi-modal inference",
        "visual question answering",
        "vision-language models",
        "fine-tuning",
        "benchmark dataset",
        "CT",
        "MRI"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ed7cbd6c6a04ef8f9a23147e56923de1ca94a48cc51c20cfa98200b90baa146_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces RadImageNet-VQA, a large-scale CT and MRI dataset with expert-curated annotations for radiologic visual question answering, designed to evaluate vision-language models on tasks like abnormality detection and pathology identification. Experiments show that current models struggle with fine-grained pathology identification, especially in open-ended settings, and the dataset avoids linguistic shortcuts as models perform near-random without image inputs.",
      "mindmap": ""
    },
    {
      "title": "Beyond Occlusion: In Search for Near Real-Time Explainability of CNN-Based Prostate Cancer Classification",
      "authors": "Martin Krebs, Jan Obdržálek, Vít Musil, Tomáš Brázdil",
      "institution": "Masaryk University",
      "link": "https://arxiv.org/pdf/2512.17416",
      "code": null,
      "tags": [
        "others",
        "occlusion",
        "GradCAM++",
        "HiResCAM",
        "Composite-L",
        "CAM",
        "explainable AI",
        "convolutional neural networks"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper seeks a faster alternative to the occlusion method for explaining CNN-based prostate cancer classification. By establishing comparison criteria and metrics, the authors evaluate several single-pass explanation methods like GradCAM++ and HiResCAM. They identify a method that reduces explanation time by at least a factor of 10 without compromising output quality, facilitating faster model development and clinical adoption.",
      "mindmap": ""
    },
    {
      "title": "AIFloodSense: A Global Aerial Imagery Dataset for Semantic Segmentation and Understanding of Flooded Environments",
      "authors": "Georgios Simantiris, Konstantinos Bacharidis, Apostolos Papanikolaou, Petros Giannakakis, Costas Panagiotakis",
      "institution": "Hellenic Mediterranean University, Institute of Computer Science, FORTH",
      "link": "https://arxiv.org/pdf/2512.17432",
      "code": null,
      "tags": [
        "computer vision",
        "disaster management",
        "semantic segmentation",
        "visual question answering",
        "image classification",
        "deep learning",
        "aerial imagery"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces AIFloodSense, a global aerial imagery dataset for flood detection, supporting tasks like semantic segmentation, image classification, and visual question answering. It establishes baseline benchmarks using state-of-the-art deep learning models. The main conclusion is that the dataset's global diversity and multi-task support advance the development of robust, domain-generalized AI tools for climate resilience and disaster assessment.",
      "mindmap": ""
    },
    {
      "title": "Xiaomi MiMo-VL-Miloco Technical Report",
      "authors": "Jiaze Li, Jingyang Chen, Yuxun Qu, Jianzhong Ju, Zhenbo Luo, Jian Luan, Shijie Xu, Zhenru Lin, Junyou Zhu, Boshen Xu, Wenhui Tan, Pei Fu",
      "institution": "Xiaomi",
      "link": "https://arxiv.org/pdf/2512.17436",
      "code": null,
      "tags": [
        "multi-modal training",
        "supervised fine-tuning",
        "reinforcement learning",
        "Group Relative Policy Optimization",
        "chain-of-thought supervision",
        "token-budget-aware reasoning",
        "quantization"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5131a57d5cef340803d1dd4e55e39db8e4fc7a8b7925e87579be976c079279c_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces MiMo-VL-Miloco-7B, a vision-language model specialized for smart-home understanding, built via a two-stage training pipeline combining supervised fine-tuning and reinforcement learning. The model achieves leading performance on home-scenario tasks like gesture recognition and also shows gains on general multimodal and language reasoning benchmarks. The authors conclude that targeted home-scenario training enhances activity understanding and can improve text-only reasoning with minimal trade-offs on other tasks.",
      "mindmap": ""
    },
    {
      "title": "LangDriveCTRL: Natural Language Controllable Driving Scene Editing with Multi-modal Agents",
      "authors": "Yun He, Francesco Pittaluga, Ziyu Jiang, Matthias Zwicker, Manmohan Chandraker, Zaid Tasneem",
      "institution": "University of Maryland, College Park, NEC Labs America, UC San Diego",
      "link": "https://arxiv.org/pdf/2512.17445",
      "code": null,
      "tags": [
        "multi-modal inference",
        "scene graph",
        "agentic pipeline",
        "video diffusion",
        "3D scene decomposition",
        "natural language control",
        "trajectory generation"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0247e2a8c71687baf6433034fd351bc4574e288c71df25dcc17678102ec16d05_w640_q70.webp",
      "contributions": "",
      "summary": "LangDriveCTRL is a framework that edits real-world driving videos using natural language instructions. It decomposes scenes into 3D graphs and uses an agentic pipeline with specialized modules for object grounding, behavior editing, and review, followed by video diffusion for refinement. The method achieves significantly higher instruction alignment and realism compared to previous state-of-the-art approaches.",
      "mindmap": ""
    },
    {
      "title": "MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation",
      "authors": "Jon Muhovič, Janez Perš",
      "institution": "University of Ljubljana",
      "link": "https://arxiv.org/pdf/2512.17450",
      "code": null,
      "tags": [
        "multi-modal training",
        "multimodal semantic segmentation",
        "deep neural network",
        "robust training strategies",
        "synchronized sensor data",
        "daytime training for nighttime performance"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6eb14eac17bf3aa4e9ce145e08ce4eae06c5adaaf8ccf31ca3fa25a7f3835fa0_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces MULTIAQUA, a multimodal maritime dataset with synchronized RGB, thermal, IR, and LIDAR data, and proposes robust training strategies for multimodal semantic segmentation. The core method enables training a deep neural network using only daytime images to achieve reliable performance in challenging conditions like near-complete darkness. The main conclusion is that this approach simplifies data acquisition and annotation while maintaining robust scene interpretation for unmanned surface vehicles.",
      "mindmap": ""
    },
    {
      "title": "3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework",
      "authors": "Tobias Sautter, Jan-Niklas Dihlmann, Hendrik P.A. Lensch",
      "institution": "University of Tübingen",
      "link": "https://arxiv.org/pdf/2512.17459",
      "code": null,
      "tags": [
        "multi-modal inference",
        "3D scene reconstruction",
        "generative models",
        "differentiable optimization",
        "compositional framework",
        "camera recovery",
        "spatial optimization"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a86f0bc63e56b2eb3e9a7eb63d3a03a4eff7de9fc52eba3ee24958d82a3a3b98_w640_q70.webp",
      "contributions": "",
      "summary": "3D-RE-GEN is a compositional framework that reconstructs a single image into textured 3D objects and a background by integrating models for detection, reconstruction, and placement, using generative models for occluded objects and a novel 4-DoF optimization for layout alignment. It achieves state-of-the-art performance in single-image 3D scene reconstruction, producing coherent, modifiable scenes suitable for VFX and game development.",
      "mindmap": ""
    },
    {
      "title": "TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis",
      "authors": "Almustapha A. Wakili, Adamu Hussaini, Abubakar A. Musa, Woosub Jung, Wei Yu",
      "institution": "Towson University",
      "link": "https://arxiv.org/pdf/2512.17488",
      "code": null,
      "tags": [
        "others",
        "federated learning",
        "digital twin",
        "Vision Transformer",
        "ViT-UNet",
        "privacy-preserving AI",
        "brain tumor segmentation"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67c12280225e186b761a6952a367a2b042a6fcd1886ac481e9bc15f5f81ee64a_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes TwinSegNet, a federated learning framework that uses a hybrid ViT-UNet model and personalized digital twins for brain tumor segmentation without sharing raw data. It achieves high accuracy on heterogeneous MRI datasets, demonstrating that privacy can be preserved without sacrificing segmentation performance in multi-institutional settings.",
      "mindmap": ""
    },
    {
      "title": "Validation of Diagnostic Artificial Intelligence Models for Prostate Pathology in a Middle Eastern Cohort",
      "authors": "Peshawa J. Muhammad Ali, Navin Vincent, Saman S. Abdulla, Han N. Mohammed Fadhl, Anders Blilie, Kelvin Szolnoky, Julia Anna Mielcarz, Xiaoyi Ji, Nita Mulliqi, Abdulbasit K. Al-Talabani, Kimmo Kartasalo",
      "institution": "Koya University, Karolinska Institutet",
      "link": "https://arxiv.org/pdf/2512.17499",
      "code": null,
      "tags": [
        "medical diagnostics",
        "artificial intelligence",
        "digital pathology",
        "prostate cancer",
        "Gleason grading",
        "external validation",
        "Cohen's quadratically weighted kappa",
        "compact scanner"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This study externally validated AI models for prostate cancer diagnosis and Gleason grading using a Middle Eastern cohort from Iraq. The AI models demonstrated performance comparable to pathologists and showed high consistency across different digital slide scanners, including low-cost compact models. The findings support the global adoption of AI in pathology, particularly in under-represented regions with limited resources.",
      "mindmap": ""
    },
    {
      "title": "LumiCtrl : Learning Illuminant Prompts for Lighting Control in Personalized Text-to-Image Models",
      "authors": "Muhammad Atif Butt, Kai Wang, Javier Vazquez-Corral, Joost Van De Weijer",
      "institution": "Computer Vision Center, Universitat Autònoma de Barcelona, City University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.17489",
      "code": null,
      "tags": [
        "diffusion training",
        "illuminant personalization",
        "physics-based illuminant augmentation",
        "edge-guided prompt disentanglement",
        "masked reconstruction loss",
        "contextual light adaptation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "LumiCtrl is a method for learning illuminant prompts from a single object image to control lighting in personalized text-to-image models. It uses physics-based augmentation, edge-guided prompt disentanglement, and a masked reconstruction loss to achieve contextual light adaptation. The method outperforms existing baselines in illuminant fidelity, aesthetic quality, and scene coherence, as confirmed by a human preference study.",
      "mindmap": ""
    },
    {
      "title": "MMLANDMARKS: a Cross-View Instance-Level Benchmark for Geo-Spatial Understanding",
      "authors": "Oskar Kristoffersen, Alba R. Sánchez, Morten R. Hannemose, Anders B. Dahl, Dim P. Papadopoulos",
      "institution": "Technical University of Denmark, Pioneer Center for AI",
      "link": "https://arxiv.org/pdf/2512.17492",
      "code": null,
      "tags": [
        "multimodal geo-spatial understanding",
        "cross-view retrieval",
        "geolocalization",
        "CLIP-inspired baseline",
        "multimodal dataset",
        "instance-level benchmark"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80d6684c681e0605e017a84878993e58d1f060cd6906ad893ab11cebf1a5f9d2_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces MMLANDMARKS, a multimodal dataset with aerial images, ground-view images, text, and GPS coordinates for geo-spatial tasks. Using a simple CLIP-inspired baseline, the authors show competitive performance across tasks like cross-view retrieval and geolocalization, highlighting the need for multimodal datasets for comprehensive geo-spatial understanding.",
      "mindmap": ""
    },
    {
      "title": "InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion",
      "authors": "Hoiyeong Jin, Hyojin Jang, Jeongho Kim, Junha Hyung, Kinam Kim, Dongjin Kim, Huijin Choi, Hyeonji Kim, Jaegul Choo",
      "institution": "KAIST AI, SK Telecom",
      "link": "https://arxiv.org/pdf/2512.17504",
      "code": null,
      "tags": [
        "diffusion inference",
        "4D scene geometry",
        "diffusion-based video generation",
        "occlusion consistency",
        "illumination-aware dataset",
        "mask generation"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f1824f6e52cce42d09258c21a8f994f87c3330105a845886e13a668bacd45e38_w640_q70.webp",
      "contributions": "",
      "summary": "The paper presents InsertAnywhere, a framework for realistic video object insertion that combines 4D scene geometry reconstruction with a diffusion-based video generation model to ensure geometric and temporal consistency. It introduces a synthetic dataset, ROSE++, for supervised training. The method outperforms existing models in producing visually coherent insertions suitable for production environments.",
      "mindmap": ""
    },
    {
      "title": "Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry",
      "authors": "Ufuk Asil, Efendi Nasibov",
      "institution": "Dokuz Eylul University",
      "link": "https://arxiv.org/pdf/2512.17505",
      "code": null,
      "tags": [
        "sensor fusion and filtering",
        "Error-State Extended Kalman Filter",
        "Scaled Unscented Kalman Filter",
        "visual-inertial odometry",
        "quaternion estimation",
        "adaptive covariance",
        "loosely coupled architecture"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0950fd135a7ca6f717b1ee73ea881e6e27ca5075f9e2e7ca11cb3c42ff286cab_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes a hybrid VIO method that combines an Error-State EKF with a targeted Scaled UKF step for orientation refinement, while dynamically adjusting visual measurement noise based on image quality metrics. The approach achieves significant improvements in accuracy over ESKF-based methods and reduces computational cost compared to a full UKF, balancing efficiency and performance in challenging UAV environments.",
      "mindmap": ""
    },
    {
      "title": "PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology",
      "authors": "Siemen Brussee, Pieter A. Valkema, Jurre A. J. Weijer, Thom Doeleman, Anne M.R. Schrader, Jesper Kers",
      "institution": "Leiden University Medical Center, Utrecht University Medical Center, Amsterdam University Medical Center",
      "link": "https://arxiv.org/pdf/2512.17517",
      "code": null,
      "tags": [
        "others",
        "multiple instance learning",
        "AutoML",
        "feature extraction",
        "whole-slide images",
        "benchmarking",
        "computational pathology"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces PathBench-MIL, an automated machine learning and benchmarking framework designed for Multiple Instance Learning in histopathology. It automates the entire pipeline from preprocessing to model aggregation, enabling standardized and reproducible evaluation of various models and feature extractors on whole-slide image datasets. The main conclusion is that this open-source framework facilitates rapid experimentation and standardization in computational pathology research.",
      "mindmap": ""
    },
    {
      "title": "Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection",
      "authors": "Sairam VCR, Rishabh Lalla, Aveen Dayal, Tejal Kulkarni, Anuj Lalla, Vineeth N Balasubramanian, Muhammad Haris Khan",
      "institution": "IIT Hyderabad, MBZUAI, UC San Diego, IIT Jodhpur, Microsoft Research India",
      "link": "https://arxiv.org/pdf/2512.17514",
      "code": null,
      "tags": [
        "computer vision",
        "source-free object detection",
        "spatial prior-aware regularization",
        "imbalance-aware noise robust pseudo-labeling",
        "mean-teacher",
        "OV-SAM",
        "domain shift"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ce0955de2a0c97faf7100d95c356843de584cdb63f24134f9bcbf1fef3e760a_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes FALCON-SFOD, a framework for source-free object detection that uses foundation model priors (SPAR) to enhance object-focused features and a noise-robust pseudo-labeling method (IRPL) to handle class imbalance. It concludes that this approach strengthens the feature space against domain shift, leading to more reliable pseudo-labels and competitive benchmark performance.",
      "mindmap": ""
    },
    {
      "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation",
      "authors": "Rang Li, Lei Li, Shuhuai Ren, Hao Tian, Shuhao Gu, Shicheng Li, Zihao Yue, Yudong Wang, Wenhan Ma, Zhe Yang, Jingyuan Ma, Zhifang Sui, Fuli Luo",
      "institution": "Peking University, Xiaomi, The University of Hong Kong, Renmin University of China",
      "link": "https://arxiv.org/pdf/2512.17495",
      "code": null,
      "tags": [
        "multimodal evaluation",
        "visual grounding",
        "benchmark",
        "multimodal large language models",
        "discriminative",
        "spatial",
        "limited",
        "rejection",
        "test-time scaling",
        "data-mixture training"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4020de87a870a946901821a37d57ee80ad4531d2b38356ac7601cb21450e17c4_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces GroundingME, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) across four challenging dimensions: discriminative, spatial, limited, and rejection tasks. The evaluation of 25 MLLMs reveals a significant performance gap, with the best model achieving only 45.1% accuracy and most failing on rejection tasks by hallucinating objects. The authors propose test-time scaling and data-mixture training as strategies to partially improve model performance on these complex grounding challenges.",
      "mindmap": ""
    },
    {
      "title": "FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views",
      "authors": "Qijian Tian, Xin Tan, Jiayu Ying, Xuhong Wang, Yuan Xie, Lizhuang Ma",
      "institution": "Shanghai Jiao Tong University, East China Normal University, Shanghai Artificial Intelligence Laboratory",
      "link": "https://arxiv.org/pdf/2512.17541",
      "code": null,
      "tags": [
        "3D reconstruction and language embedding",
        "feed-forward network",
        "3D Gaussians",
        "instance-guided contrastive learning",
        "geometry-semantic hierarchical sparsification",
        "2D-to-3D lifting"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9847bc8d208db80daeeb6fce65f8b727d7e037fbf74ca700b812950d807f8585_w640_q70.webp",
      "contributions": "",
      "summary": "FLEG is a feed-forward network that reconstructs language-embedded 3D Gaussians from arbitrary uncalibrated multi-view images without requiring 3D annotations. It uses instance-guided contrastive learning and hierarchical sparsification to align 2D semantics with 3D representations efficiently. The method outperforms existing approaches in generating accurate geometry, appearance, and language-aligned semantics from sparse or dense views.",
      "mindmap": ""
    },
    {
      "title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
      "authors": "Jiaqi Tang, Jianmin Chen, Wei Wei, Xiaogang Xu, Runtao Liu, Xiangyu Wu, Qipeng Xie, Jiafei Wu, Lei Zhang, Qifeng Chen",
      "institution": "Hong Kong University of Science and Technology, Northwestern Polytechnical University, Chinese University of Hong Kong, Nanjing University of Science and Technology, University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.17532",
      "code": null,
      "tags": [
        "multi-modal inference",
        "degradation-aware reasoning",
        "structured reasoning chains",
        "supervised fine-tuning",
        "reward-driven alignment",
        "dynamic reasoning depth scaling"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b624fc0656a14fc68753d26cc52e1c0a78d9633130c6881762bd87e498ed0875_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes Robust-R1, a framework that enhances the robustness of Multimodal Large Language Models by explicitly modeling visual degradations through structured reasoning chains. The method integrates supervised fine-tuning, reward-driven alignment, and dynamic reasoning depth scaling, and is supported by a new dataset of realistic degradations. The approach achieves state-of-the-art performance on real-world degradation benchmarks, demonstrating superior anti-degradation capabilities.",
      "mindmap": ""
    },
    {
      "title": "G3Splat: Geometrically Consistent Generalizable Gaussian Splatting",
      "authors": "Mehdi Hosseinzadeh, Shin-Fang Chng, Yi Xu, Simon Lucey, Ian Reid, Ravi Garg",
      "institution": "Australian Institute for Machine Learning, Goertek Alpha Labs, MBZUAI",
      "link": "https://arxiv.org/pdf/2512.17547",
      "code": null,
      "tags": [
        "computer vision",
        "3D Gaussian splatting",
        "geometrically consistent priors",
        "view-synthesis loss",
        "pose-free reconstruction",
        "novel-view synthesis"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/04379904f0f6b8d226bffe80f94ab3fe3f745dd41985d72f88a3585212ea8e65_w640_q70.webp",
      "contributions": "",
      "summary": "G3Splat introduces geometric priors to address the ambiguity in learning 3D Gaussian splats from images under self-supervision, enabling geometrically consistent scene reconstruction without requiring camera poses. The method outperforms prior work in geometry recovery, relative pose estimation, and novel-view synthesis, demonstrating strong zero-shot generalization on datasets like ScanNet.",
      "mindmap": ""
    },
    {
      "title": "ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image",
      "authors": "Yunqi Gao, Leyuan Liu, Yuhan Li, Changxin Gao, Yuanyuan Liu, Jingying Chen",
      "institution": "Central China Normal University, Huazhong University of Science and Technology, China University of Geosciences (Wuhan)",
      "link": "https://arxiv.org/pdf/2512.17545",
      "code": null,
      "tags": [
        "computer vision",
        "clothing tailoring",
        "body semantic estimation",
        "body edge prediction",
        "foundational human visual model (FHVM)",
        "3D mesh recovery"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a3dde00da40b964ce086e0496d0c0c6f668bf5149ef5a44e30539fa3c614601b_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes ClothHMR, a method for 3D human mesh recovery from a single image that handles diverse clothing via a clothing tailoring module to fit garments to the body silhouette and a mesh recovery module that aligns 3D representations with a foundational human vision model. It demonstrates superior performance over existing methods on benchmark datasets and in-the-wild images, with a practical web application for fashion and shopping.",
      "mindmap": ""
    },
    {
      "title": "A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points",
      "authors": "Mathilde Gajda Faanes, David Bouget, Asgeir S. Jakola, Timothy R. Smith, Vasileios K. Kavouridis, Francesco Latini, Margret Jensdottir, Peter Milos, Henrietta Nittby Redebrandt, Rickard L. Sjöberg, Rupavathana Mahesparan, Lars Kjelsberg Pedersen, Ole Solheim, Ingerid Reinertsen",
      "institution": "SINTEF Digital, University of Gothenburg, Harvard Medical School, Uppsala University Hospital, Karolinska University Hospital, Linköping University Hospital, Skåne University Hospital, Umeå University, Haukeland University Hospital, University Hospital of North Norway, Norwegian University of Science and Technology, St. Olavs University Hospital",
      "link": "https://arxiv.org/pdf/2512.17566",
      "code": null,
      "tags": [
        "others",
        "Attention U-Net",
        "FLAIR hyperintensity segmentation",
        "Dice score",
        "Raidionics"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper presents a unified deep learning model for segmenting FLAIR hyperintensities in brain tumors using an Attention U-Net architecture trained on approximately 5000 MRI scans. The model generalizes well across various tumor types and pre- and post-operative time points, achieving performance comparable to dataset-specific models. It is integrated into the open-source Raidionics software to facilitate clinical deployment.",
      "mindmap": ""
    },
    {
      "title": "RoomEditor++: A Parameter-Sharing Diffusion Architecture for High-Fidelity Furniture Synthesis",
      "authors": "Qilong Wang, Xiaofan Ming, Zhenyi Lin, Jinwen Li, Dongwei Ren, Wangmeng Zuo, Qinghua Hu",
      "institution": "Tianjin University, Harbin Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.17573",
      "code": null,
      "tags": [
        "diffusion inference",
        "diffusion model",
        "parameter-sharing dual diffusion backbone",
        "U-Net",
        "DiT",
        "inpainting",
        "geometric coherence"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/01defbd3ee3e8db15e4c76b08a2c5a3cbb6ba5c53643caf0e06f01bee07f1f49_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes RoomEditor++, a diffusion-based architecture with a parameter-sharing dual diffusion backbone for high-fidelity virtual furniture synthesis. It introduces the RoomBench++ dataset for training and evaluation. Experiments show the method outperforms state-of-the-art approaches in metrics and human preference, demonstrating strong generalization.",
      "mindmap": ""
    },
    {
      "title": "Medical Imaging AI Competitions Lack Fairness",
      "authors": "Annika Reinke, Evangelia Christodoulou, Sthuthi Sadananda, A. Emre Kavur, Khrystyna Faryna, Daan Schouten, Bennett A. Landman, Carole Sudre, Olivier Colliot, Nick Heller, Sophie Loizillon, Martin Maška, Maëlys Solal, Arya Yazdan-Panah, Vilma Bozgo, Ömer Sümer, Siem de Jong, Sophie Fischer, Michal Kozubek, Tim Rädsch, Nadim Hammoud, Fruzsina Molnár-Gábor, Steven Hicks, Michael A. Riegler, Anindo Saha, Vajira Thambawita, Pal Halvorsen, Amelia Jiménez-Sánchez, Qingyang Yang, Veronika Cheplygina, Sabrina Bottazzi, Alexander Seitel, Spyridon Bakas, Alexandros Karargyris, Kiran Vaidhya Venkadesh, Bram van Ginneken, Lena Maier-Hein",
      "institution": "German Cancer Research Center (DKFZ) Heidelberg",
      "link": "https://arxiv.org/pdf/2512.17581",
      "code": null,
      "tags": [
        "medical imaging",
        "benchmarking",
        "fairness",
        "FAIR principles",
        "dataset bias",
        "reproducibility",
        "systematic review"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper conducts a large-scale systematic study of 241 biomedical image analysis challenges to assess fairness in terms of dataset representativeness and accessibility. It finds substantial biases in dataset composition and restrictive access conditions, concluding that current benchmarks lack fairness and show a disconnect between leaderboard success and clinical relevance.",
      "mindmap": ""
    },
    {
      "title": "MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification",
      "authors": "Tosin Ige, Christopher Kiekintveld, Aritran Piplai, Asif Rahman, Olukunle Kolade, Sasidhar Kunapuli",
      "institution": "The University of Texas at El Paso, University of North Carolina",
      "link": "https://arxiv.org/pdf/2512.17594",
      "code": null,
      "tags": [
        "others",
        "Gaussian Discriminant Analysis",
        "Z-score distance analysis",
        "cluster-driven deep learning",
        "two-stage framework"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes MAD-OOD, a two-stage cluster-driven deep learning framework for out-of-distribution malware detection. It uses Gaussian Discriminant Analysis to model class embeddings and Z-score distance analysis to identify anomalies, then integrates these predictions with a neural network for final classification. The method significantly outperforms state-of-the-art approaches on benchmark datasets, achieving high AUC for detecting unseen malware families.",
      "mindmap": ""
    },
    {
      "title": "3One2: One-step Regression Plus One-step Diffusion for One-hot Modulation in Dual-path Video Snapshot Compressive Imaging",
      "authors": "Ge Wang, Xing Liu, Xin Yuan",
      "institution": "Zhejiang University, Westlake University, Westlake Institute for Optoelectronics",
      "link": "https://arxiv.org/pdf/2512.17578",
      "code": null,
      "tags": [
        "diffusion inference",
        "one-hot modulation",
        "dual optical path",
        "stochastic differential equation",
        "video inpainting",
        "one-step regression",
        "one-step diffusion"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5fc017d71c05a87d24f1951648fbc337f6dcf0174a07c6e9b18d306437014285_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes a novel algorithm for video snapshot compressive imaging using one-hot modulation, which transforms the reconstruction into a video inpainting problem solved by combining a one-step regression initialization with a one-step diffusion refinement. To address spatial degradation, it introduces a dual optical path hardware design. Experiments show the method effectively reconstructs videos and is the first to integrate diffusion models into video SCI reconstruction.",
      "mindmap": ""
    },
    {
      "title": "HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection",
      "authors": "Zhaolin Cai, Fan Li, Ziwei Zheng, Haixia Bi, Lijun He",
      "institution": "Xinjiang University, Xi’an Jiaotong University",
      "link": "https://arxiv.org/pdf/2512.17601",
      "code": null,
      "tags": [
        "video anomaly detection",
        "attention heads",
        "multi-criteria analysis",
        "tuning-free",
        "multimodal large language models (MLLMs)",
        "robust head identification"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0045f2737f70ccb56d547c71a2a55c2ab9160ecc33a3c65b7b564bc9a3329529_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes HeadHunt-VAD, a tuning-free video anomaly detection method that directly identifies and uses a sparse set of robust, anomaly-sensitive attention heads within a frozen Multimodal Large Language Model (MLLM), bypassing textual generation. It introduces a Robust Head Identification module to select expert heads based on saliency and stability across prompts, followed by a lightweight scorer for detection. The method achieves state-of-the-art performance among tuning-free approaches on major benchmarks, demonstrating the effectiveness of head-level probing in MLLMs for efficient and accurate anomaly detection.",
      "mindmap": ""
    },
    {
      "title": "MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration",
      "authors": "Svetlana Krasnova, Emiliya Starikova, Ilia Naletov, Andrey Krylov, Dmitry Sorokin",
      "institution": "Lomonosov Moscow State University, Third Opinion Platform",
      "link": "https://arxiv.org/pdf/2512.17605",
      "code": null,
      "tags": [
        "medical image registration",
        "mammography registration",
        "anatomical landmarks",
        "ANTs",
        "VoxelMorph",
        "TransMorph",
        "IDIR",
        "MammoRegNet",
        "benchmark dataset"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c1e72bae8c4d2dd504664f6eedc1a325466b12559df8aa6fc5fbe929fb95fcbf_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces MGRegBench, a public benchmark dataset with over 5,000 mammography image pairs and manual annotations for evaluating registration methods. It benchmarks classical, learning-based, and implicit neural representation approaches, finding that deep learning methods like MammoRegNet show strong performance. The dataset and code are released to enable fair comparisons and advance research in mammography registration.",
      "mindmap": ""
    },
    {
      "title": "Self-Supervised Weighted Image Guided Quantitative MRI Super-Resolution",
      "authors": "Alireza Samadifardheris, Dirk H.J. Poot, Florian Wiesinger, Stefan Klein, Juan A. Hernandez-Tamames",
      "institution": "Erasmus MC, GE Healthcare, TU Delft",
      "link": "https://arxiv.org/pdf/2512.17612",
      "code": null,
      "tags": [
        "others",
        "self-supervised learning",
        "physics-informed deep learning",
        "Bayesian maximum a posteriori inference",
        "super-resolution",
        "quantitative MRI relaxometry"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddc1b71f8c3d47afff52315d3f332374abf579939cd3a9de858b3ceab44b3ff3_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes a self-supervised, physics-informed deep learning framework for quantitative MRI super-resolution. It uses routinely acquired high-resolution weighted MRI scans as guidance to enhance low-resolution quantitative maps, eliminating the need for high-resolution ground truth during training. The method enables fast, high-quality quantitative MRI acquisitions, offering a practical pathway for clinical integration.",
      "mindmap": ""
    },
    {
      "title": "Semi-Supervised 3D Segmentation for Type-B Aortic Dissection with Slim UNETR",
      "authors": "Denis Mikhailapov, Vladimir Berikov",
      "institution": "Sobolev Institute of Mathematics SB RAS",
      "link": "https://arxiv.org/pdf/2512.17610",
      "code": null,
      "tags": [
        "others",
        "semi-supervised learning",
        "3D segmentation",
        "multi-output CNN",
        "Slim UNETR",
        "data augmentation"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abd921f37f90b9dfaf497e986d1ae89307cf4b11dd527bf09bd11e36d239652c_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes a semi-supervised learning method for multi-output 3D CNNs, specifically using Slim UNETR, to segment aortic structures in Type-B Aortic Dissection. The method leverages data augmentation techniques like rotation and flipping to utilize both labeled and unlabeled data, overcoming the challenge of limited high-quality 3D annotations. It concludes that this approach is a universal and effective strategy for improving segmentation accuracy in medical imaging with complex, multi-class outputs.",
      "mindmap": ""
    },
    {
      "title": "StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection",
      "authors": "Di Wu, Feng Yang, Wenhui Zhao, Jinwen Yu, Pan Liao, Benlian Xu, Dingwen Zhang",
      "institution": "Northwestern Polytechnical University, Suzhou University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.17620",
      "code": null,
      "tags": [
        "autonomous driving perception",
        "temporal stereo modeling",
        "dynamic confidence gating",
        "sparse query-based detection",
        "multi-view 3D object detection"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/87e1ee16ff63d70d8c3aa60f230a9ba8c43c07767bfd05ba0b8a9169945304c1_w640_q70.webp",
      "contributions": "",
      "summary": "StereoMV2D integrates temporal stereo modeling into a 2D detection-guided multi-view 3D detector to enhance depth perception by exploiting cross-temporal disparities across adjacent frames. It refines query priors efficiently within 2D regions of interest and uses a dynamic confidence gating mechanism for robust detection under occlusion. The framework achieves superior performance on nuScenes and Argoverse 2 datasets without significant computational overhead.",
      "mindmap": ""
    },
    {
      "title": "PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology",
      "authors": "Fengchun Liu, Songhan Jiang, Linghan Cai, Ziyue Wang, Yongbing Zhang",
      "institution": "Harbin Institute of Technology, Shenzhen; National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.17621",
      "code": null,
      "tags": [
        "multi-modal training",
        "PathFLIP",
        "fine-grained language-image pretraining",
        "region-level subcaptions",
        "text-conditioned region embeddings",
        "visual-language grounding",
        "large language models (LLMs)",
        "whole slide images (WSIs)",
        "computational pathology"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ccba362ad9693531c32711d08070cfa491424d893a9b07c00a878fc385f9bdae_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes PathFLIP, a framework for computational pathology that decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings for fine-grained visual-language alignment. It leverages LLMs to follow clinical instructions and adapt to diagnostic contexts. Experiments show it outperforms existing pathological VLMs on multiple benchmarks while using less training data.",
      "mindmap": ""
    },
    {
      "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
      "authors": "Zhongwei Zhang, Fuchen Long, Wei Li, Zhaofan Qiu, Wu Liu, Ting Yao, Tao Mei",
      "institution": "University of Science and Technology of China, HiDream.ai Inc.",
      "link": "https://arxiv.org/pdf/2512.17650",
      "code": null,
      "tags": [
        "diffusion training",
        "in-context generation",
        "video diffusion",
        "latent regularization",
        "attention regularization",
        "region-constraint",
        "joint denoising"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a42bfbcfb80ccf85890223bbbea68fa4b5ab3ddd21035c9e061032429b289590_w640_q70.webp",
      "contributions": "",
      "summary": "The paper presents ReCo, a region-constraint in-context generation paradigm for instructional video editing. It introduces latent and attention regularization during joint denoising to precisely modify editing regions and mitigate interference. Experiments show the method's superiority across various video editing tasks.",
      "mindmap": ""
    },
    {
      "title": "Bitbox: Behavioral Imaging Toolbox for Computational Analysis of Behavior from Videos",
      "authors": "Evangelos Sariyanidi, Gokul Nair, Lisa Yankowitz, Casey J. Zampella, Mohan Kashyap Pargi, Aashvi Manakiwala, Maya McNealis, John D. Herrington, Jeffrey Cohn, Robert T. Schultz, Birkan Tunc",
      "institution": "The Children's Hospital of Philadelphia, University of Pennsylvania, University of Pittsburgh",
      "link": "https://arxiv.org/pdf/2512.17655",
      "code": null,
      "tags": [
        "multi-modal inference",
        "computational behavioral measurement",
        "video analysis",
        "facial expression",
        "head movement",
        "body action",
        "open-source toolkit",
        "modularity",
        "interpretability"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces Bitbox, an open-source behavioral imaging toolbox that provides a standardized interface for extracting high-level behavioral measurements from video using multiple face, head, and body processors. It is designed to bridge the translational gap by making advanced computational analysis accessible to behavioral and clinical researchers without requiring engineering expertise. The authors conclude that Bitbox will accelerate the integration of computational behavioral measurement into behavioral, clinical, and mental health research.",
      "mindmap": ""
    },
    {
      "title": "Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs",
      "authors": "Zhaolin Cai, Huiyu Duan, Zitong Xu, Fan Li, Zhi Liu, Jing Liu, Wei Shen, Xiongkuo Min, Guangtao Zhai",
      "institution": "Shanghai Jiao Tong University, Xi'an Jiao Tong University, Shandong University, Tianjin University",
      "link": "https://arxiv.org/pdf/2512.17640",
      "code": null,
      "tags": [
        "multi-modal inference",
        "cognitive steering conduit (CSC)",
        "hybrid interaction representations",
        "hybrid guidance strategy",
        "language modeling loss",
        "auxiliary classification loss",
        "open-vocabulary generation"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/089843bf864a7cfe4c272a400ced858a82e7f97256cba2c56898e8f4e4591dff_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes GRASP-HOI, a framework that reformulates human-object interaction detection as an open-vocabulary generation problem. It uses a lightweight cognitive steering module to inject visual features into a frozen multi-modal LLM for reasoning and employs a hybrid loss for training. This approach achieves state-of-the-art closed-set performance and strong zero-shot generalization.",
      "mindmap": ""
    },
    {
      "title": "Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation",
      "authors": "Alexandre Personnic, Mihai Bâce",
      "institution": "KU Leuven",
      "link": "https://arxiv.org/pdf/2512.17673",
      "code": null,
      "tags": [
        "computer vision",
        "spatio-temporal feature representation",
        "channel attention",
        "self-attention",
        "recurrent neural networks",
        "video-based gaze estimation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes the Spatio-Temporal Gaze Network (ST-Gaze), which combines a CNN backbone with channel and self-attention modules to fuse eye and face features, then models intra- and inter-frame dynamics by treating features as a spatial sequence propagated through time. The method achieves state-of-the-art performance on the EVE dataset, demonstrating that preserving intra-frame spatial context is superior to premature spatial pooling for robust video-based gaze estimation.",
      "mindmap": ""
    },
    {
      "title": "An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution",
      "authors": "Yudhistira Arief Wibowo",
      "institution": "Technical University of Munich, Korea Advanced Institute of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.17675",
      "code": null,
      "tags": [
        "diffusion inference",
        "Diffusion Posterior Sampling (DPS)",
        "Manifold Constrained Gradient (MCG)",
        "conditioning step size",
        "diffusion step count",
        "ablation study"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper conducts an empirical ablation study on diffusion-based super-resolution, focusing on conditioning methods like DPS and MCG. It finds that the conditioning step size is a more critical hyperparameter than the diffusion step count for reconstruction quality. The optimal conditioning step size for best performance in their experiments falls within the range of [2.0, 3.0].",
      "mindmap": ""
    },
    {
      "title": "SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses",
      "authors": "Shaoyan Zhai, Mohamed Abdel-Aty, Chenzhu Wang, Rodrigo Vena Garcia",
      "institution": "University of Central Florida",
      "link": "https://arxiv.org/pdf/2512.17724",
      "code": null,
      "tags": [
        "multi-modal inference",
        "semantic segmentation",
        "monocular depth estimation",
        "Time-to-Collision (TTC)",
        "Generalized Extreme Value (GEV) distribution",
        "VideoLLaMA2",
        "InternVL2.5 HiCo R16",
        "domain adaptation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces SAVeD, a first-person video dataset from social media for analyzing ADAS vehicle near-misses and crashes. It proposes a framework using semantic segmentation and depth estimation to compute Time-to-Collision and uses extreme value theory to model risk. The dataset's annotations are shown to enhance the performance of video-language models through domain adaptation in complex scenarios.",
      "mindmap": ""
    },
    {
      "title": "FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation",
      "authors": "Cheng Peng, Zhuo Su, Liao Wang, Chen Guo, Zhaohu Li, Chengjiang Long, Zheng Lv, Jingxiang Sun, Chenyangguang Zhang, Yebin Liu",
      "institution": "Tsinghua University, ByteDance",
      "link": "https://arxiv.org/pdf/2512.17717",
      "code": null,
      "tags": [
        "computer vision",
        "transformer-based reconstruction",
        "3D Gaussian Splatting",
        "UV-space position maps",
        "data distribution adjustment",
        "lightweight UNet decoder"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/291c2533d43a0bee45ee61d6f189d197b685c110c8fea100f87b6b779dda6aaf_w640_q70.webp",
      "contributions": "",
      "summary": "FlexAvatar is a flexible large reconstruction model that creates high-fidelity 3D head avatars from single or sparse images without camera poses or expression labels, using a transformer-based approach with structured head query tokens and a lightweight UNet decoder for real-time detailed deformations. It achieves superior 3D consistency and dynamic realism compared to previous methods, offering a practical solution for animatable avatar creation.",
      "mindmap": ""
    },
    {
      "title": "MambaMIL+: Modeling Long-Term Contextual Patterns for Gigapixel Whole Slide Image",
      "authors": "Qian Zeng, Yihui Wang, Shu Yang, Yingxue Xu, Fengtao Zhou, Jiabo Ma, Dejia Cai, Zhengyu Zhang, Lijuan Qu, Yu Wang, Li Liang, Hao Chen",
      "institution": "Hong Kong University of Science and Technology, Southern Medical University, 900th Hospital of Joint Logistic Support Force, PLA, Zhujiang Hospital, Southern Medical University",
      "link": "https://arxiv.org/pdf/2512.17726",
      "code": null,
      "tags": [
        "multi-modal inference",
        "Mamba",
        "multiple instance learning (MIL)",
        "overlapping scanning",
        "selective stripe position encoder (S2PE)",
        "contextual token selection (CTS)"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes MambaMIL+, a multiple instance learning framework that integrates spatial context modeling and long-range dependency for analyzing gigapixel whole-slide images. It introduces overlapping scanning, a selective stripe position encoder, and a contextual token selection mechanism to overcome memory decay and limited context in long sequences. The method achieves state-of-the-art performance across 20 benchmarks for diagnostic classification, molecular prediction, and survival analysis.",
      "mindmap": ""
    },
    {
      "title": "AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection",
      "authors": "Yichen Jiang, Mohammed Talha Alam, Sohail Ahmed Khan, Duc-Tien Dang-Nguyen, Fakhri Karray",
      "institution": "University of Waterloo, MBZUAI, University of Bergen",
      "link": "https://arxiv.org/pdf/2512.17730",
      "code": null,
      "tags": [
        "deepfake detection",
        "CLIP",
        "parameter-efficient transfer learning",
        "textual prompts",
        "visual adapters",
        "layer ablation",
        "diffusion models"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3b69dc4fb6ce4ab4ca6cc4b1419cbe29e322a1c22a4599cafb0bf6800683cf49_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes AdaptPrompt, a parameter-efficient framework that adapts the CLIP vision-language model for deepfake detection by jointly learning task-specific textual prompts and visual adapters while freezing the backbone. It also introduces the Diff-Gen dataset and shows that pruning the final transformer block of the vision encoder improves the retention of high-frequency artifacts. The method achieves state-of-the-art generalization across 25 test sets, including unseen generators, and demonstrates strong few-shot performance.",
      "mindmap": ""
    },
    {
      "title": "LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence",
      "authors": "Yohanes Yudhi Adikusuma, Qixing Huang, Ying He",
      "institution": "University of Texas at Austin, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.17781",
      "code": null,
      "tags": [
        "others",
        "unsigned distance field",
        "PCA",
        "lightweight embedding",
        "geodesic distance",
        "shape correspondence"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "LiteGE introduces a lightweight method for computing geodesic distances on 3D shapes by constructing compact shape descriptors using PCA on unsigned distance field samples. This approach eliminates the need for large neural networks, enabling significant reductions in memory usage and inference time while maintaining robustness on sparse point clouds. It also facilitates fast and accurate non-isometric shape correspondence, achieving up to 1000x speedup over state-of-the-art mesh-based methods.",
      "mindmap": ""
    },
    {
      "title": "UrbanDIFF: A Denoising Diffusion Model for Spatial Gap Filling of Urban Land Surface Temperature Under Dense Cloud Cover",
      "authors": "Arya Chavoshi, Hassan Dashtian, Naveen Sudharsan, Dev Niyogi",
      "institution": "The University of Texas at Austin",
      "link": "https://arxiv.org/pdf/2512.17782",
      "code": null,
      "tags": [
        "diffusion inference",
        "denoising diffusion",
        "image inpainting",
        "spatial gap-filling",
        "pixel guided refinement",
        "MODIS Terra LST"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces UrbanDIFF, a denoising diffusion model for spatially reconstructing urban land surface temperature imagery obscured by dense clouds, using static urban structure data for conditioning and a pixel-guided refinement step. It demonstrates superior performance over baselines under high cloud coverage, with slower degradation as missing data increases.",
      "mindmap": ""
    },
    {
      "title": "Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras",
      "authors": "Ami Pandat, Punna Rajasekhar, G.Aravamuthan, Gopika Vinod, Rohit Shukla",
      "institution": "Homi Bhabha National Institute, Bhabha Atomic Research Center",
      "link": "https://arxiv.org/pdf/2512.17784",
      "code": null,
      "tags": [
        "others",
        "hybrid distortion model",
        "neural network residual correction",
        "stereo triangulation",
        "camera calibration",
        "long-range depth estimation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a hybrid framework for long-range depth estimation by extending conventional camera distortion models with higher-order terms and then enhancing them using a neural network-based residual correction model. This approach improves 3D localization accuracy for distances up to 5 kilometers using CCTV cameras. The method is validated by transforming estimated 3D coordinates to GIS maps, offering a practical calibration solution for long-range photogrammetry.",
      "mindmap": ""
    },
    {
      "title": "Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image",
      "authors": "Simon Giebenhain, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Zhe Chen, Matthias Nießner",
      "institution": "Technical University of Munich, Woven by Toyota, Toyota Motor Europe",
      "link": "https://arxiv.org/pdf/2512.17773",
      "code": null,
      "tags": [
        "computer vision",
        "vision transformer",
        "neural parametric head models",
        "3d morphable models",
        "single-image 3d reconstruction",
        "signed distance functions"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c26792d83b697ded69eb83a7cee4b4440d0b9637b6c3fbd2061ab2aef67d7bcd_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces Pix2NPHM, a method that uses a vision transformer to directly regress the parameters of a Neural Parametric Head Model from a single input image. It achieves high-fidelity 3D face reconstruction by training on a mixture of 3D data and 2D videos, and allows for further refinement through inference-time optimization. The authors conclude that their approach yields unprecedented reconstruction quality that generalizes well to in-the-wild data.",
      "mindmap": ""
    },
    {
      "title": "Animate Any Character in Any World",
      "authors": "Yitong Wang, Fangyun Wei, Hongyang Zhang, Bo Dai, Yan Lu",
      "institution": "Fudan University, Microsoft Research, University of Waterloo, The University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.17796",
      "code": null,
      "tags": [
        "multi-modal inference",
        "3DGS",
        "conditional autoregressive video generation",
        "pre-trained video generator",
        "natural language control"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d8fb0b1e8242f96e5f0b2988237b2d0603a8f364d90cc833a33b2313d43b1ae4_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces AniX, a system that animates user-provided 3D characters in 3D Gaussian Splatting (3DGS) scenes based on natural language commands. It formulates the task as a conditional autoregressive video generation problem, building upon a pre-trained video generator and a training strategy to enhance motion dynamics. The method enables open-ended character actions while preserving visual fidelity and temporal coherence in the generated video clips.",
      "mindmap": ""
    },
    {
      "title": "Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding",
      "authors": "Yue Li, Qi Ma, Runyi Yang, Mengjiao Ma, Bin Ren, Nikola Popovic, Nicu Sebe, Theo Gevers, Luc Van Gool, Danda Pani Paudel, Martin R. Oswald",
      "institution": "University of Amsterdam, ETH Zurich, INSAIT (Sofia University \"St. Kliment Ohridski\"), Nanjing University of Aeronautics and Astronautics, University of Trento",
      "link": "https://arxiv.org/pdf/2512.17817",
      "code": null,
      "tags": [
        "multi-modal training",
        "3D Gaussian Splatting",
        "multi-teacher pretraining",
        "knowledge distillation",
        "feed-forward encoder",
        "open-vocabulary segmentation",
        "render-and-distill"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/93bcfeca01370b73bf751251ff5ff1406aed5442e3fe99bf9680cea6a994f535_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces Chorus, a multi-teacher pretraining framework that learns a holistic 3D Gaussian Splatting scene encoder by distilling complementary signals from 2D foundation models. The method achieves strong performance on various 3D scene understanding tasks and demonstrates high data efficiency, requiring significantly fewer training scenes than point cloud baselines.",
      "mindmap": ""
    },
    {
      "title": "ReX-MLE: The Autonomous Agent Benchmark for Medical Imaging Challenges",
      "authors": "Roshan Kenia, Xiaoman Zhang, Pranav Rajpurkar",
      "institution": "Harvard Medical School",
      "link": "https://arxiv.org/pdf/2512.17838",
      "code": null,
      "tags": [
        "others",
        "autonomous coding agents",
        "benchmark",
        "end-to-end workflows",
        "data preprocessing",
        "model training",
        "medical imaging competitions"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55f58c5bf3f50168f0cf79114478d406f849c07ec4093b7557c2b51fc67903fb_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces ReX-MLE, a benchmark for evaluating autonomous coding agents on complex, end-to-end medical imaging challenges derived from real competitions. It finds that current state-of-the-art agents perform poorly, ranking near the 0th percentile compared to human experts, due to domain-knowledge and engineering limitations. The benchmark aims to expose these bottlenecks and guide the development of more capable, domain-aware autonomous AI systems.",
      "mindmap": ""
    },
    {
      "title": "Simulation-Driven Deep Learning Framework for Raman Spectral Denoising Under Fluorescence-Dominant Conditions",
      "authors": "Mengkun Chen, Sanidhya D. Tripathi, James W. Tunnell",
      "institution": "The University of Texas at Austin",
      "link": "https://arxiv.org/pdf/2512.17852",
      "code": null,
      "tags": [
        "others",
        "deep learning",
        "noise modeling",
        "cascaded neural network",
        "simulation-driven framework",
        "physics-informed learning"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper presents a simulation-driven deep learning framework that uses a comprehensive noise model to generate realistic Raman spectra for training a cascaded neural network. The network is designed to jointly suppress detector noise and fluorescence background. The results demonstrate that this physics-informed learning approach can improve spectral quality for faster and more accurate tissue analysis.",
      "mindmap": ""
    },
    {
      "title": "InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models",
      "authors": "Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang, Tejas Gokhale, Hamid Palangi, Hadi Jamali-Rad",
      "institution": "Delft University of Technology, University of Maryland, Baltimore County, Shell Information Technology International, Google",
      "link": "https://arxiv.org/pdf/2512.17851",
      "code": null,
      "tags": [
        "diffusion inference",
        "cross-attention maps",
        "inference-time optimization",
        "compound loss",
        "denoising step",
        "spatial alignment"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e1a251648d46bb8da396759e99d563b9a2d57eb19fc5ed8f7ece18ccb7bfeeee_w640_q70.webp",
      "contributions": "",
      "summary": "InfSplign is a training-free, inference-time method that improves spatial alignment in text-to-image diffusion models by adjusting the noise at each denoising step using a compound loss based on cross-attention maps. It achieves state-of-the-art performance on spatial reasoning benchmarks, outperforming existing inference-time and fine-tuning baselines.",
      "mindmap": ""
    },
    {
      "title": "Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN",
      "authors": "Balram Singh, Ram Prakash Sharma, Somnath Dey",
      "institution": "National Institute of Technology Hamirpur, Indian Institute of Technology Indore",
      "link": "https://arxiv.org/pdf/2512.17864",
      "code": null,
      "tags": [
        "others",
        "Convolutional Neural Network (CNN)",
        "Attention Mechanism",
        "CBAM",
        "VGG16",
        "Grad-CAM",
        "Layer-wise Relevance Propagation (LRP)"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes an interpretable plant leaf disease detection method using a CBAM-enhanced VGG16 CNN model. The model integrates attention modules to improve feature extraction and localization, achieving high accuracy on multiple datasets. The study demonstrates the effectiveness of the approach through performance evaluation and interpretability analysis using attention maps and other explainable AI techniques.",
      "mindmap": ""
    },
    {
      "title": "Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training",
      "authors": "Kristoffer Wickstrøm, Teresa Dorszewski, Siyan Chen, Michael Kampffmeyer, Elisabeth Wetzer, Robert Jenssen",
      "institution": "UiT The Arctic University of Norway, Technical University of Denmark",
      "link": "https://arxiv.org/pdf/2512.17891",
      "code": null,
      "tags": [
        "explainable AI",
        "Keypoint Counting Classifiers",
        "Vision Transformers",
        "self-explainable models",
        "keypoint matching"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3be976f0784a55cb3060e705b880d59783015427100e3ca645b5aa22bfe5f76_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces Keypoint Counting Classifiers (KCCs), a method to convert any pre-trained Vision Transformer (ViT) into a self-explainable model without requiring retraining, by leveraging ViTs' ability to identify matching keypoints between images. The method creates an interpretable decision process that is directly visualizable. The authors conclude that KCCs improve human-machine communication and represent a step towards more transparent and reliable ViT-based foundation models.",
      "mindmap": ""
    },
    {
      "title": "Visually Prompted Benchmarks Are Surprisingly Fragile",
      "authors": "Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa",
      "institution": "UC Berkeley",
      "link": "https://arxiv.org/pdf/2512.17875",
      "code": null,
      "tags": [
        "multi-modal inference",
        "visual prompting",
        "benchmark evaluation",
        "vision-language models",
        "visual marker design",
        "JPEG compression",
        "dataset size",
        "VPBench"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a1578e85cb159e2bf39916b0de61ec0b774772fc5c07fb3edc1f869eea3ea32e_w640_q70.webp",
      "contributions": "",
      "summary": "The paper evaluates vision-language models (VLMs) on visually prompted benchmarks, where questions refer to coordinates marked directly on images. It finds that model performance and leaderboard rankings are surprisingly fragile to minor changes in visual marker design (e.g., color, size) and low-level inference settings like JPEG compression. To address this instability, the authors introduce VPBench, a larger benchmark with multiple visual marker variants.",
      "mindmap": ""
    },
    {
      "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
      "authors": "Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany",
      "institution": "Technion, MIT, NVIDIA, University of Toronto, Vector Institute",
      "link": "https://arxiv.org/pdf/2512.17897",
      "code": null,
      "tags": [
        "multi-modal inference",
        "diffusion model",
        "bird's-eye-view",
        "radar cross section",
        "Doppler",
        "point cloud generation",
        "foundation models"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/42fca94c46885d829dda241902549fc6761186740477806e7cc7cc1b8d19368a_w640_q70.webp",
      "contributions": "",
      "summary": "RadarGen is a diffusion model that generates realistic automotive radar point clouds from multi-view camera images by representing radar data in bird's-eye-view and conditioning on visual cues. It uses a lightweight recovery step to reconstruct point clouds from the generated maps. Evaluations show it captures real radar statistics and reduces the performance gap for perception models trained on synthetic data.",
      "mindmap": ""
    },
    {
      "title": "Diffusion Forcing for Multi-Agent Interaction Sequence Modeling",
      "authors": "Vongani H. Maluleke, Kie Horiuchi, Lea Wilken, Evonne Ng, Jitendra Malik, Angjoo Kanazawa",
      "institution": "UC Berkeley, Sony Group Corporation, Meta",
      "link": "https://arxiv.org/pdf/2512.17900",
      "code": null,
      "tags": [
        "multi-agent motion generation",
        "diffusion forcing",
        "autoregressive diffusion",
        "transformer",
        "multi-agent interaction",
        "denoising"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3cd37ce47a0e9e2e02887aa6dab039bf60ed28e99e8eaf07d8c6aaa610b08b8a_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces MAGNet, a unified autoregressive diffusion framework for generating multi-agent human motion sequences. It extends Diffusion Forcing to explicitly model inter-agent coupling, enabling coherent coordination for both synchronized and loosely structured social interactions. The method performs on par with specialized dyadic benchmarks and naturally scales to polyadic scenarios with three or more agents.",
      "mindmap": ""
    },
    {
      "title": "InSPECT: Invariant Spectral Features Preservation of Diffusion Models",
      "authors": "Baohua Yan, Qingyuan Liu, Jennifer Kava, Xuan Di",
      "institution": "Columbia University",
      "link": "https://arxiv.org/pdf/2512.17873",
      "code": null,
      "tags": [
        "diffusion training",
        "invariant spectral features",
        "Fourier coefficients",
        "feature-preserving diffusion",
        "InSPECT",
        "DDPM"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e13641f6dbe27376e3ddb6ce03c9f7e6b0f2910fa5365cf52d7c50d8992a2550_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes InSPECT, a diffusion model that preserves invariant spectral features during the forward and backward processes, preventing the complete destruction of data into white noise. This approach leads to faster convergence, improved generation quality and diversity, and significant reductions in FID and improvements in IS compared to standard DDPM.",
      "mindmap": ""
    },
    {
      "title": "Adversarial Robustness of Vision in Open Foundation Models",
      "authors": "Jonathon Fox, William J Buchanan, Pavlos Papadopoulos",
      "institution": "Edinburgh Napier University",
      "link": "https://arxiv.org/pdf/2512.17902",
      "code": null,
      "tags": [
        "multi-modal inference",
        "Projected Gradient Descent",
        "adversarial robustness",
        "Visual Question Answering",
        "vision-language models"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper evaluates the adversarial robustness of vision-language models LLaVA-1.5-13B and Llama 3.2 Vision-8B-2 by applying untargeted Projected Gradient Descent attacks to their visual inputs and testing on a VQA v2 subset. The main conclusion is that the vision modality is a viable attack vector, and adversarial robustness does not directly correlate with standard benchmark performance, with Llama 3.2 Vision showing a smaller accuracy drop under attack despite a lower baseline.",
      "mindmap": ""
    },
    {
      "title": "Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer's Disease Classification",
      "authors": "Faisal Ahmed",
      "institution": "Embry-Riddle Aeronautical University",
      "link": "https://arxiv.org/pdf/2512.16964",
      "code": null,
      "tags": [
        "multi-modal inference",
        "Vision Transformers",
        "Pseudo-Color Enhancement",
        "MRI",
        "Multi-Class Classification"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes PseudoColorViT-Alz, a method that enhances MRI scans with pseudo-color transformations and processes them using a Vision Transformer for Alzheimer's disease classification. The model achieves state-of-the-art accuracy of 99.79% on the OASIS-1 dataset, demonstrating that combining color augmentation with Vision Transformers significantly improves classification performance over previous methods.",
      "mindmap": ""
    },
    {
      "title": "Dexterous World Models",
      "authors": "Byungjun Kim, Taeksoo Kim, Junyoung Lee, Hanbyul Joo",
      "institution": "Seoul National University, RLWRLD",
      "link": "https://arxiv.org/pdf/2512.17907",
      "code": null,
      "tags": [
        "diffusion training",
        "video diffusion",
        "egocentric hand mesh rendering",
        "hybrid interaction video dataset",
        "scene-action-conditioned generation"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0d67a89ae639d29f772540022b521f1cb6c865bf4eff8ab082f8c6e9eeeaaa6a_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces Dexterous World Models (DWM), a video diffusion framework that generates dynamic, egocentric videos of human-scene interactions by conditioning on static 3D scene renderings and hand motion sequences. It is trained on a hybrid dataset of synthetic and real-world videos. The method produces realistic and physically plausible interactions, representing a step toward interactive digital twins.",
      "mindmap": ""
    },
    {
      "title": "Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting",
      "authors": "Ananta R. Bhattarai, Helge Rhodin",
      "institution": "Bielefeld University",
      "link": "https://arxiv.org/pdf/2512.17908",
      "code": null,
      "tags": [
        "computer vision",
        "test-time refinement",
        "self-supervised learning",
        "shape from shading",
        "score distillation sampling",
        "monocular depth estimation",
        "diffusion models"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/abb5eab47c4353057728b3e9889e13b412f6c11ae6703f713d247c4724fbb909_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces Re-Depth Anything, a test-time self-supervision framework that refines monocular depth predictions by re-lighting the geometry and using a 2D diffusion model's priors via Score Distillation Sampling. It updates only specific model embeddings and the decoder to prevent collapse, rather than fine-tuning the entire network. The method shows substantial improvements in depth accuracy and realism over the baseline Depth Anything V2 model.",
      "mindmap": ""
    },
    {
      "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing",
      "authors": "Shilong Zhang, He Zhang, Zhifei Zhang, Chongjian Ge, Shuchen Xue, Shaoteng Liu, Mengwei Ren, Soo Ye Kim, Yuqian Zhou, Qing Liu, Daniil Pakhomov, Kai Zhang, Zhe Lin, Ping Luo",
      "institution": "The University of Hong Kong, Adobe Research, University of Chinese Academy of Sciences",
      "link": "https://arxiv.org/pdf/2512.17909",
      "code": null,
      "tags": [
        "diffusion training",
        "latent diffusion models",
        "representation encoders",
        "semantic-pixel reconstruction",
        "variational autoencoder",
        "text-to-image generation",
        "image editing"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ae25315fc54e46dd0abb6b1bc7b9cf8b409962a4f4b702095e19e1f36529da12_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes a framework to adapt discriminative representation encoders for generative tasks by introducing a semantic-pixel reconstruction objective, which compresses both semantic and fine-grained details into a compact latent space. The resulting model achieves state-of-the-art image reconstruction and enables unified text-to-image generation and editing, demonstrating that representation encoders can be effectively adapted into robust generative components.",
      "mindmap": ""
    },
    {
      "title": "SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis",
      "authors": "N. A. Adarsh Pritam, Jeba Shiney O, Sanyam Jain",
      "institution": "Alliance University, Østfold University College",
      "link": "https://arxiv.org/pdf/2512.17585",
      "code": null,
      "tags": [
        "diffusion training",
        "StyleGAN2-ADA",
        "Denoising Diffusion Probabilistic Models (DDPMs)",
        "FID",
        "KID",
        "Inception Score",
        "ViT-B/16",
        "synthetic data augmentation"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/497a14da92ec4e58a3716d9bb6044a8b43d59d0f8ab0aff8c6e70b0d8e5325c9_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces SkinGenBench, a benchmark that evaluates the effects of generative models (StyleGAN2-ADA and DDPMs) and preprocessing pipelines on synthetic dermoscopic image generation for melanoma diagnosis. The main conclusion is that the choice of generative architecture has a stronger impact on image quality and diagnostic utility than preprocessing complexity, with StyleGAN2-ADA outperforming diffusion models in fidelity, and synthetic augmentation significantly boosting downstream classifier performance.",
      "mindmap": ""
    },
    {
      "title": "Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data",
      "authors": "Rahul Ravi, Ruizhe Li, Tarek Abdelfatah, Stephen Chan, Xin Chen",
      "institution": "University of Nottingham, Nottingham City Hospital",
      "link": "https://arxiv.org/pdf/2512.17759",
      "code": null,
      "tags": [
        "medical imaging",
        "image registration",
        "radiomics",
        "deep learning",
        "logistic regression",
        "feature selection"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ebe1fee674daeeb2ff62e605cac35448ddb7b933f1a6948f0aa461b318710117_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes a framework using aligned longitudinal MRI and clinical data to predict breast cancer treatment response. The method involves tumor segmentation, image registration, and feature extraction, comparing radiomics and deep learning models. The main conclusion is that image registration significantly improves prediction, with radiomics features outperforming deep learning features in predicting pathologic complete response and relapse-free survival.",
      "mindmap": ""
    },
    {
      "title": "MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation",
      "authors": "Saikat Roy, Yannick Kirchhoff, Constantin Ulrich, Maximillian Rokuss, Tassilo Wald, Fabian Isensee, Klaus Maier-Hein",
      "institution": "German Cancer Research Center (DKFZ), Heidelberg University",
      "link": "https://arxiv.org/pdf/2512.17774",
      "code": null,
      "tags": [
        "multi-modal training",
        "3D ConvNeXt",
        "Global Response Normalization",
        "depth scaling",
        "width scaling",
        "context scaling",
        "supervised pretraining",
        "volumetric segmentation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces MedNeXt-v2, a compound-scaled 3D ConvNeXt architecture enhanced with a Global Response Normalization module for large-scale supervised representation learning in medical image segmentation. The authors demonstrate that scaling the backbone network's architecture is crucial for effective pretraining, and MedNeXt-v2 achieves state-of-the-art performance when fine-tuned on diverse CT and MR benchmarks. The work establishes that a stronger backbone design yields better downstream segmentation results than simply increasing dataset size.",
      "mindmap": ""
    },
    {
      "title": "D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models",
      "authors": "Javon Hickmon",
      "institution": "University of Washington",
      "link": "https://arxiv.org/pdf/2512.15747",
      "code": null,
      "tags": [
        "multi-modal inference",
        "CLIP",
        "Stable Diffusion XL",
        "zero-shot classification",
        "demographic bias mitigation",
        "data generation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes D3G, a training-free method that uses Stable Diffusion XL to generate diverse demographic data at inference time to improve zero-shot image classification with CLIP. The method is shown to boost classification accuracy while reducing harmful demographic bias in pre-trained multimodal models.",
      "mindmap": ""
    },
    {
      "title": "Surely Large Multimodal Models (Don't) Excel in Visual Species Recognition?",
      "authors": "Tian Liu, Anwesha Basu, James Caverlee, Shu Kong",
      "institution": "Texas A&M University, University of Macau, Institute of Collaborative Innovation",
      "link": "https://arxiv.org/pdf/2512.15748",
      "code": null,
      "tags": [
        "multi-modal inference",
        "few-shot learning",
        "post-hoc correction",
        "large multimodal models",
        "visual species recognition",
        "prompting",
        "re-ranking"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes Post-hoc Correction (POC), a method that uses a Large Multimodal Model (LMM) to re-rank the top predictions from a few-shot learning expert model for Visual Species Recognition. It concludes that while LMMs alone underperform on this specialized task, they can effectively correct expert model errors, and POC significantly boosts few-shot learning accuracy without additional training.",
      "mindmap": ""
    },
    {
      "title": "Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real",
      "authors": "Yan Yang, George Bebis, Mircea Nicolescu",
      "institution": "University of Nevada, Reno",
      "link": "https://arxiv.org/pdf/2512.15774",
      "code": null,
      "tags": [
        "computer vision",
        "generative adversarial networks",
        "data augmentation",
        "image-to-image translation",
        "rule-based mask warping",
        "non-mask preservation loss"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a two-step data augmentation method for masked face detection and recognition, combining rule-based mask warping with GAN-based unpaired image-to-image translation to generate realistic masked faces. It introduces a non-mask preservation loss and stochastic noise injection to improve training and diversity. The approach shows qualitative improvements over rule-based methods alone and complements other GAN-based generation techniques.",
      "mindmap": ""
    },
    {
      "title": "Human-like Working Memory from Artificial Intrinsic Plasticity Neurons",
      "authors": "Jingli Liu, Huannan Zheng, Bohao Zou, Kezhou Yang",
      "institution": "The Hong Kong University of Science and Technology (Guangzhou)",
      "link": "https://arxiv.org/pdf/2512.15829",
      "code": null,
      "tags": [
        "others",
        "neuromorphic computing",
        "intrinsic plasticity",
        "Magnetic Tunnel Junctions (MTJs)",
        "hardware-software co-design",
        "near-sensor processing",
        "working memory"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces IPNet, a neuromorphic architecture that uses Magnetic Tunnel Junction (MTJ) neurons with intrinsic plasticity to physically emulate human-like working memory. The hardware-software co-designed system achieves high accuracy on dynamic vision tasks and significantly reduces energy consumption and footprint compared to traditional models like LSTMs. The work demonstrates that bio-inspired intrinsic plasticity can provide superior processing efficiency and performance for real-time applications.",
      "mindmap": ""
    },
    {
      "title": "Large Video Planner Enables Generalizable Robot Control",
      "authors": "Boyuan Chen, Tianyuan Zhang, Haoran Geng, Kiwhan Song, Caiyi Zhang, Peihao Li, William T. Freeman, Jitendra Malik, Pieter Abbeel, Russ Tedrake, Vincent Sitzmann, Yilun Du",
      "institution": "MIT, UC Berkeley, Harvard",
      "link": "https://arxiv.org/pdf/2512.15840",
      "code": null,
      "tags": [
        "robotics",
        "foundation models",
        "large-scale video pretraining",
        "generative video planning",
        "vision-language-action (VLA)",
        "zero-shot planning",
        "internet-scale video dataset"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes an alternative paradigm for building robot foundation models by using large-scale video pretraining, rather than extending multimodal LLMs, to create a generative video planner. The model produces zero-shot video plans from novel instructions, which are then post-processed into executable robot actions. The approach demonstrates robust generalization and real-world feasibility in robot control tasks.",
      "mindmap": ""
    },
    {
      "title": "Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models",
      "authors": "Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Pier Luigi Dovesi, Shaghayegh Roohi, Mark Granroth-Wilding, Rita Cucchiara",
      "institution": "University of Modena and Reggio Emilia, AMD Silo AI",
      "link": "https://arxiv.org/pdf/2512.15885",
      "code": null,
      "tags": [
        "multi-modal training",
        "self-supervised learning",
        "vision-language alignment",
        "I-JEPA",
        "JARVIS",
        "masked predictive loss",
        "frozen vision foundation models"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces JARVIS, a self-supervised framework that integrates the I-JEPA learning paradigm into multimodal large language model (MLLM) training to enhance visual understanding. It uses frozen vision models as encoders and trains an LLM-based predictor to learn visual regularities without relying solely on textual supervision. The method consistently improves performance on vision-centric benchmarks across different LLM families without degrading multimodal reasoning abilities.",
      "mindmap": ""
    },
    {
      "title": "SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks",
      "authors": "Vegard Flovik",
      "institution": "DNV",
      "link": "https://arxiv.org/pdf/2512.15938",
      "code": null,
      "tags": [
        "mechanistic interpretability",
        "sparse autoencoder",
        "ℓ1-regularization",
        "Grad-FAM",
        "weight-space editing",
        "critical suppression threshold"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces SALVE, a framework that uses an ℓ1-regularized sparse autoencoder to discover and validate latent features in neural networks, then performs precise weight-space edits to control model behavior. It demonstrates consistent control across ResNet-18 and ViT models, providing a methodology to turn feature discovery into actionable model edits for more transparent AI systems.",
      "mindmap": ""
    },
    {
      "title": "City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs",
      "authors": "Dwip Dalal, Utkarsh Mishra, Narendra Ahuja, Nebojsa Jojic",
      "institution": "University of Illinois Urbana-Champaign, Texas A&M University, Microsoft Research",
      "link": "https://arxiv.org/pdf/2512.15933",
      "code": null,
      "tags": [
        "multi-modal inference",
        "Verbalization of Path (VoP)",
        "Sparsely Grounded Visual Navigation",
        "CityNav benchmark",
        "multimodal large language models (MLLMs)",
        "cognitive map"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces the CityNav benchmark to evaluate multimodal large language models (MLLMs) on the knowledge-intensive task of city navigation without maps or GPS. It proposes the Verbalization of Path (VoP) method, which extracts an explicit cognitive map of landmarks and directions from the MLLM to guide sequential decision-making. The authors conclude that current MLLMs and standard reasoning techniques underperform in this setting, but VoP significantly improves navigation success.",
      "mindmap": ""
    },
    {
      "title": "R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space",
      "authors": "Tin Stribor Sohn, Maximilian Dillitzer, Jason J. Corso, Eric Sax",
      "institution": "Karlsruhe Institute of Technology, Esslingen University of Applied Sciences, Dr. Ing. h.c. F. Porsche AG, University of Michigan, Voxel51 Inc.",
      "link": "https://arxiv.org/pdf/2512.15940",
      "code": null,
      "tags": [
        "multi-modal inference",
        "retrieval-augmented generation",
        "4D spatio-temporal reasoning",
        "structured memory",
        "vision-language models",
        "embodied AI"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces R4, a training-free framework that equips vision-language models with a structured, lifelong memory by anchoring object-level semantic descriptions in a 4D spatio-temporal database. This enables retrieval-augmented reasoning over space and time for embodied tasks like question answering and navigation. Experiments show that R4 substantially improves reasoning over spatio-temporal information compared to baselines.",
      "mindmap": ""
    },
    {
      "title": "The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs",
      "authors": "Tejas Anvekar, Fenil Bardoliya, Pavan K. Turaga, Chitta Baral, Vivek Gupta",
      "institution": "Arizona State University",
      "link": "https://arxiv.org/pdf/2512.15949",
      "code": null,
      "tags": [
        "multimodal evaluation",
        "perceptual grounding",
        "robustness evaluation",
        "controlled perturbations",
        "pixel-based augmentations",
        "diffusion-based stylized illusions",
        "face matching",
        "text-in-vision comprehension",
        "image matching",
        "grid pointing game",
        "attribute localization"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces The Perceptual Observatory, a framework for evaluating Multimodal Large Language Models (MLLMs) by testing their perceptual grounding and robustness through controlled tasks and systematic image perturbations. It concludes that current MLLM evaluations overemphasize end-task accuracy and lack rigorous assessment of fundamental visual understanding, revealing that model progress may rely more on textual knowledge than genuine visual perception.",
      "mindmap": ""
    },
    {
      "title": "Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models",
      "authors": "Utsav Panchal, Yuchen Liu, Luigi Palmieri, Ilche Georgievski, Marco Aiello",
      "institution": "University of Stuttgart, Bosch Research",
      "link": "https://arxiv.org/pdf/2512.15957",
      "code": null,
      "tags": [
        "multi-modal training",
        "Vision Language Model",
        "Scene Graph",
        "Supervised Fine-Tuning",
        "Direct Preference Optimization",
        "synthetic data"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces CAMP-VLM, a framework that uses a Vision Language Model enhanced with scene graphs and contextual features to predict multi-human behaviors from a third-person view. It is fine-tuned with synthetic data using SFT and DPO, and the results show it outperforms the best baseline by up to 66.9% in prediction accuracy.",
      "mindmap": ""
    },
    {
      "title": "From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection",
      "authors": "Manuel Nkegoum, Minh-Tan Pham, Élisa Fromont, Bruno Avignon, Sébastien Lefèvre",
      "institution": "Univ Bretagne Sud, IRISA; Univ Rennes, IRISA; ATERMES; UiT The Arctic University of Norway",
      "link": "https://arxiv.org/pdf/2512.15971",
      "code": null,
      "tags": [
        "computer vision",
        "vision-language models",
        "few-shot learning",
        "multispectral object detection",
        "Grounding DINO",
        "YOLO-World",
        "modality integration"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper adapts vision-language models (VLMs) like Grounding DINO and YOLO-World for few-shot multispectral object detection by integrating text, visual (RGB), and thermal (IR) modalities. It demonstrates that these VLM-based detectors outperform specialized models in data-scarce settings and achieve competitive results in fully supervised scenarios on benchmarks like FLIR and M3FD. The findings show that semantic priors from large-scale VLMs effectively transfer to unseen spectral domains, enabling data-efficient multispectral perception.",
      "mindmap": ""
    },
    {
      "title": "Are vision-language models ready to zero-shot replace supervised classification models in agriculture?",
      "authors": "Earl Ranario, Mason J. Earles",
      "institution": "University of California, Davis",
      "link": "https://arxiv.org/pdf/2512.15977",
      "code": null,
      "tags": [
        "computer vision",
        "agriculture",
        "vision-language models",
        "zero-shot classification",
        "multiple-choice prompting",
        "open-ended prompting",
        "semantic judging",
        "YOLO11",
        "AgML"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper benchmarks various open-source and closed-source vision-language models (VLMs) in a zero-shot setting on 27 agricultural classification datasets from AgML. It finds that VLMs significantly underperform a supervised YOLO11 baseline, with performance varying greatly based on prompting strategy and evaluation methodology. The authors conclude that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems but could serve as assistive tools when combined with constrained interfaces and domain-aware evaluation.",
      "mindmap": ""
    },
    {
      "title": "Eyes on the Grass: Biodiversity-Increasing Robotic Mowing Using Deep Visual Embeddings",
      "authors": "Lars Beckers, Arno Waes, Aaron Van Campenhout, Toon Goedemé",
      "institution": "KU Leuven",
      "link": "https://arxiv.org/pdf/2512.15993",
      "code": null,
      "tags": [
        "multi-modal inference",
        "ResNet50",
        "PlantNet300K",
        "deep visual embeddings",
        "feature-space analysis",
        "biodiversity estimation",
        "selective mowing algorithm"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper presents a robotic mowing system that uses a ResNet50 model, pretrained on PlantNet300K, to generate deep visual embeddings of vegetation. It estimates biodiversity from the dispersion of these embeddings and uses this to selectively deactivate the mower blades to preserve diverse patches. The results show a strong correlation between the embedding-space diversity and expert assessments, validating the approach for enhancing garden biodiversity.",
      "mindmap": ""
    },
    {
      "title": "CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion",
      "authors": "Liudi Yang, Yang Bai, George Eskandar, Fengyi Shen, Mohammad Altillawi, Dong Chen, Ziyuan Liu, Abhinav Valada",
      "institution": "University of Freiburg, Ludwig Maximilian University of Munich, Munich Center for Machine Learning (MCML), Technical University of Munich, Huawei Heisenberg Research Center (Munich)",
      "link": "https://arxiv.org/pdf/2512.16023",
      "code": null,
      "tags": [
        "multi-modal training",
        "diffusion model",
        "bridge attention",
        "action refinement"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces CoVAR, a method for co-generating video and robotic action sequences from text instructions using a multi-modal diffusion framework. It extends a pretrained video diffusion model with a parallel action diffusion model and connects them via a Bridge Attention mechanism for cross-modal interaction, along with an action refinement module. The approach demonstrates superior performance in generating high-quality videos and accurate actions compared to existing baselines, offering a scalable way to leverage video data for robotic learning.",
      "mindmap": ""
    },
    {
      "title": "Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving",
      "authors": "Jiaheng Geng, Jiatong Du, Xinyu Zhang, Ye Li, Panqu Wang, Yanjun Huang",
      "institution": "Tongji University, ZERON",
      "link": "https://arxiv.org/pdf/2512.16055",
      "code": null,
      "tags": [
        "others",
        "flow matching",
        "adversarial policy",
        "closed-loop evaluation",
        "end-to-end autonomous driving",
        "real-world image generation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a closed-loop evaluation platform for end-to-end autonomous driving that uses a flow matching-based real-world image generator and an adversarial traffic policy to create safety-critical corner cases. The platform efficiently generates realistic driving scenes to test models like UniAD and VAD, demonstrating their performance degradation in adversarial scenarios. This method effectively identifies model weaknesses to improve the safety and robustness of autonomous driving systems.",
      "mindmap": ""
    },
    {
      "title": "Auto-Vocabulary 3D Object Detection",
      "authors": "Haomeng Zhang, Kuan-Chuan Peng, Suhas Lohit, Raymond A. Yeh",
      "institution": "Purdue University, Mitsubishi Electric Research Laboratories (MERL)",
      "link": "https://arxiv.org/pdf/2512.16077",
      "code": null,
      "tags": [
        "computer vision",
        "auto-vocabulary 3D object detection",
        "vision-language models",
        "image captioning",
        "pseudo 3D box generation",
        "feature-space semantics expansion",
        "semantic score"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces Auto-Vocabulary 3D Object Detection (AV3DOD), a framework that autonomously generates class names for detected 3D objects without user input by leveraging 2D vision-language models for semantic candidate generation. It achieves state-of-the-art performance in both localization and semantic quality on standard datasets, outperforming prior methods like CoDA.",
      "mindmap": ""
    },
    {
      "title": "FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution",
      "authors": "Hao Tang, Hanyu Liu, Alessandro Perelli, Xi Chen, Chao Li",
      "institution": "University of Dundee, University of Bath, University of Cambridge",
      "link": "https://arxiv.org/pdf/2512.16075",
      "code": null,
      "tags": [
        "diffusion training",
        "3D multi-channel patch diffusion model",
        "FOD-patch adapter",
        "voxel-level conditional coordinating module",
        "SH attention module",
        "spherical harmonics"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes FOD-Diff, a 3D multi-channel patch diffusion model that predicts high angular resolution fiber orientation distributions (HAR-FOD) from low angular resolution inputs. The method incorporates a patch adapter using brain anatomy priors, a conditional coordinating module, and a spherical harmonics attention module to handle complex coefficient correlations. The experimental results demonstrate that this approach achieves state-of-the-art performance in HAR-FOD prediction.",
      "mindmap": ""
    },
    {
      "title": "LAPX: Lightweight Hourglass Network with Global Context",
      "authors": "Haopeng Zhao, Marsha Mariya Kappan, Mahdi Bamdad, Francisco Cruz",
      "institution": "University of New South Wales, Universidad Central de Chile",
      "link": "https://arxiv.org/pdf/2512.16089",
      "code": null,
      "tags": [
        "others",
        "hourglass network",
        "self-attention",
        "lightweight attention",
        "global context",
        "human pose estimation",
        "edge device"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes LAPX, a lightweight Hourglass network enhanced with self-attention to capture global context for efficient human pose estimation. It refines stage design and attention modules to balance accuracy and speed, achieving competitive results on MPII and COCO datasets with only 2.3M parameters. The model demonstrates real-time performance suitable for deployment on edge devices.",
      "mindmap": ""
    },
    {
      "title": "Collimator-assisted high-precision calibration method for event cameras",
      "authors": "Zibin Liu, Shunkun Liang, Banglei Guan, Dongcai Tan, Yang Shang, Qifeng Yu",
      "institution": "National University of Defense Technology",
      "link": "https://arxiv.org/pdf/2512.16092",
      "code": null,
      "tags": [
        "computer vision",
        "sensor calibration",
        "collimator",
        "sphere motion model",
        "nonlinear optimization",
        "flickering star-based patterns",
        "geometric calibration"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a high-precision calibration method for event cameras using a collimator with flickering star-based patterns. The method first linearly solves camera parameters via a sphere motion model and then refines them with nonlinear optimization. The authors demonstrate that this approach outperforms existing calibration methods in accuracy and reliability for long-range measurement scenarios.",
      "mindmap": ""
    },
    {
      "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times",
      "authors": "Jintao Zhang, Kaiwen Zheng, Kai Jiang, Haoxu Wang, Ion Stoica, Joseph E. Gonzalez, Jianfei Chen, Jun Zhu",
      "institution": "Tsinghua University, Shengshu Technology, UC Berkeley",
      "link": "https://arxiv.org/pdf/2512.16093",
      "code": null,
      "tags": [
        "diffusion inference",
        "SageAttention",
        "Sparse-Linear Attention (SLA)",
        "rCM",
        "W8A8 quantization",
        "step distillation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "TurboDiffusion is a framework that accelerates video diffusion models by 100-200 times using attention acceleration (low-bit SageAttention and Sparse-Linear Attention), step distillation (rCM), and W8A8 quantization. Experiments on several models show it achieves this speedup on a single GPU while maintaining comparable video quality.",
      "mindmap": ""
    }
  ]
}