{
  "label": "cs.CY",
  "slug": "cscy",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "New Exam Security Questions in the AI Era: Comparing AI-Generated Item Similarity Between Naive and Detail-Guided Prompting Approaches",
      "authors": "Ting Wang, Caroline Prendergast, Susan Lottridge",
      "institution": "American Board of Family Medicine, American Board of Surgery, Cambium Assessment",
      "link": "https://arxiv.org/pdf/2512.23729",
      "code": null,
      "tags": [
        "question generation",
        "large language models",
        "multiple-choice questions",
        "cosine similarity",
        "PubMedBERT",
        "BioBERT"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4fdc5d47472b074849e044e7c47d734f240a24f0b2daf6de26e97d0f0315992_w640_q70.webp",
      "contributions": "1. Introduced a comparative framework to assess the security risk of LLM-generated exam items by measuring the similarity between items created with proprietary (\"guided\") and publicly available (\"naive\") prompting strategies. 2. Demonstrated that LLMs using only public information can generate items highly similar to those created with proprietary guidance in narrowly defined clinical domains, identifying a specific security vulnerability. 3. Proposed concrete mitigation strategies for high-stakes exam security, including human-first AI-assisted development, separation of item pools, and systematic similarity surveillance.",
      "summary": "This paper investigates the security risk that LLMs pose to high-stakes exams by comparing the similarity of AI-generated multiple-choice questions created with two prompting strategies: a naive approach using only public information and a guided approach using proprietary exam materials. Using PubMedBERT and BioBERT embeddings to calculate cosine similarity, the study found that while guided items are generally distinct, naive prompts can produce highly similar items in constrained domains like viral pneumonia. The conclusion is that this convergence heightens item exposure risks, necessitating new safeguards like human oversight and systematic similarity checks.",
      "mindmap": "graph TB\n        A[New Exam Security Questions in the AI Era<br>AI时代的新考试安全问题] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>LLMs生成考题对考试安全的威胁] --> B1[LLMs can generate exam items from public data<br>LLMs可利用公开数据生成考题]\n        C[主要方法/Method<br>比较两种提示策略的相似性] --> C1[Naive Prompting: Public EPA descriptors<br>朴素提示: 公开的EPA描述]\n        C --> C2[Guided Prompting: Proprietary blueprints & guidelines<br>引导提示: 专有蓝图和指南]\n        C --> C3[Models: GPT-4o, Claude 4, Gemini 2.5<br>模型: GPT-4o, Claude 4, Gemini 2.5]\n        C --> C4[Similarity Analysis: PubMedBERT/BioBERT embeddings & cosine similarity<br>相似性分析: PubMedBERT/BioBERT嵌入与余弦相似度]\n        D[关键结果/Results<br>引导与朴素提示的相似性] --> D1[High internal consistency within each strategy<br>各策略内部一致性高]\n        D --> D2[Lower cross-strategy similarity overall<br>策略间总体相似性较低]\n        D --> D3[High similarity (>0.65) in narrow domains (e.g., viral pneumonia)<br>狭窄领域(如病毒性肺炎)相似性高(>0.65)]\n        D --> D4[Conclusion: Risk of item exposure, need for safeguards<br>结论: 考题暴露风险，需制定防护措施]"
    },
    {
      "title": "A Systematic Mapping on Software Fairness: Focus, Trends and Industrial Context",
      "authors": "Kessia Nepomuceno, Fabio Petrillo",
      "institution": "École de Technologie Supérieure",
      "link": "https://arxiv.org/pdf/2512.23782",
      "code": null,
      "tags": [
        "software fairness",
        "systematic mapping",
        "fairness",
        "software engineering",
        "technology readiness level",
        "group fairness"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ba1e54194de977ce396b8c19de44fec12f293e42ef1d03cfcc83d187b2d3da83_w640_q70.webp",
      "contributions": "1. A systematic literature mapping of 95 studies to categorize advancements in software fairness solutions. 2. A novel classification framework for analyzing software fairness research from the perspectives of trends, focus, and industrial viability. 3. An analysis revealing the field's focus on post-processing methods and group fairness, with limited industry collaboration and low-to-medium TRL, highlighting gaps for future work.",
      "summary": "This paper conducts a systematic mapping study to analyze research on fairness in software systems. It develops a classification framework applied to 95 studies, finding that current work is heavily algorithmic, focused on post-processing and group fairness, and lacks industrial collaboration and high readiness levels. The conclusion calls for integrating fairness across the entire software development lifecycle and increasing academia-industry partnerships.",
      "mindmap": "graph TB\n        Root[系统性映射研究:软件公平性<br/>A Systematic Mapping on Software Fairness] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br/>缺乏对软件公平性解决方案的全面理解<br/>Lack of comprehensive understanding of fairness solutions] --> Problem_Sub[挑战/Challenge<br/>研究分散，工业应用不明确<br/>Fragmented research, unclear industrial viability]\n        Method[主要方法/Method<br/>系统性文献映射<br/>Systematic Literature Mapping] --> Method_Sub[应用分类框架分析95项研究<br/>Apply classification framework to 95 studies]\n        Results[关键结果/Results<br/>研究发现/Findings] --> Results_Sub1[研究趋势/Research Trends<br/>扩展中但集中于算法<br/>Expanding but focused on algorithms]\n        Results --> Results_Sub2[研究焦点/Research Focus<br/>后处理和群体公平性<br/>Post-processing & group fairness]\n        Results --> Results_Sub3[工业背景/Industrial Context<br/>学术主导，TRL低至中<br/>Academic-led, low-to-medium TRL]"
    },
    {
      "title": "Artificial Intelligence for All? Brazilian Teachers on Ethics, Equity, and the Everyday Challenges of AI in Education",
      "authors": "Bruno Florentino, Camila Sestito, Wellington Cruz, André de Carvalho, Robson Bonidia",
      "institution": "University of São Paulo, Federal University of Technology-Paraná (UTFPR), Instituto Significare",
      "link": "https://arxiv.org/pdf/2512.23834",
      "code": null,
      "tags": [
        "AI in Education",
        "AI literacy",
        "teacher perceptions",
        "quantitative survey",
        "ethics",
        "infrastructure challenges"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e9fd8e32651f0740b552d9443daf7c424819558f55a942221ff741ca45c296c9_w640_q70.webp",
      "contributions": "1. Provides empirical data on the AI literacy levels and application interests of Brazilian K-12 teachers, revealing a high interest despite low knowledge. 2. Identifies key structural barriers (lack of training, technical support, and infrastructure) to AI adoption in Brazilian public education. 3. Highlights the critical importance teachers place on discussing ethics, digital citizenship, and responsible AI use within the pedagogical context.",
      "summary": "This study quantitatively analyzes Brazilian K-12 teachers' perceptions of AI in education through a survey of 346 educators. The results show strong teacher interest in using AI for pedagogical tasks despite limited knowledge, while identifying significant structural challenges and emphasizing the need for ethical discussions. The study concludes that effective AI integration in Brazil requires integrated public policies, teacher training, and equitable access to technology.",
      "mindmap": "graph TB\n        Root(Artificial Intelligence for All? Brazilian Teachers on Ethics, Equity, and the Everyday Challenges of AI in Education) --> Problem(核心问题/Problem)\n        Root --> Method(主要方法/Method)\n        Root --> Results(关键结果/Results)\n        Problem --> P1(巴西教师对AI的认知与态度/Brazilian Teachers' Perceptions and Attitudes towards AI)\n        Problem --> P2(AI在教育中的伦理与公平挑战/Ethical and Equity Challenges of AI in Education)\n        Method --> M1(定量问卷调查/Quantitative Questionnaire Survey)\n        M1 --> M1_1(346名巴西K-12教师/346 Brazilian K-12 Teachers)\n        Results --> R1(高兴趣但知识有限/High Interest but Limited Knowledge)\n        Results --> R2(关注伦理与结构挑战/Concerns on Ethics and Structural Challenges)\n        Results --> R3(需要政策与培训支持/Need for Policy and Training Support)"
    },
    {
      "title": "Seeking Late Night Life Lines: Experiences of Conversational AI Use in Mental Health Crisis",
      "authors": "Leah Hope Ajmani, Arka Ghosh, Benjamin Kaveladze, Eugenia Kim, Keertana Namuduri, Theresa Nguyen, Ebele Okoli, Jessica Schleider, Denae Ford, Jina Suh",
      "institution": "University of Minnesota, Northwestern University, Dartmouth College, Microsoft, Microsoft Research, Mental Health America",
      "link": "https://arxiv.org/pdf/2512.23859",
      "code": null,
      "tags": [
        "conversational ai",
        "mental health crisis",
        "stages of change model",
        "human-AI interaction",
        "testimonial survey",
        "expert interviews"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/557b0c2624da79d40758f334c7d781c4558951ee96a27d54b04a81b3f20ec2ea_w640_q70.webp",
      "contributions": "1. Provides first-person experiential data on using conversational AI during mental health crises via a testimonial survey (n=53). 2. Contrasts user experiences with mental health expert perspectives (n=16) to highlight the essential role of human connection in crisis management. 3. Proposes a responsible design framework for AI crisis intervention, positioning AI as a bridge to human support using the stages of change model.",
      "summary": "This paper investigates how people use conversational AI (e.g., ChatGPT) during mental health crises through a survey and expert interviews. It finds users turn to AI due to gaps in human support, but experts emphasize human connection is crucial. The study concludes that responsible AI should act as a bridge to human help, increasing preparedness for positive action and de-escalating crises.",
      "mindmap": "graph TB\n        A[Seeking Late Night Life Lines: Experiences of Conversational AI Use in Mental Health Crisis] --> B(核心问题/Problem: Can conversational AI responsibly support mental health crises?)\n        A --> C(主要方法/Method: Testimonial survey (n=53) & expert interviews (n=16))\n        A --> D(关键结果/Results: AI fills gaps in human support; Human connection is essential; Design AI as a bridge to human help)"
    },
    {
      "title": "Improving Reliability of Human Trafficking Alerts in Airports",
      "authors": "Nana Oye Akrofi Quarcoo, Milena Radenkovic",
      "institution": "The University of Nottingham",
      "link": "https://arxiv.org/pdf/2512.23865",
      "code": null,
      "tags": [
        "delay tolerant networks",
        "Delay Tolerant Networks",
        "Mobile Ad Hoc Networks",
        "Opportunistic Network Environment",
        "Spray and Wait",
        "Epidemic"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59e1cbec09d2308ac06a3bef24acf6366e80996c8ef8af583749661d0fe4f3bf_w640_q70.webp",
      "contributions": "1. Investigates the application of DTN protocols (Spray and Wait, Epidemic) for emergency alerting in airport human trafficking scenarios. 2. Simulates and evaluates the performance of these protocols in terms of delivery ratio and latency using the ONE simulator. 3. Discusses the potential role and limitations of DTN networks in combating human trafficking, bridging a technical evaluation with a critical real-world application.",
      "summary": "This paper investigates using Delay Tolerant Network (DTN) protocols to improve the reliability of emergency alerts for human trafficking victims in airports where conventional networks are unavailable. It simulates the scenario using the ONE simulator to evaluate the performance of Spray and Wait and Epidemic protocols on delivery ratio and latency. The study compares the protocols' advantages and limitations, concluding on their potential role in addressing this global issue.",
      "mindmap": "graph TB\n        A[Improving Reliability of Human Trafficking Alerts in Airports] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[机场人口贩卖紧急警报的可靠性/Reliability of emergency alerts for human trafficking in airports]\n        C --> C1[应用DTN协议: Spray and Wait, Epidemic/Apply DTN protocols: Spray and Wait, Epidemic]\n        C --> C2[使用ONE模拟器进行仿真/Simulation using the ONE simulator]\n        D --> D1[评估交付率和延迟/Evaluate delivery ratio and latency]\n        D --> D2[讨论协议优缺点与潜在作用/Discuss protocol pros/cons and potential role]"
    },
    {
      "title": "How Large Language Models Systematically Misrepresent American Climate Opinions",
      "authors": "Sola Kim, Jieshu Wang, Marco A. Janssen, John M. Anderies",
      "institution": "Arizona State University, Stony Brook University",
      "link": "https://arxiv.org/pdf/2512.23889",
      "code": null,
      "tags": [
        "large language model evaluation",
        "large language models",
        "public opinion simulation",
        "intersectionality",
        "bias evaluation",
        "climate policy"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7c0e5dfdc0fba7038277e876cd0c81062b3c5735782d5df732873aec991a84b3_w640_q70.webp",
      "contributions": "1. Conducted the first comparative study of LLM-generated public opinion against real human survey responses across intersecting demographic identities (race and gender). 2. Identified a systematic \"compression\" bias in LLMs, where they flatten the diversity of climate opinions by overestimating concern in less-concerned groups and underestimating it in more-concerned groups. 3. Revealed that this bias is intersectional, showing that LLMs apply uniform gender assumptions that fail for specific racial groups (e.g., misrepresenting gender patterns among Black Americans), a flaw potentially invisible to standard audits.",
      "summary": "This paper investigates how six large language models (LLMs) represent U.S. climate opinions by prompting them with profiles from a real national survey and comparing their generated responses to actual human answers. The study finds that LLMs systematically compress opinion diversity and misrepresent intersectional patterns, particularly for Black Americans, which could undermine equitable policy-making.",
      "mindmap": "graph TB\n        A[”How Large Language Models Systematically Misrepresent American Climate Opinions<br>论文标题”] --> B[”Problem: LLMs used for public opinion analysis may misrepresent diverse, intersectional views.<br>核心问题：用于公众意见分析的LLM可能歪曲多样化的交叉性观点。”]\n        A --> C[”Method: Prompt 6 LLMs with real survey respondent profiles and compare outputs to human answers.<br>主要方法：用真实调查受访者档案提示6个LLM，并将输出与人类答案比较。”]\n        A --> D[”Results: LLMs compress opinion diversity and misapply gender assumptions across racial groups.<br>关键结果：LLM压缩了意见多样性，并在不同种族群体中误用了性别假设。”]"
    },
    {
      "title": "In Memorium: The Academic Journal",
      "authors": "Russell Beale",
      "institution": "University of Birmingham",
      "link": "https://arxiv.org/pdf/2512.23915",
      "code": null,
      "tags": [
        "academic publishing",
        "academic journal",
        "peer review",
        "scholarly communication",
        "publishing model",
        "history of science"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d84666cdf04fcf7aca61e98202f63431df72a15f98090898c0d56de610147bb_w640_q70.webp",
      "contributions": "1. Provides a historical narrative tracing the evolution of the academic journal from its 17th-century origins to its modern commercial form. 2. Critically analyzes the shift in the journal's role from a tool for scientific dissemination to a metric for career advancement and gatekeeping. 3. Highlights the commercial exploitation of the academic publishing model, where publishers profit from free academic labor and a captive institutional market.",
      "summary": "This reflective piece examines the life cycle and societal impact of the academic journal. It traces its history from a scholarly dissemination tool to a commercialized metric for academic prestige, concluding that while its original ideals will be mourned, its final form had strayed so far that its passing will be less lamented.",
      "mindmap": "graph TB\n        Root[In Memoriam: The Academic Journal] --> Problem[核心问题/Problem: How did the academic journal evolve and what is its legacy?]\n        Root --> Method[主要方法/Method: Historical analysis and critical reflection on its role and influence.]\n        Root --> Results[关键结果/Results: Mourned for original ideals, but less missed due to commercial deviation.]"
    },
    {
      "title": "Disentangling Learning from Judgment: Representation Learning for Open Response Analytics",
      "authors": "Conrad Borchers, Manit Patel, Seiyon M. Lee, Anthony F. Botelho",
      "institution": "Carnegie Mellon University, University of Florida",
      "link": "https://arxiv.org/pdf/2512.23941",
      "code": null,
      "tags": [
        "educational data mining",
        "sentence embeddings",
        "rater effects",
        "residualization",
        "teacher priors",
        "interpretability"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/969e6dbddbe7eb87582932064e8e7d3a0853ca2850b5329fafc1020e7228f365_w640_q70.webp",
      "contributions": "1. An analytics-first framework that disentangles student response content from teacher grading tendencies, making rater effects visible and auditable. 2. A modeling pipeline using dynamic teacher priors and residualized sentence embeddings to mitigate prompt and rater confounds, validated temporally. 3. A projection method to surface disagreements between content and rater signals for qualitative inspection, transforming embeddings into reflective learning analytics.",
      "summary": "This paper addresses the problem of conflating student response content with teacher grading bias in automated scoring. The proposed method uses dynamic teacher priors and residualized sentence embeddings to separate these signals, with linear models quantifying their contributions. The main conclusion is that teacher priors heavily influence predictions, and adjusting for them sharpens the content representation, enabling better analysis of student understanding versus grading practices.",
      "mindmap": "graph TB\n        A[Disentangling Learning from Judgment<br>学习与评判的解耦] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Automated scoring conflates content with rater bias<br>自动评分混淆了内容与评分者偏差]\n        C --> C1[Framework separates content signals & teacher priors<br>框架分离内容信号与教师先验]\n        C --> C2[Uses centering & residualization on embeddings<br>对嵌入使用中心化与残差化]\n        C --> C3[Temporal validation & projection for inspection<br>时间验证与投影以供检查]\n        D --> D1[Teacher priors heavily influence grades<br>教师先验严重影响评分]\n        D --> D2[Combined model achieves best AUC (~0.815)<br>组合模型取得最佳AUC]\n        D --> D3[Residual content reveals student understanding<br>残差内容揭示学生理解]"
    },
    {
      "title": "Statistical Guarantees in the Search for Less Discriminatory Algorithms",
      "authors": "Chris Hays, Ben Laufer, Solon Barocas, Manish Raghavan",
      "institution": "MIT, Cornell University, Microsoft Research",
      "link": "https://arxiv.org/pdf/2512.23943",
      "code": null,
      "tags": [
        "algorithmic fairness",
        "model multiplicity",
        "optimal stopping",
        "disparate impact",
        "statistical guarantees",
        "less discriminatory algorithms"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/323b958070f176d29545b148d49cdc3b6141db385ed752500b67c6d9556a8c78_w640_q70.webp",
      "contributions": "1. Formalizes the search for less discriminatory algorithms (LDAs) as an optimal stopping problem, providing a statistical framework to define a \"good-faith effort\" in model development. 2. Proposes an adaptive stopping algorithm that yields a high-probability upper bound on the potential gains from continued search, allowing developers to certify the sufficiency of their exploration. 3. Provides a flexible framework where developers can incorporate stronger assumptions about the model distribution to obtain correspondingly stronger statistical bounds, validated on real-world datasets.",
      "summary": "The paper addresses the problem of how firms can demonstrate a good-faith effort to find less discriminatory algorithms. It proposes an adaptive stopping algorithm based on optimal stopping theory, which provides statistical guarantees on the potential benefits of further search. The method allows developers to certify that their search for fairer models was sufficient, as validated on credit, employment, and housing datasets.",
      "mindmap": "graph TB\n        Root[”Statistical Guarantees in the Search for Less Discriminatory Algorithms<br>寻找更少歧视性算法的统计保证”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>What constitutes a good-faith search for less discriminatory models?<br>什么是对更少歧视性模型的真诚搜索？”]\n        Method[”主要方法/Method<br>Formalize search as an optimal stopping problem; propose adaptive stopping algorithm.<br>将搜索形式化为最优停止问题；提出自适应停止算法。”]\n        Results[”关键结果/Results<br>High-probability bound on search gains; framework for certification.<br>搜索收益的高概率上界；用于认证的框架。”]"
    },
    {
      "title": "From artificial to circular intelligence to support the well-being of our habitat",
      "authors": "Francesca Larosa, Daniel Depellegrin, Andrea Conte, Marco Molinari, Silvia Santato, Adam Wickberg, Fermin Mallor, Anna Sperotto",
      "institution": "Royal Institute of Technology (KTH)",
      "link": "https://arxiv.org/pdf/2512.24131",
      "code": null,
      "tags": [
        "sustainable ai",
        "Circular Intelligence",
        "CIntel",
        "socio-environmental impact",
        "bottom-up approach",
        "ethical design"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0c613cbcdae547ac2c22485aba0219d44e2ac5ed102d38078c7427fe50cf1e99_w640_q70.webp",
      "contributions": "1. Proposes a novel conceptual and procedural framework called Circular Intelligence (CIntel) to address the environmental impact of AI. 2. Introduces a bottom-up, community-driven approach for AI design that learns from nature's regenerative and adaptive abilities. 3. Operationalizes the framework through a set of economic incentives promoting a shared-cost-distributed benefits paradigm.",
      "summary": "This paper identifies the significant socio-environmental impact of data- and resource-intensive AI technologies. To address this, it proposes a new framework called Circular Intelligence (CIntel), which incorporates ethical principles and a nature-inspired, community-driven design approach to promote habitat stability and human well-being. The main conclusion is that CIntel offers a pathway to develop AI tools with minimal negative impact.",
      "mindmap": "graph TB\n    A[From artificial to circular intelligence to support the well-being of our habitat] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[AI技术对地球有负面影响/AI technologies have negative impact on Earth]\n    C --> C1[提出循环智能框架/Propose Circular Intelligence (CIntel) framework]\n    C1 --> C2[自下而上、社区驱动的方法/Bottom-up, community-driven approach]\n    C1 --> C3[学习自然的再生与适应能力/Learn from nature's regeneration & adaptation]\n    C1 --> C4[设计中融入伦理原则/Incorporate ethical principles in design]\n    D --> D1[通过经济激励实现操作化/Operationalized via economic incentives]\n    D --> D2[促进共享成本-分布式收益范式/Promote shared-cost-distributed benefits paradigm]"
    },
    {
      "title": "Effects of Algorithmic Visibility on Conspiracy Communities: Reddit after Epstein's 'Suicide'",
      "authors": "Asja Attanasio, Francesco Corso, Gianmarco De Francisci Morales, Francesco Pierri",
      "institution": "Politecnico di Milano, CENTAI",
      "link": "https://arxiv.org/pdf/2512.24351",
      "code": null,
      "tags": [
        "social computing",
        "algorithmic visibility",
        "survival analysis",
        "linguistic integration",
        "toxicity scores",
        "user retention"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1539ae39cc88306cb031a18e836539c07d88fd3d29d66ab934909984f6c9fd8c_w640_q70.webp",
      "contributions": "1. Demonstrates that algorithmic homepage visibility acts as a selection mechanism, not just an amplifier, for conspiracy community membership. 2. Shows that users arriving via homepage exposure integrate less linguistically and have shorter, less stable engagement than organic discoverers. 3. Provides evidence that incidental algorithmic exposure in this context does not lead to durable radicalization, challenging a standard narrative.",
      "summary": "This paper investigates how algorithmic visibility on Reddit's homepage shapes the r/conspiracy community after Jeffrey Epstein's death. Using a computational framework combining toxicity scores, survival analysis, and lexical/semantic measures, it finds that homepage exposure selects for users who integrate poorly and leave quickly, unlike organic joiners. The results suggest algorithmic visibility reshapes community composition and limits organic growth, without producing the durable radicalization often assumed.",
      "mindmap": "graph TB\n        A[Effects of Algorithmic Visibility on Conspiracy Communities: Reddit after Epstein's 'Suicide'] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(算法可见性如何影响阴谋论社区的用户行为？<br/>How does algorithmic visibility affect user behavior in conspiracy communities?)\n        C --> C1(计算框架：毒性分数、生存分析、词汇/语义测量<br/>Computational framework: toxicity scores, survival analysis, lexical/semantic measures)\n        D --> D1(主页可见性是选择机制，非放大器<br/>Homepage visibility is a selection mechanism, not an amplifier)\n        D --> D2(有机用户整合更快更稳定<br/>Organic users integrate faster and are more stable)\n        D --> D3(算法推荐用户参与短暂，整合弱<br/>Algorithmically-recommended users participate briefly, integrate weakly)"
    },
    {
      "title": "Learning Context: A Unified Framework and Roadmap for Context-Aware AI in Education",
      "authors": "Naiming Liu, Brittany Bradford, Johaun Hatchett, Gabriel Diaz, Lorenzo Luzi, Zichao Wang, Debshila Basu Mallick, Richard Baraniuk",
      "institution": "Rice University, OpenStax, SafeInsights, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.24362",
      "code": null,
      "tags": [
        "ai for education",
        "Learning Context (LC) framework",
        "Model Context Protocol (MCP)",
        "warm-start personalization",
        "privacy-preserving data enclaves"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b97e47cc70cdc86357a1cf43eaf9957e6f2b2e079cda07dbe3e99b2877e3bb0_w640_q70.webp",
      "contributions": "1. Proposes a unified Learning Context (LC) framework to encode cognitive, affective, and sociocultural factors for holistic learner understanding. 2. Outlines a roadmap to operationalize the LC theory into an interoperable computational data structure, leveraging the Model Context Protocol (MCP) to enable AI tools with durable context for long-term personalization. 3. Details a concrete implementation strategy through the OpenStax platform and SafeInsights infrastructure, ensuring privacy-first, ethical deployment at a national scale to reduce equity gaps.",
      "summary": "This paper introduces a unified Learning Context framework to move AI in education from context-blind approaches to a holistic, principled understanding of learners. It proposes operationalizing this framework into a computational data structure using the Model Context Protocol and details an implementation plan via the OpenStax and SafeInsights ecosystems for privacy-preserving, large-scale deployment. The work aims to achieve continual, personalized learning while maintaining high ethical standards and reducing educational inequities.",
      "mindmap": "graph TB\n        A[Learning Context: A Unified Framework and Roadmap for Context-Aware AI in Education] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: AI教育缺乏情境感知 / AI in Education Lacks Context-Awareness]\n        C[主要方法/Method: 提出统一学习情境框架与MCP协议 / Propose Unified LC Framework & MCP Protocol]\n        D[关键结果/Results: 在OpenStax/SafeInsights中实现隐私优先的个性化 / Implement Privacy-First Personalization in OpenStax/SafeInsights]"
    },
    {
      "title": "From Static to Dynamic: Evaluating the Perceptual Impact of Dynamic Elements in Urban Scenes Using Generative Inpainting",
      "authors": "Zhiwei Wei, Mengzi Zhang, Boyan Lu, Zhitao Deng, Nai Yang, Hua Liao",
      "institution": "Hunan Normal University, Beijing Normal University, China University of Geosciences",
      "link": "https://arxiv.org/pdf/2512.24513",
      "code": null,
      "tags": [
        "urban scene understanding",
        "generative inpainting",
        "semantic segmentation",
        "multimodal visual features",
        "MLLM",
        "street view imagery"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8b92ccdf405994f35d8bd91d63d957a4a7b12d20cff08ed5e4958c236edcb4a3_w640_q70.webp",
      "contributions": "1. Proposed a controlled framework using semantic segmentation and MLLM-guided generative inpainting to create paired street view images with/without dynamic elements (pedestrians, vehicles) for perceptual impact analysis. 2. Conducted a perception experiment revealing that removing dynamic elements causes a significant, consistent decrease in perceived vibrancy (30.97%), with heterogeneous effects on other dimensions and individual-level variations. 3. Extended the analysis to city-scale by training machine learning models to predict vibrancy changes, finding these perceptual alterations are widespread and spatially structured, indicating a potential underestimation of urban liveliness in static-image-based assessments.",
      "summary": "This paper addresses the bias in urban perception studies that treat scenes as static by proposing a framework to isolate the effect of dynamic elements. Using semantic segmentation and MLLM-guided generative inpainting on street view images from Dongguan, China, the study found that removing pedestrians and vehicles significantly reduces perceived vibrancy. The findings were validated with machine learning models and extended to a city-scale, showing that static imagery likely underestimates urban liveliness.",
      "mindmap": "graph TB\n    A[From Static to Dynamic: Evaluating the Perceptual Impact of Dynamic Elements in Urban Scenes Using Generative Inpainting] --> B\n    A --> C\n    A --> D\n    B[核心问题/Problem: Most urban perception studies treat scenes as static, ignoring dynamic elements like pedestrians and vehicles, causing potential bias.]\n    C[主要方法/Method: Use semantic segmentation and MLLM-guided generative inpainting to create paired images (with/without dynamic elements) and conduct perception experiments.]\n    D[关键结果/Results: Removing dynamic elements causes a 30.97% decrease in vibrancy; changes are widespread and spatially structured at city scale.]"
    },
    {
      "title": "Big AI is accelerating the metacrisis: What can we do?",
      "authors": "Steven Bird",
      "institution": "Charles Darwin University",
      "link": "https://arxiv.org/pdf/2512.24863",
      "code": null,
      "tags": [
        "ethics & society",
        "metacrisis",
        "language engineers",
        "human flourishing",
        "planetary boundaries",
        "technofeudalism"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2352136b878f355e9ebcad11726708c80426973daa2249fba0b79ba62b81b583_w640_q70.webp",
      "contributions": "1. Identifies and critiques the role of \"Big AI\" and language engineers in accelerating converging global crises (ecological, meaning, language). 2. Highlights the ethical conflict between professional obligations (e.g., ACL Code of Ethics) and the harms caused by current NLP/AI development practices. 3. Proposes a paradigm shift for NLP, advocating for a future centered on human flourishing and amplifying social networks rather than scaling through large, polluting models.",
      "summary": "This paper argues that the current trajectory of \"Big AI,\" particularly in NLP, is accelerating a global metacrisis. It critiques the field's focus on scalability and value-neutral technology development, which benefits powerful interests at the expense of the public good and the planet. The paper concludes by urgently calling for an alternative, life-affirming future for NLP centered on human flourishing.",
      "mindmap": "graph TB\n        A[Big AI is accelerating the metacrisis: What can we do?] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Big AI加速生态、意义和语言危机/Big AI accelerates ecological, meaning, and language crises]\n        B --> B2[语言工程师的伦理困境/Ethical dilemma of language engineers]\n        C --> C1[批判当前可扩展性叙事/Critique current scalability narrative]\n        C --> C2[呼吁探索替代方案/Call to explore alternatives]\n        D --> D1[需要以人类繁荣为中心的未来/NLP future must center human flourishing]\n        D --> D2[利用集体智慧设计生命肯定的NLP/Design life-affirming NLP with collective intelligence]"
    },
    {
      "title": "The Impact of LLMs on Online News Consumption and Production",
      "authors": "Hangcheng Zhao, Ron Berman",
      "institution": "Rutgers Business School, The Wharton School of the University of Pennsylvania",
      "link": "https://arxiv.org/pdf/2512.24968",
      "code": null,
      "tags": [
        "ai economics",
        "staggered difference-in-differences",
        "synthetic difference-in-differences",
        "robots.txt"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7836df9e705d8a023a351c30d3595b1ff6e9614cf1d805218347987098aa3882_w640_q70.webp",
      "contributions": "1. Quantified a moderate decline in news publisher website traffic following the rise of generative AI. 2. Demonstrated that blocking GenAI bots via robots.txt can paradoxically reduce total and real consumer traffic for large publishers. 3. Provided empirical evidence that, contrary to predictions, LLMs have not yet reduced editorial hiring and have shifted publisher content strategy towards rich media and advertising.",
      "summary": "This paper empirically investigates the impact of Large Language Models (LLMs) on online news publishers using high-frequency data and causal inference methods like difference-in-differences. It finds that blocking LLM crawlers reduces publisher traffic, LLMs have not yet replaced editorial jobs, and publishers are shifting to rich content and advertising tech. The results reveal unforeseen consequences of LLM adoption on the news ecosystem.",
      "mindmap": "graph TB\n        A[The Impact of LLMs on Online News Consumption and Production] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLMs如何影响新闻生产与消费/How LLMs affect news production and consumption]\n        C --> C1[高频数据与因果推断/High-frequency data & Causal inference]\n        D --> D1[流量下降/Traffic decline]\n        D --> D2[屏蔽爬虫反效果/Blocking bots backfires]\n        D --> D3[编辑岗位未减少/Editorial jobs not reduced]\n        D --> D4[内容转向富媒体/Shift to rich content]"
    },
    {
      "title": "Pre-review to Peer review: Pitfalls of Automating Reviews using Large Language Models",
      "authors": "Akhil Pandey Akella, Harish Varma Siravuri, Shaurya Rohatgi",
      "institution": "AllSci Corp, Sunwater Capital, Kellogg School of Management (Northwestern University), Northern Illinois University, MBZUAI",
      "link": "https://arxiv.org/pdf/2512.22145",
      "code": null,
      "tags": [
        "peer review automation",
        "large language models",
        "peer review",
        "pre-review",
        "citation prediction",
        "review alignment"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff267a101523eaa0ec56d561e9fa2c165c73baa1b3016d38df1ed64dbc91dcf6_w640_q70.webp",
      "contributions": "1. Conducted a systematic evaluation of frontier open-weight LLMs for generating peer reviews, measuring alignment with human reviewers and correlation with post-publication metrics like citations and novelty. 2. Identified key pitfalls of LLMs as autonomous reviewers, including weak correlation with human scores (0.15), systematic overestimation bias (3-5 points), and uniformly high confidence scores despite errors. 3. Demonstrated the potential utility of LLMs as pre-review screening agents, as their generated reviews correlate more strongly with post-publication outcomes than with human reviewer scores, and released an open-source dataset (DLMRSD) to support further safety research.",
      "summary": "This paper evaluates the use of large language models (LLMs) for automating academic peer review by comparing LLM-generated reviews against human reviewer scores and post-publication metrics. The study finds that while LLMs show weak alignment with human reviewers and exhibit overconfidence and bias, their reviews correlate better with future citation impact, suggesting they could serve as useful pre-review screening tools rather than fully autonomous reviewers.",
      "mindmap": "graph TB\n        A[Pre-review to Peer Review: Pitfalls of Automating Reviews using Large Language Models] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[LLMs用于自动化同行评审的安全性与可靠性/Safety & Reliability of Automating Peer Review with LLMs]\n        C --> C1[使用前沿开源LLMs生成评审并与人类评分及发表后指标对比/Using Frontier Open-Weight LLMs to Generate Reviews vs. Human Scores & Post-Publication Metrics]\n        D --> D1[LLMs与人类评审员弱相关，存在高估偏差与过度自信/Weak Correlation with Humans, Overestimation Bias, High Confidence]\n        D --> D2[LLM评审与发表后指标相关性更强，适合预审筛查/LLM Reviews Correlate More with Post-Publication Metrics, Suitable for Pre-Review Screening]"
    },
    {
      "title": "AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History",
      "authors": "Qizhi Wang",
      "institution": "PingCAP, Data & AI-Innovation Lab",
      "link": "https://arxiv.org/pdf/2512.22196",
      "code": null,
      "tags": [
        "semantic change detection",
        "diachronic embeddings",
        "orthogonal Procrustes",
        "lexical drift"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/df36f3ea25509e1c01d661a7893c2b940f15cfeb346560d105c4488a5fba4140_w640_q70.webp",
      "contributions": "1. A reproducible, expert-system style pipeline for quantifying and visualizing lexical drift in historical corpora. 2. A method coupling interpretable semantic trajectories with legally meaningful axes (e.g., mercy-versus-retribution). 3. The application of the pipeline to the Old Bailey Corpus, exposing the evolution of legal concepts like justice and crime alongside historical events.",
      "summary": "This paper presents a reproducible pipeline for analyzing semantic drift in historical legal texts. The method involves training and aligning diachronic word embeddings to quantify and visualize lexical change. The analysis of the Old Bailey Corpus reveals how concepts of justice and crime evolved with penal reforms and societal debates.",
      "mindmap": "graph TB\n        A[AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(”数字人文中语义变迁分析<br>Digital Humanities Semantic Shift Analysis”)\n        C --> C1(”可复现的专家系统流程<br>Reproducible Expert-System Pipeline”)\n        C1 --> C2(”分时段词嵌入与对齐<br>Temporal Embeddings & Alignment”)\n        C2 --> C3(”几何位移与邻域变化度量<br>Geometric & Neighborhood Metrics”)\n        D --> D1(”可视化法律概念演变<br>Visualizing Legal Concept Evolution”)\n        D1 --> D2(”揭示与历史事件的关联<br>Revealing Links to Historical Events”)"
    },
    {
      "title": "Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening",
      "authors": "Shaurya Gaur, Michel Vitale, Alessa Hering, Johan Kwisthout, Colin Jacobs, Lena Philipp, Fennie van der Graaf",
      "institution": "Radboud University Medical Center, Radboud University",
      "link": "https://arxiv.org/pdf/2512.22242",
      "code": null,
      "tags": [
        "medical imaging",
        "algorithmic fairness",
        "subgroup performance analysis",
        "JustEFAB framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/783ab81ef53ced6c68b136e7001be4ece45c131979c45d04a04c21084cbf9888_w640_q70.webp",
      "contributions": "1. Conducted a fairness evaluation of three lung cancer risk estimation models (Sybil, Venkadesh21, PanCan2b) using the JustEFAB framework to assess ethically significant biases. 2. Identified and quantified statistically significant performance disparities across demographic subgroups (e.g., gender, race) that were not explained by available clinical confounders. 3. Highlighted the critical need for monitoring and improving model fairness in lung cancer screening AI to ensure equitable clinical application.",
      "summary": "This study evaluates the fairness of AI models for lung cancer risk estimation from CT scans. Using the JustEFAB framework, it assessed performance disparities across demographic groups and found significant, unexplained biases in two deep learning models. The findings underscore the importance of algorithmic fairness in medical AI to ensure equitable screening outcomes.",
      "mindmap": "graph TB\n        Root[Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening<br/>肺癌筛查风险估计模型的公平性评估] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br/>AI肺癌风险模型在不同人口亚组中的性能表现是否公平？<br/>Is AI lung cancer risk model performance fair across demographic subgroups?]\n        Method[主要方法/Method<br/>使用JustEFAB框架评估模型在NLST验证集上的性能差异<br/>Evaluate model performance disparities on NLST validation set using JustEFAB framework]\n        Results[关键结果/Results<br/>发现Sybil和Venkadesh21模型存在显著的、无法用混杂因素解释的性能差异<br/>Found significant, unexplained performance disparities in Sybil and Venkadesh21 models]"
    },
    {
      "title": "Reddit Deplatforming and Toxicity Dynamics on Generalist Voat Communities",
      "authors": "Aleksandar Tomašević, Ana Vranić, Aleksandra Alorić, Marija Mitrović Dankulov",
      "institution": "Institute of Physics Belgrade, University of Belgrade",
      "link": "https://arxiv.org/pdf/2512.22348",
      "code": null,
      "tags": [
        "social network analysis",
        "deplatforming",
        "toxicity detection",
        "dynamic reputation modeling",
        "network analysis",
        "migration regimes"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ecb07715a5eebfede95c0e808d5c2d61c5453c8fb7fa8b8c2b8260b568fa2f9_w640_q70.webp",
      "contributions": "1. Identifies and characterizes two distinct regimes (\"Hostile Takeover\" and \"Toxic Equilibrium\") of how deplatformed users transform receiving communities on alternative platforms. 2. Demonstrates that community transformation is driven by peripheral dynamics and volume, not by newcomers capturing central network positions. 3. Shows that the structure of the migrating community (loose vs. cohesive) determines whether they disperse into generalist spaces or form dedicated enclaves.",
      "summary": "This paper studies how Reddit deplatforming affects the communities on the alternative platform Voat. Using network analysis, toxicity detection, and dynamic reputation modeling, it finds that migration leads to increased toxicity through distinct phases and that platforms have a narrow window to intervene before toxic norms become entrenched.",
      "mindmap": "graph TB\n        A[Reddit Deplatforming and Toxicity Dynamics on Generalist Voat Communities] --> B(核心问题/Problem);\n        A --> C(主要方法/Method);\n        A --> D(关键结果/Results);\n        B --> B1[替代平台接收被禁用户的影响/Impact on alternative platforms receiving banned users];\n        C --> C1[网络分析/Network Analysis];\n        C --> C2[毒性检测/Toxicity Detection];\n        C --> C3[动态声誉建模/Dynamic Reputation Modeling];\n        D --> D1[敌对接管阶段/Hostile Takeover Phase (2015-2018)];\n        D --> D2[毒性平衡阶段/Toxic Equilibrium Phase (2018-2020)];\n        D --> D3[外围动态驱动转变/Peripheral Dynamics Drive Change];"
    },
    {
      "title": "Relational Mediators: LLM Chatbots as Boundary Objects in Psychotherapy",
      "authors": "Jiatao Quan, Ziyue Li, Tian Qi Zhu, Yuxuan Li, Baoying Wang, Wanda Pratt, Nan Gao",
      "institution": "University of Washington, The Hong Kong Polytechnic University, Nankai University",
      "link": "https://arxiv.org/pdf/2512.22462",
      "code": null,
      "tags": [
        "human-ai interaction",
        "boundary objects",
        "relational mediation",
        "marginalized clients",
        "therapeutic systems",
        "dynamic framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8695c76e0392473389276989f78ab825dab06f2e38bdd785d2418d8ca9a1d80_w640_q70.webp",
      "contributions": "1. Identifies enduring relational challenges in psychotherapy for marginalized clients, such as trust-building and self-disclosure burdens. 2. Proposes the Dynamic Boundary Mediation Framework, which re-conceptualizes LLMs as adaptive boundary objects. 3. Delineates three specific forms of mediation (Epistemic, Relational, Contextual) to address knowledge gaps, power asymmetries, and therapy-life discontinuities.",
      "summary": "This paper argues that current framings of LLMs in mental health overlook their potential to mediate complex therapeutic relationships. Based on interviews with therapists and marginalized clients in China, the authors propose the Dynamic Boundary Mediation Framework, which positions LLM chatbots as adaptive boundary objects to bridge knowledge, power, and contextual gaps. This offers a pathway for designing AI systems that more effectively and accountably support therapeutic relationships for marginalized users.",
      "mindmap": "graph TB\n        Root[”Relational Mediators: LLM Chatbots as Boundary Objects in Psychotherapy”] --> Problem[”核心问题/Problem”]\n        Root --> Method[”主要方法/Method”]\n        Root --> Results[”关键结果/Results”]\n        Problem --> P1[”现有视角的局限/Current Framing Limitations”]\n        Problem --> P2[”边缘化客户的关系挑战/Relational Challenges for Marginalized Clients”]\n        Method --> M1[”动态边界调解框架/Dynamic Boundary Mediation Framework”]\n        Method --> M2[”作为边界对象的LLM/LLMs as Boundary Objects”]\n        Results --> R1[”三种调解形式/Three Forms of Mediation”]\n        Results --> R2[”关系问责的AI系统/Relationally Accountable AI Systems”]"
    },
    {
      "title": "Urban Food Self-Production in the Perspective of Social Learning Theory: Empowering Self-Sustainability",
      "authors": "Ewa Duda, Adamina Korwin-Szymanowska",
      "institution": "Uniwersytet Łódzki, Uniwersytet Warszawski (University of Łódź, University of Warsaw)",
      "link": "https://arxiv.org/pdf/2512.22594",
      "code": null,
      "tags": [
        "social computing",
        "urban agriculture",
        "hydroponics",
        "qualitative study",
        "social learning theory",
        "sustainable food production"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e23a23f4c52bbdc6c863186ff1aad1ec49304917fba3f15168891df1f7d6b610_w640_q70.webp",
      "contributions": "1. Conducted a qualitative study on resident participation in an innovative urban hydroponic farming project. 2. Identified key motivations and experiences of urban residents engaging in community-based food self-production. 3. Provided insights for urban educators and policymakers on fostering sustainable food initiatives through social learning.",
      "summary": "This paper investigates urban residents' participation in a community hydroponic farming project in Poland. Using purposive sampling and in-depth interviews, the study explores the motivations, experiences, and educational pathways of participants. The findings highlight the role of social learning in empowering urban self-sustainability and offer guidance for stakeholders in urban education and development.",
      "mindmap": "graph TB\n        A[Urban Food Self-Production in the Perspective of Social Learning Theory: Empowering Self-Sustainability] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Urban food security & climate change/城市粮食安全与气候变化]\n        C --> C1[Qualitative study: interviews/定性研究：访谈]\n        C --> C2[Two communities in Poland/波兰的两个社区]\n        C --> C3[Hydroponic cabinets in flats/公寓中的水培柜]\n        D --> D1[Understand resident motivations/理解居民动机]\n        D --> D2[Outline farming experiences/概述种植经验]\n        D --> D3[Relevance for urban educators/对城市教育者的意义]"
    },
    {
      "title": "Mitigating Social Desirability Bias in Random Silicon Sampling",
      "authors": "Sashank Chapala, Maksym Mironov, Songgaojun Deng",
      "institution": "Eindhoven University of Technology",
      "link": "https://arxiv.org/pdf/2512.22725",
      "code": null,
      "tags": [
        "llm evaluation",
        "silicon sampling",
        "social desirability bias",
        "prompt engineering",
        "jensen-shannon divergence",
        "american national election study"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e514b34cb5fd24baebc22116c45f73b1898e7c713905a13d372408df0900782b_w640_q70.webp",
      "contributions": "1. Replicates and confirms the presence of persistent Social Desirability Bias (SDB) in LLM-based silicon sampling. 2. Proposes and systematically evaluates four psychologically grounded prompt-based methods (reformulated, reverse-coded, priming, preamble) for mitigating SDB. 3. Demonstrates that reformulated prompts (neutral, third-person phrasing) are the most effective method for improving alignment between silicon and human survey response distributions.",
      "summary": "This paper investigates how to reduce Social Desirability Bias in LLM-generated survey responses (silicon sampling). It tests four prompt-based mitigation methods and finds that reformulating questions into neutral, third-person phrasing most effectively aligns the LLM outputs with real human data from the American National Election Study.",
      "mindmap": "graph TB\n        A[Mitigating Social Desirability Bias in Random Silicon Sampling] --> B[核心问题/Problem: LLM硅采样存在社会期望偏差/Social Desirability Bias in Silicon Sampling]\n        A --> C[主要方法/Method: 测试四种基于提示的缓解方法/Test Four Prompt-based Mitigation Methods]\n        A --> D[关键结果/Results: 重构提示最有效，改善与人类数据对齐/Reformulated Prompts Most Effective, Improve Alignment]\n        C --> C1[重构/Reformulated]\n        C --> C2[反向编码/Reverse-coded]\n        C --> C3[启动/Priming]\n        C --> C4[序言/Preamble]"
    },
    {
      "title": "Ungraded Assignments in Introductory Computing: A Report",
      "authors": "Yehya Sleiman Tellawi, Abhishek K. Umrawal",
      "institution": "University of Illinois Urbana-Champaign",
      "link": "https://arxiv.org/pdf/2512.23004",
      "code": null,
      "tags": [
        "computing education",
        "ungraded assignments",
        "formative feedback",
        "student engagement",
        "mixed-methods",
        "introductory computing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03fe0f743f635ea37a635da0020b8df696d9ae620a38ea5dd83dd57902fd7911_w640_q70.webp",
      "contributions": "1. Developed and administered new optional ungraded assignments for a large introductory computer engineering course (ECE 120). 2. Employed a mixed-methods approach (surveys, interviews, performance analysis) to assess the impact of ungraded assignments on learning. 3. Found a positive relationship between participation in ungraded assignments and overall course performance, suggesting they appeal to high-achievers or support better outcomes.",
      "summary": "This paper investigates the effects of optional ungraded assignments in an introductory computing course. The authors developed such assignments and used surveys, interviews, and performance data to evaluate their impact. The main finding is a positive correlation between completing ungraded work and higher course grades, indicating potential benefits for student engagement and learning.",
      "mindmap": "graph TB\n        A[Ungraded Assignments in Introductory Computing: A Report] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1(评估压力与内在动机/Grade pressure vs. intrinsic motivation)\n        C --> C1(开发并实施未评分作业/Develop & administer ungraded assignments)\n        C --> C2(混合方法评估/Mixed-methods evaluation)\n        D --> D1(参与度与成绩正相关/Participation correlates with performance)"
    },
    {
      "title": "Inteligencia Artificial y Empleo: perspectiva Territorial y de Género",
      "authors": "Antoni Mestre, Xavier Naya, Manoli Albert, Vicente Pelechano",
      "institution": "Universitat de València (inferred from author names and Spanish context)",
      "link": "https://arxiv.org/pdf/2512.23059",
      "code": null,
      "tags": [
        "labor economics",
        "computational social science",
        "AI exposure index",
        "sector-based analysis",
        "territorial disaggregation",
        "gender gap",
        "CNAE incidence matrix"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/562053897ffb2e59883167293942d4f1e524304623f8d2e91e0026105669c93d_w640_q70.webp",
      "contributions": "1. Proposes a novel methodological framework for estimating AI exposure using sector-based data (CNAE classification) instead of occupation-based approaches, addressing limitations in the Spanish context. 2. Constructs an AI CNAE incidence matrix and applies it to provincial employment data (2021-2023) to provide a territorial and gender-disaggregated assessment of AI's potential impact. 3. Reveals stable structural patterns of AI exposure, identifying higher exposure in metropolitan/service regions and a consistent gender gap where female employment is more exposed across all territories.",
      "summary": "This paper proposes a sector-based methodological framework to estimate the potential exposure of employment to AI in Spain, addressing the limitations of occupation-centered approaches. By applying an AI CNAE incidence matrix to provincial employment data from 2021-2023, it provides a territorial and gender-disaggregated assessment. The results show higher AI exposure in metropolitan and service-oriented regions and a consistent gender gap, with female employment being more exposed across all territories, offering a structural perspective for policy planning rather than predicting job displacement.",
      "mindmap": "graph TB\n        A[Inteligencia Artificial y Empleo: perspectiva Territorial y de Género] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>AI对劳动力市场的不均衡影响<br>Uneven AI impact on labor markets]\n        C[主要方法/Method<br>构建行业AI暴露矩阵<br>Construct sector-based AI exposure matrix]\n        D[关键结果/Results<br>大都市区暴露更高, 存在性别差距<br>Higher exposure in metro areas, consistent gender gap]"
    },
    {
      "title": "Identifying Barriers Hindering the Acceptance of Generative AI as a Work Associate, measured with the new AGAWA scale",
      "authors": "Łukasz Sikorski, Albert Łukasik, Jacek Matulewski, Arkadiusz Gut",
      "institution": "Nicolaus Copernicus University in Toruń",
      "link": "https://arxiv.org/pdf/2512.23373",
      "code": null,
      "tags": [
        "human-ai interaction",
        "AGAWA scale",
        "technology acceptance",
        "generative AI",
        "workplace",
        "moral dilemmas"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ae604f8d4d4b73f323c396a472bef59c496be69f1ebbd75e34c80cb0805f530_w640_q70.webp",
      "contributions": "1. Proposed the AGAWA scale, a concise 4-item tool for measuring attitudes toward generative AI as a coworker, 2. Investigated key factors (concerns, human-like characteristics, sense of human uniqueness) influencing acceptance of generative AI in the workplace, 3. Confirmed the relationship between affective/moral dimensions of trust and attitudes toward generative AI at work.",
      "summary": "This paper introduces the AGAWA scale, a brief measurement tool based on TAM and UTAUT models, to study barriers to accepting generative AI as a work associate. The study found that positive attitudes toward AI coworkers are negatively correlated with concerns about interaction, human-like AI traits, and a sense of human superiority. The results highlight the link between trust dimensions and workplace AI acceptance.",
      "mindmap": "graph TB\n        Root[”Identifying Barriers to Generative AI Acceptance / 识别生成式AI接受的障碍”] --> Problem[”Core Problem: Student attitudes affect future workplace AI adoption / 核心问题: 学生态度影响未来工作场所AI采用”]\n        Root --> Method[”Method: Propose AGAWA scale (4-item tool) / 方法: 提出AGAWA量表(4项工具)”]\n        Root --> Results[”Results: Positive attitudes linked to reduced concerns, human-likeness, and superiority beliefs / 结果: 积极态度与减少的担忧、拟人特性和优越感信念相关”]"
    },
    {
      "title": "The Effect of Gender Diversity on Scientific Team Impact: A Team Roles Perspective",
      "authors": "Yi Zhao, Yongjun Zhu, Donghun Kim, Yuzhuo Wang, Heng Zhang, Chao Lu, Chengzhi Zhang",
      "institution": "Anhui University, Yonsei University, Nanjing University, Central China Normal University, Hohai University, Nanjing University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.23429",
      "code": null,
      "tags": [
        "scientometrics",
        "gender diversity",
        "team roles",
        "author contribution statements",
        "threshold regression",
        "citation impact"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a8e0d6494840d098b4c65bf9f6eb6b4b27a82bd62024c95c0f60db9e5b8fa31f_w640_q70.webp",
      "contributions": "1. Introduced a team roles perspective by classifying authors into leadership and support roles using contribution statements, moving beyond aggregate diversity measures. 2. Discovered a non-linear (inverted U-shape) relationship between gender diversity and team impact for both leadership and support groups. 3. Revealed the moderating effect of team size, showing that the impact of leadership-group gender diversity shifts from negative to positive as team size increases, while support-group diversity remains consistently positive.",
      "summary": "This study investigates how gender diversity within specific team roles (leadership vs. support) affects scientific team impact, measured by citations. By analyzing over 130,000 PLOS papers and using contribution statements to define roles, the authors employed multivariable and threshold regression. They found the relationship is an inverted U-shape, identified high-impact team compositions, and showed that team size significantly moderates the effect of leadership diversity.",
      "mindmap": "graph TB\n        A[The Effect of Gender Diversity on Scientific Team Impact<br>性别多样性对科研团队影响力的影响] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Inconsistent findings on gender diversity's effect,<br>lack of role-differentiated analysis.<br>性别多样性影响结论不一，缺乏基于团队角色的分析]\n        C[主要方法/Method<br>Analyzed 130k+ PLOS papers, used contribution<br>statements for role classification (leadership/support),<br>applied multivariable & threshold regression.<br>分析13万+PLOS论文，利用贡献声明进行角色分类，应用多元及阈值回归]\n        D[关键结果/Results<br>1. Inverted U-shape relationship.<br>2. All-female leadership + all-male support yields high impact.<br>3. Team size moderates leadership diversity effect.<br>1. 倒U型关系。<br>2. 全女性领导+全男性支持的团队影响力更高。<br>3. 团队规模调节领导组多样性效应。]"
    },
    {
      "title": "Can AI Recognize Its Own Reflection? Self-Detection Performance of LLMs in Computing Education",
      "authors": "Christopher Burger, Karmece Talley, Christina Trotter",
      "institution": "The University of Mississippi, Rust College",
      "link": "https://arxiv.org/pdf/2512.23587",
      "code": null,
      "tags": [
        "academic integrity detection",
        "Large Language Models",
        "AI-generated text detection",
        "deceptive prompts",
        "computing education",
        "self-detection"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e1ff7ce7b5924fdb6e21130ee87b352dc8c7613c8706c156c27ac0e31017f8b_w640_q70.webp",
      "contributions": "1. Evaluates the self-detection performance of three prominent LLMs (GPT-4, Claude, Gemini) in computing-specific contexts. 2. Tests detection under both standard and deceptive prompt conditions where models are instructed to evade detection. 3. Reveals significant instability in detection, showing high error rates for human-written work and susceptibility to simple prompt alterations.",
      "summary": "This paper evaluates the ability of LLMs (GPT-4, Claude, Gemini) to detect AI-generated text in computing education. It tests them under standard and deceptive prompt conditions, finding that while default AI text is easily identified, models struggle with human-written work and are highly fooled by deceptive prompts, making them unreliable for high-stakes academic judgments.",
      "mindmap": "graph TB\n        A[Can AI Recognize Its Own Reflection? Self-Detection Performance of LLMs in Computing Education] --> B[核心问题/Problem: LLMs challenge academic integrity in computing education]\n        A --> C[主要方法/Method: Evaluate GPT-4, Claude, Gemini on AI-generated text detection with standard/deceptive prompts]\n        A --> D[关键结果/Results: Models unstable; high error on human text; easily fooled by deceptive prompts; unreliable for misconduct judgments]"
    },
    {
      "title": "AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms",
      "authors": "LearnLM Team Google, Eedi, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Brian Veprek, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Irina Jurenka, Iris Hulls, James Stalley-Moores, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Lucy Dalton, Markus Kunesch, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Sara Wiltberger, Shakir Mohamed, Simon Woodhead, Vasco Brazão",
      "institution": "Google, Eedi",
      "link": "https://arxiv.org/pdf/2512.23633",
      "code": null,
      "tags": [
        "educational ai",
        "generative AI",
        "fine-tuning",
        "randomized controlled trial",
        "Socratic questioning",
        "pedagogical instruction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b430e7c78534d660126208f226397ed95756e540bade56276301199a0114bc76_w640_q70.webp",
      "contributions": "1. Conducted a rigorous, in-classroom exploratory RCT to evaluate the safety and efficacy of a generative AI tutor (LearnLM) in a real educational setting. 2. Demonstrated that a pedagogically fine-tuned AI model can reliably draft instructional content, with human tutors approving 76.4% of its messages with minimal or no edits. 3. Showed that AI-supported tutoring led to student performance at least equivalent to human-only tutoring, with a significant 5.5 percentage point improvement in solving novel problems on subsequent topics.",
      "summary": "This paper investigates whether generative AI can scale effective one-to-one tutoring. The authors integrated LearnLM, a pedagogically fine-tuned AI model, into a math tutoring platform and conducted a randomized controlled trial where human tutors supervised its outputs. The results show that LearnLM was a reliable tutor, and students using it performed as well as or better than those with human tutors alone, suggesting AI can deliver effective, individualized learning support at scale.",
      "mindmap": "graph TB\n        Root[AI Tutoring RCT in UK Classrooms] --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[个性化辅导成本高/High cost of 1-to-1 tutoring]\n        Problem --> P2[AI辅导的有效性与安全性未知/Unproven efficacy & safety of AI tutoring]\n        Method --> M1[整合LearnLM模型/Integrate LearnLM (pedagogically fine-tuned AI)]\n        Method --> M2[在Eedi平台进行RCT/Conduct RCT on Eedi platform]\n        Method --> M3[专家导师监督输出/Human tutors supervise AI drafts]\n        Results --> R1[76.4%消息被直接批准/76.4% messages approved with minimal edits]\n        Results --> R2[学生表现相当或更好/Student performance equal or better]\n        Results --> R3[解决新问题能力提升5.5%/5.5% improvement on novel problems]"
    },
    {
      "title": "SENTINEL: A Multi-Modal Early Detection Framework for Emerging Cyber Threats using Telegram",
      "authors": "Mohammad Hammas Saeed, Howie Huang",
      "institution": "George Washington University",
      "link": "https://arxiv.org/pdf/2512.21380",
      "code": null,
      "tags": [
        "cyber threat intelligence",
        "multi-modal fusion",
        "large language models",
        "graph neural networks",
        "early detection",
        "social media analysis"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3e47a235de6afcb4955afd774a9e4b3883efcafc7e4aaa97649575ef2fb34d0_w640_q70.webp",
      "contributions": "1. Proposes SENTINEL, a multi-modal framework for early cyber threat detection by aligning social media discussions with real-world attacks. 2. Combines language modeling (using LLMs) and network coordination analysis (using GNNs) to fuse textual and relational signals from platforms like Telegram. 3. Demonstrates the framework's effectiveness on a dataset of 365k messages from 16 Telegram channels, achieving an F1 score of 0.89 for threat alignment.",
      "summary": "The paper presents SENTINEL, a framework for the early detection of cyber threats by analyzing multi-modal signals from social media platforms like Telegram. It combines large language models for text understanding with graph neural networks to model user coordination, successfully aligning online discussions to real-world attacks. The evaluation on Telegram data shows the approach is effective, achieving a high F1 score of 0.89.",
      "mindmap": "graph TB\n        Root[”SENTINEL: A Multi-Modal Early Detection Framework for Emerging Cyber Threats using Telegram”] --> Problem[”核心问题/Problem: Post-hoc detection of cyber attacks is reactive; need for proactive, early warning systems.”]\n        Root --> Method[”主要方法/Method: Multi-modal framework combining LLMs (language) and GNNs (coordination graphs) to analyze social media signals.”]\n        Root --> Results[”关键结果/Results: Achieves F1 of 0.89 aligning Telegram discussions to real-world cyber threats.”]"
    },
    {
      "title": "ALETHEIA: Combating Social Media Influence Campaigns with Graph Neural Networks",
      "authors": "Mohammad Hammas Saeed, Isaiah J. King, Howie Huang",
      "institution": "George Washington University",
      "link": "https://arxiv.org/pdf/2512.21391",
      "code": null,
      "tags": [
        "social network security",
        "Graph Neural Networks",
        "influence campaigns",
        "temporal link prediction",
        "troll detection",
        "Reddit"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/473a865358e998c62938f17660edc395a7658bf360ef15e62ea79317e8734aec_w640_q70.webp",
      "contributions": "1. Proposes ALETHEIA, a system that formalizes the detection of malicious accounts in influence campaigns as a node classification and link prediction problem using a graph-based representation. 2. Demonstrates that a detection pipeline combining topological (graph) and linguistic features outperforms standard interaction and user features, achieving a 3.7% F1-score improvement. 3. Introduces a novel temporal link prediction mechanism for influence campaigns by stacking a GNN over an RNN to forecast future troll interactions (TTE/TUE) with high accuracy (96.6% AUC).",
      "summary": "This paper presents ALETHEIA, a system that uses Graph Neural Networks (GNNs) to detect malicious accounts and predict their future interactions in social media influence campaigns. By modeling campaigns as graphs and combining structural and linguistic features, it improves detection performance and forecasts troll behavior with high accuracy. The results underscore the importance of leveraging network structure to combat coordinated malicious activity online.",
      "mindmap": "graph TB\n        A[ALETHEIA: Combating Social Media Influence Campaigns with Graph Neural Networks] --> B[核心问题/Problem: Detecting and predicting malicious influence campaigns on social media]\n        A --> C[主要方法/Method: Graph Neural Networks (GNNs) with topological & linguistic features, GNN+RNN for temporal link prediction]\n        A --> D[关键结果/Results: 3.7% F1-score improvement in detection, 96.6% AUC for predicting future troll interactions]"
    },
    {
      "title": "Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments",
      "authors": "Hua Shen",
      "institution": "NYU Shanghai, New York University",
      "link": "https://arxiv.org/pdf/2512.21552",
      "code": null,
      "tags": [
        "ai for education",
        "human-ai alignment",
        "trustworthy ai",
        "adaptive learning",
        "educational technology",
        "ai ethics"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7f40fe114da7bae34b09e44db18fa32bb4f64e57bd01e75e96afc80c2ddc136_w640_q70.webp",
      "contributions": "1. Proposes the novel concept of \"bidirectional human-AI alignment\" for education, emphasizing mutual adaptation between humans and AI systems. 2. Explores the evolution of AI's role in education from a support tool to a collaborative partner, analyzing its impact on teacher roles and student agency. 3. Provides actionable strategies for policymakers, developers, and educators to ensure AI advances equity, transparency, and human flourishing in learning environments.",
      "summary": "This paper addresses the risks of AI in education, such as bias and loss of autonomy, by proposing the concept of bidirectional human-AI alignment. The method involves not only embedding human values into AI but also equipping educators and students to guide these technologies. It concludes that reframing AI adoption as a process of mutual adaptation is key to creating trustworthy learning environments where humans and AI can grow together.",
      "mindmap": "graph TB\n        A[论文标题: Bidirectional Human-AI Alignment in Education] --> B[核心问题/Problem: AI in education introduces risks to equity, privacy, and autonomy.]\n        A --> C[主要方法/Method: Proposes bidirectional alignment: embedding human values into AI and equipping humans to interpret/guide AI.]\n        A --> D[关键结果/Results: Envisions a future of mutual adaptation where AI advances equity, transparency, and human flourishing.]"
    },
    {
      "title": "Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets",
      "authors": "Matyas Bohacek, Ignacio Vilanova Echavarri",
      "institution": "Stanford University, Imperial College London",
      "link": "https://arxiv.org/pdf/2512.21775",
      "code": null,
      "tags": [
        "Data Provenance",
        "Data Provenance",
        "Compliance Rating",
        "Generative AI",
        "Dataset Ethics",
        "Transparency"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa33a1fedd52ef1c87e9bf7d9a25dad61aae942ba50660263862470b9b677745_w640_q70.webp",
      "contributions": "1. Proposes the Compliance Rating Scheme (CRS), a framework for evaluating dataset compliance with transparency, accountability, and security principles. 2. Develops and releases an open-source Python library that implements the CRS framework using data provenance technology. 3. Creates a tool that is both reactive (evaluating existing datasets) and proactive (guiding the responsible construction of new datasets).",
      "summary": "The paper addresses the lack of ethical and legal oversight in the creation and sharing of datasets for Generative AI. It proposes the Compliance Rating Scheme (CRS) framework and an accompanying open-source library to assess and ensure dataset compliance with key principles. The work aims to improve traceability and accountability in the AI data supply chain.",
      "mindmap": "graph TB\n        Root(”Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”数据集创建缺乏伦理与法律监督/Lack of ethical & legal oversight in dataset creation”)\n        Problem --> P2(”数据来源与合法性信息丢失/Loss of data origin & legitimacy info”)\n        Method --> M1(”提出合规评级方案(CRS)框架/Propose Compliance Rating Scheme (CRS) framework”)\n        Method --> M2(”开发基于数据溯源技术的开源库/Develop open-source library using data provenance”)\n        Results --> R1(”评估现有数据集的合规性/Evaluate compliance of existing datasets”)\n        Results --> R2(”指导负责任的新数据集构建/Guide responsible construction of new datasets”)"
    },
    {
      "title": "On The Conceptualization and Societal Impact of Cross-Cultural Bias",
      "authors": "Vitthal Bhandari",
      "institution": "University of Washington",
      "link": "https://arxiv.org/pdf/2512.21809",
      "code": null,
      "tags": [
        "bias and fairness",
        "cultural bias",
        "literature survey",
        "societal impact",
        "harm evaluation",
        "bias mitigation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2702d319a8125ee471012d7f7a71a4d4530da34216397d1647aba76a8e4a3842_w640_q70.webp",
      "contributions": "1. Conducts a focused survey of 20 recent (2025) papers on cultural bias in NLP, identifying gaps in current research practices. 2. Critiques the literature for lacking concrete definitions of bias, failing to identify affected stakeholders, and inadequately evaluating the harms of biased systems. 3. Advocates for a future research agenda that emphasizes robust societal impact assessment, concrete bias conceptualization, and engagement with real-world stakeholders.",
      "summary": "This paper surveys recent literature on cultural bias in NLP, finding that current research often fails to concretely define bias, engage with affected stakeholders, or thoroughly evaluate societal harms. The author proposes a set of observations to guide future work towards more robust and impactful assessments of cross-cultural bias in language technologies.",
      "mindmap": "graph TB\n    Root(”On The Conceptualization and Societal Impact of Cross-Cultural Bias”) --> Problem(”核心问题/Problem: LLMs exhibit cross-cultural bias; research often avoids real-world stakeholder engagement.”)\n    Root --> Method(”主要方法/Method: Survey and analyze 20 recent (2025) papers on cultural bias in NLP.”)\n    Root --> Results(”关键结果/Results: Identifies gaps in bias definition, harm evaluation; advocates for robust societal impact assessment.”)"
    },
    {
      "title": "Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?",
      "authors": "Naen Xu, Jinghuai Zhang, Changjiang Li, Hengyu An, Chunyi Zhou, Jun Wang, Boyu Xu, Yuyuan Li, Tianyu Du, Shouling Ji",
      "institution": "Zhejiang University, University of California, Los Angeles, Palo Alto Networks",
      "link": "https://arxiv.org/pdf/2512.21871",
      "code": "https://github.com/bluedream02/CopyGuard",
      "tags": [
        "multi-modal inference",
        "copyright compliance",
        "vision-language models",
        "tool-augmented defense",
        "benchmark dataset",
        "multimodal query"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a933ea78af16685ceab38b447862e9c50b08de435c2e6b662d59551bf5552fdc_w640_q70.webp",
      "contributions": "1. Introduced a large-scale benchmark dataset of 50,000 multimodal query-content pairs to evaluate copyright compliance in LVLMs. 2. Conducted a comprehensive evaluation revealing significant deficiencies in state-of-the-art LVLMs' ability to recognize and respect copyrighted content. 3. Proposed a novel tool-augmented defense framework to reduce copyright infringement risks in LVLM inference.",
      "summary": "This paper evaluates how large vision-language models (LVLMs) handle copyrighted visual content and finds they often fail to comply with copyright regulations. To address this, the authors propose a tool-augmented defense framework for copyright compliance. The work highlights the need for developing copyright-aware LVLMs to ensure responsible use.",
      "mindmap": "graph TB\n        Root[”Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?”]\n        Root --> Problem[”核心问题/Problem: LVLMs may infringe copyright when processing visual inputs”]\n        Root --> Method[”主要方法/Method: Benchmark dataset & Tool-augmented defense framework”]\n        Root --> Results[”关键结果/Results: Current LVLMs are deficient; Proposed framework reduces risk”]"
    },
    {
      "title": "Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management",
      "authors": "Sunil Arora, John Hastings",
      "institution": "Dakota State University",
      "link": "https://arxiv.org/pdf/2512.22060",
      "code": null,
      "tags": [
        "AI Governance & Compliance",
        "lifecycle management",
        "bias detection",
        "differential privacy",
        "federated learning",
        "terminology drift"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/113703d05215aac7e8678f24cb38d882fa4c99927066ea2264ef3d1b4c3a1d67_w640_q70.webp",
      "contributions": "1. Proposes the SC-NLP-LMF, a comprehensive six-phase framework for secure and compliant NLP model lifecycle management. 2. Integrates established technical methods (e.g., bias detection, differential privacy) with leading organizational standards (e.g., NIST AI RMF, EU AI Act). 3. Validates the framework's practicality through a healthcare case study demonstrating detection of and response to terminology drift.",
      "summary": "This paper introduces the Secure and Compliant NLP Lifecycle Management Framework (SC-NLP-LMF), a six-phase model developed from a systematic review to address security, privacy, and compliance risks in NLP systems. It integrates methods like bias detection and differential privacy with standards like NIST AI RMF and the EU AI Act. The framework provides a practical structure for organizations to manage NLP systems in high-risk environments, as illustrated by a healthcare case study on handling terminology drift.",
      "mindmap": "graph TB\n        Root[”Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>NLP systems in sensitive domains face unaddressed security, privacy, and compliance risks.”]\n        Method[”主要方法/Method<br>Proposes SC-NLP-LMF, a six-phase framework integrating standards (NIST, ISO, EU AI Act) and techniques (bias detection, differential privacy).”]\n        Results[”关键结果/Results<br>Provides a practical lifecycle structure for secure, accountable NLP systems, validated via a healthcare case study.”]"
    },
    {
      "title": "Agent-based simulation of online social networks and disinformation",
      "authors": "Alejandro Buitrago López, Alberto Ortega Pastor, David Montoro Aguilera, Mario Fernández Tárraga, Jesús Verdú Chacón, Javier Pastor-Galindo, José A. Ruipérez-Valiente",
      "institution": "University of Murcia",
      "link": "https://arxiv.org/pdf/2512.22082",
      "code": null,
      "tags": [
        "agent-based simulation",
        "agent-based simulation",
        "large language model",
        "disinformation campaigns",
        "synthetic social networks",
        "behavioral automata"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a3415bee27d71926e7770fd596253ec973faaa85d1fcba853a047ff6d08bfe3_w640_q70.webp",
      "contributions": "1. A simulation framework that models synthetic social networks using agents with demographic-based personality traits and finite-state behavioral automata for realistic and interpretable actions. 2. A generative module powered by an LLM to produce context-aware social media posts consistent with each agent's profile and memory. 3. A red module implementing DISARM-inspired workflows to orchestrate disinformation campaigns and a Mastodon-based visualization layer for real-time inspection and validation.",
      "summary": "This paper proposes an agent-based simulation framework to study online social networks and disinformation, addressing the limitations of platform opacity and data access. The framework uses LLM-powered agents with personality traits and behavioral automata to generate realistic content and simulate disinformation campaigns, with evaluation showing structural, behavioral, and linguistic realism. It provides a customizable and controllable environment for studying information dynamics.",
      "mindmap": "graph TB\n        Root[Agent-based simulation of online social networks and disinformation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[平台不透明与数据限制/Platform Opacity & Data Limits]\n        Problem --> P2[现有模拟缺乏真实性与可解释性/Existing Simulations Lack Realism & Explainability]\n        Method[主要方法/Method] --> M1[基于代理的合成社交网络/Agent-based Synthetic Social Networks]\n        Method --> M2[LLM生成上下文感知内容/LLM Generates Context-aware Content]\n        Method --> M3[红色模块模拟虚假信息活动/Red Module Simulates Disinformation Campaigns]\n        Method --> M4[Mastodon可视化层/Mastodon Visualization Layer]\n        Results[关键结果/Results] --> R1[展示结构、行为、语言真实性/Demonstrates Structural, Behavioral, Linguistic Realism]\n        Results --> R2[为研究信息动态提供可定制环境/Provides Customizable Environment for Studying Information Dynamics]"
    },
    {
      "title": "From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education",
      "authors": "Iman Reihanian, Yunfei Hou, Qingquan Sun",
      "institution": "California State University, San Bernardino",
      "link": "https://arxiv.org/pdf/2512.20714",
      "code": null,
      "tags": [
        "educational technology",
        "generative AI",
        "personalization",
        "adaptive learning",
        "large language models",
        "intelligent tutoring systems"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e46f313e494a41a4a12873eddb6320db4cd59b6fb958bb008fd6f6512729af4_w640_q70.webp",
      "contributions": "1. Identified and analyzed five key application domains for GenAI-enabled personalization in CS education: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review. 2. Synthesized four design patterns for successful implementations: context-aware tutoring anchored in student artifacts, multi-level hint structures, composition with traditional CS infrastructure, and human-in-the-loop quality assurance. 3. Proposed an exploration-first adoption framework for integrating GenAI, emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling, while pairing recurrent risks with operational mitigations.",
      "summary": "This scoping review maps how generative AI enables personalized computer science education. It analyzes design choices across 32 studies and finds that structured implementations with explanation-first guidance and artifact grounding lead to more positive learning outcomes than unconstrained chat interfaces. The paper concludes that generative AI can provide precision scaffolding when embedded in audit-ready workflows that preserve productive struggle.",
      "mindmap": "graph LR\n    A[From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education] --> B[核心问题/Problem: Does GenAI personalization support or undermine CS learning?]\n    A --> C[主要方法/Method: Scoping review of 32 studies; Analysis of design choices & patterns]\n    A --> D[关键结果/Results: Structured designs (e.g., hint ladders, artifact grounding) are more effective; Proposes an exploration-first adoption framework]"
    },
    {
      "title": "Sark: Oblivious Integrity Without Global State",
      "authors": "Alex Lynham, David Alesch, Ziyi Li, Geoff Goodell",
      "institution": "University College London (UCL)",
      "link": "https://arxiv.org/pdf/2512.20775",
      "code": null,
      "tags": [
        "distributed ledger systems",
        "oblivious integrity",
        "crash fault-tolerant blockchain",
        "integrity locus",
        "USO asset system",
        "local centrality"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b30bc13516511718c7bc6ed68789e79948291909f0ae043653903321e5a6bcc_w640_q70.webp",
      "contributions": "1. Presents Sark, a reference architecture implementing the Unforgeable, Stateful, and Oblivious (USO) asset system for oblivious, non-custodial asset management. 2. Introduces the concept of \"Integrity Locus\" as a framework to analyze and address design trade-offs related to decentralization. 3. Describes the design and implementation of Sloop, a permissioned crash fault-tolerant (CFT) blockchain, and Porters, subsystems that form the core of the Sark architecture.",
      "summary": "This paper introduces Sark, a distributed system architecture designed for managing assets with oblivious integrity, eliminating the need for a global state ledger. Its core components include the Sloop blockchain and Porters for handling client commitments, analyzed through the CIA triad. The main conclusion is that Sark offers a more decentralized trust topology by leveraging local integrity proofs instead of a global ledger, though it introduces trade-offs like local centrality.",
      "mindmap": "graph LR\n        A[Sark: Oblivious Integrity Without Global State] --> B(核心问题/Problem: How to achieve asset integrity without a global state ledger?);\n        A --> C(主要方法/Method: Implement USO asset system with Sloop CFT blockchain & Porters);\n        A --> D(关键结果/Results: Decentralized via local integrity proofs, introduces Integrity Locus concept);"
    },
    {
      "title": "Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles",
      "authors": "Ramatu Oiza Abdulsalam, Segun Aroyehun",
      "institution": "African University of Science and Technology, University of Konstanz",
      "link": "https://arxiv.org/pdf/2512.20780",
      "code": null,
      "tags": [
        "educational technology / intelligent tutoring systems",
        "large language models",
        "pedagogical quality",
        "instructional strategies",
        "linguistic analysis",
        "math tutoring"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4c157f4475efaa64bb039e61eccf65a1facd8888dde9576734865854a42e878_w640_q70.webp",
      "contributions": "1. Conducted a controlled, turn-level comparison of tutoring responses between expert human tutors, novice human tutors, and multiple large language models (LLMs) in math remediation. 2. Identified systematic differences in instructional and linguistic profiles, finding that LLMs underuse restating/revoicing strategies but produce longer, more lexically diverse, and more polite responses compared to human tutors. 3. Established statistical associations between specific instructional/linguistic features (e.g., restating, lexical diversity) and perceived pedagogical quality, showing LLMs can achieve comparable quality using different strategies.",
      "summary": "This paper investigates how closely the instructional behavior of large language models (LLMs) aligns with expert human tutors in math tutoring. By comparing responses from experts, novices, and LLMs to the same conversation turns, the study analyzes instructional strategies and linguistic features. It finds that LLMs approach expert-level pedagogical quality on average but rely on systematically different strategies, such as underusing restating/revoicing while being more verbose and polite.",
      "mindmap": "graph LR\n    A[Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles] --> B(核心问题/Problem: LLM教学行为与人类专家的一致性/Alignment of LLM instructional behavior with expert human tutors)\n    A --> C(主要方法/Method: 控制性对话轮比较/Controlled turn-level comparison of expert, novice, and LLM responses)\n    A --> D(关键结果/Results: LLM接近专家教学水平但策略不同/LLMs approach expert quality but use different instructional & linguistic strategies)"
    },
    {
      "title": "Making AI Work: An Autoethnography of a Workaround in Higher Education",
      "authors": "Shang Chieh Lee, Bhuva Narayan, Simon Buckingham Shum, Stella Ng, A. Baki Kocaballi",
      "institution": "University of Technology Sydney",
      "link": "https://arxiv.org/pdf/2512.21055",
      "code": null,
      "tags": [
        "Information Systems",
        "Autoethnography",
        "Invisible Labour",
        "Workaround",
        "Sociotechnical Systems",
        "User Innovation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c38d81a4b275e82620a51814d2cbc6d349963475ab4224ec58e70714de1ab37_w640_q70.webp",
      "contributions": "1. Provides an insider, autoethnographic account of the sociotechnical friction and \"invisible labour\" required to make enterprise GenAI functional in higher education. 2. Applies and extends Alter's theory of workarounds to interpret user-driven adaptations as integral acts of sociotechnical integration, not mere deviations. 3. Highlights the central paradox of GenAI workarounds: they enable functionality but can create unofficial \"shadow\" systems and obscure the crucial, politically charged labour involved.",
      "summary": "This study uses analytic autoethnography to examine a workaround developed when an institutional goal of empowering staff with GenAI clashed with technical and political constraints. It argues such workarounds are essential acts of sociotechnical integration that reveal the \"invisible labour\" needed to make AI functional, but this labour is often obscured, creating a paradox. The findings position this invisible labour as a core, rather than peripheral, component of practical GenAI implementation.",
      "mindmap": "graph LR\n        A[Making AI Work: An Autoethnography of a Workaround in Higher Education] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[GenAI实施中的社会技术摩擦与隐性劳动/Sociotechnical Friction & Invisible Labour in GenAI Implementation]\n        C --> C1[分析性自我民族志与工作区理论/Analytic Autoethnography & Workaround Theory]\n        D --> D1[工作区是核心的社会技术整合行为/Workarounds as Integral Sociotechnical Integration]\n        D --> D2[揭示了GenAI的整合悖论/Reveals GenAI Integration Paradox]"
    },
    {
      "title": "Beyond Context: Large Language Models Failure to Grasp Users Intent",
      "authors": "Ahmed M. Hussain, Salahuddin Salahuddin, Panos Papadimitratos",
      "institution": "KTH Royal Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.21110",
      "code": null,
      "tags": [
        "ai safety",
        "intent recognition",
        "contextual understanding",
        "safety circumvention",
        "prompt engineering",
        "transformer architectures"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55c1a596dd6375317c809bb19f466455285faf18a1f9810649d755b8027e383c_w640_q70.webp",
      "contributions": "1. Identifies and empirically demonstrates a critical vulnerability in LLMs: their inability to understand user intent and context, which allows safety mechanisms to be circumvented. 2. Evaluates multiple state-of-the-art LLMs (ChatGPT, Claude, Gemini, DeepSeek) and shows that exploitation techniques like emotional framing and progressive revelation are effective, and that reasoning capabilities can amplify this risk. 3. Proposes a paradigmatic shift in AI safety design, arguing for contextual understanding and intent recognition to be core capabilities rather than post-hoc protective mechanisms.",
      "summary": "This paper identifies a fundamental vulnerability in Large Language Models (LLMs): their lack of contextual understanding and intent recognition, which allows safety mechanisms to be systematically bypassed. The authors empirically evaluate several LLMs, showing they can be exploited through techniques like emotional framing, and find that reasoning capabilities often worsen the problem. They conclude that a paradigm shift is needed to build intent recognition directly into LLM architectures for safety.",
      "mindmap": "graph LR\n    A[Beyond Context: Large Language Models Failure to Grasp Users Intent] --> B[核心问题/Problem: LLMs缺乏上下文和意图理解能力/LLMs lack contextual understanding & intent recognition]\n    A --> C[主要方法/Method: 对多种LLM进行经验性评估/Empirical evaluation of multiple LLMs]\n    A --> D[关键结果/Results: 安全机制可被系统规避，需范式转变/Safety mechanisms can be systematically circumvented, requiring a paradigm shift]\n    B --> E[导致可利用的漏洞/Creates exploitable vulnerabilities]\n    C --> F[使用情感框架、渐进揭示等技术/Using emotional framing, progressive revelation, etc.]\n    D --> G[Claude Opus 4.1部分例外，推理能力加剧风险/Claude Opus 4.1 partial exception, reasoning amplifies risk]"
    },
    {
      "title": "Microtopia: Exploring the Impact of Interdisciplinary Projects on Ethnic Minority Female Pupils' Perceptions of Computer Science",
      "authors": "Nadine Aburumman, Ju-Ling Shih, Cigdem Sengul, Monica Pereira",
      "institution": "[Inferred from authors: Nadine Aburumman, Ju-Ling Shih, Cigdem Sengul, Monica Pereira. No explicit affiliations in provided text. Institution cannot be reliably inferred.]",
      "link": "https://arxiv.org/pdf/2512.21214",
      "code": null,
      "tags": [
        "computer science education",
        "diversity and inclusion",
        "interdisciplinary learning",
        "design thinking",
        "sustainable development goals",
        "AI/IoT/Robotics",
        "problem-based learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05065a9796e995884552033fb1ebac6397879e6ec0b2560e03b8ac9fb067c627_w640_q70.webp",
      "contributions": "1. Proposes the Microtopia programme, an interdisciplinary CS initiative integrating AI, IoT, and Robotics with design thinking and collaborative project work. 2. Demonstrates that linking CS content to real-world sustainability challenges and global issues significantly enhances engagement and perceived relevance among ethnic minority female pupils. 3. Identifies through statistical analysis that socioeconomic and ethnocultural factors (e.g., SES, perception of field as male-dominated) are underlying factors shaping pupils' perceptions of CS.",
      "summary": "This paper introduces Microtopia, an interdisciplinary programme that combines coding, AI, IoT, and Robotics with design thinking and sustainability themes to broaden participation in computer science among ethnic minority girls. The study, using pre- and post-questionnaires, found that participation significantly increased students' confidence, enjoyment, and motivation, especially when computing was presented as relevant to solving global challenges.",
      "mindmap": "graph LR\n    A[Microtopia: Exploring the Impact of Interdisciplinary Projects on Ethnic Minority Female Pupils' Perceptions of Computer Science] --> B(核心问题/Problem: Broadening participation of ethnic minority girls in CS)\n    A --> C(主要方法/Method: Interdisciplinary programme with AI/IoT/Robotics, design thinking, SDGs, collaborative projects)\n    A --> D(关键结果/Results: Increased confidence, enjoyment, motivation; CS linked to sustainability enhances engagement)"
    },
    {
      "title": "\"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme",
      "authors": "Anton Alyakin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19700",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7dd2926116b7f555fd465c856929048f8fcdbf9a408910ee765667d89ab6c77_w640_q70.webp",
      "contributions": "",
      "summary": "\"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme",
      "mindmap": ""
    },
    {
      "title": "CS-Guide: Leveraging LLMs and Student Reflections to Provide Frequent, Scalable Academic Monitoring Feedback to Computer Science Students",
      "authors": "Samuel Jacob Chacko, An-I Andy Wang, Lara Perez-Felkner, Sonia Haiduc, David Whalley, Xiuwen Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19866",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/400e33eafa2cbde4debfc18c1a5e5f0a16f436d1494edd99b4338169da7879de_w640_q70.webp",
      "contributions": "",
      "summary": "CS-Guide: Leveraging LLMs and Student Reflections to Provide Frequent, Scalable Academic Monitoring Feedback to Computer Science Students",
      "mindmap": ""
    },
    {
      "title": "Counterfactual LLM-based Framework for Measuring Rhetorical Style",
      "authors": "Jingyi Qiu, Hong Chen, Zongyi Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19908",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6db39310497bbc29a6efe3c72529c4c231f4c45f6ccb6856571045197a2f074d_w640_q70.webp",
      "contributions": "",
      "summary": "Counterfactual LLM-based Framework for Measuring Rhetorical Style",
      "mindmap": ""
    },
    {
      "title": "Prediction Air Temperature in Geothermal Heat Exchangers Using Pseudorandom Numbers: The New DARL Model",
      "authors": "C. Ramírez-Dolores, J.C. Zamora-Luria, J.A. Altamirano-Acosta, L. Sarao-Cruz, P. Jiménez-Palma, J. Moreno-Falconi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19976",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/441f7620f3757f4471a93018d5814fc4d03f1187c1a80f7194c4047e3eeea5d0_w640_q70.webp",
      "contributions": "",
      "summary": "Prediction Air Temperature in Geothermal Heat Exchangers Using Pseudorandom Numbers: The New DARL Model",
      "mindmap": ""
    },
    {
      "title": "S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test",
      "authors": "Zhe Sun, Xueyuan Yang, Yujie Lu, Zhenliang Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19992",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a7f6951c4ebc6cf6c8bd633198ae7d4c6a7c8f1e0cd348aa9e6737966dfe600_w640_q70.webp",
      "contributions": "",
      "summary": "S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test",
      "mindmap": ""
    },
    {
      "title": "Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives",
      "authors": "Karolina Drożdż, Kacper Dudzic, Anna Sterna, Marcin Moskalewicz",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20298",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e46ae09622d9142a3896f2528685617edcbaae96bc105754e16929ed630e3f0_w640_q70.webp",
      "contributions": "",
      "summary": "Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives",
      "mindmap": ""
    },
    {
      "title": "Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information",
      "authors": "İbrahim Oğuz Çetinkaya, Sajad Khodadadian, Taylan G. Topçu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20589",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bd8f6dd1aa27848e72f81ba7279a1abe238ea198e2b3aa7513fc9ca373e7554_w640_q70.webp",
      "contributions": "",
      "summary": "Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information",
      "mindmap": ""
    },
    {
      "title": "CoPE: A Small Language Model for Steerable and Scalable Content Labeling",
      "authors": "Samidh Chakrabarti, David Willner, Kevin Klyman, Tiffany Saade, Emily Capstick, Sabina Nong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18027",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/423158010714f415c807a9ae93864ccaf58e7d33b462039083dce106e5d195f2_w640_q70.webp",
      "contributions": "",
      "summary": "CoPE: A Small Language Model for Steerable and Scalable Content Labeling",
      "mindmap": ""
    },
    {
      "title": "Securing Agentic AI Systems -- A Multilayer Security Framework",
      "authors": "Sunil Arora, John Hastings",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18043",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96ca042b8b2e4295fa813434a544ac9adb3df55cbea55e3fe9b24f2dcbee4733_w640_q70.webp",
      "contributions": "",
      "summary": "Securing Agentic AI Systems -- A Multilayer Security Framework",
      "mindmap": ""
    },
    {
      "title": "Statistical laws and linguistics inform meaning in naturalistic and fictional conversation",
      "authors": "Ashley M. A. Fehr, Calla G. Beauregard, Julia Witte Zimmerman, Katie Ekström, Pablo Rosillo-Rodes, Christopher M. Danforth, Peter Sheridan Dodds",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18072",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0119bf39897b4bb6f848dd8818e4a75f43e327cbc8223e8cff6bafd0a8277d08_w640_q70.webp",
      "contributions": "",
      "summary": "Statistical laws and linguistics inform meaning in naturalistic and fictional conversation",
      "mindmap": ""
    },
    {
      "title": "Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation",
      "authors": "Lena Libon, Meghana Bhange, Rushabh Solanki, Elliot Creager, Ulrich Aïvodji",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18174",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd7baf972d08bf5c53e0a32c68fda68879e3c2febf3407ecf6537a3fcd6d36fd_w640_q70.webp",
      "contributions": "",
      "summary": "Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation",
      "mindmap": ""
    },
    {
      "title": "Measuring Fine-Grained Negotiation Tactics of Humans and LLMs in Diplomacy",
      "authors": "Wenkai Li, Lynnette Hui Xian Ng, Andy Liu, Daniel Fried",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18292",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3bca157c21b274184903d6db27d719924ad61a7ebd545421b88ee4959f34a8c8_w640_q70.webp",
      "contributions": "",
      "summary": "Measuring Fine-Grained Negotiation Tactics of Humans and LLMs in Diplomacy",
      "mindmap": ""
    },
    {
      "title": "Color, Sentiment, and Structure: A Comparative Study of Instagram Marketing Across Economies",
      "authors": "Ritesh Konka, Pranali Kurani",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18310",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e65cc34f685b4b7bf660511cdf826180cf02d70ba203ea70dda36cd297ca2194_w640_q70.webp",
      "contributions": "",
      "summary": "Color, Sentiment, and Structure: A Comparative Study of Instagram Marketing Across Economies",
      "mindmap": ""
    },
    {
      "title": "Adaptive Learning Mechanisms for Learning Management Systems: A Scoping Review and Practical Considerations",
      "authors": "Sebastian Kucharski, Iris Braun, Gregor Damnik, Matthias Wählisch",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18383",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2e9d57258150307a25d518af179c53c0ca673829d832d1ec68a69597699251d_w640_q70.webp",
      "contributions": "",
      "summary": "Adaptive Learning Mechanisms for Learning Management Systems: A Scoping Review and Practical Considerations",
      "mindmap": ""
    },
    {
      "title": "A Formal Descriptive Language for Learning Dynamics: A Five-Layer Structural Coordinate System",
      "authors": "Miyuki T. Nakata",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18525",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c5fa66d8eb7deac60effe912ce57cc5b3a8decb9a70cf6acda4def17328ab6c_w640_q70.webp",
      "contributions": "",
      "summary": "A Formal Descriptive Language for Learning Dynamics: A Five-Layer Structural Coordinate System",
      "mindmap": ""
    },
    {
      "title": "The MEVIR 2 Framework: A Virtue-Informed Moral-Epistemic Model of Human Trust Decisions",
      "authors": "Daniel Schwabe",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18539",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de4aed802558b93b84676a34a5c94c1e36401bacbc797e08c3d54cd9b553df8c_w640_q70.webp",
      "contributions": "",
      "summary": "The MEVIR 2 Framework: A Virtue-Informed Moral-Epistemic Model of Human Trust Decisions",
      "mindmap": ""
    },
    {
      "title": "Proof of Authenticity of General IoT Information with Tamper-Evident Sensors and Blockchain",
      "authors": "Kenji Saito",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18560",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32ec910f91d8d1488c0f9d63f1d11998ee65c7eeda5092f2969d5ed6f17bd9c4_w640_q70.webp",
      "contributions": "",
      "summary": "Proof of Authenticity of General IoT Information with Tamper-Evident Sensors and Blockchain",
      "mindmap": ""
    },
    {
      "title": "Measuring the Impact of Student Gaming Behaviors on Learner Modeling",
      "authors": "Qinyi Liu, Lin Li, Valdemar Švábenský, Conrad Borchers, Mohammad Khalil",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18659",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64d34ddb491ed807e5ee9ca576fcefea5ef3bafa335bbc00244fb21f30908497_w640_q70.webp",
      "contributions": "",
      "summary": "Measuring the Impact of Student Gaming Behaviors on Learner Modeling",
      "mindmap": ""
    },
    {
      "title": "\"Even GPT Can Reject Me\": Conceptualizing Abrupt Refusal Secondary Harm (ARSH) and Reimagining Psychological AI Safety with Compassionate Completion Standard (CCS)",
      "authors": "Yang Ni, Tong Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18776",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c436bec45a43d86e53531d2849399bd87e6bf3d4305bd5fa2744e56f7b83352_w640_q70.webp",
      "contributions": "",
      "summary": "\"Even GPT Can Reject Me\": Conceptualizing Abrupt Refusal Secondary Harm (ARSH) and Reimagining Psychological AI Safety with Compassionate Completion Standard (CCS)",
      "mindmap": ""
    },
    {
      "title": "Quantifying the Lifelong Impact of Resilience Interventions via Agent-Based LLM Simulation",
      "authors": "Vivienne L'Ecuyer Ming",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18803",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbb9912dc64ced0a3542815309f1cfd619aff6ad7759e8db7f3222089c75b40e_w640_q70.webp",
      "contributions": "",
      "summary": "Quantifying the Lifelong Impact of Resilience Interventions via Agent-Based LLM Simulation",
      "mindmap": ""
    },
    {
      "title": "Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,392 Adult Brazilian Workers",
      "authors": "Bruno Campello de Souza",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18871",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/446f3fb717e9b018a154458859c845d2583ea717613eb2a7397f53ffedb8700f_w640_q70.webp",
      "contributions": "",
      "summary": "Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,392 Adult Brazilian Workers",
      "mindmap": ""
    },
    {
      "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
      "authors": "Ming Li, Han Chen, Yunze Xiao, Jian Chen, Hong Jiao, Tianyi Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18880",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c02cdd0b38302d7f949dfe357cef926fc143c19edf1038c18dd4c5b1573b09_w640_q70.webp",
      "contributions": "",
      "summary": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
      "mindmap": ""
    },
    {
      "title": "Configuration Work: Four Consequences of LLMs-in-use",
      "authors": "Gabriel Alcaras, Donato Ricci",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19189",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a83a2317a41e15570e4b6245ad55aba15267903a198b04d5c65b2ef3c623e155_w640_q70.webp",
      "contributions": "",
      "summary": "Configuration Work: Four Consequences of LLMs-in-use",
      "mindmap": ""
    },
    {
      "title": "Epistemological Fault Lines Between Human and Artificial Intelligence",
      "authors": "Walter Quattrociocchi, Valerio Capraro, Matjaž Perc",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19466",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e38aa1bf279d77f222964e2fa6eaf6b1a85cc9955ae786124894e9ed3fb93c1_w640_q70.webp",
      "contributions": "",
      "summary": "Epistemological Fault Lines Between Human and Artificial Intelligence",
      "mindmap": ""
    },
    {
      "title": "Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico",
      "authors": "Martí Medina-Hern ández, Janos Kertész, Mihály Fazekas",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19491",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4c4c038ee2a9e249f2f33259b933475235132c1cbefe5a1ac37223ea8fc2367_w640_q70.webp",
      "contributions": "",
      "summary": "Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico",
      "mindmap": ""
    },
    {
      "title": "Detecting Coordinated Activities Through Temporal, Multiplex, and Collaborative Analysis",
      "authors": "Letizia Iannucci, Elisa Muratore, Antonis Matakos, Mikko Kivelä",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19677",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1131679bb8ebe54426d2398fe26bd82130fdf3a7e9489d5343133522014524c0_w640_q70.webp",
      "contributions": "",
      "summary": "Detecting Coordinated Activities Through Temporal, Multiplex, and Collaborative Analysis",
      "mindmap": ""
    },
    {
      "title": "The Role of Islamic Ethics in Preventing the Abuse of Artificial Intelligence (AI) Based Deepfakes",
      "authors": "Wisnu Uriawan, Imany Fauzy Rahman, Muhamad Zidan, Irma Rohmatillah, Muhammad Arkan Raihan, Irma Dwiyanti",
      "institution": "UIN Sunan Gunung Djati Bandung",
      "link": "https://arxiv.org/pdf/2512.17218",
      "code": null,
      "tags": [
        "ethics and society",
        "Systematic Literature Review (SLR)",
        "PRISMA",
        "Maqasid al-Shariah",
        "hifz al-ird",
        "hifz al-nafs",
        "adl",
        "tabayyun"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This study employs a Systematic Literature Review (SLISMA) to formulate an Islamic ethical framework for preventing deepfake abuse. It concludes that principles from Maqasid al-Shariah, such as protecting honor and self, provide a normative basis for shifting from punitive to preventative approaches, focusing on human dignity and the common good in the digital age.",
      "mindmap": ""
    },
    {
      "title": "Privacy-Preserving Synthetic Dataset of Individual Daily Trajectories for City-Scale Mobility Analytics",
      "authors": "Jun'ichi Ozaki, Ryosuke Susuta, Takuhiro Moriyama, Yohei Shida",
      "institution": "Not explicitly provided; cannot infer from given information.",
      "link": "https://arxiv.org/pdf/2512.17239",
      "code": null,
      "tags": [
        "privacy-preserving data synthesis",
        "multi-objective optimization",
        "origin-destination matrices",
        "dwell-travel time quantiles",
        "universal law of daily visited locations",
        "synthetic trajectory generation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a method to generate a privacy-preserving synthetic dataset of individual daily trajectories by integrating aggregated origin-destination flows with behavioral constraints in a multi-objective optimization framework. The method successfully reproduces realistic human mobility patterns in two Japanese regions, providing a practical pathway for high-resolution mobility analytics without using sensitive personal data.",
      "mindmap": ""
    },
    {
      "title": "Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?",
      "authors": "Zabir Al Nazi, G M Shahariar, Abrar Hossain, Wei Peng",
      "institution": "University of California, Riverside, University of Dhaka, Stanford University",
      "link": "https://arxiv.org/pdf/2512.17394",
      "code": null,
      "tags": [
        "vision-language models",
        "CulturalToM-VQA",
        "visual question answering",
        "chain-of-thought prompting",
        "compositional chain-of-thought prompting",
        "false belief reasoning",
        "social desirability bias"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces CulturalToM-VQA, a benchmark dataset built via a VLM-assisted human-in-the-loop pipeline to evaluate cross-cultural Theory of Mind reasoning in Vision-Language Models. It finds that while newer VLMs show strong performance on explicit tasks, they systematically struggle with false belief reasoning, and their results may be inflated by social desirability bias rather than genuine visual understanding.",
      "mindmap": ""
    },
    {
      "title": "Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding",
      "authors": "Evangelos Pournaras",
      "institution": "University of Leeds",
      "link": "https://arxiv.org/pdf/2512.17461",
      "code": null,
      "tags": [
        "democratic systems",
        "fair voting methods",
        "cumulative voting",
        "equal shares",
        "proportional representation",
        "participatory budgeting",
        "AI voting assistance"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes that combining expressive ballot formats like cumulative voting with proportional aggregation methods like equal shares constitutes a \"fair voting method.\" It concludes that such methods enhance democratic legitimacy, accelerate impactful outcomes in areas like welfare and education, and serve as a safeguard against biases in emerging AI-assisted voting scenarios.",
      "mindmap": ""
    },
    {
      "title": "Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life",
      "authors": "Corey M. Abramson",
      "institution": "Rice University, UC San Francisco",
      "link": "https://arxiv.org/pdf/2512.17850",
      "code": null,
      "tags": [
        "computational social science",
        "computational text analysis",
        "machine learning (ML)",
        "natural language processing (NLP)",
        "ethnography",
        "in-depth interviews",
        "mixed-methods"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper demonstrates how computational social science tools like machine learning and natural language processing can be integrated with traditional qualitative methods (e.g., ethnography, interviews) to study aging. It concludes that these computational methods can broaden qualitative research by streamlining workflows, scaling up projects, and enabling new multi-method insights, rather than replacing its foundational approaches.",
      "mindmap": ""
    },
    {
      "title": "Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease",
      "authors": "Carter H. Nakamoto, Lucia Lushi Chen, Agata Foryciarz, Sherri Rose",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.17340",
      "code": null,
      "tags": [
        "fair machine learning",
        "penalized regression",
        "cost-sensitive classification",
        "true positive rate disparity penalties"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a penalized fair regression framework using unfairness penalties for multiple groups, implemented via reduction to cost-sensitive classification. The method is applied to predict end-stage renal disease in a chronic kidney disease study, showing substantial fairness improvements for multiple race and ethnicity groups without appreciable loss in overall model fit.",
      "mindmap": ""
    },
    {
      "title": "Value Lens: Using Large Language Models to Understand Human Values",
      "authors": "Eduardo de la Cruz Fernández, Marcelo Karanik, Sascha Ossowski",
      "institution": "Universidad Politécnica de Madrid, Universidad Rey Juan Carlos",
      "link": "https://arxiv.org/pdf/2512.15722",
      "code": null,
      "tags": [
        "llm inference",
        "large language models",
        "value detection",
        "generative AI",
        "dual-LLM approach",
        "expert verification"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes Value Lens, a two-stage model that uses Large Language Models (LLMs) to detect human values in text. The first stage uses an LLM to conceptualize a value theory verified by experts, and the second stage employs a dual-LLM approach for detection and critical review. The results show that Value Lens performs comparably to or better than other models in similar tasks.",
      "mindmap": ""
    },
    {
      "title": "D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models",
      "authors": "Javon Hickmon",
      "institution": "University of Washington",
      "link": "https://arxiv.org/pdf/2512.15747",
      "code": null,
      "tags": [
        "multi-modal inference",
        "CLIP",
        "Stable Diffusion XL",
        "zero-shot classification",
        "demographic bias mitigation",
        "data generation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes D3G, a training-free method that uses Stable Diffusion XL to generate diverse demographic data at inference time to improve zero-shot image classification with CLIP. The method is shown to boost classification accuracy while reducing harmful demographic bias in pre-trained multimodal models.",
      "mindmap": ""
    },
    {
      "title": "Cultural Rights and the Rights to Development in the Age of AI: Implications for Global Human Rights Governance",
      "authors": "Alexander Kriebitz, Caitlin Corrigan, Aive Pevkur, Alberto Santos Ferro, Amanda Horzyk, Dirk Brand, Dohee Kim, Dodzi Koku Hattoh, Flavia Massucci, Gilles Fayad, Kamil Strzepek, Laud Ammah, Lavina Ramkissoon, Mariette Awad, Natalia Amasiadi, Nathan C. Walker, Nicole Manger, Sophia Devlin",
      "institution": "Ludwig Maximilian University of Munich, Technical University of Munich, Tallinn University of Technology, University of Edinburgh, Stellenbosch University, Changwon National University, University of Ghana, Cardinal Stefan Wyszyński University in Warsaw, African Union, American University of Beirut, University of Patras, Rutgers University, Ulster University",
      "link": "https://arxiv.org/pdf/2512.15786",
      "code": null,
      "tags": [
        "AI ethics and governance",
        "cultural rights",
        "right to development",
        "algorithmic design",
        "AI governance",
        "human rights law"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper conceptually analyzes the impact of AI on cultural rights and the right to development, examining the epistemic and normative limitations in algorithmic design. It concludes that AI governance frameworks have significant gaps in protecting these rights and risks exacerbating global inequities. The study calls for integrating cultural and developmental considerations into future AI policy and research.",
      "mindmap": ""
    },
    {
      "title": "Toward Agentic Environments: GenAI and the Convergence of AI, Sustainability, and Human-Centric Spaces",
      "authors": "Przemek Pospieszny, Dominika P. Brodowicz",
      "institution": "EPAM Systems, Warsaw School of Economics",
      "link": "https://arxiv.org/pdf/2512.15787",
      "code": null,
      "tags": [
        "others",
        "generative AI",
        "multi-agent systems",
        "edge computing",
        "sustainability",
        "ambient intelligence",
        "AI agents",
        "sustainable ecosystems"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes the concept of \"agentic environments,\" a framework leveraging generative AI, multi-agent systems, and edge computing to create sustainable, human-centric spaces. It concludes that this approach can reduce the environmental impact of AI by optimizing resource use and enhancing data privacy through decentralized, edge-driven deployment models.",
      "mindmap": ""
    },
    {
      "title": "A Systematic Analysis of Biases in Large Language Models",
      "authors": "Xulang Zhang, Rui Mao, Erik Cambria",
      "institution": "Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.15792",
      "code": null,
      "tags": [
        "fairness and bias analysis",
        "news summarization",
        "stance classification",
        "UN voting patterns",
        "multilingual story completion",
        "World Values Survey"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper systematically analyzes biases in large language models (LLMs) across political, ideological, alliance, language, and gender dimensions using experiments like news summarization and stance classification. The main conclusion is that despite being aligned for neutrality, the studied LLMs still exhibit various types of biases and affinities.",
      "mindmap": ""
    },
    {
      "title": "Evaluation of AI Ethics Tools in Language Models: A Developers' Perspective Case Stud",
      "authors": "Jhessica Silva, Diego A. B. Moreira, Gabriel O. dos Santos, Alef Ferreira, Helena Maia, Sandra Avila, Helio Pedrini",
      "institution": "Universidade Estadual de Campinas (UNICAMP), Universidade Federal de Goiás (UFG)",
      "link": "https://arxiv.org/pdf/2512.15791",
      "code": null,
      "tags": [
        "ai ethics evaluation",
        "Model Cards",
        "ALTAI",
        "FactSheets",
        "Harms Modeling",
        "literature survey",
        "interviews"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper presents a methodology to evaluate AI Ethics Tools (AIETs) for language models by selecting four tools (Model Cards, ALTAI, FactSheets, Harms Modeling) and applying them to Portuguese language models, with developer interviews. The results indicate that these tools help guide general ethical considerations but fail to address language-specific aspects like idiomatic expressions or identify negative impacts for Portuguese.",
      "mindmap": ""
    },
    {
      "title": "Explainable Ethical Assessment on Human Behaviors by Generating Conflicting Social Norms",
      "authors": "Yuxi Sun, Wei Gao, Hongzhan Lin, Jing Ma, Wenxuan Zhang",
      "institution": "Hong Kong Baptist University, Singapore Management University, Singapore University of Technology and Design",
      "link": "https://arxiv.org/pdf/2512.15793",
      "code": null,
      "tags": [
        "ethical ai",
        "contrastive learning",
        "social norms generation",
        "moral reasoning",
        "explainable ai",
        "valence prediction"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces ClarityEthic, a method that enhances ethical assessment of human actions by generating conflicting social norms to explain and predict valence (support/oppose). It uses a contrastive learning strategy to strengthen the moral reasoning of language models. Experiments show the method outperforms baselines and human evaluations confirm the generated norms provide plausible explanations.",
      "mindmap": ""
    },
    {
      "title": "Cybercrime and Computer Forensics in Epoch of Artificial Intelligence in India",
      "authors": "Sahibpreet Singh, Shikha Dhiman",
      "institution": "Guru Nanak Dev University, Amritsar",
      "link": "https://arxiv.org/pdf/2512.15799",
      "code": null,
      "tags": [
        "cybersecurity",
        "computer forensics",
        "explainable AI (XAI)",
        "data minimization",
        "algorithmic bias",
        "deepfakes",
        "adversarial AI",
        "Digital Personal Data Protection Act"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper employs a doctrinal legal methodology to analyze the integration of AI into cybercrime and forensics in India, focusing on the Digital Personal Data Protection Act, 2023. It concludes that there is a critical tension between privacy principles and forensic needs, and proposes a human-centric forensic model using explainable AI (XAI) to ensure evidence admissibility while advocating for legislative synchronization with international standards.",
      "mindmap": ""
    },
    {
      "title": "DSO: Direct Steering Optimization for Bias Mitigation",
      "authors": "Lucas Monteiro Paes, Nivedha Sivakumar, Yinong Oliver Wang, Masha Fedzechkina Donaldson, Luca Zappella, Nicholas Apostoloff",
      "institution": "Apple, Carnegie Mellon University",
      "link": "https://arxiv.org/pdf/2512.15926",
      "code": null,
      "tags": [
        "fairness and bias mitigation",
        "activation steering",
        "reinforcement learning",
        "linear transformations",
        "inference-time control"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes Direct Steering Optimization (DSO), a method using reinforcement learning to find linear transformations for steering activations in generative models to mitigate bias while maintaining performance. It demonstrates state-of-the-art trade-offs between fairness and capabilities in VLMs and LLMs, offering inference-time control over bias reduction. The work highlights the advantage of directly optimized steering strategies over heuristic-based approaches for effective bias intervention.",
      "mindmap": ""
    },
    {
      "title": "Cross-Language Bias Examination in Large Language Models",
      "authors": "Yuxuan Liang, Marwa Mahmoud",
      "institution": "Georgia Institute of Technology, University of Glasgow",
      "link": "https://arxiv.org/pdf/2512.16029",
      "code": null,
      "tags": [
        "fairness and bias evaluation",
        "multilingual bias evaluation",
        "BBQ benchmark",
        "prompt-based Implicit Association Test",
        "explicit bias",
        "implicit bias"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces a multilingual bias evaluation framework that combines explicit bias assessment using the BBQ benchmark with implicit bias measurement via a prompt-based Implicit Association Test, applied across five languages. The results show significant variation in bias across languages, with Arabic and Spanish exhibiting higher stereotype bias, and reveal contrasting patterns between explicit and implicit bias, such as age having low explicit but high implicit bias. The study highlights the importance of cross-lingual bias analysis for developing equitable multilingual LLMs.",
      "mindmap": ""
    },
    {
      "title": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams",
      "authors": "Gilad Gressel, Rahul Pankajakshan, Shir Rozenfeld, Ling Li, Ivan Franceschini, Krishnahsree Achuthan, Yisroel Mirsky",
      "institution": "Amrita Vishwa Vidyapeetham, Ca’ Foscari University of Venice, University of Melbourne, Ben Gurion University of the Negev",
      "link": "https://arxiv.org/pdf/2512.16280",
      "code": null,
      "tags": [
        "llm inference",
        "large language models",
        "safety filters",
        "conversation study",
        "social engineering",
        "automation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper investigates the role of LLMs in romance-baiting scams through interviews with insiders and victims, a blinded long-term conversation study comparing LLM agents to human operators, and an evaluation of commercial safety filters. It finds that LLMs are already widely used in scams, can elicit greater trust and compliance than humans, and that current safety filters are ineffective at detecting such dialogues, suggesting scams are ripe for full LLM automation.",
      "mindmap": ""
    },
    {
      "title": "Comprehensive AI Literacy: The Case for Centering Human Agency",
      "authors": "Sri Yash Tadimalla, Justin Cary, Gordon Hull, Jordan Register, Daniel Maxwell, David Pugalee, Tina Heafner",
      "institution": "UNC Charlotte",
      "link": "https://arxiv.org/pdf/2512.16656",
      "code": null,
      "tags": [
        "education",
        "AI literacy",
        "human agency",
        "critical thinking",
        "epistemology",
        "educational frameworks"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This position paper proposes a shift towards comprehensive AI literacy frameworks that center human agency and critical thinking in education. It concludes that educators and students must be empowered to make intentional, critical choices about AI use, rather than focusing solely on operational skills.",
      "mindmap": ""
    },
    {
      "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs",
      "authors": "Shubham Mishra, Samyek Jain, Gorang Mehrishi, Shiv Tiwari, Harsh Sharma, Pratik Narang, Dhruv Kumar",
      "institution": "Birla Institute of Technology and Science, Pilani, Carnegie Mellon University",
      "link": "https://arxiv.org/pdf/2512.16795",
      "code": null,
      "tags": [
        "llm inference",
        "retrieval-augmented generation",
        "deductive reasoning",
        "conflict-aware trust-score",
        "reasoning-trace-augmented framework",
        "supervised fine-tuning"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a reasoning-trace-augmented RAG framework that integrates a three-stage deductive reasoning process (document adjudication, conflict analysis, and grounded synthesis) to handle conflicting or unreliable retrieved evidence. It introduces a Conflict-Aware Trust-Score (CATS) evaluation pipeline. The method, tested with models like Qwen, shows substantial improvements in answer correctness and behavioral adherence over baseline RAG systems.",
      "mindmap": ""
    },
    {
      "title": "Impacts of Racial Bias in Historical Training Data for News AI",
      "authors": "Rahul Bhargava, Malene Hornstrup Jespersen, Emily Boardman Ndulue, Vivica Dsouza",
      "institution": "Northeastern University, University of Copenhagen, Media Ecosystems Analysis Group",
      "link": "https://arxiv.org/pdf/2512.16901",
      "code": null,
      "tags": [
        "algorithmic auditing",
        "multi-label classifier",
        "explainable AI",
        "word2vec",
        "New York Times Annotated Corpus"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper investigates racial bias in a multi-label text classifier trained on the New York Times Annotated Corpus using word2vec and explainable AI methods. It finds that a problematic \"blacks\" label acts as a general \"racism detector\" but fails on modern examples, demonstrating how historical training data embeds biases into AI models. The study highlights the tension for newsrooms in adopting AI tools while mitigating the reproduction of historical stereotypes in news coverage.",
      "mindmap": ""
    },
    {
      "title": "SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI",
      "authors": "Ryan Cartularo",
      "institution": "The University of Texas at Austin",
      "link": "https://arxiv.org/pdf/2512.14712",
      "code": null,
      "tags": [
        "multi-modal training",
        "Mixture-of-Experts (MoE)",
        "Hierarchical Gated Attention Network",
        "CatBoost meta-learner",
        "multimodal fusion",
        "deep fusion",
        "expert stacking",
        "Quad-Modal Ensemble"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper compares two multimodal AI architectures for sepsis prediction and antibiotic selection: a complex end-to-end deep fusion model (SepsisFusionFormer) and a leaner, context-aware Mixture-of-Experts stacking model (SepsisLateFusion). The main conclusion is that for this high-stakes, data-sparse clinical domain, the interpretable expert stacking approach, which treats modalities as orthogonal experts and uses a CatBoost meta-learner, significantly outperformed the deep fusion model, achieving state-of-the-art predictive performance and enabling a prescriptive window for intervention.",
      "mindmap": ""
    },
    {
      "title": "AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally",
      "authors": "Nadine Angela Cantonjos, Arpita Biswas",
      "institution": "Rutgers University",
      "link": "https://arxiv.org/pdf/2512.14910",
      "code": null,
      "tags": [
        "others",
        "multi-agent reasoning",
        "chain-of-responsibility",
        "modular architecture",
        "governance mechanisms",
        "multilingual interactions",
        "real-time tools"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper presents AgroAskAI, a multi-agent AI framework designed to support smallholder farmers with climate adaptation queries. It uses a modular, role-specialized architecture coordinated via a chain-of-responsibility approach, integrating real-time data and multilingual support. The experimental results show that this system delivers more actionable and grounded outputs for agricultural decision support.",
      "mindmap": ""
    },
    {
      "title": "Epistemic diversity across language models mitigates knowledge collapse",
      "authors": "Damian Hodel, Jevin D. West",
      "institution": "University of Washington",
      "link": "https://arxiv.org/pdf/2512.15011",
      "code": null,
      "tags": [
        "llm training",
        "model collapse",
        "epistemic diversity",
        "AI ecosystem",
        "self-training",
        "distributed training"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper investigates whether diversity across language models (an \"AI ecosystem\") can mitigate performance decay from training on model-generated data. It segments training data across multiple models and evaluates performance over self-training iterations. The main conclusion is that increased epistemic diversity mitigates knowledge collapse, but only up to an optimal level, with too few or too many models leading to poor performance.",
      "mindmap": ""
    },
    {
      "title": "Governing rapid technological change: Policy Delphi on the future of European AI governance",
      "authors": "Atte Ojanen, Johannes Anttila, Thilo H. K. Thelitz, Anna Bjork",
      "institution": "Demos Helsinki, University of Turku",
      "link": "https://arxiv.org/pdf/2512.15196",
      "code": null,
      "tags": [
        "policy analysis",
        "Policy Delphi",
        "anticipatory governance",
        "future-proof regulation",
        "AI Act"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper uses a two-round Policy Delphi method with European experts to study the future of AI governance. It finds a consensus that effective regulation depends more on practical implementation and enforcement than on technical specifics, and identifies a gap between desirable policy directions (like citizen participation) and their perceived feasibility.",
      "mindmap": ""
    },
    {
      "title": "ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I",
      "authors": "Seok-Hyun Ga, Chun-Yen Chang",
      "institution": "Institute for Research Excellence in Learning Sciences, National Taiwan Normal University, Seoul National University, Universitas Negeri Malang",
      "link": "https://arxiv.org/pdf/2512.15298",
      "code": null,
      "tags": [
        "multi-modal inference",
        "multimodal reasoning",
        "perception-cognition gap",
        "calculation-conceptualization discrepancy",
        "process hallucination",
        "OCR",
        "AI-resistant questions"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This study evaluates the multimodal scientific reasoning of LLMs like GPT-4o and Gemini on the Korean CSAT Earth Science I exam under different input conditions. It finds that models suffer from fundamental cognitive flaws, such as a perception-cognition gap and calculation-conceptualization discrepancy, even with optimized inputs. The paper concludes by suggesting these vulnerabilities can be exploited to design AI-resistant assessment questions to ensure academic integrity.",
      "mindmap": ""
    },
    {
      "title": "Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality",
      "authors": "Efe Bozkir, Enkelejda Kasneci",
      "institution": "Technical University of Munich",
      "link": "https://arxiv.org/pdf/2512.15343",
      "code": null,
      "tags": [
        "others",
        "crowdsourcing",
        "user study",
        "extended reality",
        "conversational agents",
        "privacy",
        "technology acceptance model"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper conducted a large-scale crowdsourcing study with 1036 participants to explore user acceptance and concerns regarding LLM-powered conversational agents in Extended Reality (XR). The study found that while users generally accept these technologies, they express significant concerns about security, privacy, social implications, and trust, with location data being the most sensitive. The results highlight the importance of practitioner transparency and that familiarity with generative AI increases acceptance, while prior XR device ownership is linked to lower acceptance.",
      "mindmap": ""
    }
  ]
}