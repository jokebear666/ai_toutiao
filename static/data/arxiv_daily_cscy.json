{
  "label": "cs.CY",
  "slug": "cscy",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "SENTINEL: A Multi-Modal Early Detection Framework for Emerging Cyber Threats using Telegram",
      "authors": "Mohammad Hammas Saeed, Howie Huang",
      "institution": "George Washington University",
      "link": "https://arxiv.org/pdf/2512.21380",
      "code": null,
      "tags": [
        "cyber threat intelligence",
        "multi-modal fusion",
        "large language models",
        "graph neural networks",
        "early detection",
        "social media analysis"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c3e47a235de6afcb4955afd774a9e4b3883efcafc7e4aaa97649575ef2fb34d0_w640_q70.webp",
      "contributions": "1. Proposes SENTINEL, a multi-modal framework for early cyber threat detection by aligning social media discussions with real-world attacks. 2. Combines language modeling (using LLMs) and network coordination analysis (using GNNs) to fuse textual and relational signals from platforms like Telegram. 3. Demonstrates the framework's effectiveness on a dataset of 365k messages from 16 Telegram channels, achieving an F1 score of 0.89 for threat alignment.",
      "summary": "The paper presents SENTINEL, a framework for the early detection of cyber threats by analyzing multi-modal signals from social media platforms like Telegram. It combines large language models for text understanding with graph neural networks to model user coordination, successfully aligning online discussions to real-world attacks. The evaluation on Telegram data shows the approach is effective, achieving a high F1 score of 0.89.",
      "mindmap": "graph TB\n        Root[”SENTINEL: A Multi-Modal Early Detection Framework for Emerging Cyber Threats using Telegram”] --> Problem[”核心问题/Problem: Post-hoc detection of cyber attacks is reactive; need for proactive, early warning systems.”]\n        Root --> Method[”主要方法/Method: Multi-modal framework combining LLMs (language) and GNNs (coordination graphs) to analyze social media signals.”]\n        Root --> Results[”关键结果/Results: Achieves F1 of 0.89 aligning Telegram discussions to real-world cyber threats.”]"
    },
    {
      "title": "ALETHEIA: Combating Social Media Influence Campaigns with Graph Neural Networks",
      "authors": "Mohammad Hammas Saeed, Isaiah J. King, Howie Huang",
      "institution": "George Washington University",
      "link": "https://arxiv.org/pdf/2512.21391",
      "code": null,
      "tags": [
        "social network security",
        "Graph Neural Networks",
        "influence campaigns",
        "temporal link prediction",
        "troll detection",
        "Reddit"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/473a865358e998c62938f17660edc395a7658bf360ef15e62ea79317e8734aec_w640_q70.webp",
      "contributions": "1. Proposes ALETHEIA, a system that formalizes the detection of malicious accounts in influence campaigns as a node classification and link prediction problem using a graph-based representation. 2. Demonstrates that a detection pipeline combining topological (graph) and linguistic features outperforms standard interaction and user features, achieving a 3.7% F1-score improvement. 3. Introduces a novel temporal link prediction mechanism for influence campaigns by stacking a GNN over an RNN to forecast future troll interactions (TTE/TUE) with high accuracy (96.6% AUC).",
      "summary": "This paper presents ALETHEIA, a system that uses Graph Neural Networks (GNNs) to detect malicious accounts and predict their future interactions in social media influence campaigns. By modeling campaigns as graphs and combining structural and linguistic features, it improves detection performance and forecasts troll behavior with high accuracy. The results underscore the importance of leveraging network structure to combat coordinated malicious activity online.",
      "mindmap": "graph TB\n        A[ALETHEIA: Combating Social Media Influence Campaigns with Graph Neural Networks] --> B[核心问题/Problem: Detecting and predicting malicious influence campaigns on social media]\n        A --> C[主要方法/Method: Graph Neural Networks (GNNs) with topological & linguistic features, GNN+RNN for temporal link prediction]\n        A --> D[关键结果/Results: 3.7% F1-score improvement in detection, 96.6% AUC for predicting future troll interactions]"
    },
    {
      "title": "Bidirectional Human-AI Alignment in Education for Trustworthy Learning Environments",
      "authors": "Hua Shen",
      "institution": "NYU Shanghai, New York University",
      "link": "https://arxiv.org/pdf/2512.21552",
      "code": null,
      "tags": [
        "ai for education",
        "human-ai alignment",
        "trustworthy ai",
        "adaptive learning",
        "educational technology",
        "ai ethics"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b7f40fe114da7bae34b09e44db18fa32bb4f64e57bd01e75e96afc80c2ddc136_w640_q70.webp",
      "contributions": "1. Proposes the novel concept of \"bidirectional human-AI alignment\" for education, emphasizing mutual adaptation between humans and AI systems. 2. Explores the evolution of AI's role in education from a support tool to a collaborative partner, analyzing its impact on teacher roles and student agency. 3. Provides actionable strategies for policymakers, developers, and educators to ensure AI advances equity, transparency, and human flourishing in learning environments.",
      "summary": "This paper addresses the risks of AI in education, such as bias and loss of autonomy, by proposing the concept of bidirectional human-AI alignment. The method involves not only embedding human values into AI but also equipping educators and students to guide these technologies. It concludes that reframing AI adoption as a process of mutual adaptation is key to creating trustworthy learning environments where humans and AI can grow together.",
      "mindmap": "graph TB\n        A[论文标题: Bidirectional Human-AI Alignment in Education] --> B[核心问题/Problem: AI in education introduces risks to equity, privacy, and autonomy.]\n        A --> C[主要方法/Method: Proposes bidirectional alignment: embedding human values into AI and equipping humans to interpret/guide AI.]\n        A --> D[关键结果/Results: Envisions a future of mutual adaptation where AI advances equity, transparency, and human flourishing.]"
    },
    {
      "title": "Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets",
      "authors": "Matyas Bohacek, Ignacio Vilanova Echavarri",
      "institution": "Stanford University, Imperial College London",
      "link": "https://arxiv.org/pdf/2512.21775",
      "code": null,
      "tags": [
        "Data Provenance",
        "Data Provenance",
        "Compliance Rating",
        "Generative AI",
        "Dataset Ethics",
        "Transparency"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fa33a1fedd52ef1c87e9bf7d9a25dad61aae942ba50660263862470b9b677745_w640_q70.webp",
      "contributions": "1. Proposes the Compliance Rating Scheme (CRS), a framework for evaluating dataset compliance with transparency, accountability, and security principles. 2. Develops and releases an open-source Python library that implements the CRS framework using data provenance technology. 3. Creates a tool that is both reactive (evaluating existing datasets) and proactive (guiding the responsible construction of new datasets).",
      "summary": "The paper addresses the lack of ethical and legal oversight in the creation and sharing of datasets for Generative AI. It proposes the Compliance Rating Scheme (CRS) framework and an accompanying open-source library to assess and ensure dataset compliance with key principles. The work aims to improve traceability and accountability in the AI data supply chain.",
      "mindmap": "graph TB\n        Root(”Compliance Rating Scheme: A Data Provenance Framework for Generative AI Datasets”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”数据集创建缺乏伦理与法律监督/Lack of ethical & legal oversight in dataset creation”)\n        Problem --> P2(”数据来源与合法性信息丢失/Loss of data origin & legitimacy info”)\n        Method --> M1(”提出合规评级方案(CRS)框架/Propose Compliance Rating Scheme (CRS) framework”)\n        Method --> M2(”开发基于数据溯源技术的开源库/Develop open-source library using data provenance”)\n        Results --> R1(”评估现有数据集的合规性/Evaluate compliance of existing datasets”)\n        Results --> R2(”指导负责任的新数据集构建/Guide responsible construction of new datasets”)"
    },
    {
      "title": "On The Conceptualization and Societal Impact of Cross-Cultural Bias",
      "authors": "Vitthal Bhandari",
      "institution": "University of Washington",
      "link": "https://arxiv.org/pdf/2512.21809",
      "code": null,
      "tags": [
        "bias and fairness",
        "cultural bias",
        "literature survey",
        "societal impact",
        "harm evaluation",
        "bias mitigation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2702d319a8125ee471012d7f7a71a4d4530da34216397d1647aba76a8e4a3842_w640_q70.webp",
      "contributions": "1. Conducts a focused survey of 20 recent (2025) papers on cultural bias in NLP, identifying gaps in current research practices. 2. Critiques the literature for lacking concrete definitions of bias, failing to identify affected stakeholders, and inadequately evaluating the harms of biased systems. 3. Advocates for a future research agenda that emphasizes robust societal impact assessment, concrete bias conceptualization, and engagement with real-world stakeholders.",
      "summary": "This paper surveys recent literature on cultural bias in NLP, finding that current research often fails to concretely define bias, engage with affected stakeholders, or thoroughly evaluate societal harms. The author proposes a set of observations to guide future work towards more robust and impactful assessments of cross-cultural bias in language technologies.",
      "mindmap": "graph TB\n    Root(”On The Conceptualization and Societal Impact of Cross-Cultural Bias”) --> Problem(”核心问题/Problem: LLMs exhibit cross-cultural bias; research often avoids real-world stakeholder engagement.”)\n    Root --> Method(”主要方法/Method: Survey and analyze 20 recent (2025) papers on cultural bias in NLP.”)\n    Root --> Results(”关键结果/Results: Identifies gaps in bias definition, harm evaluation; advocates for robust societal impact assessment.”)"
    },
    {
      "title": "Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?",
      "authors": "Naen Xu, Jinghuai Zhang, Changjiang Li, Hengyu An, Chunyi Zhou, Jun Wang, Boyu Xu, Yuyuan Li, Tianyu Du, Shouling Ji",
      "institution": "Zhejiang University, University of California, Los Angeles, Palo Alto Networks",
      "link": "https://arxiv.org/pdf/2512.21871",
      "code": "https://github.com/bluedream02/CopyGuard",
      "tags": [
        "multi-modal inference",
        "copyright compliance",
        "vision-language models",
        "tool-augmented defense",
        "benchmark dataset",
        "multimodal query"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a933ea78af16685ceab38b447862e9c50b08de435c2e6b662d59551bf5552fdc_w640_q70.webp",
      "contributions": "1. Introduced a large-scale benchmark dataset of 50,000 multimodal query-content pairs to evaluate copyright compliance in LVLMs. 2. Conducted a comprehensive evaluation revealing significant deficiencies in state-of-the-art LVLMs' ability to recognize and respect copyrighted content. 3. Proposed a novel tool-augmented defense framework to reduce copyright infringement risks in LVLM inference.",
      "summary": "This paper evaluates how large vision-language models (LVLMs) handle copyrighted visual content and finds they often fail to comply with copyright regulations. To address this, the authors propose a tool-augmented defense framework for copyright compliance. The work highlights the need for developing copyright-aware LVLMs to ensure responsible use.",
      "mindmap": "graph TB\n        Root[”Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?”]\n        Root --> Problem[”核心问题/Problem: LVLMs may infringe copyright when processing visual inputs”]\n        Root --> Method[”主要方法/Method: Benchmark dataset & Tool-augmented defense framework”]\n        Root --> Results[”关键结果/Results: Current LVLMs are deficient; Proposed framework reduces risk”]"
    },
    {
      "title": "Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management",
      "authors": "Sunil Arora, John Hastings",
      "institution": "Dakota State University",
      "link": "https://arxiv.org/pdf/2512.22060",
      "code": null,
      "tags": [
        "AI Governance & Compliance",
        "lifecycle management",
        "bias detection",
        "differential privacy",
        "federated learning",
        "terminology drift"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/113703d05215aac7e8678f24cb38d882fa4c99927066ea2264ef3d1b4c3a1d67_w640_q70.webp",
      "contributions": "1. Proposes the SC-NLP-LMF, a comprehensive six-phase framework for secure and compliant NLP model lifecycle management. 2. Integrates established technical methods (e.g., bias detection, differential privacy) with leading organizational standards (e.g., NIST AI RMF, EU AI Act). 3. Validates the framework's practicality through a healthcare case study demonstrating detection of and response to terminology drift.",
      "summary": "This paper introduces the Secure and Compliant NLP Lifecycle Management Framework (SC-NLP-LMF), a six-phase model developed from a systematic review to address security, privacy, and compliance risks in NLP systems. It integrates methods like bias detection and differential privacy with standards like NIST AI RMF and the EU AI Act. The framework provides a practical structure for organizations to manage NLP systems in high-risk environments, as illustrated by a healthcare case study on handling terminology drift.",
      "mindmap": "graph TB\n        Root[”Toward Secure and Compliant AI: Organizational Standards and Protocols for NLP Model Lifecycle Management”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>NLP systems in sensitive domains face unaddressed security, privacy, and compliance risks.”]\n        Method[”主要方法/Method<br>Proposes SC-NLP-LMF, a six-phase framework integrating standards (NIST, ISO, EU AI Act) and techniques (bias detection, differential privacy).”]\n        Results[”关键结果/Results<br>Provides a practical lifecycle structure for secure, accountable NLP systems, validated via a healthcare case study.”]"
    },
    {
      "title": "Agent-based simulation of online social networks and disinformation",
      "authors": "Alejandro Buitrago López, Alberto Ortega Pastor, David Montoro Aguilera, Mario Fernández Tárraga, Jesús Verdú Chacón, Javier Pastor-Galindo, José A. Ruipérez-Valiente",
      "institution": "University of Murcia",
      "link": "https://arxiv.org/pdf/2512.22082",
      "code": null,
      "tags": [
        "agent-based simulation",
        "agent-based simulation",
        "large language model",
        "disinformation campaigns",
        "synthetic social networks",
        "behavioral automata"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2a3415bee27d71926e7770fd596253ec973faaa85d1fcba853a047ff6d08bfe3_w640_q70.webp",
      "contributions": "1. A simulation framework that models synthetic social networks using agents with demographic-based personality traits and finite-state behavioral automata for realistic and interpretable actions. 2. A generative module powered by an LLM to produce context-aware social media posts consistent with each agent's profile and memory. 3. A red module implementing DISARM-inspired workflows to orchestrate disinformation campaigns and a Mastodon-based visualization layer for real-time inspection and validation.",
      "summary": "This paper proposes an agent-based simulation framework to study online social networks and disinformation, addressing the limitations of platform opacity and data access. The framework uses LLM-powered agents with personality traits and behavioral automata to generate realistic content and simulate disinformation campaigns, with evaluation showing structural, behavioral, and linguistic realism. It provides a customizable and controllable environment for studying information dynamics.",
      "mindmap": "graph TB\n        Root[Agent-based simulation of online social networks and disinformation] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[平台不透明与数据限制/Platform Opacity & Data Limits]\n        Problem --> P2[现有模拟缺乏真实性与可解释性/Existing Simulations Lack Realism & Explainability]\n        Method[主要方法/Method] --> M1[基于代理的合成社交网络/Agent-based Synthetic Social Networks]\n        Method --> M2[LLM生成上下文感知内容/LLM Generates Context-aware Content]\n        Method --> M3[红色模块模拟虚假信息活动/Red Module Simulates Disinformation Campaigns]\n        Method --> M4[Mastodon可视化层/Mastodon Visualization Layer]\n        Results[关键结果/Results] --> R1[展示结构、行为、语言真实性/Demonstrates Structural, Behavioral, Linguistic Realism]\n        Results --> R2[为研究信息动态提供可定制环境/Provides Customizable Environment for Studying Information Dynamics]"
    },
    {
      "title": "From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education",
      "authors": "Iman Reihanian, Yunfei Hou, Qingquan Sun",
      "institution": "California State University, San Bernardino",
      "link": "https://arxiv.org/pdf/2512.20714",
      "code": null,
      "tags": [
        "educational technology",
        "generative AI",
        "personalization",
        "adaptive learning",
        "large language models",
        "intelligent tutoring systems"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8e46f313e494a41a4a12873eddb6320db4cd59b6fb958bb008fd6f6512729af4_w640_q70.webp",
      "contributions": "1. Identified and analyzed five key application domains for GenAI-enabled personalization in CS education: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review. 2. Synthesized four design patterns for successful implementations: context-aware tutoring anchored in student artifacts, multi-level hint structures, composition with traditional CS infrastructure, and human-in-the-loop quality assurance. 3. Proposed an exploration-first adoption framework for integrating GenAI, emphasizing piloting, instrumentation, learning-preserving defaults, and evidence-based scaling, while pairing recurrent risks with operational mitigations.",
      "summary": "This scoping review maps how generative AI enables personalized computer science education. It analyzes design choices across 32 studies and finds that structured implementations with explanation-first guidance and artifact grounding lead to more positive learning outcomes than unconstrained chat interfaces. The paper concludes that generative AI can provide precision scaffolding when embedded in audit-ready workflows that preserve productive struggle.",
      "mindmap": "graph LR\n    A[From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education] --> B[核心问题/Problem: Does GenAI personalization support or undermine CS learning?]\n    A --> C[主要方法/Method: Scoping review of 32 studies; Analysis of design choices & patterns]\n    A --> D[关键结果/Results: Structured designs (e.g., hint ladders, artifact grounding) are more effective; Proposes an exploration-first adoption framework]"
    },
    {
      "title": "Sark: Oblivious Integrity Without Global State",
      "authors": "Alex Lynham, David Alesch, Ziyi Li, Geoff Goodell",
      "institution": "University College London (UCL)",
      "link": "https://arxiv.org/pdf/2512.20775",
      "code": null,
      "tags": [
        "distributed ledger systems",
        "oblivious integrity",
        "crash fault-tolerant blockchain",
        "integrity locus",
        "USO asset system",
        "local centrality"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b30bc13516511718c7bc6ed68789e79948291909f0ae043653903321e5a6bcc_w640_q70.webp",
      "contributions": "1. Presents Sark, a reference architecture implementing the Unforgeable, Stateful, and Oblivious (USO) asset system for oblivious, non-custodial asset management. 2. Introduces the concept of \"Integrity Locus\" as a framework to analyze and address design trade-offs related to decentralization. 3. Describes the design and implementation of Sloop, a permissioned crash fault-tolerant (CFT) blockchain, and Porters, subsystems that form the core of the Sark architecture.",
      "summary": "This paper introduces Sark, a distributed system architecture designed for managing assets with oblivious integrity, eliminating the need for a global state ledger. Its core components include the Sloop blockchain and Porters for handling client commitments, analyzed through the CIA triad. The main conclusion is that Sark offers a more decentralized trust topology by leveraging local integrity proofs instead of a global ledger, though it introduces trade-offs like local centrality.",
      "mindmap": "graph LR\n        A[Sark: Oblivious Integrity Without Global State] --> B(核心问题/Problem: How to achieve asset integrity without a global state ledger?);\n        A --> C(主要方法/Method: Implement USO asset system with Sloop CFT blockchain & Porters);\n        A --> D(关键结果/Results: Decentralized via local integrity proofs, introduces Integrity Locus concept);"
    },
    {
      "title": "Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles",
      "authors": "Ramatu Oiza Abdulsalam, Segun Aroyehun",
      "institution": "African University of Science and Technology, University of Konstanz",
      "link": "https://arxiv.org/pdf/2512.20780",
      "code": null,
      "tags": [
        "educational technology / intelligent tutoring systems",
        "large language models",
        "pedagogical quality",
        "instructional strategies",
        "linguistic analysis",
        "math tutoring"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b4c157f4475efaa64bb039e61eccf65a1facd8888dde9576734865854a42e878_w640_q70.webp",
      "contributions": "1. Conducted a controlled, turn-level comparison of tutoring responses between expert human tutors, novice human tutors, and multiple large language models (LLMs) in math remediation. 2. Identified systematic differences in instructional and linguistic profiles, finding that LLMs underuse restating/revoicing strategies but produce longer, more lexically diverse, and more polite responses compared to human tutors. 3. Established statistical associations between specific instructional/linguistic features (e.g., restating, lexical diversity) and perceived pedagogical quality, showing LLMs can achieve comparable quality using different strategies.",
      "summary": "This paper investigates how closely the instructional behavior of large language models (LLMs) aligns with expert human tutors in math tutoring. By comparing responses from experts, novices, and LLMs to the same conversation turns, the study analyzes instructional strategies and linguistic features. It finds that LLMs approach expert-level pedagogical quality on average but rely on systematically different strategies, such as underusing restating/revoicing while being more verbose and polite.",
      "mindmap": "graph LR\n    A[Large Language Models Approach Expert Pedagogical Quality in Math Tutoring but Differ in Instructional and Linguistic Profiles] --> B(核心问题/Problem: LLM教学行为与人类专家的一致性/Alignment of LLM instructional behavior with expert human tutors)\n    A --> C(主要方法/Method: 控制性对话轮比较/Controlled turn-level comparison of expert, novice, and LLM responses)\n    A --> D(关键结果/Results: LLM接近专家教学水平但策略不同/LLMs approach expert quality but use different instructional & linguistic strategies)"
    },
    {
      "title": "Making AI Work: An Autoethnography of a Workaround in Higher Education",
      "authors": "Shang Chieh Lee, Bhuva Narayan, Simon Buckingham Shum, Stella Ng, A. Baki Kocaballi",
      "institution": "University of Technology Sydney",
      "link": "https://arxiv.org/pdf/2512.21055",
      "code": null,
      "tags": [
        "Information Systems",
        "Autoethnography",
        "Invisible Labour",
        "Workaround",
        "Sociotechnical Systems",
        "User Innovation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c38d81a4b275e82620a51814d2cbc6d349963475ab4224ec58e70714de1ab37_w640_q70.webp",
      "contributions": "1. Provides an insider, autoethnographic account of the sociotechnical friction and \"invisible labour\" required to make enterprise GenAI functional in higher education. 2. Applies and extends Alter's theory of workarounds to interpret user-driven adaptations as integral acts of sociotechnical integration, not mere deviations. 3. Highlights the central paradox of GenAI workarounds: they enable functionality but can create unofficial \"shadow\" systems and obscure the crucial, politically charged labour involved.",
      "summary": "This study uses analytic autoethnography to examine a workaround developed when an institutional goal of empowering staff with GenAI clashed with technical and political constraints. It argues such workarounds are essential acts of sociotechnical integration that reveal the \"invisible labour\" needed to make AI functional, but this labour is often obscured, creating a paradox. The findings position this invisible labour as a core, rather than peripheral, component of practical GenAI implementation.",
      "mindmap": "graph LR\n        A[Making AI Work: An Autoethnography of a Workaround in Higher Education] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[GenAI实施中的社会技术摩擦与隐性劳动/Sociotechnical Friction & Invisible Labour in GenAI Implementation]\n        C --> C1[分析性自我民族志与工作区理论/Analytic Autoethnography & Workaround Theory]\n        D --> D1[工作区是核心的社会技术整合行为/Workarounds as Integral Sociotechnical Integration]\n        D --> D2[揭示了GenAI的整合悖论/Reveals GenAI Integration Paradox]"
    },
    {
      "title": "Beyond Context: Large Language Models Failure to Grasp Users Intent",
      "authors": "Ahmed M. Hussain, Salahuddin Salahuddin, Panos Papadimitratos",
      "institution": "KTH Royal Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.21110",
      "code": null,
      "tags": [
        "ai safety",
        "intent recognition",
        "contextual understanding",
        "safety circumvention",
        "prompt engineering",
        "transformer architectures"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/55c1a596dd6375317c809bb19f466455285faf18a1f9810649d755b8027e383c_w640_q70.webp",
      "contributions": "1. Identifies and empirically demonstrates a critical vulnerability in LLMs: their inability to understand user intent and context, which allows safety mechanisms to be circumvented. 2. Evaluates multiple state-of-the-art LLMs (ChatGPT, Claude, Gemini, DeepSeek) and shows that exploitation techniques like emotional framing and progressive revelation are effective, and that reasoning capabilities can amplify this risk. 3. Proposes a paradigmatic shift in AI safety design, arguing for contextual understanding and intent recognition to be core capabilities rather than post-hoc protective mechanisms.",
      "summary": "This paper identifies a fundamental vulnerability in Large Language Models (LLMs): their lack of contextual understanding and intent recognition, which allows safety mechanisms to be systematically bypassed. The authors empirically evaluate several LLMs, showing they can be exploited through techniques like emotional framing, and find that reasoning capabilities often worsen the problem. They conclude that a paradigm shift is needed to build intent recognition directly into LLM architectures for safety.",
      "mindmap": "graph LR\n    A[Beyond Context: Large Language Models Failure to Grasp Users Intent] --> B[核心问题/Problem: LLMs缺乏上下文和意图理解能力/LLMs lack contextual understanding & intent recognition]\n    A --> C[主要方法/Method: 对多种LLM进行经验性评估/Empirical evaluation of multiple LLMs]\n    A --> D[关键结果/Results: 安全机制可被系统规避，需范式转变/Safety mechanisms can be systematically circumvented, requiring a paradigm shift]\n    B --> E[导致可利用的漏洞/Creates exploitable vulnerabilities]\n    C --> F[使用情感框架、渐进揭示等技术/Using emotional framing, progressive revelation, etc.]\n    D --> G[Claude Opus 4.1部分例外，推理能力加剧风险/Claude Opus 4.1 partial exception, reasoning amplifies risk]"
    },
    {
      "title": "Microtopia: Exploring the Impact of Interdisciplinary Projects on Ethnic Minority Female Pupils' Perceptions of Computer Science",
      "authors": "Nadine Aburumman, Ju-Ling Shih, Cigdem Sengul, Monica Pereira",
      "institution": "[Inferred from authors: Nadine Aburumman, Ju-Ling Shih, Cigdem Sengul, Monica Pereira. No explicit affiliations in provided text. Institution cannot be reliably inferred.]",
      "link": "https://arxiv.org/pdf/2512.21214",
      "code": null,
      "tags": [
        "computer science education",
        "diversity and inclusion",
        "interdisciplinary learning",
        "design thinking",
        "sustainable development goals",
        "AI/IoT/Robotics",
        "problem-based learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/05065a9796e995884552033fb1ebac6397879e6ec0b2560e03b8ac9fb067c627_w640_q70.webp",
      "contributions": "1. Proposes the Microtopia programme, an interdisciplinary CS initiative integrating AI, IoT, and Robotics with design thinking and collaborative project work. 2. Demonstrates that linking CS content to real-world sustainability challenges and global issues significantly enhances engagement and perceived relevance among ethnic minority female pupils. 3. Identifies through statistical analysis that socioeconomic and ethnocultural factors (e.g., SES, perception of field as male-dominated) are underlying factors shaping pupils' perceptions of CS.",
      "summary": "This paper introduces Microtopia, an interdisciplinary programme that combines coding, AI, IoT, and Robotics with design thinking and sustainability themes to broaden participation in computer science among ethnic minority girls. The study, using pre- and post-questionnaires, found that participation significantly increased students' confidence, enjoyment, and motivation, especially when computing was presented as relevant to solving global challenges.",
      "mindmap": "graph LR\n    A[Microtopia: Exploring the Impact of Interdisciplinary Projects on Ethnic Minority Female Pupils' Perceptions of Computer Science] --> B(核心问题/Problem: Broadening participation of ethnic minority girls in CS)\n    A --> C(主要方法/Method: Interdisciplinary programme with AI/IoT/Robotics, design thinking, SDGs, collaborative projects)\n    A --> D(关键结果/Results: Increased confidence, enjoyment, motivation; CS linked to sustainability enhances engagement)"
    },
    {
      "title": "\"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme",
      "authors": "Anton Alyakin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19700",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f7dd2926116b7f555fd465c856929048f8fcdbf9a408910ee765667d89ab6c77_w640_q70.webp",
      "contributions": "",
      "summary": "\"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme",
      "mindmap": ""
    },
    {
      "title": "CS-Guide: Leveraging LLMs and Student Reflections to Provide Frequent, Scalable Academic Monitoring Feedback to Computer Science Students",
      "authors": "Samuel Jacob Chacko, An-I Andy Wang, Lara Perez-Felkner, Sonia Haiduc, David Whalley, Xiuwen Liu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19866",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/400e33eafa2cbde4debfc18c1a5e5f0a16f436d1494edd99b4338169da7879de_w640_q70.webp",
      "contributions": "",
      "summary": "CS-Guide: Leveraging LLMs and Student Reflections to Provide Frequent, Scalable Academic Monitoring Feedback to Computer Science Students",
      "mindmap": ""
    },
    {
      "title": "Counterfactual LLM-based Framework for Measuring Rhetorical Style",
      "authors": "Jingyi Qiu, Hong Chen, Zongyi Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19908",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6db39310497bbc29a6efe3c72529c4c231f4c45f6ccb6856571045197a2f074d_w640_q70.webp",
      "contributions": "",
      "summary": "Counterfactual LLM-based Framework for Measuring Rhetorical Style",
      "mindmap": ""
    },
    {
      "title": "Prediction Air Temperature in Geothermal Heat Exchangers Using Pseudorandom Numbers: The New DARL Model",
      "authors": "C. Ramírez-Dolores, J.C. Zamora-Luria, J.A. Altamirano-Acosta, L. Sarao-Cruz, P. Jiménez-Palma, J. Moreno-Falconi",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19976",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/441f7620f3757f4471a93018d5814fc4d03f1187c1a80f7194c4047e3eeea5d0_w640_q70.webp",
      "contributions": "",
      "summary": "Prediction Air Temperature in Geothermal Heat Exchangers Using Pseudorandom Numbers: The New DARL Model",
      "mindmap": ""
    },
    {
      "title": "S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test",
      "authors": "Zhe Sun, Xueyuan Yang, Yujie Lu, Zhenliang Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19992",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a7f6951c4ebc6cf6c8bd633198ae7d4c6a7c8f1e0cd348aa9e6737966dfe600_w640_q70.webp",
      "contributions": "",
      "summary": "S$^3$IT: A Benchmark for Spatially Situated Social Intelligence Test",
      "mindmap": ""
    },
    {
      "title": "Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives",
      "authors": "Karolina Drożdż, Kacper Dudzic, Anna Sterna, Marcin Moskalewicz",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20298",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6e46ae09622d9142a3896f2528685617edcbaae96bc105754e16929ed630e3f0_w640_q70.webp",
      "contributions": "",
      "summary": "Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives",
      "mindmap": ""
    },
    {
      "title": "Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information",
      "authors": "İbrahim Oğuz Çetinkaya, Sajad Khodadadian, Taylan G. Topçu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20589",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5bd8f6dd1aa27848e72f81ba7279a1abe238ea198e2b3aa7513fc9ca373e7554_w640_q70.webp",
      "contributions": "",
      "summary": "Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information",
      "mindmap": ""
    },
    {
      "title": "CoPE: A Small Language Model for Steerable and Scalable Content Labeling",
      "authors": "Samidh Chakrabarti, David Willner, Kevin Klyman, Tiffany Saade, Emily Capstick, Sabina Nong",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18027",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/423158010714f415c807a9ae93864ccaf58e7d33b462039083dce106e5d195f2_w640_q70.webp",
      "contributions": "",
      "summary": "CoPE: A Small Language Model for Steerable and Scalable Content Labeling",
      "mindmap": ""
    },
    {
      "title": "Securing Agentic AI Systems -- A Multilayer Security Framework",
      "authors": "Sunil Arora, John Hastings",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18043",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/96ca042b8b2e4295fa813434a544ac9adb3df55cbea55e3fe9b24f2dcbee4733_w640_q70.webp",
      "contributions": "",
      "summary": "Securing Agentic AI Systems -- A Multilayer Security Framework",
      "mindmap": ""
    },
    {
      "title": "Statistical laws and linguistics inform meaning in naturalistic and fictional conversation",
      "authors": "Ashley M. A. Fehr, Calla G. Beauregard, Julia Witte Zimmerman, Katie Ekström, Pablo Rosillo-Rodes, Christopher M. Danforth, Peter Sheridan Dodds",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18072",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0119bf39897b4bb6f848dd8818e4a75f43e327cbc8223e8cff6bafd0a8277d08_w640_q70.webp",
      "contributions": "",
      "summary": "Statistical laws and linguistics inform meaning in naturalistic and fictional conversation",
      "mindmap": ""
    },
    {
      "title": "Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation",
      "authors": "Lena Libon, Meghana Bhange, Rushabh Solanki, Elliot Creager, Ulrich Aïvodji",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18174",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fd7baf972d08bf5c53e0a32c68fda68879e3c2febf3407ecf6537a3fcd6d36fd_w640_q70.webp",
      "contributions": "",
      "summary": "Conscious Data Contribution via Community-Driven Chain-of-Thought Distillation",
      "mindmap": ""
    },
    {
      "title": "Measuring Fine-Grained Negotiation Tactics of Humans and LLMs in Diplomacy",
      "authors": "Wenkai Li, Lynnette Hui Xian Ng, Andy Liu, Daniel Fried",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18292",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3bca157c21b274184903d6db27d719924ad61a7ebd545421b88ee4959f34a8c8_w640_q70.webp",
      "contributions": "",
      "summary": "Measuring Fine-Grained Negotiation Tactics of Humans and LLMs in Diplomacy",
      "mindmap": ""
    },
    {
      "title": "Color, Sentiment, and Structure: A Comparative Study of Instagram Marketing Across Economies",
      "authors": "Ritesh Konka, Pranali Kurani",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18310",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e65cc34f685b4b7bf660511cdf826180cf02d70ba203ea70dda36cd297ca2194_w640_q70.webp",
      "contributions": "",
      "summary": "Color, Sentiment, and Structure: A Comparative Study of Instagram Marketing Across Economies",
      "mindmap": ""
    },
    {
      "title": "Adaptive Learning Mechanisms for Learning Management Systems: A Scoping Review and Practical Considerations",
      "authors": "Sebastian Kucharski, Iris Braun, Gregor Damnik, Matthias Wählisch",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18383",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d2e9d57258150307a25d518af179c53c0ca673829d832d1ec68a69597699251d_w640_q70.webp",
      "contributions": "",
      "summary": "Adaptive Learning Mechanisms for Learning Management Systems: A Scoping Review and Practical Considerations",
      "mindmap": ""
    },
    {
      "title": "A Formal Descriptive Language for Learning Dynamics: A Five-Layer Structural Coordinate System",
      "authors": "Miyuki T. Nakata",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18525",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c5fa66d8eb7deac60effe912ce57cc5b3a8decb9a70cf6acda4def17328ab6c_w640_q70.webp",
      "contributions": "",
      "summary": "A Formal Descriptive Language for Learning Dynamics: A Five-Layer Structural Coordinate System",
      "mindmap": ""
    },
    {
      "title": "The MEVIR 2 Framework: A Virtue-Informed Moral-Epistemic Model of Human Trust Decisions",
      "authors": "Daniel Schwabe",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18539",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de4aed802558b93b84676a34a5c94c1e36401bacbc797e08c3d54cd9b553df8c_w640_q70.webp",
      "contributions": "",
      "summary": "The MEVIR 2 Framework: A Virtue-Informed Moral-Epistemic Model of Human Trust Decisions",
      "mindmap": ""
    },
    {
      "title": "Proof of Authenticity of General IoT Information with Tamper-Evident Sensors and Blockchain",
      "authors": "Kenji Saito",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18560",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/32ec910f91d8d1488c0f9d63f1d11998ee65c7eeda5092f2969d5ed6f17bd9c4_w640_q70.webp",
      "contributions": "",
      "summary": "Proof of Authenticity of General IoT Information with Tamper-Evident Sensors and Blockchain",
      "mindmap": ""
    },
    {
      "title": "Measuring the Impact of Student Gaming Behaviors on Learner Modeling",
      "authors": "Qinyi Liu, Lin Li, Valdemar Švábenský, Conrad Borchers, Mohammad Khalil",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18659",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/64d34ddb491ed807e5ee9ca576fcefea5ef3bafa335bbc00244fb21f30908497_w640_q70.webp",
      "contributions": "",
      "summary": "Measuring the Impact of Student Gaming Behaviors on Learner Modeling",
      "mindmap": ""
    },
    {
      "title": "\"Even GPT Can Reject Me\": Conceptualizing Abrupt Refusal Secondary Harm (ARSH) and Reimagining Psychological AI Safety with Compassionate Completion Standard (CCS)",
      "authors": "Yang Ni, Tong Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18776",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4c436bec45a43d86e53531d2849399bd87e6bf3d4305bd5fa2744e56f7b83352_w640_q70.webp",
      "contributions": "",
      "summary": "\"Even GPT Can Reject Me\": Conceptualizing Abrupt Refusal Secondary Harm (ARSH) and Reimagining Psychological AI Safety with Compassionate Completion Standard (CCS)",
      "mindmap": ""
    },
    {
      "title": "Quantifying the Lifelong Impact of Resilience Interventions via Agent-Based LLM Simulation",
      "authors": "Vivienne L'Ecuyer Ming",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18803",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bbb9912dc64ced0a3542815309f1cfd619aff6ad7759e8db7f3222089c75b40e_w640_q70.webp",
      "contributions": "",
      "summary": "Quantifying the Lifelong Impact of Resilience Interventions via Agent-Based LLM Simulation",
      "mindmap": ""
    },
    {
      "title": "Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,392 Adult Brazilian Workers",
      "authors": "Bruno Campello de Souza",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18871",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/446f3fb717e9b018a154458859c845d2583ea717613eb2a7397f53ffedb8700f_w640_q70.webp",
      "contributions": "",
      "summary": "Psychometric Validation of the Sophotechnic Mediation Scale and a New Understanding of the Development of GenAI Mastery: Lessons from 3,392 Adult Brazilian Workers",
      "mindmap": ""
    },
    {
      "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
      "authors": "Ming Li, Han Chen, Yunze Xiao, Jian Chen, Hong Jiao, Tianyi Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18880",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69c02cdd0b38302d7f949dfe357cef926fc143c19edf1038c18dd4c5b1573b09_w640_q70.webp",
      "contributions": "",
      "summary": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
      "mindmap": ""
    },
    {
      "title": "Configuration Work: Four Consequences of LLMs-in-use",
      "authors": "Gabriel Alcaras, Donato Ricci",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19189",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a83a2317a41e15570e4b6245ad55aba15267903a198b04d5c65b2ef3c623e155_w640_q70.webp",
      "contributions": "",
      "summary": "Configuration Work: Four Consequences of LLMs-in-use",
      "mindmap": ""
    },
    {
      "title": "Epistemological Fault Lines Between Human and Artificial Intelligence",
      "authors": "Walter Quattrociocchi, Valerio Capraro, Matjaž Perc",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19466",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e38aa1bf279d77f222964e2fa6eaf6b1a85cc9955ae786124894e9ed3fb93c1_w640_q70.webp",
      "contributions": "",
      "summary": "Epistemological Fault Lines Between Human and Artificial Intelligence",
      "mindmap": ""
    },
    {
      "title": "Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico",
      "authors": "Martí Medina-Hern ández, Janos Kertész, Mihály Fazekas",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19491",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4c4c038ee2a9e249f2f33259b933475235132c1cbefe5a1ac37223ea8fc2367_w640_q70.webp",
      "contributions": "",
      "summary": "Learning from sanctioned government suppliers: A machine learning and network science approach to detecting fraud and corruption in Mexico",
      "mindmap": ""
    },
    {
      "title": "Detecting Coordinated Activities Through Temporal, Multiplex, and Collaborative Analysis",
      "authors": "Letizia Iannucci, Elisa Muratore, Antonis Matakos, Mikko Kivelä",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19677",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1131679bb8ebe54426d2398fe26bd82130fdf3a7e9489d5343133522014524c0_w640_q70.webp",
      "contributions": "",
      "summary": "Detecting Coordinated Activities Through Temporal, Multiplex, and Collaborative Analysis",
      "mindmap": ""
    },
    {
      "title": "The Role of Islamic Ethics in Preventing the Abuse of Artificial Intelligence (AI) Based Deepfakes",
      "authors": "Wisnu Uriawan, Imany Fauzy Rahman, Muhamad Zidan, Irma Rohmatillah, Muhammad Arkan Raihan, Irma Dwiyanti",
      "institution": "UIN Sunan Gunung Djati Bandung",
      "link": "https://arxiv.org/pdf/2512.17218",
      "code": null,
      "tags": [
        "ethics and society",
        "Systematic Literature Review (SLR)",
        "PRISMA",
        "Maqasid al-Shariah",
        "hifz al-ird",
        "hifz al-nafs",
        "adl",
        "tabayyun"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This study employs a Systematic Literature Review (SLISMA) to formulate an Islamic ethical framework for preventing deepfake abuse. It concludes that principles from Maqasid al-Shariah, such as protecting honor and self, provide a normative basis for shifting from punitive to preventative approaches, focusing on human dignity and the common good in the digital age.",
      "mindmap": ""
    },
    {
      "title": "Privacy-Preserving Synthetic Dataset of Individual Daily Trajectories for City-Scale Mobility Analytics",
      "authors": "Jun'ichi Ozaki, Ryosuke Susuta, Takuhiro Moriyama, Yohei Shida",
      "institution": "Not explicitly provided; cannot infer from given information.",
      "link": "https://arxiv.org/pdf/2512.17239",
      "code": null,
      "tags": [
        "privacy-preserving data synthesis",
        "multi-objective optimization",
        "origin-destination matrices",
        "dwell-travel time quantiles",
        "universal law of daily visited locations",
        "synthetic trajectory generation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a method to generate a privacy-preserving synthetic dataset of individual daily trajectories by integrating aggregated origin-destination flows with behavioral constraints in a multi-objective optimization framework. The method successfully reproduces realistic human mobility patterns in two Japanese regions, providing a practical pathway for high-resolution mobility analytics without using sensitive personal data.",
      "mindmap": ""
    },
    {
      "title": "Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?",
      "authors": "Zabir Al Nazi, G M Shahariar, Abrar Hossain, Wei Peng",
      "institution": "University of California, Riverside, University of Dhaka, Stanford University",
      "link": "https://arxiv.org/pdf/2512.17394",
      "code": null,
      "tags": [
        "vision-language models",
        "CulturalToM-VQA",
        "visual question answering",
        "chain-of-thought prompting",
        "compositional chain-of-thought prompting",
        "false belief reasoning",
        "social desirability bias"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces CulturalToM-VQA, a benchmark dataset built via a VLM-assisted human-in-the-loop pipeline to evaluate cross-cultural Theory of Mind reasoning in Vision-Language Models. It finds that while newer VLMs show strong performance on explicit tasks, they systematically struggle with false belief reasoning, and their results may be inflated by social desirability bias rather than genuine visual understanding.",
      "mindmap": ""
    },
    {
      "title": "Fair Voting Methods as a Catalyst for Democratic Resilience: A Trilogy on Legitimacy, Impact and AI Safeguarding",
      "authors": "Evangelos Pournaras",
      "institution": "University of Leeds",
      "link": "https://arxiv.org/pdf/2512.17461",
      "code": null,
      "tags": [
        "democratic systems",
        "fair voting methods",
        "cumulative voting",
        "equal shares",
        "proportional representation",
        "participatory budgeting",
        "AI voting assistance"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes that combining expressive ballot formats like cumulative voting with proportional aggregation methods like equal shares constitutes a \"fair voting method.\" It concludes that such methods enhance democratic legitimacy, accelerate impactful outcomes in areas like welfare and education, and serve as a safeguard against biases in emerging AI-assisted voting scenarios.",
      "mindmap": ""
    },
    {
      "title": "Integrating Computational Methods and AI into Qualitative Studies of Aging and Later Life",
      "authors": "Corey M. Abramson",
      "institution": "Rice University, UC San Francisco",
      "link": "https://arxiv.org/pdf/2512.17850",
      "code": null,
      "tags": [
        "computational social science",
        "computational text analysis",
        "machine learning (ML)",
        "natural language processing (NLP)",
        "ethnography",
        "in-depth interviews",
        "mixed-methods"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper demonstrates how computational social science tools like machine learning and natural language processing can be integrated with traditional qualitative methods (e.g., ethnography, interviews) to study aging. It concludes that these computational methods can broaden qualitative research by streamlining workflows, scaling up projects, and enabling new multi-method insights, rather than replacing its foundational approaches.",
      "mindmap": ""
    },
    {
      "title": "Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease",
      "authors": "Carter H. Nakamoto, Lucia Lushi Chen, Agata Foryciarz, Sherri Rose",
      "institution": "Stanford University",
      "link": "https://arxiv.org/pdf/2512.17340",
      "code": null,
      "tags": [
        "fair machine learning",
        "penalized regression",
        "cost-sensitive classification",
        "true positive rate disparity penalties"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a penalized fair regression framework using unfairness penalties for multiple groups, implemented via reduction to cost-sensitive classification. The method is applied to predict end-stage renal disease in a chronic kidney disease study, showing substantial fairness improvements for multiple race and ethnicity groups without appreciable loss in overall model fit.",
      "mindmap": ""
    },
    {
      "title": "Value Lens: Using Large Language Models to Understand Human Values",
      "authors": "Eduardo de la Cruz Fernández, Marcelo Karanik, Sascha Ossowski",
      "institution": "Universidad Politécnica de Madrid, Universidad Rey Juan Carlos",
      "link": "https://arxiv.org/pdf/2512.15722",
      "code": null,
      "tags": [
        "llm inference",
        "large language models",
        "value detection",
        "generative AI",
        "dual-LLM approach",
        "expert verification"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes Value Lens, a two-stage model that uses Large Language Models (LLMs) to detect human values in text. The first stage uses an LLM to conceptualize a value theory verified by experts, and the second stage employs a dual-LLM approach for detection and critical review. The results show that Value Lens performs comparably to or better than other models in similar tasks.",
      "mindmap": ""
    },
    {
      "title": "D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models",
      "authors": "Javon Hickmon",
      "institution": "University of Washington",
      "link": "https://arxiv.org/pdf/2512.15747",
      "code": null,
      "tags": [
        "multi-modal inference",
        "CLIP",
        "Stable Diffusion XL",
        "zero-shot classification",
        "demographic bias mitigation",
        "data generation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes D3G, a training-free method that uses Stable Diffusion XL to generate diverse demographic data at inference time to improve zero-shot image classification with CLIP. The method is shown to boost classification accuracy while reducing harmful demographic bias in pre-trained multimodal models.",
      "mindmap": ""
    },
    {
      "title": "Cultural Rights and the Rights to Development in the Age of AI: Implications for Global Human Rights Governance",
      "authors": "Alexander Kriebitz, Caitlin Corrigan, Aive Pevkur, Alberto Santos Ferro, Amanda Horzyk, Dirk Brand, Dohee Kim, Dodzi Koku Hattoh, Flavia Massucci, Gilles Fayad, Kamil Strzepek, Laud Ammah, Lavina Ramkissoon, Mariette Awad, Natalia Amasiadi, Nathan C. Walker, Nicole Manger, Sophia Devlin",
      "institution": "Ludwig Maximilian University of Munich, Technical University of Munich, Tallinn University of Technology, University of Edinburgh, Stellenbosch University, Changwon National University, University of Ghana, Cardinal Stefan Wyszyński University in Warsaw, African Union, American University of Beirut, University of Patras, Rutgers University, Ulster University",
      "link": "https://arxiv.org/pdf/2512.15786",
      "code": null,
      "tags": [
        "AI ethics and governance",
        "cultural rights",
        "right to development",
        "algorithmic design",
        "AI governance",
        "human rights law"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper conceptually analyzes the impact of AI on cultural rights and the right to development, examining the epistemic and normative limitations in algorithmic design. It concludes that AI governance frameworks have significant gaps in protecting these rights and risks exacerbating global inequities. The study calls for integrating cultural and developmental considerations into future AI policy and research.",
      "mindmap": ""
    },
    {
      "title": "Toward Agentic Environments: GenAI and the Convergence of AI, Sustainability, and Human-Centric Spaces",
      "authors": "Przemek Pospieszny, Dominika P. Brodowicz",
      "institution": "EPAM Systems, Warsaw School of Economics",
      "link": "https://arxiv.org/pdf/2512.15787",
      "code": null,
      "tags": [
        "others",
        "generative AI",
        "multi-agent systems",
        "edge computing",
        "sustainability",
        "ambient intelligence",
        "AI agents",
        "sustainable ecosystems"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes the concept of \"agentic environments,\" a framework leveraging generative AI, multi-agent systems, and edge computing to create sustainable, human-centric spaces. It concludes that this approach can reduce the environmental impact of AI by optimizing resource use and enhancing data privacy through decentralized, edge-driven deployment models.",
      "mindmap": ""
    },
    {
      "title": "A Systematic Analysis of Biases in Large Language Models",
      "authors": "Xulang Zhang, Rui Mao, Erik Cambria",
      "institution": "Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.15792",
      "code": null,
      "tags": [
        "fairness and bias analysis",
        "news summarization",
        "stance classification",
        "UN voting patterns",
        "multilingual story completion",
        "World Values Survey"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper systematically analyzes biases in large language models (LLMs) across political, ideological, alliance, language, and gender dimensions using experiments like news summarization and stance classification. The main conclusion is that despite being aligned for neutrality, the studied LLMs still exhibit various types of biases and affinities.",
      "mindmap": ""
    },
    {
      "title": "Evaluation of AI Ethics Tools in Language Models: A Developers' Perspective Case Stud",
      "authors": "Jhessica Silva, Diego A. B. Moreira, Gabriel O. dos Santos, Alef Ferreira, Helena Maia, Sandra Avila, Helio Pedrini",
      "institution": "Universidade Estadual de Campinas (UNICAMP), Universidade Federal de Goiás (UFG)",
      "link": "https://arxiv.org/pdf/2512.15791",
      "code": null,
      "tags": [
        "ai ethics evaluation",
        "Model Cards",
        "ALTAI",
        "FactSheets",
        "Harms Modeling",
        "literature survey",
        "interviews"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper presents a methodology to evaluate AI Ethics Tools (AIETs) for language models by selecting four tools (Model Cards, ALTAI, FactSheets, Harms Modeling) and applying them to Portuguese language models, with developer interviews. The results indicate that these tools help guide general ethical considerations but fail to address language-specific aspects like idiomatic expressions or identify negative impacts for Portuguese.",
      "mindmap": ""
    },
    {
      "title": "Explainable Ethical Assessment on Human Behaviors by Generating Conflicting Social Norms",
      "authors": "Yuxi Sun, Wei Gao, Hongzhan Lin, Jing Ma, Wenxuan Zhang",
      "institution": "Hong Kong Baptist University, Singapore Management University, Singapore University of Technology and Design",
      "link": "https://arxiv.org/pdf/2512.15793",
      "code": null,
      "tags": [
        "ethical ai",
        "contrastive learning",
        "social norms generation",
        "moral reasoning",
        "explainable ai",
        "valence prediction"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces ClarityEthic, a method that enhances ethical assessment of human actions by generating conflicting social norms to explain and predict valence (support/oppose). It uses a contrastive learning strategy to strengthen the moral reasoning of language models. Experiments show the method outperforms baselines and human evaluations confirm the generated norms provide plausible explanations.",
      "mindmap": ""
    },
    {
      "title": "Cybercrime and Computer Forensics in Epoch of Artificial Intelligence in India",
      "authors": "Sahibpreet Singh, Shikha Dhiman",
      "institution": "Guru Nanak Dev University, Amritsar",
      "link": "https://arxiv.org/pdf/2512.15799",
      "code": null,
      "tags": [
        "cybersecurity",
        "computer forensics",
        "explainable AI (XAI)",
        "data minimization",
        "algorithmic bias",
        "deepfakes",
        "adversarial AI",
        "Digital Personal Data Protection Act"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper employs a doctrinal legal methodology to analyze the integration of AI into cybercrime and forensics in India, focusing on the Digital Personal Data Protection Act, 2023. It concludes that there is a critical tension between privacy principles and forensic needs, and proposes a human-centric forensic model using explainable AI (XAI) to ensure evidence admissibility while advocating for legislative synchronization with international standards.",
      "mindmap": ""
    },
    {
      "title": "DSO: Direct Steering Optimization for Bias Mitigation",
      "authors": "Lucas Monteiro Paes, Nivedha Sivakumar, Yinong Oliver Wang, Masha Fedzechkina Donaldson, Luca Zappella, Nicholas Apostoloff",
      "institution": "Apple, Carnegie Mellon University",
      "link": "https://arxiv.org/pdf/2512.15926",
      "code": null,
      "tags": [
        "fairness and bias mitigation",
        "activation steering",
        "reinforcement learning",
        "linear transformations",
        "inference-time control"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes Direct Steering Optimization (DSO), a method using reinforcement learning to find linear transformations for steering activations in generative models to mitigate bias while maintaining performance. It demonstrates state-of-the-art trade-offs between fairness and capabilities in VLMs and LLMs, offering inference-time control over bias reduction. The work highlights the advantage of directly optimized steering strategies over heuristic-based approaches for effective bias intervention.",
      "mindmap": ""
    },
    {
      "title": "Cross-Language Bias Examination in Large Language Models",
      "authors": "Yuxuan Liang, Marwa Mahmoud",
      "institution": "Georgia Institute of Technology, University of Glasgow",
      "link": "https://arxiv.org/pdf/2512.16029",
      "code": null,
      "tags": [
        "fairness and bias evaluation",
        "multilingual bias evaluation",
        "BBQ benchmark",
        "prompt-based Implicit Association Test",
        "explicit bias",
        "implicit bias"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces a multilingual bias evaluation framework that combines explicit bias assessment using the BBQ benchmark with implicit bias measurement via a prompt-based Implicit Association Test, applied across five languages. The results show significant variation in bias across languages, with Arabic and Spanish exhibiting higher stereotype bias, and reveal contrasting patterns between explicit and implicit bias, such as age having low explicit but high implicit bias. The study highlights the importance of cross-lingual bias analysis for developing equitable multilingual LLMs.",
      "mindmap": ""
    },
    {
      "title": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams",
      "authors": "Gilad Gressel, Rahul Pankajakshan, Shir Rozenfeld, Ling Li, Ivan Franceschini, Krishnahsree Achuthan, Yisroel Mirsky",
      "institution": "Amrita Vishwa Vidyapeetham, Ca’ Foscari University of Venice, University of Melbourne, Ben Gurion University of the Negev",
      "link": "https://arxiv.org/pdf/2512.16280",
      "code": null,
      "tags": [
        "llm inference",
        "large language models",
        "safety filters",
        "conversation study",
        "social engineering",
        "automation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper investigates the role of LLMs in romance-baiting scams through interviews with insiders and victims, a blinded long-term conversation study comparing LLM agents to human operators, and an evaluation of commercial safety filters. It finds that LLMs are already widely used in scams, can elicit greater trust and compliance than humans, and that current safety filters are ineffective at detecting such dialogues, suggesting scams are ripe for full LLM automation.",
      "mindmap": ""
    },
    {
      "title": "Comprehensive AI Literacy: The Case for Centering Human Agency",
      "authors": "Sri Yash Tadimalla, Justin Cary, Gordon Hull, Jordan Register, Daniel Maxwell, David Pugalee, Tina Heafner",
      "institution": "UNC Charlotte",
      "link": "https://arxiv.org/pdf/2512.16656",
      "code": null,
      "tags": [
        "education",
        "AI literacy",
        "human agency",
        "critical thinking",
        "epistemology",
        "educational frameworks"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This position paper proposes a shift towards comprehensive AI literacy frameworks that center human agency and critical thinking in education. It concludes that educators and students must be empowered to make intentional, critical choices about AI use, rather than focusing solely on operational skills.",
      "mindmap": ""
    },
    {
      "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs",
      "authors": "Shubham Mishra, Samyek Jain, Gorang Mehrishi, Shiv Tiwari, Harsh Sharma, Pratik Narang, Dhruv Kumar",
      "institution": "Birla Institute of Technology and Science, Pilani, Carnegie Mellon University",
      "link": "https://arxiv.org/pdf/2512.16795",
      "code": null,
      "tags": [
        "llm inference",
        "retrieval-augmented generation",
        "deductive reasoning",
        "conflict-aware trust-score",
        "reasoning-trace-augmented framework",
        "supervised fine-tuning"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a reasoning-trace-augmented RAG framework that integrates a three-stage deductive reasoning process (document adjudication, conflict analysis, and grounded synthesis) to handle conflicting or unreliable retrieved evidence. It introduces a Conflict-Aware Trust-Score (CATS) evaluation pipeline. The method, tested with models like Qwen, shows substantial improvements in answer correctness and behavioral adherence over baseline RAG systems.",
      "mindmap": ""
    },
    {
      "title": "Impacts of Racial Bias in Historical Training Data for News AI",
      "authors": "Rahul Bhargava, Malene Hornstrup Jespersen, Emily Boardman Ndulue, Vivica Dsouza",
      "institution": "Northeastern University, University of Copenhagen, Media Ecosystems Analysis Group",
      "link": "https://arxiv.org/pdf/2512.16901",
      "code": null,
      "tags": [
        "algorithmic auditing",
        "multi-label classifier",
        "explainable AI",
        "word2vec",
        "New York Times Annotated Corpus"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper investigates racial bias in a multi-label text classifier trained on the New York Times Annotated Corpus using word2vec and explainable AI methods. It finds that a problematic \"blacks\" label acts as a general \"racism detector\" but fails on modern examples, demonstrating how historical training data embeds biases into AI models. The study highlights the tension for newsrooms in adopting AI tools while mitigating the reproduction of historical stereotypes in news coverage.",
      "mindmap": ""
    },
    {
      "title": "SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI",
      "authors": "Ryan Cartularo",
      "institution": "The University of Texas at Austin",
      "link": "https://arxiv.org/pdf/2512.14712",
      "code": null,
      "tags": [
        "multi-modal training",
        "Mixture-of-Experts (MoE)",
        "Hierarchical Gated Attention Network",
        "CatBoost meta-learner",
        "multimodal fusion",
        "deep fusion",
        "expert stacking",
        "Quad-Modal Ensemble"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper compares two multimodal AI architectures for sepsis prediction and antibiotic selection: a complex end-to-end deep fusion model (SepsisFusionFormer) and a leaner, context-aware Mixture-of-Experts stacking model (SepsisLateFusion). The main conclusion is that for this high-stakes, data-sparse clinical domain, the interpretable expert stacking approach, which treats modalities as orthogonal experts and uses a CatBoost meta-learner, significantly outperformed the deep fusion model, achieving state-of-the-art predictive performance and enabling a prescriptive window for intervention.",
      "mindmap": ""
    },
    {
      "title": "AgroAskAI: A Multi-Agentic AI Framework for Supporting Smallholder Farmers' Enquiries Globally",
      "authors": "Nadine Angela Cantonjos, Arpita Biswas",
      "institution": "Rutgers University",
      "link": "https://arxiv.org/pdf/2512.14910",
      "code": null,
      "tags": [
        "others",
        "multi-agent reasoning",
        "chain-of-responsibility",
        "modular architecture",
        "governance mechanisms",
        "multilingual interactions",
        "real-time tools"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper presents AgroAskAI, a multi-agent AI framework designed to support smallholder farmers with climate adaptation queries. It uses a modular, role-specialized architecture coordinated via a chain-of-responsibility approach, integrating real-time data and multilingual support. The experimental results show that this system delivers more actionable and grounded outputs for agricultural decision support.",
      "mindmap": ""
    },
    {
      "title": "Epistemic diversity across language models mitigates knowledge collapse",
      "authors": "Damian Hodel, Jevin D. West",
      "institution": "University of Washington",
      "link": "https://arxiv.org/pdf/2512.15011",
      "code": null,
      "tags": [
        "llm training",
        "model collapse",
        "epistemic diversity",
        "AI ecosystem",
        "self-training",
        "distributed training"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper investigates whether diversity across language models (an \"AI ecosystem\") can mitigate performance decay from training on model-generated data. It segments training data across multiple models and evaluates performance over self-training iterations. The main conclusion is that increased epistemic diversity mitigates knowledge collapse, but only up to an optimal level, with too few or too many models leading to poor performance.",
      "mindmap": ""
    },
    {
      "title": "Governing rapid technological change: Policy Delphi on the future of European AI governance",
      "authors": "Atte Ojanen, Johannes Anttila, Thilo H. K. Thelitz, Anna Bjork",
      "institution": "Demos Helsinki, University of Turku",
      "link": "https://arxiv.org/pdf/2512.15196",
      "code": null,
      "tags": [
        "policy analysis",
        "Policy Delphi",
        "anticipatory governance",
        "future-proof regulation",
        "AI Act"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper uses a two-round Policy Delphi method with European experts to study the future of AI governance. It finds a consensus that effective regulation depends more on practical implementation and enforcement than on technical specifics, and identifies a gap between desirable policy directions (like citizen participation) and their perceived feasibility.",
      "mindmap": ""
    },
    {
      "title": "ChatGPT and Gemini participated in the Korean College Scholastic Ability Test -- Earth Science I",
      "authors": "Seok-Hyun Ga, Chun-Yen Chang",
      "institution": "Institute for Research Excellence in Learning Sciences, National Taiwan Normal University, Seoul National University, Universitas Negeri Malang",
      "link": "https://arxiv.org/pdf/2512.15298",
      "code": null,
      "tags": [
        "multi-modal inference",
        "multimodal reasoning",
        "perception-cognition gap",
        "calculation-conceptualization discrepancy",
        "process hallucination",
        "OCR",
        "AI-resistant questions"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This study evaluates the multimodal scientific reasoning of LLMs like GPT-4o and Gemini on the Korean CSAT Earth Science I exam under different input conditions. It finds that models suffer from fundamental cognitive flaws, such as a perception-cognition gap and calculation-conceptualization discrepancy, even with optimized inputs. The paper concludes by suggesting these vulnerabilities can be exploited to design AI-resistant assessment questions to ensure academic integrity.",
      "mindmap": ""
    },
    {
      "title": "Exploring User Acceptance and Concerns toward LLM-powered Conversational Agents in Immersive Extended Reality",
      "authors": "Efe Bozkir, Enkelejda Kasneci",
      "institution": "Technical University of Munich",
      "link": "https://arxiv.org/pdf/2512.15343",
      "code": null,
      "tags": [
        "others",
        "crowdsourcing",
        "user study",
        "extended reality",
        "conversational agents",
        "privacy",
        "technology acceptance model"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper conducted a large-scale crowdsourcing study with 1036 participants to explore user acceptance and concerns regarding LLM-powered conversational agents in Extended Reality (XR). The study found that while users generally accept these technologies, they express significant concerns about security, privacy, social implications, and trust, with location data being the most sensitive. The results highlight the importance of practitioner transparency and that familiarity with generative AI increases acceptance, while prior XR device ownership is linked to lower acceptance.",
      "mindmap": ""
    }
  ]
}