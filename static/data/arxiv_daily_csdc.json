{
  "label": "cs.DC",
  "slug": "csdc",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum",
      "authors": "Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda",
      "institution": "Spark Works Ltd., IONOS SE, Iquadrat Informática S.L.",
      "link": "https://arxiv.org/pdf/2512.21340",
      "code": null,
      "tags": [
        "on-device ai",
        "data spaces",
        "cloud-edge continuum",
        "containerized microservices",
        "edge AI",
        "intelligent infrastructure monitoring"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ae37cae2dac445e3682b661f116dd30e5d680105ac5070eb7b48c049ab9aff3_w640_q70.webp",
      "contributions": "1. A real-world implementation of intelligent infrastructure monitoring within a data space-enabled cloud-edge framework, demonstrating practical integration. 2. Leveraging edge computing, containerized microservices, and interoperable data sharing to address challenges like sensor integration, data privacy, and scalability. 3. Showcasing the training and deployment of AI/ML services directly at the edge for optimized resource use and timely decision-making in smart city applications.",
      "summary": "This paper presents a real-world use case for smart cities, implementing an intelligent climate monitoring system within a cloud-edge continuum framework enabled by data spaces. The method combines edge computing, containerized microservices, and secure data sharing to facilitate localized analytics and AI deployment. The conclusion highlights the transformative potential of integrating AI, edge computing, and data spaces for building efficient and resilient smart city infrastructures.",
      "mindmap": "graph TB\n        A[Harnessing Data Spaces for Smart City Infrastructures] --> B[核心问题/Problem: Enhancing smart city efficiency, sustainability, and resilience with secure, interoperable data exchange]\n        A --> C[主要方法/Method: Data space-enabled cloud-edge framework with edge computing, containerized microservices, and edge AI/ML]\n        A --> D[关键结果/Results: Demonstrates practical use case for intelligent monitoring, enabling localized analytics, real-time inference, and trusted data collaboration]"
    },
    {
      "title": "DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction",
      "authors": "Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Lukić, Franck Cappello",
      "institution": "University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory",
      "link": "https://arxiv.org/pdf/2512.21433",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "lossy compression",
        "quality prediction",
        "deep-surrogate",
        "mixture-of-experts",
        "feature-extraction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp",
      "contributions": "1) A generalizable surrogate model for predicting compression quality across different compressors, quality metrics, and datasets. 2) A novel two-stage design that decouples expensive feature extraction from lightweight prediction for efficient training and modular inference. 3) A mixture-of-experts design to optimize performance and robustness for time-evolving scientific data.",
      "summary": "This paper proposes DeepCQ, a deep-surrogate framework to efficiently predict the quality of data after lossy compression, which is traditionally computationally expensive to assess. The method uses a two-stage and mixture-of-experts design for generalizability and robustness across different compressors, metrics, and time-evolving datasets. The framework achieves high predictive accuracy (errors under 10%) on real-world applications, enabling informed compression decisions and reducing I/O and computational overhead.",
      "mindmap": "graph TB\n        A[DeepCQ: 通用深度代理框架用于有损压缩质量预测] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[评估压缩后数据质量的计算成本高/Expensive to assess post-compression data quality]\n        C --> C1[两阶段设计: 特征提取 + 轻量预测/Two-stage design: feature extraction + lightweight prediction]\n        C --> C2[专家混合设计处理时变数据/Mixture-of-experts for time-evolving data]\n        D --> D1[预测误差普遍低于10%/Prediction errors generally under 10%]\n        D --> D2[显著优于现有方法/Significantly outperforms existing methods]"
    },
    {
      "title": "Demystifying ARM SME to Optimize General Matrix Multiplications",
      "authors": "Chencheng Deng, Weiling Yang, Jianbin Fang, Dezun Dong",
      "institution": "College of Computer Science and Technology, National University of Defense Technology",
      "link": "https://arxiv.org/pdf/2512.21473",
      "code": null,
      "tags": [
        "gpu kernels",
        "ARM SME",
        "GEMM",
        "cache-aware partitioning",
        "micro-kernels",
        "on-the-fly transposition"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3bc3a0f3452bf6376dcfa53f5f9e56a621c5b526537a2008e7f55c112b765095_w640_q70.webp",
      "contributions": "1. A systematic characterization of the ARM SME architecture that derives optimization guidelines for GEMM. 2. The design and implementation of MpGEMM, an open-source library featuring cache-aware partitioning and efficient data packing with on-the-fly transposition. 3. Specialized micro-kernels that fully utilize SME's multi-vector loads and all available tile registers to maximize performance.",
      "summary": "This paper addresses the underutilization of ARM's Scalable Matrix Extension (SME) hardware for large-scale General Matrix Multiplication (GEMM). It proposes MpGEMM, an open-source library that optimizes GEMM through cache-aware partitioning, efficient data packing, and specialized micro-kernels tailored for SME. Evaluations on an Apple M4 Pro show MpGEMM achieves a 1.23x speedup over the vendor-optimized Apple Accelerate library.",
      "mindmap": "graph TB\n        Root(”Demystifying ARM SME to Optimize General Matrix Multiplications”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”现有库未能充分利用ARM SME硬件/Existing libraries fail to exploit ARM SME”)\n        Problem --> P2(”大规模GEMM性能瓶颈/Large-scale GEMM performance bottlenecks”)\n        Method --> M1(”系统化架构分析/Systematic SME characterization”)\n        Method --> M2(”设计MpGEMM库/Design MpGEMM library”)\n        M2 --> M2a(”缓存感知分区/Cache-aware partitioning”)\n        M2 --> M2b(”高效数据打包/Efficient data packing”)\n        M2 --> M2c(”专用微内核/Specialized micro-kernels”)\n        Results --> R1(”性能超越Apple Accelerate库/Outperforms Apple Accelerate”)\n        Results --> R2(”显著优于其他开源方案/Significantly beats other open-source alternatives”)"
    },
    {
      "title": "Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism",
      "authors": "Xinglin Pan, Shaohuai Shi, Wenxiang Lin, Yuxin Wang, Zhenheng Tang, Wei Wang, Xiaowen Chu",
      "institution": "The Hong Kong University of Science and Technology (Guangzhou), Harbin Institute of Technology (Shenzhen), Hong Kong Baptist University, The Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.21487",
      "code": null,
      "tags": [
        "llm inference",
        "mixture-of-experts (MoE)",
        "disaggregated expert parallelism (DEP)",
        "task scheduling",
        "inference throughput",
        "fine-grained pipelining"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44cc55e59c66470ffb4e47c93ad8e48f60e8377f30eff6289fbad1cfcb862c96_w640_q70.webp",
      "contributions": "1) Partitioning intensive computation and communication tasks into smaller, fine-grained tasks to enable pipelining, including support for shared experts. 2) Formulating a fine-grained task scheduling optimization problem that supports variable task granularity and ordering. 3) Developing an efficient solver to navigate the large solution space and derive a near-optimal task schedule.",
      "summary": "This paper addresses the memory-intensive inference problem in Mixture-of-Experts (MoE) models by proposing FinDEP, a fine-grained task scheduling algorithm for Disaggregated Expert Parallelism (DEP). FinDEP improves inference throughput by maximizing task overlap through computational partitioning and optimized scheduling. Experiments on systems with up to 32 GPUs show throughput improvements of up to 1.61x over prior methods.",
      "mindmap": "graph TB\n        A[FinDEP: Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[MoE推理内存密集，现有DEP调度效率低/MoE inference is memory-intensive, existing DEP scheduling is inefficient]\n        C --> C1[细粒度任务划分与调度优化/Fine-grained task partitioning and scheduling optimization]\n        D --> D1[吞吐量最高提升1.61倍/Throughput improved by up to 1.61x]"
    },
    {
      "title": "nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures",
      "authors": "Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang",
      "institution": "Canaan Inc.",
      "link": "https://arxiv.org/pdf/2512.21571",
      "code": "https://github.com/kendryte/nncase",
      "tags": [
        "compiler & ir",
        "e-graph",
        "term rewriting",
        "phase ordering",
        "NUMA abstraction",
        "auto vectorize"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a555b38ecd80e8c11606362e5402405bbf0053e11754e06f720d50ce213ee0d_w640_q70.webp",
      "contributions": "1. Proposes an end-to-end compilation framework (nncase) that unifies LLM deployment across heterogeneous memory architectures using a NUMA abstraction for a \"compile once, adapt everywhere\" capability. 2. Introduces an e-graph-based term rewriting engine with equality saturation to mitigate the phase ordering problem and enable global optimization of computation and data movement. 3. Integrates three key automated optimization modules: Auto Vectorize for heterogeneous computing units, Auto Distribution for parallel strategies with communication optimization, and Auto Schedule for on-chip cache locality.",
      "summary": "The paper presents nncase, an end-to-end compiler framework designed to tackle the challenge of efficiently deploying large language models on heterogeneous memory architectures. Its core innovation is an e-graph-based rewriting engine that avoids the phase ordering problem, enabling unified optimization across diverse hardware targets. Evaluations show nncase outperforms frameworks like MLC LLM and Intel IPEX, achieving performance close to hand-optimized llama.cpp, demonstrating the viability of automated compilation for high-performance LLM deployment.",
      "mindmap": "graph TB\n        A[nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures] --> B[核心问题/Problem: LLM部署受限于内存架构异构性，传统编译器流程碎片化/Memory architecture heterogeneity hinders efficient LLM deployment, traditional compilers have fragmented workflows.]\n        A --> C[主要方法/Method: 基于e-graph的项重写引擎，统一NUMA抽象，集成自动向量化、分布、调度模块/E-graph-based term rewriting engine, unified NUMA abstraction, integrates Auto Vectorize, Distribution, Schedule modules.]\n        A --> D[关键结果/Results: 性能超越MLC LLM和Intel IPEX，接近手工优化的llama.cpp/Outperforms MLC LLM & Intel IPEX, achieves performance comparable to hand-optimized llama.cpp.]"
    },
    {
      "title": "Embedding Samples Dispatching for Recommendation Model Training in Edge Environments",
      "authors": "Guopeng Li, Haisheng Tan, Chi Zhang, Hongqiu Ni, Zilong Wang, Xinyue Zhang, Yang Xu, Han Tian",
      "institution": "University of Science and Technology of China (USTC), Hefei University of Technology",
      "link": "https://arxiv.org/pdf/2512.21615",
      "code": null,
      "tags": [
        "memory & caching",
        "edge computing",
        "embedding cache",
        "parameter server",
        "sample dispatching",
        "transmission cost"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f94cc43f65f5fd909f8762bda535a33eea94f879931ebe1a563280cb0db1be81_w640_q70.webp",
      "contributions": "1. Proposed ESD, a novel mechanism to optimize the dispatch of input embedding samples to edge workers to minimize embedding transmission cost. 2. Designed HybridDis, a dispatch decision method that combines an optimal algorithm and a heuristic to balance decision quality and resource consumption. 3. Implemented a prototype and demonstrated significant reductions in transmission cost (up to 36.76%) and training speedup (up to 1.74x) on real-world workloads.",
      "summary": "This paper addresses the high communication cost of embedding transmission during Deep Learning Recommendation Model (DLRM) training in edge environments. It proposes ESD, a mechanism that dispatches input samples to edge workers to minimize expected transmission cost, using a hybrid decision method called HybridDis. Experimental results show that ESD significantly reduces transmission cost and speeds up end-to-end training compared to state-of-the-art methods.",
      "mindmap": "graph TB\n        Root[”Embedding Samples Dispatching for Recommendation Model Training in Edge Environments<br>边缘环境中推荐模型训练的嵌入样本调度”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>DLRM边缘训练中嵌入传输成本高”] --> P1[”挑战/Challenges<br>异构网络，资源受限”]\n        Method[”主要方法/Method<br>ESD机制与HybridDis调度”] --> M1[”方法核心/Core<br>基于预期传输成本的样本调度”]\n        Results[”关键结果/Results<br>减少传输成本，加速训练”] --> R1[”性能提升/Improvement<br>成本降低36.76%，速度提升1.74倍”]"
    },
    {
      "title": "LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol for Multicore Real-Time Systems",
      "authors": "Nan Chen, Xiaotian Dai, Tong Cheng, Alan Burns, Iain Bate, Shuai Zhao",
      "institution": "University of York, Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.21701",
      "code": null,
      "tags": [
        "real-time systems",
        "lock-free",
        "fault-tolerance",
        "resource sharing",
        "multicore",
        "worst-case response time analysis"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb19a780f9199527b92c55981536e4b4108e6135efe187882739942a46ebf5ed_w640_q70.webp",
      "contributions": "1. Proposes the LEFT-RS protocol, a lock-free design that allows concurrent read access to global resources and parallel entry into critical sections, improving efficiency. 2. Enhances fault resilience by limiting overhead and enabling tasks to complete earlier if others experience faults, reducing blocking. 3. Provides a comprehensive worst-case response time analysis to ensure timing guarantees for the proposed protocol.",
      "summary": "The paper proposes LEFT-RS, a lock-free and fault-tolerant resource sharing protocol for multicore real-time systems. It allows tasks to concurrently access resources and enter critical sections in parallel, improving efficiency and resilience to transient faults. Evaluation shows it significantly outperforms existing methods, achieving up to an 84.5% average improvement in schedulability.",
      "mindmap": "graph TB\n        Root[LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Faults in critical sections cause error propagation; locking protocols lack fault tolerance, increasing blocking.]\n        Method[主要方法/Method: LEFT-RS protocol enables concurrent read access and parallel critical section entry for fault resilience.]\n        Results[关键结果/Results: Up to 84.5% average schedulability improvement over existing approaches.]"
    },
    {
      "title": "Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference",
      "authors": "Linyi Jiang, Yifei Zhu, Hao Yin, Bo Li",
      "institution": "Shanghai Jiao Tong University, Tsinghua University, Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.21730",
      "code": null,
      "tags": [
        "multi-modal inference",
        "vision transformer",
        "cloud-device collaboration",
        "dynamic scheduling",
        "patch-level importance",
        "weighted ensembling"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/883099a5be7486c0821b7ffc4858fa9de1fb7c6f3487310e5eb913db9f04c63e_w640_q70.webp",
      "contributions": "1. A collaboration-aware importance scorer that identifies critical regions at the patch level for selective processing. 2. A dynamic scheduler that adaptively adjusts patch transmission quality to balance latency and accuracy under changing network conditions. 3. A weighted ensembler that fuses edge and cloud inference results to improve overall accuracy.",
      "summary": "This paper presents Hyperion, a cloud-device collaborative framework designed to enable low-latency inference on Ultra-HD video using off-the-shelf vision transformers. It tackles computational and transmission bottlenecks by selectively processing critical patches, dynamically adjusting transmission quality, and fusing results. Experiments show Hyperion improves frame processing rate by up to 1.61x and accuracy by up to 20.2% compared to baselines.",
      "mindmap": "graph TB\n        A[Hyperion: Low-Latency Ultra-HD Video Analytics<br>Hyperion: 低延迟超高清视频分析] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Ultra-HD视频处理的计算与传输瓶颈<br>Computational & Transmission Bottleneck for Ultra-HD Video]\n        C[主要方法/Method<br>云-端协作的Vision Transformer推理框架<br>Cloud-Device Collaborative ViT Inference Framework]\n        D[关键结果/Results<br>处理率提升1.61倍，准确率提升20.2%<br>1.61x Faster Frame Rate, 20.2% Higher Accuracy]"
    },
    {
      "title": "Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers",
      "authors": "Krishna Chaitanya Sunkara, Rambabu Konakanchi",
      "institution": "Oracle, Charles Schwab",
      "link": "https://arxiv.org/pdf/2512.21801",
      "code": null,
      "tags": [
        "cluster infrastructure",
        "LSTM",
        "Random Forest",
        "MQTT",
        "InfluxDB",
        "Streamlit"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp",
      "contributions": "1. Probabilistic LSTM forecasting validated within ±30-minute windows for coolant leaks, 2. 96.5% F1-score Random Forest detection for immediate leak identification, 3. Integrated smart IoT architecture design with MQTT streaming, InfluxDB storage, and Streamlit dashboards for energy-efficient data center operations.",
      "summary": "The paper proposes a smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. The system, tested on synthetic data, achieves 96.5% detection accuracy and 87% forecasting accuracy, potentially preventing significant energy waste through proactive maintenance.",
      "mindmap": "graph TB\n        A[Smart IoT-Based Leak Forecasting and Detection] --> B[核心问题/Problem: Coolant leaks cause energy loss in AI data centers]\n        A --> C[主要方法/Method: LSTM for forecasting + Random Forest for detection with IoT sensors]\n        A --> D[关键结果/Results: 96.5% detection accuracy, 87% forecasting accuracy, 1,500 kWh energy saved]"
    },
    {
      "title": "LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices",
      "authors": "Mingyu Sun, Xiao Zhang, Shen Qu, Yan Li, Mengbai Xiao, Yuan Yuan, Dongxiao Yu",
      "institution": "Shandong University",
      "link": "https://arxiv.org/pdf/2512.21835",
      "code": null,
      "tags": [
        "llm inference",
        "collaborative inference",
        "pipeline parallelism",
        "model offloading",
        "memory adaptation",
        "edge computing"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4d2da6a7be206646ebc1b92d8a0053408a991ec073254f89b4182ecdc54fe1b_w640_q70.webp",
      "contributions": "1. Proposes LIME, a collaborative system for lossless LLM inference across multiple memory-constrained edge devices under limited bandwidth. 2. Employs an interleaved pipeline parallelism with model offloading to dynamically balance computation and communication. 3. Introduces a fine-grained offline allocation scheduler and an online memory adaptation strategy to optimize resource usage and minimize inference latency.",
      "summary": "This paper proposes LIME, a system that enables lossless, collaborative LLM inference on multiple memory-constrained edge devices by using interleaved pipeline parallelism and model offloading, along with offline scheduling and online memory adaptation. Experiments on four Nvidia Jetson devices with LLaMA3.3-70B show that LIME achieves significant speedups over baselines without accuracy loss.",
      "mindmap": "graph TB\n        Root[LIME: 协作式无损LLM推理 / Collaborative Lossless LLM Inference] --> Problem[边缘设备内存受限 / Memory-Constrained Edge Devices]\n        Root --> Method[交织流水线并行与模型卸载 / Interleaved Pipeline Parallelism & Offloading]\n        Root --> Results[实现无损加速 / Achieves Lossless Speedup]"
    },
    {
      "title": "Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models",
      "authors": "Tingyang Sun, Ting He, Bo Ji, Parimal Parag",
      "institution": "Pennsylvania State University, Virginia Tech, Indian Institute of Science",
      "link": "https://arxiv.org/pdf/2512.21884",
      "code": null,
      "tags": [
        "llm inference",
        "distributed inference",
        "block placement",
        "request routing",
        "performance modeling",
        "resource allocation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25938e1be55cbd072ba066aea4bb0e492f8b8c2a83e48eaa7e09e800b8697383_w640_q70.webp",
      "contributions": "1. Developed experimentally validated performance models for distributed LLM inference under given block placement and request routing decisions. 2. Formulated the offline optimization problem as a MILP, proved its NP-hardness, and designed a polynomial-complexity algorithm with performance guarantees. 3. Adapted the offline algorithm for the online setting with the same performance guarantee under bounded load.",
      "summary": "This paper addresses the resource allocation problem for geographically-distributed LLM inference, focusing on optimizing block placement and request routing. It proposes performance models, offline and online algorithms with theoretical guarantees, and a lightweight CPU-only simulator. The solution significantly reduces inference time compared to the state-of-the-art in diverse distributed settings.",
      "mindmap": "graph TB\n        A[Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 分布式LLM推理的资源分配优化/Optimizing resource allocation for distributed LLM inference]\n        C[主要方法/Method: 性能建模与优化算法/Performance modeling and optimization algorithms]\n        D[关键结果/Results: 显著降低推理时间/Substantially reduces inference time]"
    },
    {
      "title": "BLEST: Blazingly Efficient BFS using Tensor Cores",
      "authors": "Deniz Elbek, Kamer Kaya",
      "institution": "Sabanci University",
      "link": "https://arxiv.org/pdf/2512.21967",
      "code": null,
      "tags": [
        "gpu kernels",
        "BFS",
        "Tensor Cores",
        "SpMSpV",
        "Graph Reordering",
        "Kernel Fusion"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da59a50541aea7cf054914628e80911f7ca77a9353af15a6a412588e726ca791_w640_q70.webp",
      "contributions": "1. Introduces Binarised Virtual Slice Sets (BVSS) for warp-level load balancing and eliminating frontier-oblivious work assignment in BFS., 2. Applies two complementary graph reordering strategies (compression-oriented and bandwidth-reducing) to improve memory efficiency and update locality., 3. Develops a batched SpMSpV multiplication pattern using bitwise Tensor Core tiles and combines kernel fusion with a lazy vertex update scheme to reduce synchronization and atomic overheads.",
      "summary": "The paper presents BLEST, a framework that accelerates Breadth-First Search (BFS) on GPUs by efficiently mapping the irregular computation onto dense-math Tensor Cores. The method reformulates the BFS pipeline using a bitmap-oriented structure, specialized load balancing, graph reordering, and kernel fusion. Experiments show that BLEST achieves significant speedups (3.58x to 4.9x) over state-of-the-art GPU-based BFS implementations.",
      "mindmap": "graph TB\n        A[BLEST: Blazingly Efficient BFS using Tensor Cores] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[如何将不规则图BFS映射到密集张量核心/Map irregular BFS to dense Tensor Cores]\n        C --> C1[二值化虚拟切片集/Binarised Virtual Slice Sets (BVSS)]\n        C --> C2[图重排序策略/Graph Reordering Strategies]\n        C --> C3[批处理SpMSpV与核融合/Batched SpMSpV & Kernel Fusion]\n        D --> D1[平均3.58-4.9倍加速/Average 3.58-4.9x Speedup]"
    },
    {
      "title": "Proceedings First Workshop on Adaptable Cloud Architectures",
      "authors": "Giuseppe De Palma, Saverio Giallorenzo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.22054",
      "code": null,
      "tags": [],
      "day": "2025-12-29",
      "thumbnail": null,
      "contributions": "",
      "summary": "",
      "mindmap": ""
    },
    {
      "title": "FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion",
      "authors": "Zhuoran Zhu, Chunyang Zhu, Hao Lin, Xu Fu, Yiming Zhou, Quanlu Zhang, Zhenhua Li, Feng Qian, Chao Yu, Boxun Li, Guohao Dai, Yu Wang",
      "institution": "Tsinghua University, Infinigence AI, University of Southern California, Zhongguancun Academy, Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.22036",
      "code": null,
      "tags": [
        "communication & networking",
        "Mixture-of-Experts",
        "expert parallelism",
        "data shuffling",
        "transformation-communication fusion",
        "collective communication"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7cf69e647d44d7f0b5d2cdef643280359c8d359bbdf2f836c065bb3b6fb214ae_w640_q70.webp",
      "contributions": "1. Identifies the root cause of inefficiency in MoE data shuffling as the misalignment between expert-major and device-major data layouts, requiring disaggregated transformation and communication. 2. Proposes FUSCO, a communication library that fuses data transformation and communication operations into a single, efficient pipeline to eliminate redundant data movement. 3. Introduces lightweight planning and load-balancing mechanisms to eliminate redundant communication and disperse traffic, further optimizing the shuffling process.",
      "summary": "This paper addresses the performance bottleneck of distributed data shuffling in Mixture-of-Experts (MoE) model training and inference. It proposes FUSCO, a communication library that fuses data transformation and communication to align expert-major and device-major data layouts efficiently. Evaluations show FUSCO achieves significant speedups over existing libraries like NCCL and DeepEP, reducing both training and inference latency.",
      "mindmap": "graph TB\n        Root[FUSCO: High-Performance Distributed Data Shuffling] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: MoE专家并行中的数据混洗开销大/High overhead of data shuffling in MoE expert parallelism]\n        Method[主要方法/Method: 通过融合数据转换与通信实现高效混洗/Efficient shuffling via transformation-communication fusion]\n        Results[关键结果/Results: 相比NCCL和DeepEP实现显著加速/Significant speedups over NCCL and DeepEP]"
    },
    {
      "title": "Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View",
      "authors": "Yanmeng Wang, Zhiwen Dai, Shuai Wang, Jian Zhou, Fu Xiao, Tony Q. S. Quek, Tsung-Hui Chang",
      "institution": "The affiliations include IEEE members, suggesting multiple institutions. Based on common patterns, likely institutions include The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen) and/or other Chinese universities/tech institutes, given authors like Tsung-Hui Chang and Tony Q. S. Quek are affiliated with such institutions.",
      "link": "https://arxiv.org/pdf/2512.22035",
      "code": null,
      "tags": [
        "federated learning",
        "federated fine-tuning",
        "connection failures",
        "adaptive aggregation",
        "data heterogeneity",
        "convergence guarantee"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09b9295640a8e9a219a19de76effe76cb1ea0696845676f4a5d9a059161538fb_w640_q70.webp",
      "contributions": "1. Proposes FedAuto, a novel Federated Fine-Tuning framework that mitigates the combined effects of unreliable connections and data heterogeneity via adaptive aggregation, requiring no prior knowledge of network conditions. 2. Establishes a rigorous, per-round convergence guarantee for FedAuto that holds for each individual realization, removing common assumptions on failure probabilities or client selection. 3. Demonstrates through extensive experiments that FedAuto outperforms state-of-the-art baselines under diverse failure scenarios for both full and partial-parameter fine-tuning (e.g., LoRA).",
      "summary": "The paper addresses the performance degradation of Federated Fine-Tuning (FFT) in real-world networks with unreliable connections and heterogeneous data. It proposes FedAuto, a framework that uses adaptive aggregation to handle these issues without prior network knowledge or infrastructure changes. Experiments show FedAuto consistently outperforms existing methods and provides stronger theoretical convergence guarantees.",
      "mindmap": "graph TB\n        A[Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[FFT性能受不可靠连接和数据异构性影响/FFT performance degraded by unreliable connections & data heterogeneity]\n        C --> C1[FedAuto: 通过自适应聚合的FFT框架/FedAuto: FFT framework with adaptive aggregation]\n        C --> C2[无需先验网络知识/No prior network knowledge needed]\n        D --> D1[实验表现超越SOTA/Outperforms SOTA baselines]\n        D --> D2[提供严格收敛保证/Provides rigorous convergence guarantee]"
    },
    {
      "title": "Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications",
      "authors": "Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer",
      "institution": "University of Illinois at Urbana-Champaign, IBM Research",
      "link": "https://arxiv.org/pdf/2512.22113",
      "code": null,
      "tags": [
        "agent system",
        "root cause analysis",
        "service dependency graph",
        "program dependence graph",
        "LLM agent",
        "cloud incident"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62ebd8a01fd966235e0d8d40581cb8352024a391331fada8ea23868c2235ada9_w640_q70.webp",
      "contributions": "1. PRAXIS, an agentic approach for cloud incident RCA with structured, LLM-driven graph reasoning and traversal over microservice and program dependency graphs. 2. An application of the hammock block program dependence graph for agentic RCA, leveraging its hierarchical structure for multi-granular code analysis. 3. A Code-Cloud-RCA Benchmark consisting of 30 real-world incident scenarios injected in a live Kubernetes environment.",
      "summary": "This paper introduces PRAXIS, an orchestrator that uses an LLM-driven agent to traverse service dependency graphs and program dependence graphs to diagnose the root cause of code- and configuration-related cloud incidents. Compared to ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x, as demonstrated on a benchmark of 30 real-world incidents.",
      "mindmap": "graph TB\n        A[Agentic Structured Graph Traversal for Root Cause Analysis<br/>基于智能体结构化图遍历的云应用根因分析] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>High cost of unresolved cloud incidents; Need for effective root cause analysis]\n        C[主要方法/Method<br/>PRAXIS: LLM-driven traversal over Service Dependency Graph and Program Dependence Graph]\n        D[关键结果/Results<br/>3.1x higher RCA accuracy, 3.8x lower token consumption vs. ReAct baselines]"
    },
    {
      "title": "PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation",
      "authors": "Yuma Ichikawa, Naoya Takagi, Takumi Nakagawa, Yuzi Kanazawa, Akira Sakai",
      "institution": "Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University",
      "link": "https://arxiv.org/pdf/2512.20687",
      "code": null,
      "tags": [
        "llm inference",
        "hierarchical autoregressive model",
        "KV-cache optimization",
        "memory-bound inference",
        "multi-resolution context",
        "throughput-quality trade-off"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp",
      "contributions": "1. Proposes PHOTON, a hierarchical autoregressive model that replaces the Transformer's flat token-by-token scanning with a vertical, multi-resolution context access pattern. 2. Introduces a persistent hierarchy of latent streams, with a bottom-up encoder compressing tokens and lightweight top-down decoders reconstructing token representations, reducing decode-time KV-cache traffic. 3. Demonstrates significant improvements in throughput per unit memory (up to 10^3x) and advantages in long-context and multi-query tasks compared to Transformer-based models.",
      "summary": "The paper identifies that Transformer inference becomes memory-bound due to ever-growing KV-cache reads/writes during autoregressive decoding. To solve this, it proposes PHOTON, a hierarchical model that accesses context vertically at multiple resolutions instead of scanning tokens horizontally. This architectural change drastically reduces memory traffic, yielding orders-of-magnitude higher throughput per unit memory while maintaining quality.",
      "mindmap": "graph LR\n    A[PHOTON: Hierarchical Autoregressive Modeling] --> B[核心问题/Problem: Transformer水平扫描导致KV缓存读写成为内存瓶颈/Horizontal scanning causes memory-bound KV-cache bottleneck]\n    A --> C[主要方法/Method: 用垂直多分辨率层次模型替代/Replace with vertical multi-resolution hierarchical model]\n    A --> D[关键结果/Results: 内存效率与吞吐量大幅提升/Significant improvement in memory efficiency & throughput]"
    },
    {
      "title": "SoK: Speedy Secure Finality",
      "authors": "Yash Saraswat, Abhimanyu Nag",
      "institution": "Indian Institute of Technology, Roorkee; University of Alberta",
      "link": "https://arxiv.org/pdf/2512.20715",
      "code": null,
      "tags": [
        "blockchain consensus",
        "finality",
        "consensus protocol",
        "Ethereum",
        "Gasper",
        "reorg resilience"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/280bac4449b7f86317e6367c908a04f23445bf30d94bffc4957a4d305fac0548_w640_q70.webp",
      "contributions": "1. Provides a systematic survey of the state-of-the-art in Speedy Secure Finality (SSF) protocols, tracing their evolution from foundational works like Goldfish to RLMD-GHOST. 2. Introduces and explains core theoretical primitives for understanding SSF, such as reorganization resilience and the generalized sleepy model. 3. Analyzes the practical trade-offs of Single Slot Finality and surveys the 3-Slot Finality (3SF) protocol as a pragmatic solution balancing fast finality with Ethereum's engineering constraints.",
      "summary": "This paper surveys research on Speedy Secure Finality (SSF) to reduce the long confirmation latency in Ethereum's Gasper protocol. It reviews the evolution of fast finality protocols, analyzes their design trade-offs, and highlights the 3-Slot Finality protocol as a practical synthesis. The main conclusion is that 3SF offers a viable path to achieve faster, secure finality while addressing the network's practical limitations.",
      "mindmap": "graph LR\n    A[SoK: Speedy Secure Finality] --> B[核心问题/Problem: Ethereum Gasper协议15分钟最终确认延迟导致重组攻击和MEV提取]\n    A --> C[主要方法/Method: 系统综述快速最终性协议，分析单时隙最终性的瓶颈，调查3时隙最终性协议]\n    A --> D[关键结果/Results: 3SF协议在理论安全保证与工程约束间取得平衡，是实用的快速最终性方案]"
    },
    {
      "title": "RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale",
      "authors": "Aymen Alsaadi, Mason Hooten, Mariya Goliyad, Andre Merzky, Andrew Shao, Mikhail Titov, Tianle Wang, Yian Chen, Maria Kalantzi, Kent Lee, Andrew Park, Indira Pimpalkhare, Nick Radcliffe, Colin Wahl, Pete Mendygral, Matteo Turilli, Shantenu Jha",
      "institution": "Rutgers University, Hewlett Packard Enterprise, Brookhaven National Laboratory",
      "link": "https://arxiv.org/pdf/2512.20795",
      "code": null,
      "tags": [
        "cluster infrastructure",
        "multi-runtime middleware",
        "hybrid AI-HPC workflows",
        "uniform abstractions",
        "Dragon",
        "vLLM"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/548c8dcfddec763ea481a17e825529b0e0dab60751d6a7f48f3ad27c7f29ea25_w640_q70.webp",
      "contributions": "1. Proposes RHAPSODY, a multi-runtime middleware that composes and coordinates existing runtimes to support heterogeneous AI-HPC workloads within a single job allocation. 2. Introduces uniform abstractions for tasks, services, resources, and execution policies to manage conflicting requirements of simulations, AI services, and agentic workflows. 3. Demonstrates minimal runtime overhead, scalability for inference workloads, and efficient AI-HPC coupling in evaluations on leadership-class HPC platforms.",
      "summary": "The paper addresses the challenge of executing hybrid AI-HPC workflows, which combine simulations, training, and inference with conflicting runtime requirements. It proposes RHAPSODY, a middleware that coordinates existing runtimes through uniform abstractions instead of replacing them. Evaluation shows RHAPSODY enables efficient, scalable execution of these heterogeneous workloads with minimal overhead.",
      "mindmap": "graph LR\n    A[RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale] --> B[核心问题/Problem: 现有系统无法规模化支持混合AI-HPC工作流的异构需求 / Existing systems cannot support heterogeneous requirements of hybrid AI-HPC workflows at scale]\n    A --> C[主要方法/Method: 多运行时中间件，通过统一抽象协调现有运行时 / Multi-runtime middleware coordinating existing runtimes via uniform abstractions]\n    A --> D[关键结果/Results: 开销最小，支持规模化异构与近线性推理扩展 / Minimal overhead, sustains heterogeneity at scale, near-linear inference scaling]"
    },
    {
      "title": "Stochastic well-structured transition systems",
      "authors": "James Aspnes",
      "institution": "Yale University",
      "link": "https://arxiv.org/pdf/2512.20939",
      "code": null,
      "tags": [
        "distributed computing theory",
        "well-structured transition systems",
        "population protocols",
        "probabilistic scheduling",
        "computational complexity",
        "BPP"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6b86e00331b0080766e8bd0e99c46088867daf6771188ff7f0462c5c277cb00_w640_q70.webp",
      "contributions": "1. Defines a new class of stochastic well-structured transition systems (SWSTSs) that unifies models like population protocols and chemical reaction networks under a probabilistic scheduling rule. 2. Proves fundamental limitations on phase clocks in SWSTSs, showing they either stop or tick too fast in expected polynomial time. 3. Provides an exact characterization of computational power, showing augmented SWSTSs compute exactly BPP languages, while unaugmented ones compute symmetric BPL languages.",
      "summary": "This paper extends the theory of well-structured transition systems by incorporating probabilistic scheduling, creating a new class called stochastic well-structured transition systems (SWSTSs). It proves that any phase clock implementation in these systems has polynomial expected duration, and that terminating computations finish in expected polynomial time. These results lead to an exact characterization of computational power, showing that augmented SWSTSs compute exactly the languages in BPP.",
      "mindmap": "graph LR\n    A[Stochastic well-structured transition systems] --> B(核心问题/Problem: Extend WSTS theory to probabilistic scheduling models)\n    A --> C(主要方法/Method: Define SWSTS class unifying population protocols, CRNs, gossip models)\n    A --> D(关键结果/Results: Phase clock limitations; Polynomial expected termination; Computational power = BPP/BPL)"
    },
    {
      "title": "AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences",
      "authors": "Zhe Wang, Jinghang Li, Yifei Zhu",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.20943",
      "code": null,
      "tags": [
        "communication & networking",
        "4D Gaussian Splatting",
        "video streaming",
        "integer linear programming",
        "pruning",
        "keyframe selection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp",
      "contributions": "1. Proposes a streaming-optimized 4DGS framework that converts Gaussian streams into multi-channel 2D formats and uses intelligent keyframe identification to enhance reconstruction quality and reduce training time. 2. Models the 4DGS delivery problem as an integer linear programming problem and designs a lightweight pruning algorithm to adaptively prune Gaussian updates for bandwidth-efficient transmission. 3. Demonstrates significant improvements in quality stability (reducing PSNR deviation by >20%), training speed (6x acceleration), and transmission efficiency (50% size reduction) compared to state-of-the-art methods.",
      "summary": "The paper presents AirGS, a framework that optimizes the training and delivery pipeline for 4D Gaussian Splatting to enable real-time free-viewpoint video streaming. It addresses quality degradation and high bandwidth overhead by introducing a 2D representation format, keyframe selection, and an adaptive pruning algorithm for transmission. Experiments show AirGS significantly improves quality stability, accelerates training, and reduces transmission size.",
      "mindmap": "graph LR\n    A[AirGS: Real-Time 4D Gaussian Streaming] --> B[核心问题/Problem: 4DGS质量下降与高带宽开销/4DGS Quality Degradation & High Bandwidth Overhead]\n    A --> C[主要方法/Method: 流优化框架与自适应剪枝/Streaming-Optimized Framework & Adaptive Pruning]\n    A --> D[关键结果/Results: 质量稳定、训练加速、传输减小/Quality Stable, Training Faster, Transmission Smaller]"
    },
    {
      "title": "Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications",
      "authors": "Yuxiao Wang, Yuedong Xu, Qingyang Duan, Yuxuan Liu, Lei Jiao, Yinghao Yu, Jun Wu",
      "institution": "Fudan University",
      "link": "https://arxiv.org/pdf/2512.20953",
      "code": null,
      "tags": [
        "llm training",
        "3D parallelism",
        "heterogeneous GPUs",
        "spot instances",
        "load balancing",
        "fault-tolerance"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60ccbe9f51f850501dbeed0dfb992113708962be3840d0248e7a8d0677723ce1_w640_q70.webp",
      "contributions": "1. Introduces AutoHet, a system that automatically identifies optimal parallelism plans for distributed training on heterogeneous GPUs, supporting asymmetric 3D parallelism. 2. Proposes a theoretical optimization model for device grouping and load balancing to minimize per-iteration training time across GPUs with diverse capabilities. 3. Presents an efficient recovery strategy for spot instance preemption that prioritizes retrieving training states locally to minimize checkpoint downloads from cloud storage.",
      "summary": "This paper addresses the challenge of distributed training of large language models on heterogeneous GPU clusters, particularly with spot instances. It proposes AutoHet, a system that automatically optimizes 3D parallelism plans and load balancing for such environments and includes an efficient fault-tolerance mechanism. Evaluations show AutoHet achieves significant speedups in both training throughput and recovery speed compared to existing systems.",
      "mindmap": "graph LR\n    A[论文标题 / Paper Title:<br>Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs] --> B[核心问题 / Problem:<br>Heterogeneous GPU & Spot Instance Training];\n    A --> C[主要方法 / Method:<br>AutoHet System & Optimization Model];\n    A --> D[关键结果 / Results:<br>1.79x Training & 4.38x Recovery Speedup];"
    },
    {
      "title": "Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality",
      "authors": "Sirui Chen, Jingji Chen, Siqi Zhu, Ziheng Jiang, Yanghua Peng, Xuehai Qian",
      "institution": "Tsinghua University, Purdue University, University of Illinois Urbana-Champaign, ByteDance Seed",
      "link": "https://arxiv.org/pdf/2512.20968",
      "code": null,
      "tags": [
        "llm training",
        "distributed attention",
        "communication efficiency",
        "Ring-Attention",
        "communication-computation ratio",
        "scalability"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4944eec84564de9a1d27e811d1317c483f5220256be0880e9e87af0f1df84b8e_w640_q70.webp",
      "contributions": "1. Proposes Mesh-Attention, a new distributed attention algorithm using a matrix-based model that assigns 2D computation tiles to GPUs for lower communication-computation ratio. 2. Introduces a greedy algorithm to efficiently search the scheduling space within a tile under communication constraints. 3. Provides theoretical analysis and extensive experiments showing Mesh-Attention significantly reduces communication volume and achieves speedup compared to state-of-the-art methods.",
      "summary": "This paper addresses the communication bottleneck in scaling LLM context windows by proposing Mesh-Attention, a new distributed attention algorithm that uses 2D computation tiling to reduce communication overhead. It demonstrates superior performance, achieving up to 3.4x speedup and 85.4% communication reduction on 256 GPUs, and shows good scalability for large-scale deployments.",
      "mindmap": "graph LR\n    A[Mesh-Attention<br>论文标题/Paper Title] --> B[核心问题/Problem: 分布式注意力通信开销大<br>High Communication in Distributed Attention]\n    A --> C[主要方法/Method: 基于2D计算块划分的Mesh-Attention算法<br>Mesh-Attention with 2D Tile Assignment]\n    A --> D[关键结果/Results: 通信量减少85.4%, 速度提升3.4倍<br>85.4% Comm Reduction, 3.4x Speedup]"
    },
    {
      "title": "Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions",
      "authors": "Linggao Kong, Yuedong Xu, Lei Jiao, Chuan Xu",
      "institution": "Fudan University, University of Oregon, Inria",
      "link": "https://arxiv.org/pdf/2512.20967",
      "code": null,
      "tags": [
        "llm training",
        "spot instance",
        "online scheduling",
        "deadline-aware",
        "LoRA",
        "integer programming"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6addc088c50bc798ccb2da0947c0b05adb32b7751a390fde44b4c5be3c5b85f3_w640_q70.webp",
      "contributions": "1. Formulated an integer programming problem for deadline-aware LLM fine-tuning using a mix of volatile spot and reliable on-demand GPU instances. 2. Proposed a prediction-based online allocation algorithm and a complementary algorithm without predictions, with a policy selection algorithm that learns the best policy from a parameterized pool. 3. Provided theoretical analysis showing the prediction-based algorithm's performance improves with prediction accuracy and that the policy selection algorithm has a sublinear regret bound, with experiments showing up to 54.8% utility improvement.",
      "summary": "This paper tackles the challenge of cost-effective, deadline-aware scheduling for fine-tuning large language models (LLMs) on volatile GPU spot instances. It proposes an online framework that uses a mix of spot and on-demand instances, featuring a prediction-based algorithm, a non-prediction algorithm, and a policy selection mechanism. The framework adapts to market dynamics, is theoretically grounded, and significantly outperforms baselines in experiments.",
      "mindmap": "graph LR\n    A[Deadline-Aware Online Scheduling for LLM Fine-Tuning] --> B(核心问题/Problem: Expensive LLM fine-tuning with volatile spot instances)\n    A --> C(主要方法/Method: Mixed instance scheduling with prediction & online policy selection)\n    A --> D(关键结果/Results: O(√T) regret, up to 54.8% utility gain)"
    },
    {
      "title": "ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting",
      "authors": "S. M. Shovan, Arindam Khanda, Sanjukta Bhowmick, Sajal K. Das",
      "institution": "Missouri University of Science and Technology, University of North Texas",
      "link": "https://arxiv.org/pdf/2512.21009",
      "code": null,
      "tags": [
        "parallel computing",
        "hypergraph",
        "GPU",
        "dynamic data structure",
        "triad counting",
        "parallel algorithm"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/335f41f5fe79d6bd117abec93eaf4a43675ba18ef5f201690d075fa6085dfc56_w640_q70.webp",
      "contributions": "1. Proposed ESCHER, a novel GPU-centric parallel data structure for efficient representation and management of large-scale dynamic hypergraphs. 2. Designed a hypergraph triad-count update framework that minimizes redundant computation by leveraging ESCHER's dynamic operation capabilities. 3. Demonstrated significant performance improvements, achieving speedups of up to 104.5x, 473.7x, and 112.5x for different triad counting types on real-world and synthetic datasets.",
      "summary": "The paper addresses the computational challenge of analyzing large, dynamic hypergraphs, which lack efficient specialized data structures. It proposes ESCHER, a GPU-centric data structure for representing hypergraph evolution, and a corresponding triad-counting update framework. The method achieves substantial speedups over state-of-the-art approaches in counting various types of hypergraph triads.",
      "mindmap": "graph LR\n    A[ESCHER: Efficient and Scalable Hypergraph Evolution Representation] --> B[核心问题/Problem: 缺乏分析大规模动态超图的高效数据结构]\n    A --> C[主要方法/Method: 提出GPU并行的ESCHER数据结构与三元组计数更新框架]\n    A --> D[关键结果/Results: 性能显著提升，最高达473.7倍加速]"
    },
    {
      "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
      "authors": "Savvy Sharma, George Petrovic, Sarthak Kaushik",
      "institution": "George Brown Polytechnic",
      "link": "https://arxiv.org/pdf/2512.21048",
      "code": null,
      "tags": [
        "federated learning",
        "zero-knowledge proofs",
        "trusted execution environments",
        "blockchain",
        "medical AI",
        "verifiable aggregation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaf3da543259b835ceee63f63b6675f8ca4fdd248035da93b3582ae546b60093_w640_q70.webp",
      "contributions": "1. Proposes a novel architecture (zkFL-Health) that integrates Federated Learning with Zero-Knowledge Proofs and Trusted Execution Environments to ensure privacy and verifiable correctness in medical AI training. 2. Introduces a protocol where the aggregator, operating within a TEE, generates a succinct ZK proof to attest it used the correct inputs and aggregation rule, without revealing client updates. 3. Leverages a blockchain to provide an immutable audit trail of cryptographic commitments and proof verification, removing the need to trust a single party and enhancing regulatory compliance.",
      "summary": "The paper addresses privacy leakage and trust issues in federated learning for healthcare by proposing zkFL-Health, a framework that combines FL with zero-knowledge proofs and TEEs to enable verifiable and private model aggregation. The method ensures the aggregator's computations are provably correct and recorded on a blockchain for auditability. The conclusion is that this approach provides strong confidentiality, integrity, and auditability, which are crucial for clinical adoption.",
      "mindmap": "graph LR\n    A[zkFL-Health] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[隐私泄露与聚合器信任/Privacy Leakage & Aggregator Trust]\n    C --> C1[FL+ZKP+TEE/FL+ZKP+TEE]\n    C --> C2[链上验证/On-chain Verification]\n    D --> D1[可验证的隐私保护/Verifiable Privacy]\n    D --> D2[审计与合规/Auditability & Compliance]"
    },
    {
      "title": "Declarative distributed broadcast using three-valued modal logic and semitopologies",
      "authors": "Murdoch J. Gabbay",
      "institution": "Heriot-Watt University (inferred from author's affiliation)",
      "link": "https://arxiv.org/pdf/2512.21137",
      "code": null,
      "tags": [
        "distributed algorithms",
        "modal logic",
        "declarative specification",
        "semitopologies",
        "three-valued logic",
        "axiomatic theories"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13c32160b520274be9e08e195b14474c8b52625ca44889f1659468dad3c6d782_w640_q70.webp",
      "contributions": "1. Proposes a novel method to formally specify distributed algorithms as declarative axiomatic theories using modal logic, 2. Demonstrates the method's application and scalability on concrete protocols (voting, broadcast, agreement), 3. Shows the method's practical utility by finding errors in a proposed industrial protocol.",
      "summary": "This paper proposes a novel declarative approach for specifying distributed algorithms using three-valued modal logic and semitopologies. It demonstrates the method on protocols like Bracha Broadcast, providing a compact, human-readable specification that abstracts away low-level implementation details. The approach enables precise reasoning about correctness and has been used to find errors in industrial protocols.",
      "mindmap": "graph LR\n    A[Declarative distributed broadcast using three-valued modal logic and semitopologies] --> B[核心问题/Problem: 如何对分布式算法进行形式化、声明式规范？/How to formally specify distributed algorithms declaratively?]\n    A --> C[主要方法/Method: 使用三值模态逻辑和半拓扑作为公理化理论/Using three-valued modal logic and semitopologies as axiomatic theories]\n    A --> D[关键结果/Results: 创建了精确、紧凑的规范，可发现协议错误，支持验证/Creates precise, compact specifications that can find protocol errors and support verification]"
    },
    {
      "title": "Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning",
      "authors": "Antonio Tarizzo, Mohammad Kazemi, Deniz Gündüz",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19777",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b386ba4532b788c41eccda5b3c48b9585db890467bbb5e150328901a4ad2208_w640_q70.webp",
      "contributions": "",
      "summary": "Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning",
      "mindmap": ""
    },
    {
      "title": "Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform",
      "authors": "Andrea Sordello, Marco Mellia, Idilio Drago, Rodolfo Valentim, Francesco Musumeci, Massimo Tornatore, Federico Cerutti, Martino Trevisan, Alessio Botta, Willen Borges Coelho",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19842",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e888d5285a786a3c767579c7eaaca0375fc971ad3a2fb2064daa28b1f1f886b9_w640_q70.webp",
      "contributions": "",
      "summary": "Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform",
      "mindmap": ""
    },
    {
      "title": "An Adaptive Distributed Stencil Abstraction for GPUs",
      "authors": "Aditya Bhosale, Laxmikant Kale",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19851",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c440cc2cd05a524e05f6d514a65e60b4224e6fad90cc6a9250733b6134c5563a_w640_q70.webp",
      "contributions": "",
      "summary": "An Adaptive Distributed Stencil Abstraction for GPUs",
      "mindmap": ""
    },
    {
      "title": "UCCL-EP: Portable Expert-Parallel Communication",
      "authors": "Ziming Mao, Yihan Zhang, Chihan Cui, Kaichao You, Zhongjie Chen, Zhiying Xu, Scott Shenker, Costin Raiciu, Yang Zhou, Ion Stoica",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19849",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb2d143c0a9d64bd2c5bdf4142e8e3a096290fd8319372321c00c3c17d53b658_w640_q70.webp",
      "contributions": "",
      "summary": "UCCL-EP: Portable Expert-Parallel Communication",
      "mindmap": ""
    },
    {
      "title": "Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions",
      "authors": "Pengchao Han, Xi Huang, Yi Fang, Guojun Han",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19972",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68a57a4b48fd01d2fd3b5670a8a2ba0afbbd643b8ddcf8604e3c9fdeb4782d83_w640_q70.webp",
      "contributions": "",
      "summary": "Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions",
      "mindmap": ""
    },
    {
      "title": "Scaling Point-based Differentiable Rendering for Large-scale Reconstruction",
      "authors": "Hexu Zhao, Xiaoteng Liu, Xiwen Min, Jianhao Huang, Youming Deng, Yanfei Li, Ang Li, Jinyang Li, Aurojit Panda",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20017",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ce1e013da49adc5236a2b8d6111015f3c345c5b5d1cd6d9c9375d46d54a5c3d_w640_q70.webp",
      "contributions": "",
      "summary": "Scaling Point-based Differentiable Rendering for Large-scale Reconstruction",
      "mindmap": ""
    },
    {
      "title": "FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling",
      "authors": "Yaojian Chen, Si-Qiu Gong, Lin Gan, Yanfei Liu, An Yang, Yinuo Wang, Chao-yang Lu, Guangwen Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20064",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/daf5920f62b1d9628f22f64fd603d044ea22b1d4cbcf93ff65d5699edb214a6c_w640_q70.webp",
      "contributions": "",
      "summary": "FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling",
      "mindmap": ""
    },
    {
      "title": "Population Protocols Revisited: Parity and Beyond",
      "authors": "Leszek Gąsieniec, Tytus Grodzicki, Tomasz Jurdziński, Jakub Kowalski, Grzegorz Stachowiak",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20163",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8aad104fc51d7c782bd987b120b6adc44b9209ecea22bf9d780ac0054be6ece3_w640_q70.webp",
      "contributions": "",
      "summary": "Population Protocols Revisited: Parity and Beyond",
      "mindmap": ""
    },
    {
      "title": "SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication",
      "authors": "Chen Zhuang, Lingqi Zhang, Benjamin Brock, Du Wu, Peng Chen, Toshio Endo, Satoshi Matsuoka, Mohamed Wahib",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20178",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/340dca9738b4b1930a1331103d9fe185151f34d58d7be73cc31d211665f20128_w640_q70.webp",
      "contributions": "",
      "summary": "SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication",
      "mindmap": ""
    },
    {
      "title": "Reaching Agreement Among Reasoning LLM Agents",
      "authors": "Chaoyi Ruan, Yiliang Wang, Ziji Shi, Jialin Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20184",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/300a553c72ebfc0b096f1fc824a5f548ba652ad4ed3a63bd7596ea2c6fa4c4a9_w640_q70.webp",
      "contributions": "",
      "summary": "Reaching Agreement Among Reasoning LLM Agents",
      "mindmap": ""
    },
    {
      "title": "Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs",
      "authors": "Yinan Ni, Xiao Yang, Yuqi Tang, Zhimin Qiu, Chen Wang, Tingzhou Yuan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20210",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbb6bb6b2735659dd1638f766c8c42f1c8f809b00b87f15a65d394ddfae14463_w640_q70.webp",
      "contributions": "",
      "summary": "Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs",
      "mindmap": ""
    },
    {
      "title": "Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning",
      "authors": "Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20363",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59262adfe1a821db6db6b416af65f0ab7cc67a6943505bf052c9306bf801e87f_w640_q70.webp",
      "contributions": "",
      "summary": "Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning",
      "mindmap": ""
    },
    {
      "title": "Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults",
      "authors": "Mohammad Walid Charrwi, Zaid Hussain",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20394",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b39d3fd875a1d18d18c3b4c5a175ce223ca72ea88ffe8906fbefdd667cb5178_w640_q70.webp",
      "contributions": "",
      "summary": "Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults",
      "mindmap": ""
    },
    {
      "title": "WOC: Dual-Path Weighted Object Consensus Made Efficient",
      "authors": "Tanisha Fonseca, Gengrui Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20485",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5815c48f85f174b306c1a113a0f90bbdad3e1dbaf0212a44c24b093feff6ff3_w640_q70.webp",
      "contributions": "",
      "summary": "WOC: Dual-Path Weighted Object Consensus Made Efficient",
      "mindmap": ""
    },
    {
      "title": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
      "authors": "Rui Pan, Zhuofu Chen, Ravi Netravali",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20573",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab6d51f1421d2f929752b850a0d24b6a7af807b50f1d54c1101b257d175a44f_w640_q70.webp",
      "contributions": "",
      "summary": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
      "mindmap": ""
    },
    {
      "title": "Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation",
      "authors": "Nihir Chadderwala",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17913",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ebf691e752728d3961062fe7df081d6a92c7729c4f9968ddd1c3f083bb93df_w640_q70.webp",
      "contributions": "",
      "summary": "Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation",
      "mindmap": ""
    },
    {
      "title": "QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments",
      "authors": "Irwindeep Singh, Sukhpal Singh Gill, Jinzhao Sun, Jan Mol",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17918",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ffe2289ec69b8a1b313e066cc4c4de736d5becc210f22958bfd52daff8ff5e6_w640_q70.webp",
      "contributions": "",
      "summary": "QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments",
      "mindmap": ""
    },
    {
      "title": "Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA",
      "authors": "Allison Li, Kristjan Greenewald, Thomas Parnell, Navid Azizan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17910",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69e52c70ff632453dafd08b1f3a3463c9d2df49fdb8300df0a3e128192436717_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA",
      "mindmap": ""
    },
    {
      "title": "Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU",
      "authors": "Bin Xu, Ayan Banerjee, Midhat Urooj, Sandeep K.S. Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17941",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8208ecda566e64a777495316e0aac16897f35ec183a5fb04050258d853af7cf7_w640_q70.webp",
      "contributions": "",
      "summary": "Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU",
      "mindmap": ""
    },
    {
      "title": "Fast Online Digital Twinning on FPGA for Mission Critical Applications",
      "authors": "Bin Xu, Ayan Banerjee, Sandeep K. S. Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17942",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16ee82367ac1a35aebfd7c95c003a129158965297946e5c0f25e447a72aa119c_w640_q70.webp",
      "contributions": "",
      "summary": "Fast Online Digital Twinning on FPGA for Mission Critical Applications",
      "mindmap": ""
    },
    {
      "title": "ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training",
      "authors": "Yi Yang, Ziyu Lin, Liesheng Wei",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18127",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0b4f1e094718ec73523430923a11d6db1ca267faf57d9dfeb12c670c753f795_w640_q70.webp",
      "contributions": "",
      "summary": "ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training",
      "mindmap": ""
    },
    {
      "title": "Constrained Cuts, Flows, and Lattice-Linearity",
      "authors": "Robert Streit, Vijay K. Garg",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18141",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4988963d0df9feb5d6d93950c8695a282967332a6a2d340e258ed92a056f4c86_w640_q70.webp",
      "contributions": "",
      "summary": "Constrained Cuts, Flows, and Lattice-Linearity",
      "mindmap": ""
    },
    {
      "title": "TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale",
      "authors": "Dongha Yoon, Younghoon Min, Hoshik Kim, Sam H. Noh, Jongryool Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18194",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf126804dfba85c7a794b9b5687408dce6800961fba23c8342730d926fc068da_w640_q70.webp",
      "contributions": "",
      "summary": "TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale",
      "mindmap": ""
    },
    {
      "title": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
      "authors": "Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18318",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp",
      "contributions": "",
      "summary": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
      "mindmap": ""
    },
    {
      "title": "Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching",
      "authors": "Hussein Amro, Basel Fakhri, Amer E. Mouawad, Izzat El Hajj",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18334",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b31bc47ecb1dc74e237f8d5df239958727df951c6edb1120b903f3fd7b5c55be_w640_q70.webp",
      "contributions": "",
      "summary": "Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching",
      "mindmap": ""
    },
    {
      "title": "Snowveil: A Framework for Decentralised Preference Discovery",
      "authors": "Grammateia Kotsialou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18444",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8761ec2c77d131e1f61b3af656dc162d45194670910e8c8975220f93bfc1af6_w640_q70.webp",
      "contributions": "",
      "summary": "Snowveil: A Framework for Decentralised Preference Discovery",
      "mindmap": ""
    },
    {
      "title": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing",
      "authors": "Wentao Liu, Yuhao Hu, Ruiting Zhou, Baochun Li, Ne Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18674",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b0a6c1ba7d729d7d1a45d1f2d74caedc5189c982e32587fba450b708786cd88_w640_q70.webp",
      "contributions": "",
      "summary": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing",
      "mindmap": ""
    },
    {
      "title": "A Real-Time Digital Twin for Adaptive Scheduling",
      "authors": "Yihe Zhang, Yash Kurkure, Yiheng Tao, Michael E. Papka, Zhiling Lan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18894",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/228b6575bd24119fca42b0b1a9ba6a42e15166f06d7d2264d4bc894aff71d4ed_w640_q70.webp",
      "contributions": "",
      "summary": "A Real-Time Digital Twin for Adaptive Scheduling",
      "mindmap": ""
    },
    {
      "title": "QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits",
      "authors": "Ivan Čilić, Ivana Podnar Žarko, Pantelis Frangoudis, Schahram Dustdar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18915",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9dc5e7bd9261487c82b2942a0f628fd37b770391416336515d537b8d9c7608d_w640_q70.webp",
      "contributions": "",
      "summary": "QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits",
      "mindmap": ""
    },
    {
      "title": "Timely Parameter Updating in Over-the-Air Federated Learning",
      "authors": "Jiaqi Zhu, Zhongyuan Zhao, Xiao Li, Ruihao Du, Shi Jin, Howard H.Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19103",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fc52e9cecab5d074f9763764f769e7c7bd5aaa186ce1d747f2bbd3fa2caf41_w640_q70.webp",
      "contributions": "",
      "summary": "Timely Parameter Updating in Over-the-Air Federated Learning",
      "mindmap": ""
    },
    {
      "title": "Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT",
      "authors": "Murtaza Rangwala, Richard O. Sinnott, Rajkumar Buyya",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19131",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5ab0acf932ea7b2d42fac6b935c509aaf0582fb4879eb9fae7e0fe0a8e7f766_w640_q70.webp",
      "contributions": "",
      "summary": "Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT",
      "mindmap": ""
    },
    {
      "title": "L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling",
      "authors": "Yitao Yuan, Chenqi Zhao, Bohan Zhao, Zane Cao, Yongchao He, Wenfei Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19179",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a06667660cf3663082a8dcc7b41e7338eae070225a51b761512a3dfc2c89548_w640_q70.webp",
      "contributions": "",
      "summary": "L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling",
      "mindmap": ""
    },
    {
      "title": "Simulations between Strongly Sublinear MPC and Node-Capacitated Clique",
      "authors": "Philipp Schneider, Julian Werthmann",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19326",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8644a50509dfa5e351f02bea3451463ff322106a9319b73a700313df6f2ab2a4_w640_q70.webp",
      "contributions": "",
      "summary": "Simulations between Strongly Sublinear MPC and Node-Capacitated Clique",
      "mindmap": ""
    },
    {
      "title": "Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives",
      "authors": "Kiril Dichev, Filip Pawlowski, Albert-Jan Yzelman",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19342",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2babd04ddf70f201df2fa1a003998a91a1e266029f2a3a118314f226a7ce88f0_w640_q70.webp",
      "contributions": "",
      "summary": "Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives",
      "mindmap": ""
    },
    {
      "title": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference",
      "authors": "George Karfakis, Faraz Tahmasebi, Binglu Chen, Lime Yao, Saptarshi Mitra, Tianyue Pan, Hyoukjun Kwon, Puneet Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19606",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba70ec7306532ca1c35d62f262fda2524e63fa17cc3a261b1800c846a6c06b2_w640_q70.webp",
      "contributions": "",
      "summary": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference",
      "mindmap": ""
    },
    {
      "title": "EuroHPC SPACE CoE: Redesigning Scalable Parallel Astrophysical Codes for Exascale",
      "authors": "Nitin Shukla, Alessandro Romeo, Caterina Caravita, Lubomir Riha, Ondrej Vysocky, Petr Strakos, Milan Jaros, João Barbosa, Radim Vavrik, Andrea Mignone, Marco Rossazza, Stefano Truzzi, Vittoria Berta, Iacopo Colonnelli, Doriana Medić, Elisabetta Boella, Daniele Gregori, Eva Sciacca, Luca Tornatore, Giuliano Taffoni, Pranab J. Deka, Fabio Bacchini, Rostislav-Paul Wilhelm, Georgios Doulis, Khalil Pierre, Luciano Rezzolla, Tine Colman, Benoît Commerçon, Othman Bouizi, Matthieu Kuhn, Erwan Raffin, Marc Sergent, Robert Wissing, Guillermo Marin, Klaus Dolag, Geray S. Karademir, Gino Perna, Marisa Zanotti, Sebastian Trujillo-Gomez",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18883",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/858132a58cba05e698ccaa1c8a830fb0c87d0b8772070099bf19455acf265c4e_w640_q70.webp",
      "contributions": "",
      "summary": "EuroHPC SPACE CoE: Redesigning Scalable Parallel Astrophysical Codes for Exascale",
      "mindmap": ""
    },
    {
      "title": "Dion2: A Simple Method to Shrink Matrix in Muon",
      "authors": "Kwangjun Ahn, Noah Amsel, John Langford",
      "institution": "Microsoft Research, AI Frontiers, NYU",
      "link": "https://arxiv.org/pdf/2512.16928",
      "code": null,
      "tags": [
        "llm training",
        "Muon optimizer",
        "orthonormalization",
        "matrix shrinking",
        "sampling",
        "Newton-Schulz iterations",
        "FSDP2"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces Dion2, a simple method to reduce the computational cost of the Muon optimizer by sampling a fraction of rows or columns for orthonormalization at each iteration. This sparsifies the update, lowering computation and communication overhead. The method maintains update quality close to full Muon while improving scalability, as shown in training benchmarks.",
      "mindmap": ""
    },
    {
      "title": "Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning",
      "authors": "Baolei Zhang, Minghong Fang, Zhuqing Liu, Biao Yi, Peizhao Zhou, Yuan Wang, Tong Li, Zheli Liu",
      "institution": "Nankai University, University of Louisville, University of North Texas",
      "link": "https://arxiv.org/pdf/2512.17254",
      "code": null,
      "tags": [
        "fault-tolerance",
        "federated learning",
        "byzantine-robust aggregation",
        "privacy-preserving",
        "dimensionality reduction",
        "secure multi-party computation",
        "adaptive tuning"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes ABBR, a practical framework for federated learning that combines Byzantine-robust aggregation with privacy-preserving techniques. Its core method uses dimensionality reduction to speed up private computations and an adaptive tuning strategy to minimize the impact of malicious models. The framework is shown to run significantly faster with minimal overhead while maintaining strong Byzantine resilience.",
      "mindmap": ""
    },
    {
      "title": "Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs",
      "authors": "Ivan Kralj, Lodovico Giaretta, Gordan Ježić, Ivana Podnar Žarko, Šarūnas Girdzijauskas",
      "institution": "University of Zagreb, Faculty of Electrical Engineering and Computing; RISE Research Institutes of Sweden; KTH Royal Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.17352",
      "code": null,
      "tags": [
        "others",
        "adaptive graph pruning",
        "Spatio-Temporal Graph Neural Networks (ST-GNNs)",
        "Sudden Event Prediction Accuracy (SEPA)",
        "online semi-decentralized training",
        "Federated Learning (FL)",
        "Gossip Learning"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a9feeec6bf4367fbf82a987484881d47da3c3be2e4b769373b648eb200942b_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes an adaptive graph pruning algorithm for Spatio-Temporal Graph Neural Networks (ST-GNNs) to reduce communication overhead in online semi-decentralized traffic prediction systems. It also introduces a novel evaluation metric, SEPA, to measure responsiveness to sudden traffic events. The method maintains prediction accuracy while significantly lowering communication costs across different decentralized learning settings.",
      "mindmap": ""
    },
    {
      "title": "Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing",
      "authors": "Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng",
      "institution": "Wuhan University",
      "link": "https://arxiv.org/pdf/2512.17574",
      "code": null,
      "tags": [
        "multi-modal inference",
        "GPU-internal scheduling",
        "resource sharing",
        "collaborative multi-GPU video decoding",
        "logically decoupled execution",
        "FlashCodec",
        "UnifiedServe"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65d961bcef453cdff273d0991a97111419acf476f8d34e21f66dbd356156dfa7_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes FlashCodec and UnifiedServe, a framework that optimizes multimodal large language model (MLLM) serving by accelerating video decoding and enabling resource sharing across the vision and text stages. This approach reduces latency and eliminates inter-stage blocking, leading to significantly higher throughput and better SLO adherence compared to existing systems.",
      "mindmap": ""
    },
    {
      "title": "SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference",
      "authors": "Jeff Smith",
      "institution": "2nd Set AI",
      "link": "https://arxiv.org/pdf/2512.15742",
      "code": null,
      "tags": [
        "llm inference",
        "vector quantization",
        "spline networks",
        "memory optimization",
        "cache optimization",
        "hardware-aware compilation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces SHARe-KAN, a framework that uses Gain-Shape-Bias Vector Quantization to compress Kolmogorov-Arnold Networks (KANs) by exploiting functional redundancy while preserving their dense, holographic topology. Coupled with a hardware-aware compiler called LUTHAM, it achieves an 88x reduction in runtime memory while matching baseline accuracy, effectively decoupling the workload from DRAM bandwidth constraints.",
      "mindmap": ""
    },
    {
      "title": "LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models",
      "authors": "Yijie Zhi, Yayu Cao, Jianhua Dai, Xiaoyang Han, Jingwen Pu, Qingran Wu, Sheng Cheng, Ming Cai",
      "institution": "Zhejiang University, Zhejiang Institute of Administration, Beijing ShenZhou Aerospace Software Technology Ltd.",
      "link": "https://arxiv.org/pdf/2512.15766",
      "code": null,
      "tags": [
        "llm inference",
        "retrieval-augmented generation",
        "loop transformation",
        "static control part",
        "feedback-based iterative mechanism",
        "equivalence checking"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes LOOPRAG, a retrieval-augmented generation framework that guides Large Language Models (LLMs) to perform effective loop transformation optimization. It uses a loop-aware retrieval algorithm and a feedback-based iterative mechanism with compilation and testing to generate correct and efficient code. The evaluation shows LOOPRAG achieves significant speedups over both traditional compilers and base LLMs on standard benchmark suites.",
      "mindmap": ""
    },
    {
      "title": "Optimizing Agentic Language Model Inference via Speculative Tool Calls",
      "authors": "Daniel Nichols, Prajwal Singhania, Charles Jekel, Abhinav Bhatele, Harshitha Menon",
      "institution": "Lawrence Livermore National Laboratory, University of Maryland",
      "link": "https://arxiv.org/pdf/2512.15834",
      "code": null,
      "tags": [
        "llm inference",
        "speculative tool calls",
        "tool cache",
        "vLLM",
        "prefix-caching"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces system optimizations for language model agents that use external tools, specifically by speculating future tool calls and keeping sequences resident in the inference engine to reduce overhead. These methods lead to significant throughput improvements of hundreds of tokens per second. The authors also propose a new \"tool cache\" API to facilitate adoption of these optimizations.",
      "mindmap": ""
    },
    {
      "title": "Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference",
      "authors": "Jian Tian, Shuailong Li, Yang Cao, Wenbo Cui, Minghan Zhu, Wenkang Wu, Jianming Zhang, Yanpeng Wang, Zhiwen Xiao, Zhenyu Hou, Dou Shen",
      "institution": "Baidu Inc.",
      "link": "https://arxiv.org/pdf/2512.16134",
      "code": null,
      "tags": [
        "llm inference",
        "staggered batch scheduling",
        "load-aware global allocation",
        "DP+EP",
        "time-to-first-token",
        "throughput",
        "data parallelism",
        "expert parallelism"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes Staggered Batch Scheduling (SBS), a method that buffers requests to form optimal batches before dispatching them to a DP+EP inference cluster, eliminating internal queuing. It also introduces a Load-Aware Global Allocation strategy to balance computational load. The system reduces Time-to-First-Token by 30-40% and improves throughput by 15-20% compared to immediate scheduling baselines.",
      "mindmap": ""
    },
    {
      "title": "Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference",
      "authors": "Dhruv Deshmukh, Saurabh Goyal, Nipun Kwatra, Ramachandran Ramjee",
      "institution": "Microsoft Research India",
      "link": "https://arxiv.org/pdf/2512.16391",
      "code": null,
      "tags": [
        "llm inference",
        "sparse attention",
        "dynamic programming",
        "top-k selection",
        "anchor layers",
        "reuse layers",
        "FlashAttention-3"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes Kascade, a training-free sparse attention method that accelerates long-context LLM inference by computing exact Top-k indices in selected anchor layers and reusing them in intermediate layers, based on the stability of high-weight keys across layers. It uses a dynamic programming algorithm to select anchor layers and achieves significant speedups in both prefill and decode phases while maintaining accuracy close to dense attention on benchmarks.",
      "mindmap": ""
    },
    {
      "title": "AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research",
      "authors": "Ignacio Heredia, Álvaro López García, Germán Moltó, Amanda Calatrava, Valentin Kozlov, Alessandro Costantini, Viet Tran, Mario David, Daniel San Martín, Marcin Płóciennik, Marta Obregón Ruiz, Saúl Fernandez, Judith Sáinz-Pardo Díaz, Miguel Caballer, Caterina Alarcón Marín, Stefan Dlugolinsky, Martin Šeleng, Lisana Berberi, Khadijeh Alibabaei, Borja Esteban Sanchis, Pedro Castro, Giacinto Donvito, Diego Aguirre, Sergio Langarita, Vicente Rodriguez, Leonhard Duda, Andrés Heredia Canales, Susana Rebolledo Ruiz, João Machado, Giang Nguyen, Fernando Aguilar Gómez, Jaime Díez",
      "institution": "Instituto de Física de Cantabria (IFCA), Instituto de Instrumentación para Imagen Molecular (I3M), Institute of Informatics, Slovak Academy of Sciences (IISAS), Istituto Nazionale di Fisica Nucleare (INFN), Karlsruher Institut für Technologie, Poznańskie Centrum Superkomputerowo Sieciowe, Centro Nacional de Computação Avançada (CNCA)",
      "link": "https://arxiv.org/pdf/2512.16455",
      "code": null,
      "tags": [
        "cluster infrastructure",
        "federated cloud platform",
        "service catalogue",
        "interactive development environments",
        "GPU resources",
        "annotation tools",
        "experiment tracking",
        "federated learning",
        "model deployment",
        "traceability",
        "reproducibility"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper presents AI4EOSC, a federated cloud platform designed to support the full machine learning lifecycle for scientific research. The platform integrates distributed e-Infrastructures to provide consistent access to development environments, GPU training, annotation tools, and deployment options. Its main conclusion is that this integrated, customizable platform lowers adoption barriers and facilitates reproducible AI workflows for the scientific community.",
      "mindmap": ""
    },
    {
      "title": "Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint",
      "authors": "Endar Suprih Wihidayat, Sieteng Soh, Kwan-Wu Chin, Duc-son Pham",
      "institution": "Sebelas Maret University, Curtin University, University of Wollongong",
      "link": "https://arxiv.org/pdf/2512.16792",
      "code": null,
      "tags": [
        "edge computing",
        "mixed integer linear programming",
        "heuristic algorithm",
        "task offloading",
        "server deployment",
        "budget constraint"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a Multi-stage Edge Server Upgrade (M-ESU) framework, solved via an optimal Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). The main conclusion is that the heuristic solution performs close to optimal for small networks and significantly outperforms alternative strategies in large-scale networks, improving task satisfaction by up to 21.57% under budget and demand growth constraints.",
      "mindmap": ""
    },
    {
      "title": "Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning",
      "authors": "Bahman Abolhassani, Tugba Erpek, Kemal Davaslioglu, Yalin E. Sagduyu, Sastry Kompella",
      "institution": "Nexcepta",
      "link": "https://arxiv.org/pdf/2512.16813",
      "code": null,
      "tags": [
        "others",
        "multi-agent reinforcement learning",
        "QMIX",
        "reactive jamming",
        "channel hopping",
        "power control",
        "Upper Confidence Bound"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a multi-agent reinforcement learning framework based on the QMIX algorithm to coordinate anti-jamming strategies in swarm networks. The method enables agents to jointly select transmission channels and power levels to counter an adaptive reactive jammer. The results show that QMIX achieves near-optimal performance, higher throughput, and lower jamming incidence compared to baseline policies, demonstrating its effectiveness for securing swarm communications.",
      "mindmap": ""
    },
    {
      "title": "Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies",
      "authors": "Astrid Brull, Sara Aguti, Véronique Bolduc, Ying Hu, Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del-Rio, Oleksii Sliusarenko, Haiyan Zhou, Francesco Muntoni, Carsten G. Bönnemann, Xabi Uribe-Etxebarria",
      "institution": "National Institute of Neurological Disorders and Stroke, National Institutes of Health; University College London; Sherpa.ai",
      "link": "https://arxiv.org/pdf/2512.16876",
      "code": null,
      "tags": [
        "others",
        "federated learning",
        "immunofluorescence microscopy",
        "collagen VI-related dystrophies",
        "rare disease diagnosis",
        "decentralized training"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper applies Federated Learning (FL) to train a diagnostic model for a rare disease using collagen VI immunofluorescence images from decentralized datasets across multiple institutions. This approach addresses data scarcity and privacy concerns by keeping patient data local. The resulting FL model outperformed single-institution models, demonstrating improved diagnostic accuracy and generalizability for classifying collagen VI-related dystrophies.",
      "mindmap": ""
    },
    {
      "title": "Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation",
      "authors": "Unai Laskurain, Aitor Aguirre-Ortuzar, Urko Zurutuza",
      "institution": "Mondragon Unibertsitatea",
      "link": "https://arxiv.org/pdf/2512.14767",
      "code": null,
      "tags": [
        "others",
        "Shapley-CMI",
        "Private Set Intersection",
        "Conditional Mutual Information",
        "Vertical Federated Learning",
        "data valuation"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a privacy-preserving method for evaluating feature contributions in Vertical Federated Learning (VFL) using Shapley-CMI and a Private Set Intersection (PSI) server to compute encrypted intersection sizes without sharing raw data. The system enables secure and fair data valuation before model training. Initial experiments confirm the approach's correctness and privacy, demonstrating its viability for secure feature contribution estimation in VFL.",
      "mindmap": ""
    },
    {
      "title": "LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs",
      "authors": "Erik Schultheis, Dan Alistarh",
      "institution": "IST Austria",
      "link": "https://arxiv.org/pdf/2512.15306",
      "code": null,
      "tags": [
        "llm training",
        "8-bit training",
        "activation checkpointing",
        "offloading",
        "copy-engine collectives",
        "dynamic tensor-level scaling",
        "ZeRO-1"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "LLMQ is an end-to-end CUDA/C++ framework that enables efficient 8-bit pretraining and fine-tuning of medium-sized language models on consumer GPUs by employing optimizations like activation checkpointing, offloading, and copy-engine based collectives to overcome memory and communication bottlenecks. It demonstrates that models up to 32B parameters can be trained on affordable hardware like a 4xRTX 4090 workstation while maintaining high FLOP utilization, rivaling the efficiency of production systems on more expensive cloud-grade GPUs.",
      "mindmap": ""
    },
    {
      "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX",
      "authors": "Xuting Liu, Daniel Alexander, Siva Kesava Reddy Kakarla, Behnaz Arzani, Vincent Liu",
      "institution": "University of Pennsylvania, Microsoft Research",
      "link": "https://arxiv.org/pdf/2512.15705",
      "code": null,
      "tags": [
        "llm inference",
        "early-exit",
        "dynamic rebatching",
        "copy-free buffer",
        "SLA-aware scheduler",
        "KV cache",
        "state-copying"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes Dynamic Rebatching and the DREX system to efficiently batch requests in Early-Exit LLMs, where tokens can exit at different layers. DREX dynamically reorganizes batches at exit points using a copy-free buffer and a predictive scheduler, improving throughput by 2-12% while eliminating involuntary exits and preserving output quality.",
      "mindmap": ""
    }
  ]
}