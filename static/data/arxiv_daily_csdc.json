{
  "label": "cs.DC",
  "slug": "csdc",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems",
      "authors": "Jithin VG, Ditto PS",
      "institution": "Bud Ecosystem Inc",
      "link": "https://arxiv.org/pdf/2512.22125",
      "code": "https://github.com/BudEcosystem/GPU-Virt-Bench",
      "tags": [
        "llm inference",
        "GPU Virtualization",
        "Benchmarking",
        "Multi-tenancy",
        "CUDA",
        "Performance Isolation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0a1c9f2d4dfba1fc452a424ad0f1298f01afe6d95dfd39dd2ff3f0c1bac9430c_w640_q70.webp",
      "contributions": "1. Proposed GPU-Virt-Bench, a comprehensive benchmarking framework with 56 metrics across 10 categories for evaluating software-based GPU virtualization systems. 2. Enabled systematic comparison between software virtualization approaches (e.g., HAMi-core, BUD-FCSP) and ideal hardware-based MIG behavior. 3. Demonstrated the framework's utility by revealing critical performance characteristics for production deployment decisions in multi-tenant environments.",
      "summary": "This paper addresses the lack of standardized evaluation for software-based GPU virtualization systems, which are needed for efficient GPU sharing in AI/LLM workloads. The authors propose GPU-Virt-Bench, a comprehensive benchmarking framework that measures performance across multiple critical dimensions. The framework provides actionable insights for practitioners by comparing software solutions against hardware-based baselines.",
      "mindmap": "graph TB\n        A[GPU-Virt-Bench: A Comprehensive Benchmarking Framework] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[GPU资源共享需求高，但软件虚拟化方案缺乏标准化评估/High demand for GPU sharing, but software virtualization lacks standardized evaluation]\n        C --> C1[提出包含56个指标、10个类别的综合基准测试框架/Propose a comprehensive benchmarking framework with 56 metrics across 10 categories]\n        D --> D1[系统比较软件方案与MIG，为生产部署提供关键性能洞察/Systematic comparison between software approaches and MIG provides key performance insights for deployment]"
    },
    {
      "title": "SoDA: An Efficient Interaction Paradigm for the Agentic Web",
      "authors": "Zicai Cui, Zhouyuan Jian, Weiwen Liu, Weinan Zhang",
      "institution": "Shanghai Jiao Tong University, Shanghai Innovation Institute",
      "link": "https://arxiv.org/pdf/2512.22135",
      "code": null,
      "tags": [
        "agent system",
        "Sovereign Digital Avatar",
        "Intent-Permission Handshake",
        "orthogonal decoupling",
        "A2A protocols",
        "dual-factor adaptive routing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ef4bb1bba1b84bcf1102a80dc39b16d212412d68a7abea9ab0aac1dc9e23dedb_w640_q70.webp",
      "contributions": "1. Proposes a user sovereignty interaction paradigm for the Agentic Web, decoupling memory from application logic to break data lock-in and shifting from explicit instruction to implicit intent alignment to reduce cognitive load. 2. Implements the paradigm via the Sovereign Digital Avatar (SoDA) with an orthogonal decoupling design of storage, computation, and interaction, establishing the principle of \"data as a persistent asset, model as a transient tool\". 3. Designs an Intent-Permission Handshake Mechanism based on A2A protocols with dual-factor adaptive routing for active risk governance in zero-trust environments.",
      "summary": "This paper proposes the Sovereign Digital Avatar (SoDA), a new interaction paradigm for the Agentic Web that decouples user memory from applications and uses intent alignment to reduce cognitive load. It introduces an architecture with orthogonal decoupling and a secure handshake mechanism for zero-trust environments. Empirical results show it significantly reduces token consumption and user cognitive load compared to existing methods.",
      "mindmap": "graph TB\n        A[SoDA: An Efficient Interaction Paradigm for the Agentic Web] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[数据锁定/Data Lock-in]\n        B --> B2[认知过载/Cognitive Overload]\n        C --> C1[主权数字化身/Sovereign Digital Avatar (SoDA)]\n        C --> C2[正交解耦设计/Orthogonal Decoupling Design]\n        C --> C3[意图-权限握手机制/Intent-Permission Handshake Mechanism]\n        D --> D1[降低令牌消耗/Reduces Token Consumption by 27-35%]\n        D --> D2[降低认知负载/Reduces Cognitive Load by 72% vs RAG, 88% vs Manual]"
    },
    {
      "title": "SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware",
      "authors": "Mahadev Sunil Kumar, Arnab Raha, Debayan Das, Gopakumar G, Amitava Mukherjee",
      "institution": "Accenture PLC, Intel Corporation, Indian Institute of Science, Amrita Vishwa Vidyapeetham, Birla Institute of Technology and Science",
      "link": "https://arxiv.org/pdf/2512.22136",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "Structured Pruning",
        "Multi-Objective Optimization",
        "Edge Inference",
        "MVCNN",
        "View-Adaptive Compression"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7687c3e58bfa2b22573745dffb608fb7c36a0c339dd9216829f78a284f51e662_w640_q70.webp",
      "contributions": "1. Proposes a framework for lightweight DNN deployment that integrates structured pruning with multi-objective optimization to meet heterogeneous hardware constraints. 2. Demonstrates the framework on MVCNN by quantifying the contribution of individual views to accuracy for view-adaptive pruning budget allocation. 3. Shows experimentally that the compressed models meet user-specified accuracy and memory bounds while achieving 1.2x to 5.0x inference speedup across diverse hardware.",
      "summary": "The paper addresses the challenge of deploying large DNNs on resource-constrained edge devices. It proposes SlimEdge, a method that combines structured pruning and multi-objective optimization to compress models like MVCNN while preserving task performance. The results show that this approach successfully meets specified accuracy and memory constraints while significantly reducing inference latency on various edge hardware platforms.",
      "mindmap": "graph TB\n        A[SlimEdge: Lightweight Distributed DNN Deployment] --> B(核心问题/Problem: DNN部署在资源受限的边缘设备上/DNN deployment on resource-constrained edge devices)\n        A --> C(主要方法/Method: 结构化剪枝与多目标优化/Structured Pruning & Multi-Objective Optimization)\n        A --> D(关键结果/Results: 满足精度与内存约束，推理延迟降低1.2x-5.0x/Meets accuracy & memory bounds, 1.2x-5.0x latency reduction)"
    },
    {
      "title": "HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration",
      "authors": "Jiangwen Dong, Jiayu Li, Wanyu Lin",
      "institution": "The Hong Kong Polytechnic University",
      "link": "https://arxiv.org/pdf/2512.22137",
      "code": null,
      "tags": [
        "llm inference",
        "edge-cloud collaboration",
        "task decomposition",
        "adaptive routing",
        "parallel execution",
        "token-efficient inference"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a14e12f757065eef8f857ead7c55331977412b8a4d1ba64499c5c1457d797284_w640_q70.webp",
      "contributions": "1. Proposes HybridFlow, a resource-adaptive inference framework for collaborative reasoning between edge and cloud LLMs. 2. Introduces a two-stage method involving dynamic task decomposition for parallel execution and a learned router for resource-aware subtask assignment. 3. Demonstrates effectiveness in reducing end-to-end inference time and token usage while maintaining accuracy on multiple reasoning benchmarks.",
      "summary": "This paper addresses the challenges of high latency and token cost for LLM inference on edge devices by proposing HybridFlow, a framework that dynamically decomposes queries into parallel subtasks and adaptively routes them between edge and cloud models. The method reduces inference time and token consumption while preserving competitive accuracy, as validated on several reasoning benchmarks.",
      "mindmap": "graph TB\n        Root(”HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”LLM推理延迟高，Token消耗大/High LLM inference latency & token cost”)\n        Problem --> P2(”边缘设备资源受限/Resource-limited edge devices”)\n        Problem --> P3(”现有协作方法粗粒度，效率低/Existing coarse-grained collaboration is inefficient”)\n        Method --> M1(”任务分解与并行执行/Task Decomposition & Parallel Execution”)\n        Method --> M2(”资源感知子任务路由/Resource-Aware Subtask Routing”)\n        Results --> R1(”减少端到端推理时间/Reduces end-to-end inference time”)\n        Results --> R2(”降低总体Token使用/Lowers overall token usage”)\n        Results --> R3(”保持有竞争力的准确率/Maintains competitive accuracy”)"
    },
    {
      "title": "HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA",
      "authors": "Amur Saqib Pal, Muhammad Mohsin Ghaffar, Faisal Shafait, Christian Weis, Norbert Wehn",
      "institution": "National University of Sciences and Technology (Pakistan), RPTU Kaiserslautern-Landau (Germany)",
      "link": "https://arxiv.org/pdf/2512.22139",
      "code": "https://github.com/dll-ncai/HLS4PC",
      "tags": [
        "on-device ai",
        "FPGA",
        "HLS",
        "Point Cloud",
        "Model Compression",
        "Fixed-Point"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/21433bfda0767bbdfcee46f13fb3acd9373d13bb741d87a755643767c9ad74f9_w640_q70.webp",
      "contributions": "1. Proposed HLS4PC, a parameterizable HLS framework for accelerating point-based 3D point cloud models on FPGA. 2. Introduced PointMLP-Lite, a 4x less complex model variant created via hardware-aware compression techniques (URS, quantization, pruning, fusion). 3. Demonstrated FPGA acceleration achieving 3.56x higher throughput than prior work and outperforming GPU/CPU implementations.",
      "summary": "This paper addresses the challenge of real-time 3D point cloud processing by proposing HLS4PC, a parameterizable FPGA acceleration framework. The method combines algorithmic optimizations and hardware-aware model compression to create an efficient fixed-point implementation, which significantly outperforms previous accelerators and GPU/CPU baselines in throughput.",
      "mindmap": "graph TB\n        Root[HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[GPU under-utilization due to sparse, unstructured point cloud data]\n        P1 --> P2[High memory/computation demand hinders real-time performance]\n        Method[主要方法/Method] --> M1[Parameterizable HLS framework for FPGA]\n        M1 --> M2[Hardware-aware compression: URS, quantization, pruning, fusion]\n        M2 --> M3[Creates PointMLP-Lite model]\n        Results[关键结果/Results] --> R1[PointMLP-Lite: 4x less complex, ~2% accuracy drop]\n        R1 --> R2[3.56x higher throughput vs. prior work]\n        R2 --> R3[2.3x (GPU) and 22x (CPU) higher throughput]"
    },
    {
      "title": "On Harnessing Idle Compute at the Edge for Foundation Model Training",
      "authors": "Leyang Xue, Meghana Madhyastha, Myungjin Lee, Amos Storkey, Randal Burns, Mahesh K. Marina",
      "institution": "The University of Edinburgh, Johns Hopkins University, Cisco Research",
      "link": "https://arxiv.org/pdf/2512.22142",
      "code": null,
      "tags": [
        "llm training",
        "edge computing",
        "tensor parallelism",
        "parameter server",
        "device heterogeneity",
        "fault-tolerance"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fefe0f72fa70ae7be2cad54f15e74f96fd08cc506630060214e298521278148_w640_q70.webp",
      "contributions": "1. A novel selective hybrid tensor parallelism method to finely partition training operations for edge devices. 2. A parameter server-centric training framework to cope with device memory limits and avoid communication bottlenecks. 3. A cost optimization model to guide device selection and workload distribution, effectively handling device heterogeneity and churn.",
      "summary": "The paper addresses the challenge of decentralized foundation model training on edge devices, which is hindered by memory limits, communication overhead, and device heterogeneity. It proposes Cleave, a new paradigm that uses selective hybrid tensor parallelism and a parameter server framework to partition training efficiently. The evaluation shows Cleave matches cloud-based training performance, scales to thousands of devices, and handles failures with much faster recovery than prior methods.",
      "mindmap": "graph TB\n    A[On Harnessing Idle Compute at the Edge for Foundation Model Training] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[现有边缘训练方法性能不足/Existing edge training falls short]\n    B --> B2[设备内存与通信瓶颈/Device memory & communication bottlenecks]\n    B --> B3[设备异构性与动态性/Device heterogeneity & dynamism]\n    C --> C1[选择性混合张量并行/Selective hybrid tensor parallelism]\n    C --> C2[参数服务器框架/Parameter server framework]\n    C --> C3[成本优化模型/Cost optimization model]\n    D --> D1[匹配云端训练性能/Matches cloud-based training]\n    D --> D2[扩展至数千设备/Scales to thousands of devices]\n    D --> D3[快速故障恢复/Fast failure recovery]"
    },
    {
      "title": "GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs",
      "authors": "Ruifan Chu, Anbang Wang, Xiuxiu Bai, Shuai Liu, Xiaoshe Dong",
      "institution": "School of Software Engineering, Xi’an Jiaotong University",
      "link": "https://arxiv.org/pdf/2512.22147",
      "code": null,
      "tags": [
        "gpu kernels",
        "Minimal Executable Program (MEP)",
        "Automatic Error Repair",
        "Performance Pattern Inheritance",
        "iterative optimization",
        "cross-platform"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp",
      "contributions": "1. Proposes an end-to-end LLM framework that optimizes GPU kernels by constructing Minimal Executable Programs (MEPs) to avoid expensive full application builds and executions. 2. Introduces Automatic Error Repair and Performance Pattern Inheritance to automatically fix faults and reuse effective optimization strategies, reducing search cost. 3. Demonstrates cross-platform portability and effectiveness on NVIDIA GPUs and the Haiguang DCU platform, achieving significant speedups over direct LLM optimization.",
      "summary": "The paper addresses the high cost of full builds for GPU kernel optimization in large HPC applications by proposing an LLM framework that uses Minimal Executable Programs (MEPs) for iterative optimization. The method integrates automatic error repair and performance pattern inheritance to maintain correctness and reuse strategies. It achieves substantial speedups across different hardware platforms without requiring full-source dependencies.",
      "mindmap": "graph TB\n        A[GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Full builds & runs are expensive in large applications/大型应用中完整构建与运行成本高]\n        C --> C1[Construct Minimal Executable Program (MEP) for kernel/为内核构建最小可执行程序]\n        C --> C2[Multi-round iterative optimization with LLM feedback/基于LLM反馈的多轮迭代优化]\n        C --> C3[Integrate Automatic Error Repair & Performance Pattern Inheritance/集成自动错误修复与性能模式继承]\n        D --> D1[Achieves significant speedups (e.g., 5.05x, 7.77x)/获得显著加速比]\n        D --> D2[Cross-platform portability (NVIDIA, DCU)/跨平台可移植性]\n        D --> D3[Surpasses direct LLM optimization/超越直接LLM优化]"
    },
    {
      "title": "Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments",
      "authors": "Guilin Zhang, Wulan Guo, Ziqi Tan",
      "institution": "George Washington University",
      "link": "https://arxiv.org/pdf/2512.22149",
      "code": null,
      "tags": [
        "agent system",
        "serverless computing",
        "GPU resource allocation",
        "workload scheduling",
        "multi-agent systems",
        "collaborative reasoning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2fe7e30427c00e4689f161fb9912d4d11cc091ed6dd1dae3c4ea2c5805084e3b_w640_q70.webp",
      "contributions": "1. An adaptive GPU resource allocation framework for multi-agent systems in serverless environments that dynamically adjusts resources based on workload characteristics, agent priorities, and minimum requirements. 2. An O(N) complexity algorithm for real-time adaptation, enabling millisecond-scale reallocation to handle dynamic workload fluctuations. 3. A comprehensive evaluation demonstrating the framework's superiority over static and round-robin strategies, achieving 85% latency reduction while maintaining throughput and improving GPU utilization and cost-efficiency.",
      "summary": "This paper proposes an adaptive GPU resource allocation framework to address the challenge of efficiently deploying heterogeneous multi-agent AI systems on serverless platforms. The method dynamically allocates resources using a real-time algorithm to handle varying computational demands and workload fluctuations. The results show it significantly reduces latency compared to baseline schedulers while maintaining throughput, offering a cost-effective solution for serverless multi-agent deployment.",
      "mindmap": "graph TB\n        Root[”Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments<br/>面向无服务器环境的多智能体协同推理的自适应GPU资源分配”] --> Problem[”核心问题/Problem<br/>Heterogeneous agent workloads & dynamic demands on serverless GPU platforms<br/>多智能体工作负载异构与无服务器GPU平台动态需求”]\n        Root --> Method[”主要方法/Method<br/>Adaptive GPU resource allocation framework with O(N) real-time algorithm<br/>基于O(N)实时算法的自适应GPU资源分配框架”]\n        Root --> Results[”关键结果/Results<br/>85% latency reduction vs. round-robin, maintains throughput<br/>相比轮询调度延迟降低85%，保持吞吐量”]"
    },
    {
      "title": "TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures",
      "authors": "Wei Li, Zhenyu Bai, Heru Wang, Pranav Dangi, Zhiqiang Zhang, Cheng Tan, Huiying Lan, Weng-Fai Wong, Tulika Mitra",
      "institution": "National University of Singapore, Arizona State University, Google, Lumai Ltd.",
      "link": "https://arxiv.org/pdf/2512.22168",
      "code": null,
      "tags": [
        "compiler & ir",
        "spatial dataflow",
        "tile-based compilation",
        "MLIR",
        "on-chip network",
        "hardware representation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cce8d87d1dd357c986e9809985cc87b6430fd568820f39521500addb34e7eef7_w640_q70.webp",
      "contributions": "1. An end-to-end compiler framework (TL) that compiles tile-based programs (e.g., Triton kernels) onto spatial dataflow architectures, focusing on distributing tile instances across cores. 2. A novel hardware representation that captures interconnect topology, memory hierarchy, and compute capabilities to enable architecture-specific optimizations and support diverse targets. 3. A practical implementation built on the MLIR ecosystem, providing a generic entry point for different front-ends and an end point for different back-ends, demonstrated with performance gains over vendor libraries.",
      "summary": "This paper presents TL, an end-to-end compiler framework that tackles the limited programmability of spatial dataflow accelerators by automatically mapping tile-based workloads across distributed cores to optimize data reuse and reduce communications. TL introduces a hardware-aware representation and is built on MLIR to support diverse targets. Experiments show it can match or exceed the performance of hand-tuned vendor libraries on kernels like GEMM and FlashAttention.",
      "mindmap": "graph TB\n        Root[TL: Automatic End-to-End Compiler] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[Limited Programmability of Spatial Accelerators<br/>空间加速器的有限可编程性]\n        Problem --> P2[Poor Performance of Naive Mappings<br/>朴素映射性能差]\n        Method[主要方法/Method] --> M1[End-to-End Tile-Based Compiler Framework<br/>端到端基于分块的编译器框架]\n        Method --> M2[Hardware Representation for Topology & Memory<br/>用于拓扑和内存的硬件表示]\n        Method --> M3[Built on MLIR Ecosystem<br/>基于MLIR生态系统构建]\n        Results[关键结果/Results] --> R1[Performance on par with/vs Vendor Library (GEMM)<br/>性能与厂商库相当/超越(GEMM)]\n        Results --> R2[Significant Speedup for FlashAttention<br/>FlashAttention显著加速]"
    },
    {
      "title": "BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs",
      "authors": "Muhammad Zeeshan Karamat, Sadman Saif, Christiana Chamon Garcia",
      "institution": "Virginia Tech",
      "link": "https://arxiv.org/pdf/2512.22174",
      "code": null,
      "tags": [
        "fault-tolerance",
        "bit-flip faults",
        "fault localization",
        "transformer reliability",
        "residual-path perturbation",
        "loss-sensitivity profiling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e931785a8ed1d0dca51ed3c75265de72147ccd6e3d68df21de1c7cad78a1d912_w640_q70.webp",
      "contributions": "1. Introduces BitFlipScope, a scalable software framework for localizing bit-flip corruptions in transformer-based LLMs under two deployment scenarios (with and without a clean reference model). 2. Proposes differential analysis for fault localization when a reference model is available and residual-path perturbation/loss-sensitivity profiling for localization when no reference exists. 3. Enables lightweight performance recovery for corrupted models without requiring costly fine-tuning or full retraining.",
      "summary": "This paper introduces BitFlipScope, a framework for localizing and recovering from bit-flip corruptions in LLMs. It uses differential analysis with a reference model or perturbation-based profiling without one to identify fault-affected regions, enabling targeted recovery without full retraining. The work aims to improve fault resilience for LLMs in hardware-prone and adversarial environments.",
      "mindmap": "graph TB\n        A[BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs] --> B[核心问题/Problem: Bit-flip faults corrupt LLM parameters, causing unpredictable behavior]\n        A --> C[主要方法/Method: Differential analysis with reference model; Residual-path perturbation & loss-sensitivity profiling without reference]\n        A --> D[关键结果/Results: Enables fault localization and lightweight recovery, improving fault-resilient LLM deployment]"
    },
    {
      "title": "AiiDAlab: on the route to accelerate science",
      "authors": "Aliaksandr V.Yakutovich, Jusong Yu, Daniel Hollas, Edan Bainglass, Corsin Battaglia, Miki Bonacci, Lucas Fernandez Vilanova, Stephan Henne, Anders Kaestner, Michel Kenzelmann, Graham Kimbell, Jakob Lass, Fabio Lopes, Daniel G. Mazzone, Andres Ortega-Guerrero, Xing Wang, Nicola Marzari, Carlo A. Pignedoli, Giovanni Pizzi",
      "institution": "Empa, Paul Scherrer Institute, École Polytechnique Fédérale de Lausanne, University of Bristol",
      "link": "https://arxiv.org/pdf/2512.22173",
      "code": null,
      "tags": [
        "scientific workflow management",
        "AiiDAlab",
        "AiiDA",
        "provenance tracking",
        "FAIR principles",
        "web-based interface"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ffb3ec2c93f3c0a0b3a1f69f46586695b4825664dea8e67ccbdf005064367c46_w640_q70.webp",
      "contributions": "1. Development of the AiiDAlab platform, a web-based interface that simplifies access to and execution of complex computational workflows on supercomputers, lowering the barrier to entry for non-experts. 2. Maturation and expansion of the platform from its origins in computational materials science to support diverse scientific disciplines including quantum chemistry, atmospheric modeling, and experimental data analysis. 3. Integration with electronic laboratory notebooks (ELNs) and emphasis on automatic provenance tracking via AiiDA to enforce reproducibility and adherence to FAIR principles for generating Open Research Data.",
      "summary": "The paper presents AiiDAlab, a web-based platform designed to simplify the execution of complex computational workflows on supercomputers. It abstracts away technical details, provides an intuitive interface, and automatically tracks simulation provenance to ensure reproducibility. The platform has evolved to accelerate scientific discovery across multiple disciplines by allowing researchers to focus on their science rather than computational challenges.",
      "mindmap": "graph TB\n        Root[AiiDAlab: on the route to accelerate science]\n        Root --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[复杂工作流执行需要专业知识/Complex workflow execution requires technical expertise]\n        Method --> M1[提供基于浏览器的用户界面/Provide web-browser-based user interface]\n        Method --> M2[底层AiiDA引擎自动追踪溯源/Underlying AiiDA engine automatically tracks provenance]\n        Results --> R1[跨多学科加速科学发现/Accelerates scientific discovery across multiple disciplines]\n        Results --> R2[确保可重复性与FAIR原则/Ensures reproducibility and FAIR principles]"
    },
    {
      "title": "iOS as Acceleration",
      "authors": "Alexander K. Chen",
      "institution": "Independent High School Researcher (No institutional affiliation inferred)",
      "link": "https://arxiv.org/pdf/2512.22180",
      "code": null,
      "tags": [
        "on-device ai",
        "distributed pipeline parallelism",
        "mobile acceleration",
        "iOS",
        "memory constraints",
        "thermal throttling"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2533767e76bdf97e302af13359b973b06a9948269cc9017131b6e880553cb6b9_w640_q70.webp",
      "contributions": "1. Proposes a novel proof-of-concept system using distributed pipeline parallelism to harness iOS devices as computational accelerators for local ML tasks. 2. Demonstrates the system's effectiveness in accelerating modest model training (e.g., ResNet-34) and agentic LRM tool-usage, achieving a 44% decrease in training time in a specific setup. 3. Explores the unique potential of ubiquitous mobile devices with powerful processors and sensors (e.g., LiDAR, GPS) as cost-effective resources for embodied agentic AI and local compute, discussing practical use-cases and limitations.",
      "summary": "This paper addresses the barrier of expensive compute for local machine learning by proposing a system that uses distributed pipeline parallelism to leverage underutilized iOS phones as accelerators. The method partitions model weights to circumvent mobile memory limits, successfully accelerating tasks like training ResNet-34. The work concludes that commonplace mobile devices have significant potential to contribute to ML, especially for local, cost-sensitive, or sensor-driven applications.",
      "mindmap": "graph TB\n        A[iOS as Acceleration] --> B[核心问题/Problem: Powerful compute is a barrier for local ML; Cloud is not always viable]\n        A --> C[主要方法/Method: Use distributed pipeline parallelism to harness iOS devices as accelerators]\n        A --> D[关键结果/Results: Achieved faster training for modest models; Highlights mobile potential for ML]"
    },
    {
      "title": "MatKV: Trading Compute for Flash Storage in LLM Inference",
      "authors": "Kun-Woo Shin, Jay H. Park, Moonwook Oh, Yohan Jo, Jaeyoung Do, Sang-Won Lee",
      "institution": "Seoul National University, Samsung Electronics",
      "link": "https://arxiv.org/pdf/2512.22195",
      "code": "https://github.com/kunwooshin/MatKV",
      "tags": [
        "llm inference",
        "retrieval-augmented generation",
        "key-value cache",
        "flash storage",
        "prefill optimization",
        "power efficiency"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3d47111ab2d615579a09c00ff5a99f391f035b974cc23e324cddfbabf4d23cec_w640_q70.webp",
      "contributions": "1. Proposes MatKV, a scheme to precompute and materialize KV vectors of RAG documents in flash storage to avoid recomputation during inference. 2. Demonstrates that MatKV reduces inference time and power consumption by half for RAG workloads with minimal accuracy impact. 3. Shows MatKV enables additional optimizations like overlapping KV loading with decoding and enabling the use of low-end GPUs for decoding.",
      "summary": "The paper addresses the high compute and energy cost of the prefill phase in RAG-based LLM inference. It proposes MatKV, which precomputes and stores key-value vectors of documents in flash storage for reuse, trading compute for storage. Experiments show this approach halves inference time and power consumption while maintaining accuracy and enabling further hardware optimizations.",
      "mindmap": "graph TB\n        Root[”MatKV: Trading Compute for Flash Storage in LLM Inference”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>RAG推理中prefill阶段计算开销大<br>High compute cost of prefill in RAG inference”]\n        Method[”主要方法/Method<br>预计算并物化KV向量到闪存<br>Precompute & materialize KVs to flash storage”]\n        Results[”关键结果/Results<br>推理时间与能耗减半<br>Halves inference time & power consumption”]"
    },
    {
      "title": "SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM",
      "authors": "Simone Bnà, Giuseppe Giaquinto, Ettore Fadiga, Tommaso Zanelli, Francesco Bottau",
      "institution": "Cineca Supercomputing Centre, Università degli Studi di Napoli Federico II",
      "link": "https://arxiv.org/pdf/2512.22215",
      "code": null,
      "tags": [
        "computational fluid dynamics",
        "GPU porting",
        "unified memory",
        "memory pool manager",
        "OpenFOAM",
        "scalability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ea486a67026343b5f3c7d85db1ef1ff1202af04d0bd147ef25d9dc29565e2b1_w640_q70.webp",
      "contributions": "1. Presents SPUMA, a full GPU porting of OpenFOAM targeting both NVIDIA and AMD GPUs. 2. Implements a portable programming model with a memory pool manager leveraging unified memory for efficient GPU utilization. 3. Demonstrates significant performance and energy efficiency gains through extensive testing on pre-exascale clusters, showing up to 82% energy reduction compared to CPU simulations.",
      "summary": "This paper addresses the challenge of GPU programmability for open-source CFD by introducing SPUMA, a portable GPU port of OpenFOAM that uses a memory pool manager with unified memory. The method was tested on LUMI and Leonardo clusters, showing strong scalability up to 65% efficiency and weak scalability up to 85%, while reducing energy consumption by up to 82% compared to CPU-based simulations.",
      "mindmap": "graph TB\n        Root[SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: GPU programmability challenge in open-source CFD] --> P1[GPU可编程性挑战/GPU Programmability Challenge]\n        Method[主要方法/Method: Portable GPU porting with memory pool] --> M1[便携式编程模型/Portable Programming Model]\n        Method --> M2[内存池管理器/Memory Pool Manager]\n        Method --> M3[利用统一内存/Leverages Unified Memory]\n        Results[关键结果/Results: Performance and energy efficiency on pre-exascale clusters] --> R1[强可扩展性达65%/Strong Scalability 65%]\n        Results --> R2[弱可扩展性达85%/Weak Scalability 85%]\n        Results --> R3[能耗降低82%/Energy Reduction 82%]"
    },
    {
      "title": "Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs",
      "authors": "Xinhao Cheng, Zhihao Zhang, Yu Zhou, Jianan Ji, Jinchen Jiang, Zepeng Zhao, Ziruo Xiao, Zihao Ye, Yingyi Huang, Ruihang Lai, Hongyi Jin, Bohan Hou, Mengdi Wu, Yixin Dong, Anthony Yip, Zihao Ye, Songting Wang, Wenqin Yang, Xupeng Miao, Tianqi Chen, Zhihao Jia",
      "institution": "Carnegie Mellon University, Tsinghua University, NVIDIA, University of Michigan, Purdue University",
      "link": "https://arxiv.org/pdf/2512.22219",
      "code": "https://github.com/mirage-project/mirage",
      "tags": [
        "llm inference",
        "megakernel",
        "kernel fusion",
        "SM-level graph",
        "software pipelining",
        "CUDA"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f886cd06ce6c0f773c062fa38aae0fa982d862cc66ef54da9fbbfd6cf62dd86a_w640_q70.webp",
      "contributions": "1. Introduces an SM-level graph representation for capturing fine-grained data dependencies across GPU streaming multiprocessors. 2. Develops a compiler and an in-kernel parallel runtime that automatically transforms multi-operator inference into a single, high-performance mega-kernel. 3. Enables previously infeasible GPU optimizations like cross-operator software pipelining and fine-grained kernel overlap, significantly reducing inference latency.",
      "summary": "The paper introduces Mirage Persistent Kernel (MPK), a compiler and runtime system that automatically fuses multiple GPU kernels for model inference into a single, optimized mega-kernel. It achieves this by using a novel SM-level graph representation and decentralized scheduling to enable fine-grained optimizations like software pipelining. Evaluation shows MPK reduces LLM inference latency by up to 1.7x, pushing performance close to hardware limits.",
      "mindmap": "graph TB\n        A[Mirage Persistent Kernel<br>幻影持久内核] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Kernel-per-operator execution<br>limits GPU optimization<br>逐算子内核执行限制GPU优化]\n        C --> C1[SM-level graph &<br>mega-kernel runtime<br>SM级图与巨型内核运行时]\n        D --> D1[Reduces inference latency<br>by up to 1.7x<br>推理延迟降低高达1.7倍]"
    },
    {
      "title": "Scalable Cloud-Native Architectures for Intelligent PMU Data Processing",
      "authors": "Nachiappan Chockalingam, Akshay Deshpande, Lokesh Butra, Ram Sekhar Bodala, Nitin Saksena, Adithya Parthasarathy, Balakrishna Pothineni, Akash Kumar Agarwal",
      "institution": "IEEE, NTT Data, Amtrak, Albertsons Companies",
      "link": "https://arxiv.org/pdf/2512.22231",
      "code": null,
      "tags": [
        "cluster infrastructure",
        "cloud-native",
        "distributed stream processing",
        "containerized microservices",
        "elastic resource orchestration",
        "edge-cloud hybrid"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59263c4210b1af52fedb9e9660a5117d937ac4a63d70c41f31a04dc3c553429f_w640_q70.webp",
      "contributions": "1. A comprehensive theoretical framework for AI-enhanced cloud-based PMU analytics. 2. Mathematical formulations for distributed machine learning optimized for PMU time-series data. 3. Analysis of edge-cloud hybrid architectures with integrated security and privacy considerations.",
      "summary": "This paper proposes a scalable cloud-native architecture to address the latency and scalability challenges of processing high-frequency data from Phasor Measurement Units (PMUs) in smart grids. The method integrates AI with edge and cloud computing, using distributed stream processing and containerized microservices for real-time analytics. The analysis shows the architecture can achieve sub-second response times while scaling to large deployments, providing a robust foundation for next-generation grid analytics.",
      "mindmap": "graph TB\n        Root[”Scalable Cloud-Native Architectures for Intelligent PMU Data Processing”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>PMU数据规模大，传统架构延迟高，可扩展性差”]\n        Method[”主要方法/Method<br>云原生架构，集成AI、边缘与云计算，使用分布式流处理和微服务”]\n        Results[”关键结果/Results<br>实现亚秒级响应，可扩展至大规模部署，提供安全可靠的基础”]"
    },
    {
      "title": "Valori: A Deterministic Memory Substrate for AI Systems",
      "authors": "Varshith Gudur",
      "institution": "Independent Researcher (Valori Kernel Project)",
      "link": "https://arxiv.org/pdf/2512.22280",
      "code": "https://github.com/varshith-Git/Valori-Kernel",
      "tags": [
        "memory & caching",
        "deterministic memory",
        "fixed-point arithmetic",
        "vector embeddings",
        "approximate nearest neighbor search",
        "state machine"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2abb7ed17a8a06e6a2b8760f08fa9345391995aee74a3542cacf55dd051b383f_w640_q70.webp",
      "contributions": "1. Identifies and characterizes the fundamental non-determinism in AI memory systems caused by hardware-dependent floating-point arithmetic, which leads to divergent memory states and retrieval results. 2. Proposes Valori, a deterministic AI memory substrate that replaces floating-point operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. 3. Demonstrates that Valori guarantees bit-identical memory states, snapshots, and search results across different hardware platforms (e.g., x86 vs. ARM), establishing deterministic memory as a necessary primitive for trustworthy AI.",
      "summary": "The paper identifies non-determinism in AI memory systems due to hardware-dependent floating-point arithmetic, which compromises replayability and auditability. It proposes Valori, a memory substrate using fixed-point arithmetic and a state machine model to guarantee bit-identical behavior across platforms. The work concludes that deterministic memory is essential for building trustworthy AI systems.",
      "mindmap": "graph TB\n        A[Valori: A Deterministic Memory Substrate for AI Systems] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: AI内存非确定性/AI Memory Non-Determinism]\n        C[主要方法/Method: 固定点算术与状态机/Fixed-Point Arithmetic & State Machine]\n        D[关键结果/Results: 跨平台比特一致性/Cross-Platform Bit-Identical Results]"
    },
    {
      "title": "Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries",
      "authors": "Saurabh Deochake, Debajyoti Mukhopadhyay",
      "institution": "SentinelOne, WIDiCoReL Research Lab",
      "link": "https://arxiv.org/pdf/2512.22364",
      "code": null,
      "tags": [
        "llm inference",
        "Text-to-SQL",
        "Cloud Cost Optimization",
        "Query Efficiency",
        "Large Language Models",
        "Google BigQuery"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06f17566f5fb65cb73b79b0dbb64bde11c2f87d177f02865af7fc2d8910e3ac4_w640_q70.webp",
      "contributions": "1. Introduced a cloud-native cost evaluation methodology for Text-to-SQL systems, measuring bytes processed, slot utilization, and estimated query cost on production infrastructure. 2. Conducted an empirical evaluation of six LLMs on Google BigQuery, demonstrating that reasoning models achieve significantly lower cloud compute costs while maintaining high correctness. 3. Quantified cost variance across models, identified prevalent inefficiency patterns (e.g., missing partition filters), and provided deployment guidelines for cost-sensitive environments.",
      "summary": "This paper studies the cloud compute costs of SQL queries generated by Large Language Models (LLMs) for Text-to-SQL tasks. By evaluating six state-of-the-art LLMs on Google BigQuery, it finds that reasoning models are more cost-efficient, processing far fewer bytes, and that execution time is a poor proxy for cloud cost. The work provides a new cost-focused evaluation methodology and guidelines for deploying cost-aware Text-to-SQL systems.",
      "mindmap": "graph TB\n        A[Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Existing efficiency metrics (e.g., VES) measure time, not cloud compute costs.] --> B1[问题背景/Context<br>LLMs achieve high Text-to-SQL accuracy, but cost efficiency in cloud deployments is unknown.]\n        C[主要方法/Method<br>Systematic evaluation of 6 LLMs on Google BigQuery (StackOverflow dataset).] --> C1[评估指标/Metrics<br>Measure bytes processed, slot utilization, estimated cost, and correctness.]\n        D[关键结果/Results] --> D1[发现1/Finding 1<br>Reasoning models process 44.5% fewer bytes with equivalent correctness.]\n        D --> D2[发现2/Finding 2<br>Weak correlation (r=0.16) between execution time and query cost.]\n        D --> D3[发现3/Finding 3<br>Up to 3.4x cost variance; standard models produce high-cost outliers.]"
    },
    {
      "title": "Efficient Multi-Model Orchestration for Self-Hosted Large Language Models",
      "authors": "Bhanu Prakash Vangala, Tanu Malik",
      "institution": "University of Missouri",
      "link": "https://arxiv.org/pdf/2512.22402",
      "code": null,
      "tags": [
        "llm inference",
        "Kubernetes",
        "Helm",
        "DistilBERT",
        "scale-to-zero",
        "hybrid routing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/14e9270400ac5f7fbf1ac4048cb82d9527c762106e232f9cd97653eb0ab3bdb4_w640_q70.webp",
      "contributions": "1. A unified Helm-based deployment system for self-hosted LLMs on Kubernetes, 2. An adaptive scale-to-zero automation mechanism for efficient GPU resource utilization, 3. A hybrid routing module combining keyword heuristics and a lightweight DistilBERT classifier to balance cost, latency, and accuracy.",
      "summary": "The paper introduces \"Pick and Spin,\" a framework for efficient orchestration of self-hosted large language models. It addresses challenges in GPU utilization and workload routing by integrating Kubernetes-based deployment, adaptive scaling, and a hybrid routing strategy. The system demonstrates significant improvements in success rate, latency, and cost compared to static deployments.",
      "mindmap": "graph TB\n        A[Efficient Multi-Model Orchestration for Self-Hosted LLMs] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Self-hosted LLM deployment challenges: GPU utilization, workload routing, reliability/自托管LLM部署挑战：GPU利用率、工作负载路由、可靠性]\n        C --> C1[Pick and Spin Framework: Kubernetes, Helm, scale-to-zero, hybrid routing/Pick and Spin框架：Kubernetes, Helm, 缩容至零, 混合路由]\n        D --> D1[21.6% higher success rate, 30% lower latency, 33% lower cost/成功率提升21.6%，延迟降低30%，成本降低33%]"
    },
    {
      "title": "Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving",
      "authors": "Rui Li, Zhaoning Zhang, Libo Zhang, Huaimin Wang, Xiang Fu, Zhiquan Lai",
      "institution": "National University of Defense Technology",
      "link": "https://arxiv.org/pdf/2512.22420",
      "code": null,
      "tags": [
        "llm inference",
        "speculative decoding",
        "dynamic adaptation",
        "multi-armed bandit",
        "throughput optimization",
        "latency reduction"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c6466394cd16e760ca78e05f13eba9852a284e7e8231b58de2c71fbee1e7b39_w640_q70.webp",
      "contributions": "1. Identifies the critical trade-off in speculative decoding: beneficial in memory-bound (low-load) scenarios but detrimental in compute-bound (high-load) scenarios due to verification overhead. 2. Proposes Nightjar, a novel learning-based algorithm that dynamically adapts the speculative length (or disables SD) based on real-time request load and batch size. 3. Demonstrates significant performance gains, achieving up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding.",
      "summary": "The paper addresses the inefficiency of fixed-length speculative decoding in LLM serving, which fails to adapt to dynamic request loads. It proposes Nightjar, a learning-based algorithm that dynamically selects the optimal speculative length. Experiments show Nightjar significantly improves throughput and reduces latency compared to standard speculative decoding.",
      "mindmap": "graph TB\n        A[Nightjar: Dynamic Adaptive Speculative Decoding] --> B[核心问题/Problem: Fixed speculative length fails under dynamic loads]\n        A --> C[主要方法/Method: Learning-based algorithm adapts speculative length]\n        A --> D[关键结果/Results: Higher throughput, lower latency]"
    },
    {
      "title": "Role-Based Fault Tolerance System for LLM RL Post-Training",
      "authors": "Zhenqian Chen, Baoquan Zhong, Xiang Li, Qing Dai, Xinkui Zhao, Miao Ye, Ren Cheng, Lufei Zhang, Jianwei Yin",
      "institution": "Zhejiang University, State Key Laboratory of Mathematical Engineering and Advanced Computing",
      "link": "https://arxiv.org/pdf/2512.22492",
      "code": null,
      "tags": [
        "fault-tolerance",
        "role-based fault tolerance",
        "RL post-training",
        "UCX communication",
        "warm standby",
        "Effective Training Time Ratio (ETTR)"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ceff77cfb9d0c7d7e763916b77716ee6534c3aa3d54d41253fef6ea4d9a86ec0_w640_q70.webp",
      "contributions": "1. Proposes a role-based fault isolation and recovery system (RobustRL) for RL post-training, enabling recovery of only the failed component (trainer, rollout) instead of restarting the entire task. 2. Introduces a role-aware monitoring mechanism to accurately detect failures and avoid false positives/delays specific to different RL roles. 3. Implements dynamic, UCX-based point-to-point communication to reconnect recovered roles and synchronize weights immediately, replacing static collective communication.",
      "summary": "The paper addresses the lack of fault tolerance for RL post-training of LLMs, which interleaves training and inference workloads. It proposes RobustRL, a system that isolates and recovers failed roles (e.g., trainer, rollout) individually using a Detect-Restart-Reconnect paradigm, instead of restarting the entire job. This approach significantly improves the Effective Training Time Ratio and reduces end-to-end training time compared to baseline methods.",
      "mindmap": "graph TB\n        A[Role-Based Fault Tolerance System for LLM RL Post-Training] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[RL后训练混合训练与推理工作负载，易受双方故障影响/RL post-training mixes training & inference, vulnerable to faults from both]\n        B --> B2[现有容错框架未针对RL的异步执行优化/Existing FT frameworks not optimized for RL's async execution]\n        C --> C1[基于角色的故障隔离与恢复/Role-based fault isolation & recovery]\n        C --> C2[检测-重启-重连范式/Detect-Restart-Reconnect paradigm]\n        C2 --> C21[角色感知监控/Role-aware monitoring]\n        C2 --> C22[非中断式重启/Non-disruptive restart with warm standbys]\n        C2 --> C23[动态UCX点对点通信重连/Dynamic UCX P2P reconnection]\n        D --> D1[ETTR超过80%，优于基线的60%/ETTR >80%, better than baseline 60%]\n        D --> D2[端到端训练时间加快8.4%-17.4%/End-to-end training time 8.4%-17.4% faster]"
    },
    {
      "title": "Object Abstraction To Streamline Edge-Cloud-Native Application Development",
      "authors": "Pawissanutt Lertpongrujikorn",
      "institution": "University of North Texas",
      "link": "https://arxiv.org/pdf/2512.22534",
      "code": null,
      "tags": [
        "serverless computing",
        "edge computing",
        "Object-as-a-Service (OaaS)",
        "edge-cloud continuum",
        "serverless",
        "FaaS",
        "declarative SLA"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fcd35442b8b6cfbc12c61e295e970899c2bfb10184fe06c88745b8fbf0137055_w640_q70.webp",
      "contributions": "1. Proposed the Object-as-a-Service (OaaS) paradigm, unifying resource, state, and workflow management with the Oparaca prototype. 2. Extended OaaS to the edge-cloud continuum with OaaS-IoT/EdgeWeaver, improving performance and reducing code complexity. 3. Established an empirical methodology and commercialization pathway for cloud-native research grounded in practitioner needs.",
      "summary": "This dissertation addresses the complexity and fragmentation in serverless and cloud-native development by proposing the Object-as-a-Service (OaaS) paradigm. It introduces a unified abstraction for resources, state, and workflows, and extends it to the edge-cloud continuum, demonstrating improved developer productivity and system performance. The work concludes that OaaS effectively hides infrastructure complexity, allowing developers to focus on application logic.",
      "mindmap": "graph TB\n        Root(”Object Abstraction for Edge-Cloud-Native Apps<br>面向边缘云原生应用的对象抽象”) --> Problem\n        Root --> Method\n        Root --> Results\n        Problem(”核心问题/Problem”) --> P1(”Serverless 承诺与实践存在差距<br>Gap in serverless promise vs. practice”)\n        Problem --> P2(”基础设施碎片化与复杂化<br>Infrastructure fragmentation & complexity”)\n        Method(”主要方法/Method”) --> M1(”提出 OaaS 范式<br>Propose OaaS paradigm”)\n        Method --> M2(”开发 Oparaca 原型<br>Develop Oparaca prototype”)\n        Method --> M3(”扩展至边缘云连续体<br>Extend to edge-cloud continuum (OaaS-IoT)”)\n        Results(”关键结果/Results”) --> R1(”统一资源、状态、工作流管理<br>Unified resource, state, workflow management”)\n        Results --> R2(”性能开销可忽略，可扩展性领先<br>Negligible overhead, state-of-the-art scalability”)\n        Results --> R3(”任务完成更快，代码行数减少<br>Faster task completion, reduced lines of code”)"
    },
    {
      "title": "RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure",
      "authors": "Wei Gao, Yuheng Zhao, Tianyuan Wu, Shaopan Xiong, Weixun Wang, Dakai An, Lunxi Cao, Dilxat Muhtar, Zichen Liu, Haizhou Zhao, Ju Huang, Siran Yang, Yongbin Li, Wenbo Su, Jiamang Wang, Lin Qu, Bo Zheng, Wei Wang",
      "institution": "HKUST, Alibaba Group",
      "link": "https://arxiv.org/pdf/2512.22560",
      "code": "https://github.com/alibaba/ROLL",
      "tags": [
        "agent system",
        "disaggregated infrastructure",
        "hardware-affinity mapping",
        "fine-grained asynchrony"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dc8e408c613097b7b60bc32e41ec3137faa8dd94184658c8a802b65cbf57c593_w640_q70.webp",
      "contributions": "1. A hardware-affinity workload mapping strategy that routes compute-bound and bandwidth-bound tasks to best-fit GPU devices. 2. A fine-grained asynchrony mechanism that manages execution at the trajectory level to mitigate resource bubbles and improve utilization. 3. A statefulness-aware computation design that offloads stateless components to serverless infrastructure for elastic scaling.",
      "summary": "The paper presents RollArt, a distributed system designed to scale agentic reinforcement learning training on disaggregated infrastructure. It addresses the heterogeneity of agentic RL workloads by proposing three core techniques: hardware-affinity workload mapping, fine-grained asynchrony, and statefulness-aware computation. The system demonstrates significant improvements in training throughput, achieving 1.35-2.05x speedup over baselines, and scales to thousands of GPUs.",
      "mindmap": "graph TB\n        Root[”RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure”] --> Problem[”核心问题/Problem: Agentic RL workloads are heterogeneous, causing inefficiency in monolithic infrastructure.”]\n        Root --> Method[”主要方法/Method: Disaggregated system with hardware-affinity mapping, fine-grained asynchrony, and statefulness-aware computation.”]\n        Root --> Results[”关键结果/Results: Achieves 1.35-2.05x training speedup and scales to >3000 GPUs.”]"
    },
    {
      "title": "Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference",
      "authors": "Mona Moghadampanah, Adib Rezaei Shahmirzadi, Farhana Amin, Dimitrios S. Nikolopoulos",
      "institution": "Virginia Tech",
      "link": "https://arxiv.org/pdf/2512.22695",
      "code": null,
      "tags": [
        "multi-modal inference",
        "energy efficiency",
        "dynamic voltage and frequency scaling (DVFS)",
        "GPU underutilization",
        "visual token sequences",
        "stage-level analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3f6dd42f6aa45e4d0992cdd9fa407ab5c4268e31f45ecafdc9b44861ceb445e1_w640_q70.webp",
      "contributions": "1. Provides the first detailed, stage-level energy characterization of MLLM inference, identifying modality inflation as a key inefficiency. 2. Quantifies the significant energy overhead (17%-94%) of multimodal inference and reveals diverse bottlenecks (vision encoder vs. prefill) and GPU underutilization. 3. Demonstrates stage-wise DVFS as an effective optimization to reduce energy consumption with minimal performance impact.",
      "summary": "This paper investigates the energy inefficiency of multimodal large language model (MLLM) inference, termed \"modality inflation,\" where extra encoding stages and longer token sequences increase energy consumption. It provides a stage-level energy analysis on GPUs, quantifying overheads and identifying bottlenecks, and proposes stage-wise dynamic voltage and frequency scaling (DVFS) as an effective optimization to save energy with modest performance loss.",
      "mindmap": "graph TB\n        A[Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference] --> B[核心问题/Problem: Multimodal inference introduces unexplored energy trade-offs and inefficiencies (modality inflation)]\n        A --> C[主要方法/Method: Stage-level energy analysis (vision encoding, prefill, decode) on GPU, and proposes stage-wise DVFS optimization]\n        A --> D[关键结果/Results: Quantifies 17%-94% energy overhead, identifies bottlenecks and GPU underutilization, demonstrates DVFS saves energy with minor impact]"
    },
    {
      "title": "OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads",
      "authors": "Ertza Warraich, Ali Imran, Annus Zulfiqar, Shay Vargaftik, Sonia Fahmy, Muhammad Shahbaz",
      "institution": "Purdue University, Broadcom, University of Michigan",
      "link": "https://arxiv.org/pdf/2512.22743",
      "code": null,
      "tags": [
        "communication & networking",
        "RDMA",
        "tail latency",
        "collective communication",
        "reliability",
        "domain-specific transport"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3143921b3b275baf220b75c6f927e8a49b4fa31653bbd117324490a1b8f8da93_w640_q70.webp",
      "contributions": "1. Proposes OptiNIC, a domain-specific RDMA transport that eliminates retransmissions and in-order delivery from the NIC, shifting to a best-effort, out-of-order model. 2. Introduces adaptive timeouts to trigger forward progress in case of data loss or delay, decoupling completion signaling from complete data delivery. 3. Shifts loss recovery to the ML pipeline (e.g., via Hadamard Transform and Erasure Coding) while retaining standard congestion control, improving performance and resilience for distributed ML workloads.",
      "summary": "The paper identifies tail latency in collective communication as a major bottleneck for distributed ML. It proposes OptiNIC, a new RDMA transport that relaxes strict reliability guarantees based on ML's tolerance for data loss, using adaptive timeouts and moving recovery to the application layer. Evaluation shows OptiNIC significantly improves time-to-accuracy, throughput, and tail latency while reducing hardware resource usage.",
      "mindmap": "graph TB\n        A[OptiNIC: A Resilient and Tail-Optimal RDMA NIC] --> B[核心问题/Problem: Tail latency in collective communication bottlenecks distributed ML scaling]\n        A --> C[主要方法/Method: Domain-specific RDMA transport with best-effort delivery, adaptive timeouts, and loss recovery in ML pipeline]\n        A --> D[关键结果/Results: Improves TTA 2x, throughput 1.6x, lowers 99th% latency 3.5x, cuts BRAM usage 2.7x]"
    },
    {
      "title": "Two-Robot Computational Landscape: A Complete Characterization of Model Power in Minimal Mobile Robot Systems",
      "authors": "Naoki Kitamura, Yuichi Sudo, Koichi Wada",
      "institution": "The University of Osaka, Hosei University",
      "link": "https://arxiv.org/pdf/2512.22770",
      "code": null,
      "tags": [
        "distributed computing",
        "autonomous mobile robots",
        "Look-Compute-Move (LCM)",
        "computational power hierarchy",
        "finite-state robots",
        "robots with lights"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ddce34ada1bbaebc271a0c17bbc6cf8413606d2887ebd22bfe77cc4c0e90d34a_w640_q70.webp",
      "contributions": "1. Proves that under full synchrony, the FSTA (finite-state) and LUMI (robots with lights) models coincide for two robots, showing perfect synchrony can substitute for memory and communication at this minimal scale. 2. Shows that the FSTA and FCOM (finite-communication) models are orthogonal (bidirectionally incomparable), completing the landscape of incomparability. 3. Provides the first complete and exact characterization of the computational power hierarchy for two robots across all major models and schedulers using a novel simulation-free method.",
      "summary": "This paper provides the first complete characterization of the computational power of two autonomous mobile robots across major models (OBLOT, FSTA, FCOM, LUMI) and schedulers. Using a novel simulation-free method, it reveals a landscape distinct from the general n-robot case, showing that perfect synchrony can substitute for memory and communication for two robots, and that FSTA and FCOM are orthogonal. This yields the first exact computational hierarchy for minimal robot systems.",
      "mindmap": "graph TB\n        A[Two-Robot Computational Landscape] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Two-robot computational hierarchy unresolved]\n        C --> C1[Simulation-free analysis method]\n        D --> D1[FSTA^F = LUMI^F under full sync]\n        D --> D2[FSTA and FCOM are orthogonal]\n        D --> D3[Complete landscape for two robots]"
    },
    {
      "title": "Argus: Token Aware Distributed LLM Inference Optimization",
      "authors": "Panlong Wu, Yifei Zhong, Danyang Chen, Ting Wang, Fangxin Wang",
      "institution": "The Chinese University of Hong Kong, Shenzhen (CUHK-SZ)",
      "link": "https://arxiv.org/pdf/2512.22925",
      "code": null,
      "tags": [
        "llm inference",
        "token-aware offloading",
        "Lyapunov optimization",
        "length prediction",
        "edge-cloud systems",
        "distributed inference"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2e6bf58921ba8401e4cc5e40322f2ce1b65861ebe0ac5f35cfead3e0339c7f09_w640_q70.webp",
      "contributions": "1. A Length-Aware Semantics (LAS) module that predicts output token lengths for prompts using a fine-tuned language model with token-length-sensitive feature modulation. 2. A Lyapunov-guided Offloading Optimization (LOO) module that formulates long-term Quality-of-Experience optimization considering both LLM prefilling and decoding costs. 3. A novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to solve the resulting integer nonlinear programming problem under time-varying constraints.",
      "summary": "This paper presents Argus, a token-aware distributed LLM inference framework for edge-cloud systems. It addresses inference time variability by predicting output token lengths and using Lyapunov optimization for efficient task offloading. Evaluations show Argus achieves robust and efficient performance in dynamic, heterogeneous environments.",
      "mindmap": "graph TB\n        A[Argus: Token Aware Distributed LLM Inference Optimization] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[LLM推理时间可变性高 / High LLM Inference Time Variability]\n        B --> B2[动态异构边缘云环境 / Dynamic Heterogeneous Edge-Cloud Environment]\n        C --> C1[LAS: 输出长度预测 / LAS: Output Length Prediction]\n        C --> C2[LOO: 李雅普诺夫优化卸载 / LOO: Lyapunov Optimization Offloading]\n        C --> C3[IODCC: 迭代卸载算法 / IODCC: Iterative Offloading Algorithm]\n        D --> D1[鲁棒性能 / Robust Performance]\n        D --> D2[高效推理 / Efficient Inference]"
    },
    {
      "title": "A Domain Decomposition-based Solver for Acoustic Wave propagation in Two-Dimensional Random Media",
      "authors": "Sudhi Sharma Padillath Vasudevan",
      "institution": "Carleton University",
      "link": "https://arxiv.org/pdf/2512.23027",
      "code": null,
      "tags": [
        "uncertainty quantification",
        "stochastic Galerkin method",
        "polynomial chaos expansion",
        "domain decomposition",
        "Neumann-Neumann preconditioner"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9a4a9c029830a2f5bbff0493a205dfd33bea894daf2a861a9f131c15f02f4b9d_w640_q70.webp",
      "contributions": "1. Applies an intrusive stochastic Galerkin method with Polynomial Chaos Expansion to solve acoustic wave propagation in random media, transforming the stochastic PDE into a deterministic system. 2. Employs Domain Decomposition-based solvers to address the high computational cost associated with large-scale, high-dimensional stochastic systems. 3. Utilizes a conjugate gradient iterative solver with a two-level Neumann-Neumann preconditioner, demonstrating efficient scalability for the problem.",
      "summary": "This paper tackles the high computational cost of simulating acoustic wave propagation in two-dimensional random media. It proposes a method combining an intrusive stochastic Galerkin approach with Polynomial Chaos Expansion and a Domain Decomposition-based linear solver preconditioned with a two-level Neumann-Neumann method. The results show that this approach provides an efficiently scalable solution for the problem.",
      "mindmap": "graph TB\n        Root[基于域分解的二维随机介质声波传播求解器<br>A Domain Decomposition-based Solver for Acoustic Wave Propagation in 2D Random Media]\n        Root --> Problem[核心问题/Problem]\n        Root --> Method[主要方法/Method]\n        Root --> Results[关键结果/Results]\n        Problem --> P1[计算成本高<br>High Computational Cost]\n        P1 --> P2[网格、时间步、随机参数增加<br>Increasing Mesh, Time Step, Random Parameters]\n        Method --> M1[侵入式随机伽辽金法<br>Intrusive Stochastic Galerkin]\n        M1 --> M2[多项式混沌展开<br>Polynomial Chaos Expansion (PCE)]\n        Method --> M3[域分解求解器<br>Domain Decomposition Solver]\n        M3 --> M4[共轭梯度法+两层Neumann-Neumann预处理器<br>Conjugate Gradient with Two-level Neumann-Neumann Preconditioner]\n        Results --> R1[高效可扩展性<br>Efficient Scalability]"
    },
    {
      "title": "Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware",
      "authors": "Alex Khalil, Guillaume Heilles, Maria Parraga, Simon Heilles",
      "institution": "UCLouvain, Universidad Espíritu Santo, DENEM Labs",
      "link": "https://arxiv.org/pdf/2512.23029",
      "code": null,
      "tags": [
        "llm inference",
        "quantization",
        "mixture-of-experts",
        "on-premise deployment",
        "consumer-grade hardware",
        "benchmark analysis"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ed34c10397ed5cae19c39a4a8e2a5a1f0fd64e2f76183b8ba093c74b9a79fe51_w640_q70.webp",
      "contributions": "1. A comprehensive benchmarking framework for evaluating both the intrinsic model capabilities and the server-side performance (latency, throughput, scalability) of a private LLM deployment. 2. A practical demonstration and performance analysis of deploying a quantized, large-scale (30B parameter) Mixture-of-Experts model (Qwen3) on next-generation consumer-grade hardware (NVIDIA RTX 5090). 3. Evidence that a carefully configured on-premises LLM server can achieve performance comparable to cloud services, offering SMBs a viable, cost-effective, and privacy-preserving alternative.",
      "summary": "This paper investigates the feasibility of deploying a private, high-performance LLM server for Small and Medium Businesses using consumer-grade hardware. It benchmarks a quantized Qwen3-30B model on an NVIDIA RTX 5090, evaluating both model capability and server performance under load. The results show that such an on-premises setup can achieve performance close to cloud services at a lower cost and with full data privacy.",
      "mindmap": "graph TB\n        A[Viability and Performance of a Private LLM Server for SMBs<br>SMB私有LLM服务器的可行性与性能] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Cloud reliance: cost, privacy, sovereignty for SMBs<br>云依赖：成本、隐私、SMB主权]\n        C[主要方法/Method<br>Benchmark quantized Qwen3-30B on consumer hardware (RTX 5090)<br>在消费级硬件上对量化Qwen3-30B进行基准测试]\n        D[关键结果/Results<br>On-premises performance rivals cloud, viable for SMBs<br>本地性能媲美云端，对SMB可行]"
    },
    {
      "title": "Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation",
      "authors": "Mario Colosi, Reza Farahani, Maria Fazio, Radu Prodan, Massimo Villari",
      "institution": "University of Messina, University of Klagenfurt, University of Innsbruck",
      "link": "https://arxiv.org/pdf/2512.23096",
      "code": null,
      "tags": [
        "federated learning",
        "self-supervised learning",
        "representation learning",
        "distributed learning",
        "decentralized clustering",
        "contextual data"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f2b452fb94443ab9846af33524787f0bc6c709b6e90ae3be4e653738c6fe592b_w640_q70.webp",
      "contributions": "1. Introduces Osmotic Learning (OSM-L), a novel self-supervised paradigm for learning from distributed data without raw data exchange. 2. Proposes an \"osmosis\" process that aligns local representations to converge to a dynamic equilibrium, capturing contextual patterns. 3. Demonstrates that OSM-L functions as a decentralized clustering mechanism, identifying correlated data groups during training.",
      "summary": "This paper proposes Osmotic Learning (OSM-L), a self-supervised distributed learning paradigm that extracts higher-level latent knowledge from decentralized data sources without sharing raw data. It achieves this through an iterative \"osmosis\" process that aligns local representations to converge to a contextual equilibrium, also enabling decentralized clustering. Experimental results show OSM-L achieves high accuracy in local information alignment while preserving contextual integrity.",
      "mindmap": "graph TB\n        A[Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation] --> B[核心问题/Problem: Extracting meaningful knowledge from distributed, heterogeneous data without raw data exchange]\n        A --> C[主要方法/Method: Osmotic Learning (OSM-L) - self-supervised paradigm using iterative alignment and ”osmosis” for representation convergence]\n        A --> D[关键结果/Results: Achieves >0.99 alignment accuracy and preserves contextual integrity; enables decentralized clustering]"
    },
    {
      "title": "FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs",
      "authors": "Zihao Zhou, Shusen Yang, Fangyuan Zhao, Xuebin Ren",
      "institution": "Xi'an Jiaotong University",
      "link": "https://arxiv.org/pdf/2512.23235",
      "code": null,
      "tags": [
        "federated learning",
        "graph federated learning",
        "fairness",
        "overlapping subgraphs",
        "privacy-preserving",
        "weighted aggregation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c67614889dfdf1e0e6de4fd0bd950e8649eb4d988fb1661c29ef6c14b73bba25_w640_q70.webp",
      "contributions": "1. Uncover and theoretically analyze the unfairness issue in graph federated learning caused by imbalanced overlapping subgraphs across clients. 2. Propose FairGFL, a novel algorithm that uses a privacy-preserving estimation of overlapping ratios and an interpretable weighted aggregation approach to enhance cross-client fairness. 3. Improve the tradeoff between model utility and fairness by integrating a carefully crafted regularizer into the federated composite loss function.",
      "summary": "The paper identifies a fairness problem in graph federated learning when client subgraphs overlap in an imbalanced way. To solve this, it proposes FairGFL, a method that uses privacy-preserving overlap estimation and a fairness-aware regularizer to balance utility and fairness. Experiments show FairGFL outperforms baselines in both utility and fairness on benchmark datasets.",
      "mindmap": "graph TB\n    A(FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs) --> B(核心问题/Problem: Imbalanced overlapping subgraphs cause unfairness in GFL)\n    A --> C(主要方法/Method: FairGFL with privacy-preserving overlap estimation, weighted aggregation, and fairness regularizer)\n    A --> D(关键结果/Results: Outperforms baselines in model utility and fairness)"
    },
    {
      "title": "Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL",
      "authors": "Abolfazl Younesi, Abbas Shabrang Maryan, Elyas Oustad, Zahra Najafabadi Samani, Mohsen Ansari, Thomas Fahringer",
      "institution": "University of Innsbruck, Sharif University of Technology",
      "link": "https://arxiv.org/pdf/2512.23310",
      "code": null,
      "tags": [
        "llm inference",
        "Lyapunov Optimization",
        "Deep Reinforcement Learning",
        "Edge-Cloud Partitioning",
        "Transformer Decomposition",
        "Queue Stability"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/121b71aa214f4a7c1671c9df76bf67a9cd64f3cb9e74186606a380ce86f7634f_w640_q70.webp",
      "contributions": "1. Proposes a fine-grained, adaptive partitioning framework (Splitwise) that decomposes transformer layers into attention heads and feed-forward sub-blocks, enabling exponentially more partition choices than layer-wise schemes. 2. Introduces a hierarchical DRL policy guided by Lyapunov optimization to jointly optimize latency, energy, and accuracy while guaranteeing queue stability under stochastic workloads and variable bandwidth. 3. Ensures robustness through partition checkpoints with exponential backoff recovery for communication failures, validated on real edge devices with large models.",
      "summary": "The paper proposes Splitwise, a Lyapunov-assisted DRL framework for dynamically partitioning LLM inference between edge and cloud at a fine-grained sub-layer level. It aims to minimize latency and energy while maintaining accuracy under fluctuating network conditions. Experiments show Splitwise significantly reduces latency and energy consumption compared to existing methods.",
      "mindmap": "graph TB\n        A[Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL] --> B[核心问题/Problem: LLMs are hard to deploy on edge devices; cloud-only is slow; static partitions fail with bandwidth changes.]\n        A --> C[主要方法/Method: Fine-grained partition of transformer layers; Lyapunov-assisted DRL for adaptive optimization; checkpointing for robustness.]\n        A --> D[关键结果/Results: Reduces latency 1.4x-2.8x; cuts energy up to 41%; lowers 95th-percentile latency by 53-61%.]"
    },
    {
      "title": "An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes",
      "authors": "Vinoth Punniyamoorthy, Bikesh Kumar, Sumit Saha, Lokesh Butra, Mayilsamy Palanigounder, Akash Kumar Agarwal, Kabilan Kannan",
      "institution": "IEEE, East West Bank, NTT Data, Albertsons",
      "link": "https://arxiv.org/pdf/2512.23415",
      "code": null,
      "tags": [
        "cluster infrastructure",
        "Kubernetes",
        "Autoscaling",
        "AIOps",
        "Service Level Objectives",
        "Cost Optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ce581ba4d1249deeba9f1bffa6739ebbe74df663542fef3893eee5e0a117ae2e_w640_q70.webp",
      "contributions": "1. A gap-driven analysis of existing Kubernetes autoscaling approaches, highlighting their limitations. 2. A safe and explainable multi-signal autoscaling framework that integrates SLO-aware and cost-conscious control with demand forecasting. 3. Experimental evaluation demonstrating significant improvements in SLO violation duration, scaling response time, and infrastructure cost compared to baselines.",
      "summary": "This paper addresses SLO violations and cost inefficiencies in Kubernetes autoscaling by proposing an AIOps-driven framework that uses multi-signal control and lightweight forecasting. The method integrates SLO and cost awareness to improve responsiveness and stability. Evaluation shows it reduces SLO violations by up to 31%, improves response time by 24%, and lowers cost by 18% compared to standard Kubernetes autoscalers.",
      "mindmap": "graph TB\n        Root(”An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”SLO违反与成本低效/SLO Violations & Cost Inefficiency”)\n        Problem --> P2(”反应式扩展与不透明逻辑/Reactive Scaling & Opaque Logic”)\n        Method --> M1(”AIOps驱动的多信号框架/AIOps-Driven Multi-Signal Framework”)\n        Method --> M2(”SLO与成本感知控制/SLO & Cost-Aware Control”)\n        Method --> M3(”轻量级需求预测/Lightweight Demand Forecasting”)\n        Results --> R1(”SLO违反时长减少31%/SLO Violation Duration Reduced by 31%”)\n        Results --> R2(”扩展响应时间提升24%/Scaling Response Time Improved by 24%”)\n        Results --> R3(”基础设施成本降低18%/Infrastructure Cost Lowered by 18%”)"
    },
    {
      "title": "Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates",
      "authors": "Yongjie Guan",
      "institution": "Zhejiang University of Technology",
      "link": "https://arxiv.org/pdf/2512.23434",
      "code": null,
      "tags": [
        "distributed systems",
        "consistent hashing",
        "rendezvous hashing",
        "load balancing",
        "cache locality",
        "minimal churn"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c40c222a782553ce278b2cbc43564ea8beed18effebd850b7b92ac28f04bda05_w640_q70.webp",
      "contributions": "1. Introduces Local Rendezvous Hashing (LRH), which restricts HRW selection to a cache-local window of C distinct neighboring physical nodes on a ring. 2. Proposes next-distinct offsets to enforce bounded distinct candidate enumeration in exactly C ring steps. 3. Demonstrates that under fixed-candidate liveness failover, LRH achieves 0% excess churn while maintaining high throughput and good load balance.",
      "summary": "This paper addresses the trade-off between load balance and performance in consistent hashing for distributed systems. It proposes Local Rendezvous Hashing (LRH), a method that performs a Highest Random Weight selection within a small, cache-local window of nodes on a ring. LRH achieves near-optimal load balance with minimal key churn and significantly higher lookup throughput compared to multi-probe consistent hashing.",
      "mindmap": "graph TB\n        A[Local Rendezvous Hashing] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Ring-based consistent hashing has high load imbalance or scattered memory accesses.]\n        C --> C1[Restrict HRW selection to a cache-local window of C distinct nodes.]\n        D --> D1[Reduces Max/Avg load to 1.0947 and achieves 60.05 Mkeys/s throughput.]"
    },
    {
      "title": "Bitcoin-IPC: Scaling Bitcoin with a Network of Proof-of-Stake Subnets",
      "authors": "Marko Vukolić, Orestis Alpos, Jakov Mitrovski, Themis Papameletiou, Nikola Ristić, Dionysis Zindros",
      "institution": "Bitcoin Scaling Labs, Common Prefix",
      "link": "https://arxiv.org/pdf/2512.23439",
      "code": null,
      "tags": [
        "blockchain scalability",
        "Bitcoin",
        "Layer-2",
        "Proof-of-Stake",
        "interoperability",
        "SegWit"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c762e73812d8ecf4dff85e3904f4f2897f640f483515f122ffbda6dd2edfabcc_w640_q70.webp",
      "contributions": "1. Introduces Bitcoin-IPC, a protocol enabling permissionless creation of Proof-of-Stake Layer-2 subnets with stake denominated in Bitcoin (BTC). 2. Proposes a novel design embedded within Bitcoin's SegWit mechanism, inspired by SWIFT messaging, for seamless cross-subnet value transfer routed through Bitcoin L1. 3. Achieves significant scalability improvements, reducing transaction cost by up to 23x and increasing throughput from 7 to over 160 tps without modifying Bitcoin L1.",
      "summary": "The paper addresses Bitcoin's limited transaction throughput for use as a Medium of Exchange. It proposes Bitcoin-IPC, a protocol that creates a network of programmable Proof-of-Stake Layer-2 chains (subnets) that use Bitcoin for security and settlement. The design significantly increases transaction throughput and reduces cost without requiring changes to the Bitcoin base layer.",
      "mindmap": "graph TB\n        A[Bitcoin-IPC: Scaling Bitcoin with a Network of Proof-of-Stake Subnets] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[比特币作为交换媒介的可扩展性不足/Bitcoin's limited scalability as Medium of Exchange]\n        C --> C1[基于SegWit和SWIFT启发的L2 PoS子网协议/L2 PoS Subnet protocol inspired by SegWit & SWIFT]\n        D --> D1[吞吐量从7 tps提升至160+ tps/Throughput increased from 7 to 160+ tps]\n        D --> D2[每笔交易成本降低高达23倍/Tx cost reduced up to 23x]"
    },
    {
      "title": "Decoupling Adaptive Control in TeaStore",
      "authors": "Eddy Truyen",
      "institution": "DistriNet, KU Leuven",
      "link": "https://arxiv.org/pdf/2512.23495",
      "code": null,
      "tags": [
        "self-adaptive systems",
        "self-adaptation",
        "microservices",
        "control loop",
        "operator pattern",
        "software architecture"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/78e96b6ab1b10ae0a32ddc4e7967c5bedce7be1b0fe5db13bec216c9e0b657ae_w640_q70.webp",
      "contributions": "1. Analyzes how software architectural methods, the cloud-native Operator pattern, and legacy programming techniques can decouple adaptive control from the application logic in a microservice system. 2. Examines the trade-offs between fine-grained expressive adaptation and system-wide control, highlighting when reuse of adaptation strategies is effective. 3. Proposes that these approaches are complementary and can be combined into a multi-tiered architecture for self-adaptive microservices.",
      "summary": "This paper discusses the implementation of self-adaptation in the Adaptable TeaStore microservice benchmark. It examines different technical approaches (software architecture, Operator pattern, programming techniques) for decoupling the adaptive control logic from the application, analyzing their trade-offs. The main conclusion is that these approaches can be combined into a multi-tiered architecture for effective self-adaptive microservices.",
      "mindmap": "graph TB\n        A[Decoupling Adaptive Control in TeaStore] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[实现微服务中的细粒度自适应/Implementing fine-grained self-adaptation in microservices]\n        C --> C1[软件架构方法/Software architectural methods]\n        C --> C2[Operator模式/Operator pattern]\n        C --> C3[传统编程技术/Legacy programming techniques]\n        D --> D1[权衡细粒度与系统范围控制/Trade-offs between fine-grained and system-wide control]\n        D --> D2[可组合的多层架构/Composable multi-tiered architecture]"
    },
    {
      "title": "Fancy Some Chips for Your TeaStore? Modeling the Control of an Adaptable Discrete System",
      "authors": "Anna Gallone, Simon Bliudze, Sophie Cerf, Olga Kouchnarenko",
      "institution": "Université Marie et Louis Pasteur (FEMTO-ST), Univ. Lille (Inria, CNRS, CRIStAL)",
      "link": "https://arxiv.org/pdf/2512.23496",
      "code": "https://github.com/NwaitDev/Chips_Public,",
      "tags": [
        "modeling languages",
        "control theory",
        "distributed systems",
        "Chips",
        "control theory",
        "component-based modeling",
        "Adaptable TeaStore",
        "BIP"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80812c1a02bcd560c40919556c5ed13e3adfb056050e40a68f66bb940765d6f7_w640_q70.webp",
      "contributions": "1. Introduces Chips, a novel language for designing models of complex, intertwined systems by mixing control theory with general-purpose programming concepts. 2. Enables systematic design, modeling, and analysis of adaptable systems through functional block descriptions. 3. Demonstrates the language's application and utility using a variation of the Adaptable TeaStore as a concrete running example.",
      "summary": "This paper introduces Chips, a modeling language that combines control theory with programming concepts to facilitate the design and analysis of robust, component-based systems. The method is demonstrated on an Adaptable TeaStore application, showing how Chips can be used to systematically model complex, interacting entities like software, hardware, and services. The main conclusion is that Chips aids in ensuring system robustness and quality of service for web applications and cyber-physical systems.",
      "mindmap": "graph TB\n        Root[Fancy Some Chips for Your TeaStore?<br/>Modeling the Control of an Adaptable Discrete System] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br/>Web应用需管理复杂、相互依赖的资源以确保鲁棒性] --> Problem_Detail[系统复杂/Complex System<br/>软件、硬件、网络、微服务交织]\n        Method[主要方法/Method<br/>提出Chips建模语言] --> Method_Detail1[混合概念/Mixed Concepts<br/>控制理论 + 通用编程语言]\n        Method --> Method_Detail2[功能块描述/Functional Blocks<br/>生成鲁棒的组件模型]\n        Results[关键结果/Results<br/>系统化设计、建模与分析] --> Results_Detail[案例演示/Case Study<br/>使用Adaptable TeaStore变体验证]"
    },
    {
      "title": "Optimal Configuration of API Resources in Cloud Native Computing",
      "authors": "Eddy Truyen, Wouter Joosen",
      "institution": "DistriNet, KU Leuven",
      "link": "https://arxiv.org/pdf/2512.23494",
      "code": null,
      "tags": [
        "cloud computing",
        "Kubernetes",
        "resource optimization",
        "microservices",
        "DevOps",
        "Bayesian optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fd4cec4e268968c920e0f358d7cea15fb1e4dc177e4b11b53180a3f5172ef65_w640_q70.webp",
      "contributions": "1. Applies an existing black-box optimization framework to the largely unexplored problem of fine-tuning CPU and memory allocation during the DevOps Release phase, before deployment. 2. Empirically evaluates the framework using the TeaStore microservice application and provides a statistical comparison of different optimization algorithms, analyzing their trade-offs. 3. Provides practical guidance on when to use factor screening (for optimal configuration or algorithm comparison with a budget) versus pure Bayesian optimization (for finding a near-optimal configuration).",
      "summary": "This paper applies a black-box optimization framework to tune Kubernetes CPU and memory resource configurations for microservices during the DevOps Release phase, a problem often overlooked in favor of runtime autoscaling. The evaluation on the TeaStore application shows that factor screening is useful for finding the optimal configuration within a budget, but Bayesian optimization without screening is better for finding a near-optimal solution.",
      "mindmap": "graph TB\n        Root[”Optimal Configuration of API Resources in Cloud Native Computing<br/>云原生计算中API资源的最优配置”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br/>Untuned resource allocation before deployment<br/>部署前未调优的资源分配”] --> P1[”子问题/Sub-Problem<br/>Focus on Release phase, not Ops<br/>关注发布阶段，而非运维阶段”]\n        Method[”主要方法/Method<br/>Apply black-box optimization framework<br/>应用黑盒优化框架”] --> M1[”技术/Technique<br/>Factor screening & Bayesian optimization<br/>因子筛选与贝叶斯优化”]\n        Method --> M2[”评估/Evaluation<br/>Use TeaStore microservice app<br/>使用TeaStore微服务应用”]\n        Results[”关键结果/Results<br/>Guidance on screening vs. no screening<br/>关于是否使用筛选的指导”] --> R1[”结果1/Result 1<br/>Screening helps find optimal config with budget<br/>筛选有助于在预算内找到最优配置”]\n        Results --> R2[”结果2/Result 2<br/>Pure BO better for near-optimal config<br/>纯贝叶斯优化对寻找近似最优配置更好”]"
    },
    {
      "title": "AdaptiFlow: An Extensible Framework for Event-Driven Autonomy in Cloud Microservices",
      "authors": "Brice Arléon Zemtsop Ndadji, Simon Bliudze, Clément Quinton",
      "institution": "Univ. Lille, CNRS, Inria, Centrale Lille, CRIStAL",
      "link": "https://arxiv.org/pdf/2512.23499",
      "code": null,
      "tags": [
        "autonomic computing",
        "MAPE-K loop",
        "decentralized adaptation",
        "event-driven",
        "rule-based",
        "microservices"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6cea3a497714ae1ca6ead099b1f31177f6fe2bf3d4bc5a184a21835b1744db4a_w640_q70.webp",
      "contributions": "1. A framework (AdaptiFlow) providing abstraction layers for the Monitor and Execute phases of the MAPE-K loop to enable autonomous microservices. 2. A lightweight, event-driven and rule-based mechanism for specifying adaptation logic, decoupling it from metrics collection and action execution. 3. A workflow for service instrumentation and evidence that decentralized adaptation can emerge from localized decisions without global coordination, validated through three adaptation scenarios (self-healing, self-protection, self-optimization).",
      "summary": "This paper presents AdaptiFlow, a framework for building self-adaptive cloud microservices by decoupling metrics collection and action execution from adaptation logic using an event-driven, rule-based approach. It enables decentralized autonomy, allowing services to adapt locally without global coordination. The framework was validated on a benchmark, demonstrating practical implementation of self-healing, self-protection, and self-optimization scenarios with minimal code changes.",
      "mindmap": "graph TB\n        A[AdaptiFlow: An Extensible Framework for Event-Driven Autonomy in Cloud Microservices] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有方案集中式控制不适用于微服务/Existing centralized control ill-suited for microservices]\n        C --> C1[基于MAPE-K的抽象层与事件驱动规则/MAPE-K abstraction layers & event-driven rules]\n        C --> C2[解耦监控、执行与逻辑/Decouple Monitor/Execute from adaptation logic]\n        D --> D1[实现三种自治场景/Implemented three autonomy scenarios]\n        D --> D2[去中心化适应无需全局协调/Decentralized adaptation without global coordination]"
    },
    {
      "title": "Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space",
      "authors": "Sergey Salishev",
      "institution": "Saint Petersburg State University",
      "link": "https://arxiv.org/pdf/2512.22676",
      "code": null,
      "tags": [
        "digital signal processing",
        "computer arithmetic",
        "high-performance computing",
        "energy-efficient computing",
        "integer-friendly approximation",
        "conflict-free memory access",
        "fast Fourier transform",
        "fast Schur algorithm"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6fe50589b6f9e67a8e1acb929b6cfc7dcaecddcc5c1837d218c89e297ca79994_w640_q70.webp",
      "contributions": "1. A power/energy consumption model for clocked CMOS logic to select optimal parallelism. 2. Integer-friendly approximation methods for elementary functions using constrained piecewise-polynomials to reduce lookup-table size. 3. Provably conflict-free data placement and execution order schemes for mixed-radix streaming FFT on multi-bank/single-port memories.",
      "summary": "This thesis develops signal-processing algorithms and implementation schemes under constraints of minimal parallelism and memory space to improve energy efficiency. It proposes a power model, approximation methods, and conflict-free memory access schemes for FFT and fast Schur algorithms. The results provide constructive theorems and design trade-offs for building efficient specialized accelerators.",
      "mindmap": "graph TB\n        Root[”Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space<br>信号处理算法在最小并行度和内存空间约束下的综合”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>Improving energy efficiency of low-power computing hardware<br>提高低功耗计算硬件的能效”]\n        Method[”主要方法/Method<br>1. Power/energy model for CMOS logic<br>CMOS逻辑功耗/能耗模型<br>2. Integer-friendly function approximation<br>整数友好函数近似<br>3. Conflict-free FFT schedules<br>无冲突FFT调度<br>4. Parallelism/memory analysis for fast Schur algorithm<br>快速Schur算法的并行度/内存分析”]\n        Results[”关键结果/Results<br>Constructive theorems, schedules, and design trade-offs for efficient specialized accelerators<br>为高效专用加速器提供构造性定理、调度方案和设计权衡”]"
    },
    {
      "title": "Revisiting finite Abelian hidden subgroup problem and its distributed exact quantum algorithm",
      "authors": "Ziyuan Dong, Xiang Fan, Tengxun Zhong, Daowen Qiu",
      "institution": "Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.22959",
      "code": null,
      "tags": [
        "quantum computing",
        "hidden subgroup problem",
        "exact quantum algorithm",
        "distributed quantum algorithm",
        "amplitude amplification",
        "Chinese Remainder Theorem"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a7feb61cf81eb163415a6709dd5e0b72019ffd85d693a4f2bc910ebd710ff58b_w640_q70.webp",
      "contributions": "1. Proposes a new, more concise exact quantum algorithm for the finite Abelian hidden subgroup problem using amplitude amplification. 2. Introduces a distributed exact quantum algorithm for the same problem that reduces resource requirements and avoids quantum communication by leveraging the Chinese Remainder Theorem. 3. Develops a parallel exact classical algorithm with reduced query complexity, where the total queries across nodes do not exceed the centralized version under mild conditions.",
      "summary": "This paper revisits the finite Abelian hidden subgroup problem (AHSP). It proposes a new exact quantum algorithm, a distributed quantum algorithm that requires fewer resources and no quantum communication, and a parallel classical algorithm. The main conclusion is that these methods offer more concise, resource-efficient, and scalable solutions for solving the AHSP.",
      "mindmap": "graph TB\n        A[Revisiting finite Abelian hidden subgroup problem and its distributed exact quantum algorithm] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[有限阿贝尔隐藏子群问题 / Finite Abelian Hidden Subgroup Problem]\n        C --> C1[振幅放大 / Amplitude Amplification]\n        C --> C2[中国剩余定理 / Chinese Remainder Theorem]\n        D --> D1[精确量子算法 / Exact Quantum Algorithm]\n        D --> D2[分布式量子算法 / Distributed Quantum Algorithm]\n        D --> D3[并行经典算法 / Parallel Classical Algorithm]"
    },
    {
      "title": "Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity",
      "authors": "Krishna Harsha Kovelakuntla Huthasana, Alireza Olama, Andreas Lundell",
      "institution": "Åbo Akademi University",
      "link": "https://arxiv.org/pdf/2512.23071",
      "code": null,
      "tags": [
        "federated learning",
        "L0 regularization",
        "probabilistic gates",
        "communication efficiency",
        "model sparsity",
        "federated stochastic gradient descent"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/259d06fa8e807f154e153891f40b44308796040886ed51e218b65ee4e67a8c9c_w640_q70.webp",
      "contributions": "1. Proposes a novel federated learning method that enforces an L0 constraint on model parameters using probabilistic gates and their continuous relaxation to achieve target sparsity. 2. Derives the L0 constrained stochastic minimization objective from an entropy maximization problem of the stochastic gates. 3. Demonstrates that the method can achieve high target sparsity (down to ρ=0.005) under data and client heterogeneity with minimal loss in statistical performance, outperforming magnitude pruning-based methods.",
      "summary": "The paper addresses the problem of poor generalizability and communication inefficiency in Federated Learning due to overly dense models. It proposes a method to enforce L0 sparsity constraints via probabilistic gates, deriving the objective from entropy maximization and implementing it with federated stochastic gradient descent. The method is shown to be communication-efficient and achieves high target sparsity with better statistical performance than pruning-based baselines on synthetic and real datasets.",
      "mindmap": "graph TB\n        Root[Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: 数据与模型固有的稀疏性未被解决，导致模型过密、泛化性差，且存在数据和客户端参与异质性。]\n        Method[主要方法/Method: 通过概率门及其连续松弛对非零参数密度施加L0约束，目标源自随机门的熵最大化问题，并基于联邦随机梯度下降。]\n        Results[关键结果/Results: 在数据和客户端异质性下，能达到目标密度(ρ)，统计性能损失最小，且比基于幅度的剪枝方法更优、通信高效。]"
    },
    {
      "title": "Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum",
      "authors": "Dimitrios Amaxilatis, Themistoklis Sarantakos, Nikolaos Tsironis, Souvik Sengupta, Kostas Ramantas, Jhofre Ojeda",
      "institution": "Spark Works Ltd., IONOS SE, Iquadrat Informática S.L.",
      "link": "https://arxiv.org/pdf/2512.21340",
      "code": null,
      "tags": [
        "on-device ai",
        "data spaces",
        "cloud-edge continuum",
        "containerized microservices",
        "edge AI",
        "intelligent infrastructure monitoring"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5ae37cae2dac445e3682b661f116dd30e5d680105ac5070eb7b48c049ab9aff3_w640_q70.webp",
      "contributions": "1. A real-world implementation of intelligent infrastructure monitoring within a data space-enabled cloud-edge framework, demonstrating practical integration. 2. Leveraging edge computing, containerized microservices, and interoperable data sharing to address challenges like sensor integration, data privacy, and scalability. 3. Showcasing the training and deployment of AI/ML services directly at the edge for optimized resource use and timely decision-making in smart city applications.",
      "summary": "This paper presents a real-world use case for smart cities, implementing an intelligent climate monitoring system within a cloud-edge continuum framework enabled by data spaces. The method combines edge computing, containerized microservices, and secure data sharing to facilitate localized analytics and AI deployment. The conclusion highlights the transformative potential of integrating AI, edge computing, and data spaces for building efficient and resilient smart city infrastructures.",
      "mindmap": "graph TB\n        A[Harnessing Data Spaces for Smart City Infrastructures] --> B[核心问题/Problem: Enhancing smart city efficiency, sustainability, and resilience with secure, interoperable data exchange]\n        A --> C[主要方法/Method: Data space-enabled cloud-edge framework with edge computing, containerized microservices, and edge AI/ML]\n        A --> D[关键结果/Results: Demonstrates practical use case for intelligent monitoring, enabling localized analytics, real-time inference, and trusted data collaboration]"
    },
    {
      "title": "DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction",
      "authors": "Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Lukić, Franck Cappello",
      "institution": "University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory",
      "link": "https://arxiv.org/pdf/2512.21433",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "lossy compression",
        "quality prediction",
        "deep-surrogate",
        "mixture-of-experts",
        "feature-extraction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp",
      "contributions": "1) A generalizable surrogate model for predicting compression quality across different compressors, quality metrics, and datasets. 2) A novel two-stage design that decouples expensive feature extraction from lightweight prediction for efficient training and modular inference. 3) A mixture-of-experts design to optimize performance and robustness for time-evolving scientific data.",
      "summary": "This paper proposes DeepCQ, a deep-surrogate framework to efficiently predict the quality of data after lossy compression, which is traditionally computationally expensive to assess. The method uses a two-stage and mixture-of-experts design for generalizability and robustness across different compressors, metrics, and time-evolving datasets. The framework achieves high predictive accuracy (errors under 10%) on real-world applications, enabling informed compression decisions and reducing I/O and computational overhead.",
      "mindmap": "graph TB\n        A[DeepCQ: 通用深度代理框架用于有损压缩质量预测] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[评估压缩后数据质量的计算成本高/Expensive to assess post-compression data quality]\n        C --> C1[两阶段设计: 特征提取 + 轻量预测/Two-stage design: feature extraction + lightweight prediction]\n        C --> C2[专家混合设计处理时变数据/Mixture-of-experts for time-evolving data]\n        D --> D1[预测误差普遍低于10%/Prediction errors generally under 10%]\n        D --> D2[显著优于现有方法/Significantly outperforms existing methods]"
    },
    {
      "title": "Demystifying ARM SME to Optimize General Matrix Multiplications",
      "authors": "Chencheng Deng, Weiling Yang, Jianbin Fang, Dezun Dong",
      "institution": "College of Computer Science and Technology, National University of Defense Technology",
      "link": "https://arxiv.org/pdf/2512.21473",
      "code": null,
      "tags": [
        "gpu kernels",
        "ARM SME",
        "GEMM",
        "cache-aware partitioning",
        "micro-kernels",
        "on-the-fly transposition"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3bc3a0f3452bf6376dcfa53f5f9e56a621c5b526537a2008e7f55c112b765095_w640_q70.webp",
      "contributions": "1. A systematic characterization of the ARM SME architecture that derives optimization guidelines for GEMM. 2. The design and implementation of MpGEMM, an open-source library featuring cache-aware partitioning and efficient data packing with on-the-fly transposition. 3. Specialized micro-kernels that fully utilize SME's multi-vector loads and all available tile registers to maximize performance.",
      "summary": "This paper addresses the underutilization of ARM's Scalable Matrix Extension (SME) hardware for large-scale General Matrix Multiplication (GEMM). It proposes MpGEMM, an open-source library that optimizes GEMM through cache-aware partitioning, efficient data packing, and specialized micro-kernels tailored for SME. Evaluations on an Apple M4 Pro show MpGEMM achieves a 1.23x speedup over the vendor-optimized Apple Accelerate library.",
      "mindmap": "graph TB\n        Root(”Demystifying ARM SME to Optimize General Matrix Multiplications”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”现有库未能充分利用ARM SME硬件/Existing libraries fail to exploit ARM SME”)\n        Problem --> P2(”大规模GEMM性能瓶颈/Large-scale GEMM performance bottlenecks”)\n        Method --> M1(”系统化架构分析/Systematic SME characterization”)\n        Method --> M2(”设计MpGEMM库/Design MpGEMM library”)\n        M2 --> M2a(”缓存感知分区/Cache-aware partitioning”)\n        M2 --> M2b(”高效数据打包/Efficient data packing”)\n        M2 --> M2c(”专用微内核/Specialized micro-kernels”)\n        Results --> R1(”性能超越Apple Accelerate库/Outperforms Apple Accelerate”)\n        Results --> R2(”显著优于其他开源方案/Significantly beats other open-source alternatives”)"
    },
    {
      "title": "Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism",
      "authors": "Xinglin Pan, Shaohuai Shi, Wenxiang Lin, Yuxin Wang, Zhenheng Tang, Wei Wang, Xiaowen Chu",
      "institution": "The Hong Kong University of Science and Technology (Guangzhou), Harbin Institute of Technology (Shenzhen), Hong Kong Baptist University, The Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.21487",
      "code": null,
      "tags": [
        "llm inference",
        "mixture-of-experts (MoE)",
        "disaggregated expert parallelism (DEP)",
        "task scheduling",
        "inference throughput",
        "fine-grained pipelining"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/44cc55e59c66470ffb4e47c93ad8e48f60e8377f30eff6289fbad1cfcb862c96_w640_q70.webp",
      "contributions": "1) Partitioning intensive computation and communication tasks into smaller, fine-grained tasks to enable pipelining, including support for shared experts. 2) Formulating a fine-grained task scheduling optimization problem that supports variable task granularity and ordering. 3) Developing an efficient solver to navigate the large solution space and derive a near-optimal task schedule.",
      "summary": "This paper addresses the memory-intensive inference problem in Mixture-of-Experts (MoE) models by proposing FinDEP, a fine-grained task scheduling algorithm for Disaggregated Expert Parallelism (DEP). FinDEP improves inference throughput by maximizing task overlap through computational partitioning and optimized scheduling. Experiments on systems with up to 32 GPUs show throughput improvements of up to 1.61x over prior methods.",
      "mindmap": "graph TB\n        A[FinDEP: Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[MoE推理内存密集，现有DEP调度效率低/MoE inference is memory-intensive, existing DEP scheduling is inefficient]\n        C --> C1[细粒度任务划分与调度优化/Fine-grained task partitioning and scheduling optimization]\n        D --> D1[吞吐量最高提升1.61倍/Throughput improved by up to 1.61x]"
    },
    {
      "title": "nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures",
      "authors": "Hui Guo, Qihang Zheng, Chenghai Huo, Dongliang Guo, Haoqi Yang, Yang Zhang",
      "institution": "Canaan Inc.",
      "link": "https://arxiv.org/pdf/2512.21571",
      "code": "https://github.com/kendryte/nncase",
      "tags": [
        "compiler & ir",
        "e-graph",
        "term rewriting",
        "phase ordering",
        "NUMA abstraction",
        "auto vectorize"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a555b38ecd80e8c11606362e5402405bbf0053e11754e06f720d50ce213ee0d_w640_q70.webp",
      "contributions": "1. Proposes an end-to-end compilation framework (nncase) that unifies LLM deployment across heterogeneous memory architectures using a NUMA abstraction for a \"compile once, adapt everywhere\" capability. 2. Introduces an e-graph-based term rewriting engine with equality saturation to mitigate the phase ordering problem and enable global optimization of computation and data movement. 3. Integrates three key automated optimization modules: Auto Vectorize for heterogeneous computing units, Auto Distribution for parallel strategies with communication optimization, and Auto Schedule for on-chip cache locality.",
      "summary": "The paper presents nncase, an end-to-end compiler framework designed to tackle the challenge of efficiently deploying large language models on heterogeneous memory architectures. Its core innovation is an e-graph-based rewriting engine that avoids the phase ordering problem, enabling unified optimization across diverse hardware targets. Evaluations show nncase outperforms frameworks like MLC LLM and Intel IPEX, achieving performance close to hand-optimized llama.cpp, demonstrating the viability of automated compilation for high-performance LLM deployment.",
      "mindmap": "graph TB\n        A[nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures] --> B[核心问题/Problem: LLM部署受限于内存架构异构性，传统编译器流程碎片化/Memory architecture heterogeneity hinders efficient LLM deployment, traditional compilers have fragmented workflows.]\n        A --> C[主要方法/Method: 基于e-graph的项重写引擎，统一NUMA抽象，集成自动向量化、分布、调度模块/E-graph-based term rewriting engine, unified NUMA abstraction, integrates Auto Vectorize, Distribution, Schedule modules.]\n        A --> D[关键结果/Results: 性能超越MLC LLM和Intel IPEX，接近手工优化的llama.cpp/Outperforms MLC LLM & Intel IPEX, achieves performance comparable to hand-optimized llama.cpp.]"
    },
    {
      "title": "Embedding Samples Dispatching for Recommendation Model Training in Edge Environments",
      "authors": "Guopeng Li, Haisheng Tan, Chi Zhang, Hongqiu Ni, Zilong Wang, Xinyue Zhang, Yang Xu, Han Tian",
      "institution": "University of Science and Technology of China (USTC), Hefei University of Technology",
      "link": "https://arxiv.org/pdf/2512.21615",
      "code": null,
      "tags": [
        "memory & caching",
        "edge computing",
        "embedding cache",
        "parameter server",
        "sample dispatching",
        "transmission cost"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f94cc43f65f5fd909f8762bda535a33eea94f879931ebe1a563280cb0db1be81_w640_q70.webp",
      "contributions": "1. Proposed ESD, a novel mechanism to optimize the dispatch of input embedding samples to edge workers to minimize embedding transmission cost. 2. Designed HybridDis, a dispatch decision method that combines an optimal algorithm and a heuristic to balance decision quality and resource consumption. 3. Implemented a prototype and demonstrated significant reductions in transmission cost (up to 36.76%) and training speedup (up to 1.74x) on real-world workloads.",
      "summary": "This paper addresses the high communication cost of embedding transmission during Deep Learning Recommendation Model (DLRM) training in edge environments. It proposes ESD, a mechanism that dispatches input samples to edge workers to minimize expected transmission cost, using a hybrid decision method called HybridDis. Experimental results show that ESD significantly reduces transmission cost and speeds up end-to-end training compared to state-of-the-art methods.",
      "mindmap": "graph TB\n        Root[”Embedding Samples Dispatching for Recommendation Model Training in Edge Environments<br>边缘环境中推荐模型训练的嵌入样本调度”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>DLRM边缘训练中嵌入传输成本高”] --> P1[”挑战/Challenges<br>异构网络，资源受限”]\n        Method[”主要方法/Method<br>ESD机制与HybridDis调度”] --> M1[”方法核心/Core<br>基于预期传输成本的样本调度”]\n        Results[”关键结果/Results<br>减少传输成本，加速训练”] --> R1[”性能提升/Improvement<br>成本降低36.76%，速度提升1.74倍”]"
    },
    {
      "title": "LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol for Multicore Real-Time Systems",
      "authors": "Nan Chen, Xiaotian Dai, Tong Cheng, Alan Burns, Iain Bate, Shuai Zhao",
      "institution": "University of York, Sun Yat-sen University",
      "link": "https://arxiv.org/pdf/2512.21701",
      "code": null,
      "tags": [
        "real-time systems",
        "lock-free",
        "fault-tolerance",
        "resource sharing",
        "multicore",
        "worst-case response time analysis"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb19a780f9199527b92c55981536e4b4108e6135efe187882739942a46ebf5ed_w640_q70.webp",
      "contributions": "1. Proposes the LEFT-RS protocol, a lock-free design that allows concurrent read access to global resources and parallel entry into critical sections, improving efficiency. 2. Enhances fault resilience by limiting overhead and enabling tasks to complete earlier if others experience faults, reducing blocking. 3. Provides a comprehensive worst-case response time analysis to ensure timing guarantees for the proposed protocol.",
      "summary": "The paper proposes LEFT-RS, a lock-free and fault-tolerant resource sharing protocol for multicore real-time systems. It allows tasks to concurrently access resources and enter critical sections in parallel, improving efficiency and resilience to transient faults. Evaluation shows it significantly outperforms existing methods, achieving up to an 84.5% average improvement in schedulability.",
      "mindmap": "graph TB\n        Root[LEFT-RS: A Lock-Free Fault-Tolerant Resource Sharing Protocol] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Faults in critical sections cause error propagation; locking protocols lack fault tolerance, increasing blocking.]\n        Method[主要方法/Method: LEFT-RS protocol enables concurrent read access and parallel critical section entry for fault resilience.]\n        Results[关键结果/Results: Up to 84.5% average schedulability improvement over existing approaches.]"
    },
    {
      "title": "Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference",
      "authors": "Linyi Jiang, Yifei Zhu, Hao Yin, Bo Li",
      "institution": "Shanghai Jiao Tong University, Tsinghua University, Hong Kong University of Science and Technology",
      "link": "https://arxiv.org/pdf/2512.21730",
      "code": null,
      "tags": [
        "multi-modal inference",
        "vision transformer",
        "cloud-device collaboration",
        "dynamic scheduling",
        "patch-level importance",
        "weighted ensembling"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/883099a5be7486c0821b7ffc4858fa9de1fb7c6f3487310e5eb913db9f04c63e_w640_q70.webp",
      "contributions": "1. A collaboration-aware importance scorer that identifies critical regions at the patch level for selective processing. 2. A dynamic scheduler that adaptively adjusts patch transmission quality to balance latency and accuracy under changing network conditions. 3. A weighted ensembler that fuses edge and cloud inference results to improve overall accuracy.",
      "summary": "This paper presents Hyperion, a cloud-device collaborative framework designed to enable low-latency inference on Ultra-HD video using off-the-shelf vision transformers. It tackles computational and transmission bottlenecks by selectively processing critical patches, dynamically adjusting transmission quality, and fusing results. Experiments show Hyperion improves frame processing rate by up to 1.61x and accuracy by up to 20.2% compared to baselines.",
      "mindmap": "graph TB\n        A[Hyperion: Low-Latency Ultra-HD Video Analytics<br>Hyperion: 低延迟超高清视频分析] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br>Ultra-HD视频处理的计算与传输瓶颈<br>Computational & Transmission Bottleneck for Ultra-HD Video]\n        C[主要方法/Method<br>云-端协作的Vision Transformer推理框架<br>Cloud-Device Collaborative ViT Inference Framework]\n        D[关键结果/Results<br>处理率提升1.61倍，准确率提升20.2%<br>1.61x Faster Frame Rate, 20.2% Higher Accuracy]"
    },
    {
      "title": "Smart IoT-Based Leak Forecasting and Detection for Energy-Efficient Liquid Cooling in AI Data Centers",
      "authors": "Krishna Chaitanya Sunkara, Rambabu Konakanchi",
      "institution": "Oracle, Charles Schwab",
      "link": "https://arxiv.org/pdf/2512.21801",
      "code": null,
      "tags": [
        "cluster infrastructure",
        "LSTM",
        "Random Forest",
        "MQTT",
        "InfluxDB",
        "Streamlit"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa113d59b2dc6ec7a3bf00f9eebc8541689a6235867cf59ce376cdcfa30a2a5c_w640_q70.webp",
      "contributions": "1. Probabilistic LSTM forecasting validated within ±30-minute windows for coolant leaks, 2. 96.5% F1-score Random Forest detection for immediate leak identification, 3. Integrated smart IoT architecture design with MQTT streaming, InfluxDB storage, and Streamlit dashboards for energy-efficient data center operations.",
      "summary": "The paper proposes a smart IoT monitoring system combining LSTM neural networks for probabilistic leak forecasting and Random Forest classifiers for instant detection in liquid-cooled AI data centers. The system, tested on synthetic data, achieves 96.5% detection accuracy and 87% forecasting accuracy, potentially preventing significant energy waste through proactive maintenance.",
      "mindmap": "graph TB\n        A[Smart IoT-Based Leak Forecasting and Detection] --> B[核心问题/Problem: Coolant leaks cause energy loss in AI data centers]\n        A --> C[主要方法/Method: LSTM for forecasting + Random Forest for detection with IoT sensors]\n        A --> D[关键结果/Results: 96.5% detection accuracy, 87% forecasting accuracy, 1,500 kWh energy saved]"
    },
    {
      "title": "LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices",
      "authors": "Mingyu Sun, Xiao Zhang, Shen Qu, Yan Li, Mengbai Xiao, Yuan Yuan, Dongxiao Yu",
      "institution": "Shandong University",
      "link": "https://arxiv.org/pdf/2512.21835",
      "code": null,
      "tags": [
        "llm inference",
        "collaborative inference",
        "pipeline parallelism",
        "model offloading",
        "memory adaptation",
        "edge computing"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a4d2da6a7be206646ebc1b92d8a0053408a991ec073254f89b4182ecdc54fe1b_w640_q70.webp",
      "contributions": "1. Proposes LIME, a collaborative system for lossless LLM inference across multiple memory-constrained edge devices under limited bandwidth. 2. Employs an interleaved pipeline parallelism with model offloading to dynamically balance computation and communication. 3. Introduces a fine-grained offline allocation scheduler and an online memory adaptation strategy to optimize resource usage and minimize inference latency.",
      "summary": "This paper proposes LIME, a system that enables lossless, collaborative LLM inference on multiple memory-constrained edge devices by using interleaved pipeline parallelism and model offloading, along with offline scheduling and online memory adaptation. Experiments on four Nvidia Jetson devices with LLaMA3.3-70B show that LIME achieves significant speedups over baselines without accuracy loss.",
      "mindmap": "graph TB\n        Root[LIME: 协作式无损LLM推理 / Collaborative Lossless LLM Inference] --> Problem[边缘设备内存受限 / Memory-Constrained Edge Devices]\n        Root --> Method[交织流水线并行与模型卸载 / Interleaved Pipeline Parallelism & Offloading]\n        Root --> Results[实现无损加速 / Achieves Lossless Speedup]"
    },
    {
      "title": "Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models",
      "authors": "Tingyang Sun, Ting He, Bo Ji, Parimal Parag",
      "institution": "Pennsylvania State University, Virginia Tech, Indian Institute of Science",
      "link": "https://arxiv.org/pdf/2512.21884",
      "code": null,
      "tags": [
        "llm inference",
        "distributed inference",
        "block placement",
        "request routing",
        "performance modeling",
        "resource allocation"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/25938e1be55cbd072ba066aea4bb0e492f8b8c2a83e48eaa7e09e800b8697383_w640_q70.webp",
      "contributions": "1. Developed experimentally validated performance models for distributed LLM inference under given block placement and request routing decisions. 2. Formulated the offline optimization problem as a MILP, proved its NP-hardness, and designed a polynomial-complexity algorithm with performance guarantees. 3. Adapted the offline algorithm for the online setting with the same performance guarantee under bounded load.",
      "summary": "This paper addresses the resource allocation problem for geographically-distributed LLM inference, focusing on optimizing block placement and request routing. It proposes performance models, offline and online algorithms with theoretical guarantees, and a lightweight CPU-only simulator. The solution significantly reduces inference time compared to the state-of-the-art in diverse distributed settings.",
      "mindmap": "graph TB\n        A[Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 分布式LLM推理的资源分配优化/Optimizing resource allocation for distributed LLM inference]\n        C[主要方法/Method: 性能建模与优化算法/Performance modeling and optimization algorithms]\n        D[关键结果/Results: 显著降低推理时间/Substantially reduces inference time]"
    },
    {
      "title": "BLEST: Blazingly Efficient BFS using Tensor Cores",
      "authors": "Deniz Elbek, Kamer Kaya",
      "institution": "Sabanci University",
      "link": "https://arxiv.org/pdf/2512.21967",
      "code": null,
      "tags": [
        "gpu kernels",
        "BFS",
        "Tensor Cores",
        "SpMSpV",
        "Graph Reordering",
        "Kernel Fusion"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/da59a50541aea7cf054914628e80911f7ca77a9353af15a6a412588e726ca791_w640_q70.webp",
      "contributions": "1. Introduces Binarised Virtual Slice Sets (BVSS) for warp-level load balancing and eliminating frontier-oblivious work assignment in BFS., 2. Applies two complementary graph reordering strategies (compression-oriented and bandwidth-reducing) to improve memory efficiency and update locality., 3. Develops a batched SpMSpV multiplication pattern using bitwise Tensor Core tiles and combines kernel fusion with a lazy vertex update scheme to reduce synchronization and atomic overheads.",
      "summary": "The paper presents BLEST, a framework that accelerates Breadth-First Search (BFS) on GPUs by efficiently mapping the irregular computation onto dense-math Tensor Cores. The method reformulates the BFS pipeline using a bitmap-oriented structure, specialized load balancing, graph reordering, and kernel fusion. Experiments show that BLEST achieves significant speedups (3.58x to 4.9x) over state-of-the-art GPU-based BFS implementations.",
      "mindmap": "graph TB\n        A[BLEST: Blazingly Efficient BFS using Tensor Cores] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[如何将不规则图BFS映射到密集张量核心/Map irregular BFS to dense Tensor Cores]\n        C --> C1[二值化虚拟切片集/Binarised Virtual Slice Sets (BVSS)]\n        C --> C2[图重排序策略/Graph Reordering Strategies]\n        C --> C3[批处理SpMSpV与核融合/Batched SpMSpV & Kernel Fusion]\n        D --> D1[平均3.58-4.9倍加速/Average 3.58-4.9x Speedup]"
    },
    {
      "title": "Proceedings First Workshop on Adaptable Cloud Architectures",
      "authors": "Giuseppe De Palma, Saverio Giallorenzo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.22054",
      "code": null,
      "tags": [],
      "day": "2025-12-29",
      "thumbnail": null,
      "contributions": "",
      "summary": "",
      "mindmap": ""
    },
    {
      "title": "FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion",
      "authors": "Zhuoran Zhu, Chunyang Zhu, Hao Lin, Xu Fu, Yiming Zhou, Quanlu Zhang, Zhenhua Li, Feng Qian, Chao Yu, Boxun Li, Guohao Dai, Yu Wang",
      "institution": "Tsinghua University, Infinigence AI, University of Southern California, Zhongguancun Academy, Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.22036",
      "code": null,
      "tags": [
        "communication & networking",
        "Mixture-of-Experts",
        "expert parallelism",
        "data shuffling",
        "transformation-communication fusion",
        "collective communication"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7cf69e647d44d7f0b5d2cdef643280359c8d359bbdf2f836c065bb3b6fb214ae_w640_q70.webp",
      "contributions": "1. Identifies the root cause of inefficiency in MoE data shuffling as the misalignment between expert-major and device-major data layouts, requiring disaggregated transformation and communication. 2. Proposes FUSCO, a communication library that fuses data transformation and communication operations into a single, efficient pipeline to eliminate redundant data movement. 3. Introduces lightweight planning and load-balancing mechanisms to eliminate redundant communication and disperse traffic, further optimizing the shuffling process.",
      "summary": "This paper addresses the performance bottleneck of distributed data shuffling in Mixture-of-Experts (MoE) model training and inference. It proposes FUSCO, a communication library that fuses data transformation and communication to align expert-major and device-major data layouts efficiently. Evaluations show FUSCO achieves significant speedups over existing libraries like NCCL and DeepEP, reducing both training and inference latency.",
      "mindmap": "graph TB\n        Root[FUSCO: High-Performance Distributed Data Shuffling] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: MoE专家并行中的数据混洗开销大/High overhead of data shuffling in MoE expert parallelism]\n        Method[主要方法/Method: 通过融合数据转换与通信实现高效混洗/Efficient shuffling via transformation-communication fusion]\n        Results[关键结果/Results: 相比NCCL和DeepEP实现显著加速/Significant speedups over NCCL and DeepEP]"
    },
    {
      "title": "Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View",
      "authors": "Yanmeng Wang, Zhiwen Dai, Shuai Wang, Jian Zhou, Fu Xiao, Tony Q. S. Quek, Tsung-Hui Chang",
      "institution": "The affiliations include IEEE members, suggesting multiple institutions. Based on common patterns, likely institutions include The Chinese University of Hong Kong, Shenzhen (CUHK-Shenzhen) and/or other Chinese universities/tech institutes, given authors like Tsung-Hui Chang and Tony Q. S. Quek are affiliated with such institutions.",
      "link": "https://arxiv.org/pdf/2512.22035",
      "code": null,
      "tags": [
        "federated learning",
        "federated fine-tuning",
        "connection failures",
        "adaptive aggregation",
        "data heterogeneity",
        "convergence guarantee"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09b9295640a8e9a219a19de76effe76cb1ea0696845676f4a5d9a059161538fb_w640_q70.webp",
      "contributions": "1. Proposes FedAuto, a novel Federated Fine-Tuning framework that mitigates the combined effects of unreliable connections and data heterogeneity via adaptive aggregation, requiring no prior knowledge of network conditions. 2. Establishes a rigorous, per-round convergence guarantee for FedAuto that holds for each individual realization, removing common assumptions on failure probabilities or client selection. 3. Demonstrates through extensive experiments that FedAuto outperforms state-of-the-art baselines under diverse failure scenarios for both full and partial-parameter fine-tuning (e.g., LoRA).",
      "summary": "The paper addresses the performance degradation of Federated Fine-Tuning (FFT) in real-world networks with unreliable connections and heterogeneous data. It proposes FedAuto, a framework that uses adaptive aggregation to handle these issues without prior network knowledge or infrastructure changes. Experiments show FedAuto consistently outperforms existing methods and provides stronger theoretical convergence guarantees.",
      "mindmap": "graph TB\n        A[Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[FFT性能受不可靠连接和数据异构性影响/FFT performance degraded by unreliable connections & data heterogeneity]\n        C --> C1[FedAuto: 通过自适应聚合的FFT框架/FedAuto: FFT framework with adaptive aggregation]\n        C --> C2[无需先验网络知识/No prior network knowledge needed]\n        D --> D1[实验表现超越SOTA/Outperforms SOTA baselines]\n        D --> D2[提供严格收敛保证/Provides rigorous convergence guarantee]"
    },
    {
      "title": "Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications",
      "authors": "Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer",
      "institution": "University of Illinois at Urbana-Champaign, IBM Research",
      "link": "https://arxiv.org/pdf/2512.22113",
      "code": null,
      "tags": [
        "agent system",
        "root cause analysis",
        "service dependency graph",
        "program dependence graph",
        "LLM agent",
        "cloud incident"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/62ebd8a01fd966235e0d8d40581cb8352024a391331fada8ea23868c2235ada9_w640_q70.webp",
      "contributions": "1. PRAXIS, an agentic approach for cloud incident RCA with structured, LLM-driven graph reasoning and traversal over microservice and program dependency graphs. 2. An application of the hammock block program dependence graph for agentic RCA, leveraging its hierarchical structure for multi-granular code analysis. 3. A Code-Cloud-RCA Benchmark consisting of 30 real-world incident scenarios injected in a live Kubernetes environment.",
      "summary": "This paper introduces PRAXIS, an orchestrator that uses an LLM-driven agent to traverse service dependency graphs and program dependence graphs to diagnose the root cause of code- and configuration-related cloud incidents. Compared to ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x, as demonstrated on a benchmark of 30 real-world incidents.",
      "mindmap": "graph TB\n        A[Agentic Structured Graph Traversal for Root Cause Analysis<br/>基于智能体结构化图遍历的云应用根因分析] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem<br/>High cost of unresolved cloud incidents; Need for effective root cause analysis]\n        C[主要方法/Method<br/>PRAXIS: LLM-driven traversal over Service Dependency Graph and Program Dependence Graph]\n        D[关键结果/Results<br/>3.1x higher RCA accuracy, 3.8x lower token consumption vs. ReAct baselines]"
    },
    {
      "title": "PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation",
      "authors": "Yuma Ichikawa, Naoya Takagi, Takumi Nakagawa, Yuzi Kanazawa, Akira Sakai",
      "institution": "Fujitsu Limited, RIKEN Center for AIP, Institute of Science Tokyo, Tokai University",
      "link": "https://arxiv.org/pdf/2512.20687",
      "code": null,
      "tags": [
        "llm inference",
        "hierarchical autoregressive model",
        "KV-cache optimization",
        "memory-bound inference",
        "multi-resolution context",
        "throughput-quality trade-off"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6824d7ad660d8d52e1568c90187924380b5fd436a69942bfac67084af3298d40_w640_q70.webp",
      "contributions": "1. Proposes PHOTON, a hierarchical autoregressive model that replaces the Transformer's flat token-by-token scanning with a vertical, multi-resolution context access pattern. 2. Introduces a persistent hierarchy of latent streams, with a bottom-up encoder compressing tokens and lightweight top-down decoders reconstructing token representations, reducing decode-time KV-cache traffic. 3. Demonstrates significant improvements in throughput per unit memory (up to 10^3x) and advantages in long-context and multi-query tasks compared to Transformer-based models.",
      "summary": "The paper identifies that Transformer inference becomes memory-bound due to ever-growing KV-cache reads/writes during autoregressive decoding. To solve this, it proposes PHOTON, a hierarchical model that accesses context vertically at multiple resolutions instead of scanning tokens horizontally. This architectural change drastically reduces memory traffic, yielding orders-of-magnitude higher throughput per unit memory while maintaining quality.",
      "mindmap": "graph LR\n    A[PHOTON: Hierarchical Autoregressive Modeling] --> B[核心问题/Problem: Transformer水平扫描导致KV缓存读写成为内存瓶颈/Horizontal scanning causes memory-bound KV-cache bottleneck]\n    A --> C[主要方法/Method: 用垂直多分辨率层次模型替代/Replace with vertical multi-resolution hierarchical model]\n    A --> D[关键结果/Results: 内存效率与吞吐量大幅提升/Significant improvement in memory efficiency & throughput]"
    },
    {
      "title": "SoK: Speedy Secure Finality",
      "authors": "Yash Saraswat, Abhimanyu Nag",
      "institution": "Indian Institute of Technology, Roorkee; University of Alberta",
      "link": "https://arxiv.org/pdf/2512.20715",
      "code": null,
      "tags": [
        "blockchain consensus",
        "finality",
        "consensus protocol",
        "Ethereum",
        "Gasper",
        "reorg resilience"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/280bac4449b7f86317e6367c908a04f23445bf30d94bffc4957a4d305fac0548_w640_q70.webp",
      "contributions": "1. Provides a systematic survey of the state-of-the-art in Speedy Secure Finality (SSF) protocols, tracing their evolution from foundational works like Goldfish to RLMD-GHOST. 2. Introduces and explains core theoretical primitives for understanding SSF, such as reorganization resilience and the generalized sleepy model. 3. Analyzes the practical trade-offs of Single Slot Finality and surveys the 3-Slot Finality (3SF) protocol as a pragmatic solution balancing fast finality with Ethereum's engineering constraints.",
      "summary": "This paper surveys research on Speedy Secure Finality (SSF) to reduce the long confirmation latency in Ethereum's Gasper protocol. It reviews the evolution of fast finality protocols, analyzes their design trade-offs, and highlights the 3-Slot Finality protocol as a practical synthesis. The main conclusion is that 3SF offers a viable path to achieve faster, secure finality while addressing the network's practical limitations.",
      "mindmap": "graph LR\n    A[SoK: Speedy Secure Finality] --> B[核心问题/Problem: Ethereum Gasper协议15分钟最终确认延迟导致重组攻击和MEV提取]\n    A --> C[主要方法/Method: 系统综述快速最终性协议，分析单时隙最终性的瓶颈，调查3时隙最终性协议]\n    A --> D[关键结果/Results: 3SF协议在理论安全保证与工程约束间取得平衡，是实用的快速最终性方案]"
    },
    {
      "title": "RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale",
      "authors": "Aymen Alsaadi, Mason Hooten, Mariya Goliyad, Andre Merzky, Andrew Shao, Mikhail Titov, Tianle Wang, Yian Chen, Maria Kalantzi, Kent Lee, Andrew Park, Indira Pimpalkhare, Nick Radcliffe, Colin Wahl, Pete Mendygral, Matteo Turilli, Shantenu Jha",
      "institution": "Rutgers University, Hewlett Packard Enterprise, Brookhaven National Laboratory",
      "link": "https://arxiv.org/pdf/2512.20795",
      "code": null,
      "tags": [
        "cluster infrastructure",
        "multi-runtime middleware",
        "hybrid AI-HPC workflows",
        "uniform abstractions",
        "Dragon",
        "vLLM"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/548c8dcfddec763ea481a17e825529b0e0dab60751d6a7f48f3ad27c7f29ea25_w640_q70.webp",
      "contributions": "1. Proposes RHAPSODY, a multi-runtime middleware that composes and coordinates existing runtimes to support heterogeneous AI-HPC workloads within a single job allocation. 2. Introduces uniform abstractions for tasks, services, resources, and execution policies to manage conflicting requirements of simulations, AI services, and agentic workflows. 3. Demonstrates minimal runtime overhead, scalability for inference workloads, and efficient AI-HPC coupling in evaluations on leadership-class HPC platforms.",
      "summary": "The paper addresses the challenge of executing hybrid AI-HPC workflows, which combine simulations, training, and inference with conflicting runtime requirements. It proposes RHAPSODY, a middleware that coordinates existing runtimes through uniform abstractions instead of replacing them. Evaluation shows RHAPSODY enables efficient, scalable execution of these heterogeneous workloads with minimal overhead.",
      "mindmap": "graph LR\n    A[RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale] --> B[核心问题/Problem: 现有系统无法规模化支持混合AI-HPC工作流的异构需求 / Existing systems cannot support heterogeneous requirements of hybrid AI-HPC workflows at scale]\n    A --> C[主要方法/Method: 多运行时中间件，通过统一抽象协调现有运行时 / Multi-runtime middleware coordinating existing runtimes via uniform abstractions]\n    A --> D[关键结果/Results: 开销最小，支持规模化异构与近线性推理扩展 / Minimal overhead, sustains heterogeneity at scale, near-linear inference scaling]"
    },
    {
      "title": "Stochastic well-structured transition systems",
      "authors": "James Aspnes",
      "institution": "Yale University",
      "link": "https://arxiv.org/pdf/2512.20939",
      "code": null,
      "tags": [
        "distributed computing theory",
        "well-structured transition systems",
        "population protocols",
        "probabilistic scheduling",
        "computational complexity",
        "BPP"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6b86e00331b0080766e8bd0e99c46088867daf6771188ff7f0462c5c277cb00_w640_q70.webp",
      "contributions": "1. Defines a new class of stochastic well-structured transition systems (SWSTSs) that unifies models like population protocols and chemical reaction networks under a probabilistic scheduling rule. 2. Proves fundamental limitations on phase clocks in SWSTSs, showing they either stop or tick too fast in expected polynomial time. 3. Provides an exact characterization of computational power, showing augmented SWSTSs compute exactly BPP languages, while unaugmented ones compute symmetric BPL languages.",
      "summary": "This paper extends the theory of well-structured transition systems by incorporating probabilistic scheduling, creating a new class called stochastic well-structured transition systems (SWSTSs). It proves that any phase clock implementation in these systems has polynomial expected duration, and that terminating computations finish in expected polynomial time. These results lead to an exact characterization of computational power, showing that augmented SWSTSs compute exactly the languages in BPP.",
      "mindmap": "graph LR\n    A[Stochastic well-structured transition systems] --> B(核心问题/Problem: Extend WSTS theory to probabilistic scheduling models)\n    A --> C(主要方法/Method: Define SWSTS class unifying population protocols, CRNs, gossip models)\n    A --> D(关键结果/Results: Phase clock limitations; Polynomial expected termination; Computational power = BPP/BPL)"
    },
    {
      "title": "AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences",
      "authors": "Zhe Wang, Jinghang Li, Yifei Zhu",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.20943",
      "code": null,
      "tags": [
        "communication & networking",
        "4D Gaussian Splatting",
        "video streaming",
        "integer linear programming",
        "pruning",
        "keyframe selection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp",
      "contributions": "1. Proposes a streaming-optimized 4DGS framework that converts Gaussian streams into multi-channel 2D formats and uses intelligent keyframe identification to enhance reconstruction quality and reduce training time. 2. Models the 4DGS delivery problem as an integer linear programming problem and designs a lightweight pruning algorithm to adaptively prune Gaussian updates for bandwidth-efficient transmission. 3. Demonstrates significant improvements in quality stability (reducing PSNR deviation by >20%), training speed (6x acceleration), and transmission efficiency (50% size reduction) compared to state-of-the-art methods.",
      "summary": "The paper presents AirGS, a framework that optimizes the training and delivery pipeline for 4D Gaussian Splatting to enable real-time free-viewpoint video streaming. It addresses quality degradation and high bandwidth overhead by introducing a 2D representation format, keyframe selection, and an adaptive pruning algorithm for transmission. Experiments show AirGS significantly improves quality stability, accelerates training, and reduces transmission size.",
      "mindmap": "graph LR\n    A[AirGS: Real-Time 4D Gaussian Streaming] --> B[核心问题/Problem: 4DGS质量下降与高带宽开销/4DGS Quality Degradation & High Bandwidth Overhead]\n    A --> C[主要方法/Method: 流优化框架与自适应剪枝/Streaming-Optimized Framework & Adaptive Pruning]\n    A --> D[关键结果/Results: 质量稳定、训练加速、传输减小/Quality Stable, Training Faster, Transmission Smaller]"
    },
    {
      "title": "Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications",
      "authors": "Yuxiao Wang, Yuedong Xu, Qingyang Duan, Yuxuan Liu, Lei Jiao, Yinghao Yu, Jun Wu",
      "institution": "Fudan University",
      "link": "https://arxiv.org/pdf/2512.20953",
      "code": null,
      "tags": [
        "llm training",
        "3D parallelism",
        "heterogeneous GPUs",
        "spot instances",
        "load balancing",
        "fault-tolerance"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/60ccbe9f51f850501dbeed0dfb992113708962be3840d0248e7a8d0677723ce1_w640_q70.webp",
      "contributions": "1. Introduces AutoHet, a system that automatically identifies optimal parallelism plans for distributed training on heterogeneous GPUs, supporting asymmetric 3D parallelism. 2. Proposes a theoretical optimization model for device grouping and load balancing to minimize per-iteration training time across GPUs with diverse capabilities. 3. Presents an efficient recovery strategy for spot instance preemption that prioritizes retrieving training states locally to minimize checkpoint downloads from cloud storage.",
      "summary": "This paper addresses the challenge of distributed training of large language models on heterogeneous GPU clusters, particularly with spot instances. It proposes AutoHet, a system that automatically optimizes 3D parallelism plans and load balancing for such environments and includes an efficient fault-tolerance mechanism. Evaluations show AutoHet achieves significant speedups in both training throughput and recovery speed compared to existing systems.",
      "mindmap": "graph LR\n    A[论文标题 / Paper Title:<br>Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs] --> B[核心问题 / Problem:<br>Heterogeneous GPU & Spot Instance Training];\n    A --> C[主要方法 / Method:<br>AutoHet System & Optimization Model];\n    A --> D[关键结果 / Results:<br>1.79x Training & 4.38x Recovery Speedup];"
    },
    {
      "title": "Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality",
      "authors": "Sirui Chen, Jingji Chen, Siqi Zhu, Ziheng Jiang, Yanghua Peng, Xuehai Qian",
      "institution": "Tsinghua University, Purdue University, University of Illinois Urbana-Champaign, ByteDance Seed",
      "link": "https://arxiv.org/pdf/2512.20968",
      "code": null,
      "tags": [
        "llm training",
        "distributed attention",
        "communication efficiency",
        "Ring-Attention",
        "communication-computation ratio",
        "scalability"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4944eec84564de9a1d27e811d1317c483f5220256be0880e9e87af0f1df84b8e_w640_q70.webp",
      "contributions": "1. Proposes Mesh-Attention, a new distributed attention algorithm using a matrix-based model that assigns 2D computation tiles to GPUs for lower communication-computation ratio. 2. Introduces a greedy algorithm to efficiently search the scheduling space within a tile under communication constraints. 3. Provides theoretical analysis and extensive experiments showing Mesh-Attention significantly reduces communication volume and achieves speedup compared to state-of-the-art methods.",
      "summary": "This paper addresses the communication bottleneck in scaling LLM context windows by proposing Mesh-Attention, a new distributed attention algorithm that uses 2D computation tiling to reduce communication overhead. It demonstrates superior performance, achieving up to 3.4x speedup and 85.4% communication reduction on 256 GPUs, and shows good scalability for large-scale deployments.",
      "mindmap": "graph LR\n    A[Mesh-Attention<br>论文标题/Paper Title] --> B[核心问题/Problem: 分布式注意力通信开销大<br>High Communication in Distributed Attention]\n    A --> C[主要方法/Method: 基于2D计算块划分的Mesh-Attention算法<br>Mesh-Attention with 2D Tile Assignment]\n    A --> D[关键结果/Results: 通信量减少85.4%, 速度提升3.4倍<br>85.4% Comm Reduction, 3.4x Speedup]"
    },
    {
      "title": "Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions",
      "authors": "Linggao Kong, Yuedong Xu, Lei Jiao, Chuan Xu",
      "institution": "Fudan University, University of Oregon, Inria",
      "link": "https://arxiv.org/pdf/2512.20967",
      "code": null,
      "tags": [
        "llm training",
        "spot instance",
        "online scheduling",
        "deadline-aware",
        "LoRA",
        "integer programming"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6addc088c50bc798ccb2da0947c0b05adb32b7751a390fde44b4c5be3c5b85f3_w640_q70.webp",
      "contributions": "1. Formulated an integer programming problem for deadline-aware LLM fine-tuning using a mix of volatile spot and reliable on-demand GPU instances. 2. Proposed a prediction-based online allocation algorithm and a complementary algorithm without predictions, with a policy selection algorithm that learns the best policy from a parameterized pool. 3. Provided theoretical analysis showing the prediction-based algorithm's performance improves with prediction accuracy and that the policy selection algorithm has a sublinear regret bound, with experiments showing up to 54.8% utility improvement.",
      "summary": "This paper tackles the challenge of cost-effective, deadline-aware scheduling for fine-tuning large language models (LLMs) on volatile GPU spot instances. It proposes an online framework that uses a mix of spot and on-demand instances, featuring a prediction-based algorithm, a non-prediction algorithm, and a policy selection mechanism. The framework adapts to market dynamics, is theoretically grounded, and significantly outperforms baselines in experiments.",
      "mindmap": "graph LR\n    A[Deadline-Aware Online Scheduling for LLM Fine-Tuning] --> B(核心问题/Problem: Expensive LLM fine-tuning with volatile spot instances)\n    A --> C(主要方法/Method: Mixed instance scheduling with prediction & online policy selection)\n    A --> D(关键结果/Results: O(√T) regret, up to 54.8% utility gain)"
    },
    {
      "title": "ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting",
      "authors": "S. M. Shovan, Arindam Khanda, Sanjukta Bhowmick, Sajal K. Das",
      "institution": "Missouri University of Science and Technology, University of North Texas",
      "link": "https://arxiv.org/pdf/2512.21009",
      "code": null,
      "tags": [
        "parallel computing",
        "hypergraph",
        "GPU",
        "dynamic data structure",
        "triad counting",
        "parallel algorithm"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/335f41f5fe79d6bd117abec93eaf4a43675ba18ef5f201690d075fa6085dfc56_w640_q70.webp",
      "contributions": "1. Proposed ESCHER, a novel GPU-centric parallel data structure for efficient representation and management of large-scale dynamic hypergraphs. 2. Designed a hypergraph triad-count update framework that minimizes redundant computation by leveraging ESCHER's dynamic operation capabilities. 3. Demonstrated significant performance improvements, achieving speedups of up to 104.5x, 473.7x, and 112.5x for different triad counting types on real-world and synthetic datasets.",
      "summary": "The paper addresses the computational challenge of analyzing large, dynamic hypergraphs, which lack efficient specialized data structures. It proposes ESCHER, a GPU-centric data structure for representing hypergraph evolution, and a corresponding triad-counting update framework. The method achieves substantial speedups over state-of-the-art approaches in counting various types of hypergraph triads.",
      "mindmap": "graph LR\n    A[ESCHER: Efficient and Scalable Hypergraph Evolution Representation] --> B[核心问题/Problem: 缺乏分析大规模动态超图的高效数据结构]\n    A --> C[主要方法/Method: 提出GPU并行的ESCHER数据结构与三元组计数更新框架]\n    A --> D[关键结果/Results: 性能显著提升，最高达473.7倍加速]"
    },
    {
      "title": "zkFL-Health: Blockchain-Enabled Zero-Knowledge Federated Learning for Medical AI Privacy",
      "authors": "Savvy Sharma, George Petrovic, Sarthak Kaushik",
      "institution": "George Brown Polytechnic",
      "link": "https://arxiv.org/pdf/2512.21048",
      "code": null,
      "tags": [
        "federated learning",
        "zero-knowledge proofs",
        "trusted execution environments",
        "blockchain",
        "medical AI",
        "verifiable aggregation"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eaf3da543259b835ceee63f63b6675f8ca4fdd248035da93b3582ae546b60093_w640_q70.webp",
      "contributions": "1. Proposes a novel architecture (zkFL-Health) that integrates Federated Learning with Zero-Knowledge Proofs and Trusted Execution Environments to ensure privacy and verifiable correctness in medical AI training. 2. Introduces a protocol where the aggregator, operating within a TEE, generates a succinct ZK proof to attest it used the correct inputs and aggregation rule, without revealing client updates. 3. Leverages a blockchain to provide an immutable audit trail of cryptographic commitments and proof verification, removing the need to trust a single party and enhancing regulatory compliance.",
      "summary": "The paper addresses privacy leakage and trust issues in federated learning for healthcare by proposing zkFL-Health, a framework that combines FL with zero-knowledge proofs and TEEs to enable verifiable and private model aggregation. The method ensures the aggregator's computations are provably correct and recorded on a blockchain for auditability. The conclusion is that this approach provides strong confidentiality, integrity, and auditability, which are crucial for clinical adoption.",
      "mindmap": "graph LR\n    A[zkFL-Health] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[隐私泄露与聚合器信任/Privacy Leakage & Aggregator Trust]\n    C --> C1[FL+ZKP+TEE/FL+ZKP+TEE]\n    C --> C2[链上验证/On-chain Verification]\n    D --> D1[可验证的隐私保护/Verifiable Privacy]\n    D --> D2[审计与合规/Auditability & Compliance]"
    },
    {
      "title": "Declarative distributed broadcast using three-valued modal logic and semitopologies",
      "authors": "Murdoch J. Gabbay",
      "institution": "Heriot-Watt University (inferred from author's affiliation)",
      "link": "https://arxiv.org/pdf/2512.21137",
      "code": null,
      "tags": [
        "distributed algorithms",
        "modal logic",
        "declarative specification",
        "semitopologies",
        "three-valued logic",
        "axiomatic theories"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13c32160b520274be9e08e195b14474c8b52625ca44889f1659468dad3c6d782_w640_q70.webp",
      "contributions": "1. Proposes a novel method to formally specify distributed algorithms as declarative axiomatic theories using modal logic, 2. Demonstrates the method's application and scalability on concrete protocols (voting, broadcast, agreement), 3. Shows the method's practical utility by finding errors in a proposed industrial protocol.",
      "summary": "This paper proposes a novel declarative approach for specifying distributed algorithms using three-valued modal logic and semitopologies. It demonstrates the method on protocols like Bracha Broadcast, providing a compact, human-readable specification that abstracts away low-level implementation details. The approach enables precise reasoning about correctness and has been used to find errors in industrial protocols.",
      "mindmap": "graph LR\n    A[Declarative distributed broadcast using three-valued modal logic and semitopologies] --> B[核心问题/Problem: 如何对分布式算法进行形式化、声明式规范？/How to formally specify distributed algorithms declaratively?]\n    A --> C[主要方法/Method: 使用三值模态逻辑和半拓扑作为公理化理论/Using three-valued modal logic and semitopologies as axiomatic theories]\n    A --> D[关键结果/Results: 创建了精确、紧凑的规范，可发现协议错误，支持验证/Creates precise, compact specifications that can find protocol errors and support verification]"
    },
    {
      "title": "Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning",
      "authors": "Antonio Tarizzo, Mohammad Kazemi, Deniz Gündüz",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19777",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7b386ba4532b788c41eccda5b3c48b9585db890467bbb5e150328901a4ad2208_w640_q70.webp",
      "contributions": "",
      "summary": "Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning",
      "mindmap": ""
    },
    {
      "title": "Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform",
      "authors": "Andrea Sordello, Marco Mellia, Idilio Drago, Rodolfo Valentim, Francesco Musumeci, Massimo Tornatore, Federico Cerutti, Martino Trevisan, Alessio Botta, Willen Borges Coelho",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19842",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e888d5285a786a3c767579c7eaaca0375fc971ad3a2fb2064daa28b1f1f886b9_w640_q70.webp",
      "contributions": "",
      "summary": "Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform",
      "mindmap": ""
    },
    {
      "title": "An Adaptive Distributed Stencil Abstraction for GPUs",
      "authors": "Aditya Bhosale, Laxmikant Kale",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19851",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c440cc2cd05a524e05f6d514a65e60b4224e6fad90cc6a9250733b6134c5563a_w640_q70.webp",
      "contributions": "",
      "summary": "An Adaptive Distributed Stencil Abstraction for GPUs",
      "mindmap": ""
    },
    {
      "title": "UCCL-EP: Portable Expert-Parallel Communication",
      "authors": "Ziming Mao, Yihan Zhang, Chihan Cui, Kaichao You, Zhongjie Chen, Zhiying Xu, Scott Shenker, Costin Raiciu, Yang Zhou, Ion Stoica",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19849",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eb2d143c0a9d64bd2c5bdf4142e8e3a096290fd8319372321c00c3c17d53b658_w640_q70.webp",
      "contributions": "",
      "summary": "UCCL-EP: Portable Expert-Parallel Communication",
      "mindmap": ""
    },
    {
      "title": "Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions",
      "authors": "Pengchao Han, Xi Huang, Yi Fang, Guojun Han",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19972",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/68a57a4b48fd01d2fd3b5670a8a2ba0afbbd643b8ddcf8604e3c9fdeb4782d83_w640_q70.webp",
      "contributions": "",
      "summary": "Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions",
      "mindmap": ""
    },
    {
      "title": "Scaling Point-based Differentiable Rendering for Large-scale Reconstruction",
      "authors": "Hexu Zhao, Xiaoteng Liu, Xiwen Min, Jianhao Huang, Youming Deng, Yanfei Li, Ang Li, Jinyang Li, Aurojit Panda",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20017",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0ce1e013da49adc5236a2b8d6111015f3c345c5b5d1cd6d9c9375d46d54a5c3d_w640_q70.webp",
      "contributions": "",
      "summary": "Scaling Point-based Differentiable Rendering for Large-scale Reconstruction",
      "mindmap": ""
    },
    {
      "title": "FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling",
      "authors": "Yaojian Chen, Si-Qiu Gong, Lin Gan, Yanfei Liu, An Yang, Yinuo Wang, Chao-yang Lu, Guangwen Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20064",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/daf5920f62b1d9628f22f64fd603d044ea22b1d4cbcf93ff65d5699edb214a6c_w640_q70.webp",
      "contributions": "",
      "summary": "FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling",
      "mindmap": ""
    },
    {
      "title": "Population Protocols Revisited: Parity and Beyond",
      "authors": "Leszek Gąsieniec, Tytus Grodzicki, Tomasz Jurdziński, Jakub Kowalski, Grzegorz Stachowiak",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20163",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8aad104fc51d7c782bd987b120b6adc44b9209ecea22bf9d780ac0054be6ece3_w640_q70.webp",
      "contributions": "",
      "summary": "Population Protocols Revisited: Parity and Beyond",
      "mindmap": ""
    },
    {
      "title": "SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication",
      "authors": "Chen Zhuang, Lingqi Zhang, Benjamin Brock, Du Wu, Peng Chen, Toshio Endo, Satoshi Matsuoka, Mohamed Wahib",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20178",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/340dca9738b4b1930a1331103d9fe185151f34d58d7be73cc31d211665f20128_w640_q70.webp",
      "contributions": "",
      "summary": "SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication",
      "mindmap": ""
    },
    {
      "title": "Reaching Agreement Among Reasoning LLM Agents",
      "authors": "Chaoyi Ruan, Yiliang Wang, Ziji Shi, Jialin Li",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20184",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/300a553c72ebfc0b096f1fc824a5f548ba652ad4ed3a63bd7596ea2c6fa4c4a9_w640_q70.webp",
      "contributions": "",
      "summary": "Reaching Agreement Among Reasoning LLM Agents",
      "mindmap": ""
    },
    {
      "title": "Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs",
      "authors": "Yinan Ni, Xiao Yang, Yuqi Tang, Zhimin Qiu, Chen Wang, Tingzhou Yuan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20210",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cbb6bb6b2735659dd1638f766c8c42f1c8f809b00b87f15a65d394ddfae14463_w640_q70.webp",
      "contributions": "",
      "summary": "Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs",
      "mindmap": ""
    },
    {
      "title": "Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning",
      "authors": "Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20363",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/59262adfe1a821db6db6b416af65f0ab7cc67a6943505bf052c9306bf801e87f_w640_q70.webp",
      "contributions": "",
      "summary": "Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning",
      "mindmap": ""
    },
    {
      "title": "Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults",
      "authors": "Mohammad Walid Charrwi, Zaid Hussain",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20394",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9b39d3fd875a1d18d18c3b4c5a175ce223ca72ea88ffe8906fbefdd667cb5178_w640_q70.webp",
      "contributions": "",
      "summary": "Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults",
      "mindmap": ""
    },
    {
      "title": "WOC: Dual-Path Weighted Object Consensus Made Efficient",
      "authors": "Tanisha Fonseca, Gengrui Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20485",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a5815c48f85f174b306c1a113a0f90bbdad3e1dbaf0212a44c24b093feff6ff3_w640_q70.webp",
      "contributions": "",
      "summary": "WOC: Dual-Path Weighted Object Consensus Made Efficient",
      "mindmap": ""
    },
    {
      "title": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
      "authors": "Rui Pan, Zhuofu Chen, Ravi Netravali",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20573",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/bab6d51f1421d2f929752b850a0d24b6a7af807b50f1d54c1101b257d175a44f_w640_q70.webp",
      "contributions": "",
      "summary": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
      "mindmap": ""
    },
    {
      "title": "Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation",
      "authors": "Nihir Chadderwala",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17913",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/48ebf691e752728d3961062fe7df081d6a92c7729c4f9968ddd1c3f083bb93df_w640_q70.webp",
      "contributions": "",
      "summary": "Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation",
      "mindmap": ""
    },
    {
      "title": "QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments",
      "authors": "Irwindeep Singh, Sukhpal Singh Gill, Jinzhao Sun, Jan Mol",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17918",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ffe2289ec69b8a1b313e066cc4c4de736d5becc210f22958bfd52daff8ff5e6_w640_q70.webp",
      "contributions": "",
      "summary": "QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments",
      "mindmap": ""
    },
    {
      "title": "Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA",
      "authors": "Allison Li, Kristjan Greenewald, Thomas Parnell, Navid Azizan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17910",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69e52c70ff632453dafd08b1f3a3463c9d2df49fdb8300df0a3e128192436717_w640_q70.webp",
      "contributions": "",
      "summary": "Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA",
      "mindmap": ""
    },
    {
      "title": "Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU",
      "authors": "Bin Xu, Ayan Banerjee, Midhat Urooj, Sandeep K.S. Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17941",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8208ecda566e64a777495316e0aac16897f35ec183a5fb04050258d853af7cf7_w640_q70.webp",
      "contributions": "",
      "summary": "Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU",
      "mindmap": ""
    },
    {
      "title": "Fast Online Digital Twinning on FPGA for Mission Critical Applications",
      "authors": "Bin Xu, Ayan Banerjee, Sandeep K. S. Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17942",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/16ee82367ac1a35aebfd7c95c003a129158965297946e5c0f25e447a72aa119c_w640_q70.webp",
      "contributions": "",
      "summary": "Fast Online Digital Twinning on FPGA for Mission Critical Applications",
      "mindmap": ""
    },
    {
      "title": "ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training",
      "authors": "Yi Yang, Ziyu Lin, Liesheng Wei",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18127",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f0b4f1e094718ec73523430923a11d6db1ca267faf57d9dfeb12c670c753f795_w640_q70.webp",
      "contributions": "",
      "summary": "ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training",
      "mindmap": ""
    },
    {
      "title": "Constrained Cuts, Flows, and Lattice-Linearity",
      "authors": "Robert Streit, Vijay K. Garg",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18141",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4988963d0df9feb5d6d93950c8695a282967332a6a2d340e258ed92a056f4c86_w640_q70.webp",
      "contributions": "",
      "summary": "Constrained Cuts, Flows, and Lattice-Linearity",
      "mindmap": ""
    },
    {
      "title": "TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale",
      "authors": "Dongha Yoon, Younghoon Min, Hoshik Kim, Sam H. Noh, Jongryool Kim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18194",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf126804dfba85c7a794b9b5687408dce6800961fba23c8342730d926fc068da_w640_q70.webp",
      "contributions": "",
      "summary": "TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale",
      "mindmap": ""
    },
    {
      "title": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
      "authors": "Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18318",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp",
      "contributions": "",
      "summary": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
      "mindmap": ""
    },
    {
      "title": "Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching",
      "authors": "Hussein Amro, Basel Fakhri, Amer E. Mouawad, Izzat El Hajj",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18334",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b31bc47ecb1dc74e237f8d5df239958727df951c6edb1120b903f3fd7b5c55be_w640_q70.webp",
      "contributions": "",
      "summary": "Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching",
      "mindmap": ""
    },
    {
      "title": "Snowveil: A Framework for Decentralised Preference Discovery",
      "authors": "Grammateia Kotsialou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18444",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f8761ec2c77d131e1f61b3af656dc162d45194670910e8c8975220f93bfc1af6_w640_q70.webp",
      "contributions": "",
      "summary": "Snowveil: A Framework for Decentralised Preference Discovery",
      "mindmap": ""
    },
    {
      "title": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing",
      "authors": "Wentao Liu, Yuhao Hu, Ruiting Zhou, Baochun Li, Ne Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18674",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0b0a6c1ba7d729d7d1a45d1f2d74caedc5189c982e32587fba450b708786cd88_w640_q70.webp",
      "contributions": "",
      "summary": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing",
      "mindmap": ""
    },
    {
      "title": "A Real-Time Digital Twin for Adaptive Scheduling",
      "authors": "Yihe Zhang, Yash Kurkure, Yiheng Tao, Michael E. Papka, Zhiling Lan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18894",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/228b6575bd24119fca42b0b1a9ba6a42e15166f06d7d2264d4bc894aff71d4ed_w640_q70.webp",
      "contributions": "",
      "summary": "A Real-Time Digital Twin for Adaptive Scheduling",
      "mindmap": ""
    },
    {
      "title": "QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits",
      "authors": "Ivan Čilić, Ivana Podnar Žarko, Pantelis Frangoudis, Schahram Dustdar",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18915",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9dc5e7bd9261487c82b2942a0f628fd37b770391416336515d537b8d9c7608d_w640_q70.webp",
      "contributions": "",
      "summary": "QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits",
      "mindmap": ""
    },
    {
      "title": "Timely Parameter Updating in Over-the-Air Federated Learning",
      "authors": "Jiaqi Zhu, Zhongyuan Zhao, Xiao Li, Ruihao Du, Shi Jin, Howard H.Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19103",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d5fc52e9cecab5d074f9763764f769e7c7bd5aaa186ce1d747f2bbd3fa2caf41_w640_q70.webp",
      "contributions": "",
      "summary": "Timely Parameter Updating in Over-the-Air Federated Learning",
      "mindmap": ""
    },
    {
      "title": "Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT",
      "authors": "Murtaza Rangwala, Richard O. Sinnott, Rajkumar Buyya",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19131",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f5ab0acf932ea7b2d42fac6b935c509aaf0582fb4879eb9fae7e0fe0a8e7f766_w640_q70.webp",
      "contributions": "",
      "summary": "Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT",
      "mindmap": ""
    },
    {
      "title": "L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling",
      "authors": "Yitao Yuan, Chenqi Zhao, Bohan Zhao, Zane Cao, Yongchao He, Wenfei Wu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19179",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1a06667660cf3663082a8dcc7b41e7338eae070225a51b761512a3dfc2c89548_w640_q70.webp",
      "contributions": "",
      "summary": "L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling",
      "mindmap": ""
    },
    {
      "title": "Simulations between Strongly Sublinear MPC and Node-Capacitated Clique",
      "authors": "Philipp Schneider, Julian Werthmann",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19326",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8644a50509dfa5e351f02bea3451463ff322106a9319b73a700313df6f2ab2a4_w640_q70.webp",
      "contributions": "",
      "summary": "Simulations between Strongly Sublinear MPC and Node-Capacitated Clique",
      "mindmap": ""
    },
    {
      "title": "Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives",
      "authors": "Kiril Dichev, Filip Pawlowski, Albert-Jan Yzelman",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19342",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2babd04ddf70f201df2fa1a003998a91a1e266029f2a3a118314f226a7ce88f0_w640_q70.webp",
      "contributions": "",
      "summary": "Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives",
      "mindmap": ""
    },
    {
      "title": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference",
      "authors": "George Karfakis, Faraz Tahmasebi, Binglu Chen, Lime Yao, Saptarshi Mitra, Tianyue Pan, Hyoukjun Kwon, Puneet Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19606",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba70ec7306532ca1c35d62f262fda2524e63fa17cc3a261b1800c846a6c06b2_w640_q70.webp",
      "contributions": "",
      "summary": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference",
      "mindmap": ""
    },
    {
      "title": "EuroHPC SPACE CoE: Redesigning Scalable Parallel Astrophysical Codes for Exascale",
      "authors": "Nitin Shukla, Alessandro Romeo, Caterina Caravita, Lubomir Riha, Ondrej Vysocky, Petr Strakos, Milan Jaros, João Barbosa, Radim Vavrik, Andrea Mignone, Marco Rossazza, Stefano Truzzi, Vittoria Berta, Iacopo Colonnelli, Doriana Medić, Elisabetta Boella, Daniele Gregori, Eva Sciacca, Luca Tornatore, Giuliano Taffoni, Pranab J. Deka, Fabio Bacchini, Rostislav-Paul Wilhelm, Georgios Doulis, Khalil Pierre, Luciano Rezzolla, Tine Colman, Benoît Commerçon, Othman Bouizi, Matthieu Kuhn, Erwan Raffin, Marc Sergent, Robert Wissing, Guillermo Marin, Klaus Dolag, Geray S. Karademir, Gino Perna, Marisa Zanotti, Sebastian Trujillo-Gomez",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18883",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/858132a58cba05e698ccaa1c8a830fb0c87d0b8772070099bf19455acf265c4e_w640_q70.webp",
      "contributions": "",
      "summary": "EuroHPC SPACE CoE: Redesigning Scalable Parallel Astrophysical Codes for Exascale",
      "mindmap": ""
    },
    {
      "title": "Dion2: A Simple Method to Shrink Matrix in Muon",
      "authors": "Kwangjun Ahn, Noah Amsel, John Langford",
      "institution": "Microsoft Research, AI Frontiers, NYU",
      "link": "https://arxiv.org/pdf/2512.16928",
      "code": null,
      "tags": [
        "llm training",
        "Muon optimizer",
        "orthonormalization",
        "matrix shrinking",
        "sampling",
        "Newton-Schulz iterations",
        "FSDP2"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces Dion2, a simple method to reduce the computational cost of the Muon optimizer by sampling a fraction of rows or columns for orthonormalization at each iteration. This sparsifies the update, lowering computation and communication overhead. The method maintains update quality close to full Muon while improving scalability, as shown in training benchmarks.",
      "mindmap": ""
    },
    {
      "title": "Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning",
      "authors": "Baolei Zhang, Minghong Fang, Zhuqing Liu, Biao Yi, Peizhao Zhou, Yuan Wang, Tong Li, Zheli Liu",
      "institution": "Nankai University, University of Louisville, University of North Texas",
      "link": "https://arxiv.org/pdf/2512.17254",
      "code": null,
      "tags": [
        "fault-tolerance",
        "federated learning",
        "byzantine-robust aggregation",
        "privacy-preserving",
        "dimensionality reduction",
        "secure multi-party computation",
        "adaptive tuning"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes ABBR, a practical framework for federated learning that combines Byzantine-robust aggregation with privacy-preserving techniques. Its core method uses dimensionality reduction to speed up private computations and an adaptive tuning strategy to minimize the impact of malicious models. The framework is shown to run significantly faster with minimal overhead while maintaining strong Byzantine resilience.",
      "mindmap": ""
    },
    {
      "title": "Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs",
      "authors": "Ivan Kralj, Lodovico Giaretta, Gordan Ježić, Ivana Podnar Žarko, Šarūnas Girdzijauskas",
      "institution": "University of Zagreb, Faculty of Electrical Engineering and Computing; RISE Research Institutes of Sweden; KTH Royal Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.17352",
      "code": null,
      "tags": [
        "others",
        "adaptive graph pruning",
        "Spatio-Temporal Graph Neural Networks (ST-GNNs)",
        "Sudden Event Prediction Accuracy (SEPA)",
        "online semi-decentralized training",
        "Federated Learning (FL)",
        "Gossip Learning"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/09a9feeec6bf4367fbf82a987484881d47da3c3be2e4b769373b648eb200942b_w640_q70.webp",
      "contributions": "",
      "summary": "This paper proposes an adaptive graph pruning algorithm for Spatio-Temporal Graph Neural Networks (ST-GNNs) to reduce communication overhead in online semi-decentralized traffic prediction systems. It also introduces a novel evaluation metric, SEPA, to measure responsiveness to sudden traffic events. The method maintains prediction accuracy while significantly lowering communication costs across different decentralized learning settings.",
      "mindmap": ""
    },
    {
      "title": "Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing",
      "authors": "Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng",
      "institution": "Wuhan University",
      "link": "https://arxiv.org/pdf/2512.17574",
      "code": null,
      "tags": [
        "multi-modal inference",
        "GPU-internal scheduling",
        "resource sharing",
        "collaborative multi-GPU video decoding",
        "logically decoupled execution",
        "FlashCodec",
        "UnifiedServe"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/65d961bcef453cdff273d0991a97111419acf476f8d34e21f66dbd356156dfa7_w640_q70.webp",
      "contributions": "",
      "summary": "The paper proposes FlashCodec and UnifiedServe, a framework that optimizes multimodal large language model (MLLM) serving by accelerating video decoding and enabling resource sharing across the vision and text stages. This approach reduces latency and eliminates inter-stage blocking, leading to significantly higher throughput and better SLO adherence compared to existing systems.",
      "mindmap": ""
    },
    {
      "title": "SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference",
      "authors": "Jeff Smith",
      "institution": "2nd Set AI",
      "link": "https://arxiv.org/pdf/2512.15742",
      "code": null,
      "tags": [
        "llm inference",
        "vector quantization",
        "spline networks",
        "memory optimization",
        "cache optimization",
        "hardware-aware compilation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces SHARe-KAN, a framework that uses Gain-Shape-Bias Vector Quantization to compress Kolmogorov-Arnold Networks (KANs) by exploiting functional redundancy while preserving their dense, holographic topology. Coupled with a hardware-aware compiler called LUTHAM, it achieves an 88x reduction in runtime memory while matching baseline accuracy, effectively decoupling the workload from DRAM bandwidth constraints.",
      "mindmap": ""
    },
    {
      "title": "LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models",
      "authors": "Yijie Zhi, Yayu Cao, Jianhua Dai, Xiaoyang Han, Jingwen Pu, Qingran Wu, Sheng Cheng, Ming Cai",
      "institution": "Zhejiang University, Zhejiang Institute of Administration, Beijing ShenZhou Aerospace Software Technology Ltd.",
      "link": "https://arxiv.org/pdf/2512.15766",
      "code": null,
      "tags": [
        "llm inference",
        "retrieval-augmented generation",
        "loop transformation",
        "static control part",
        "feedback-based iterative mechanism",
        "equivalence checking"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes LOOPRAG, a retrieval-augmented generation framework that guides Large Language Models (LLMs) to perform effective loop transformation optimization. It uses a loop-aware retrieval algorithm and a feedback-based iterative mechanism with compilation and testing to generate correct and efficient code. The evaluation shows LOOPRAG achieves significant speedups over both traditional compilers and base LLMs on standard benchmark suites.",
      "mindmap": ""
    },
    {
      "title": "Optimizing Agentic Language Model Inference via Speculative Tool Calls",
      "authors": "Daniel Nichols, Prajwal Singhania, Charles Jekel, Abhinav Bhatele, Harshitha Menon",
      "institution": "Lawrence Livermore National Laboratory, University of Maryland",
      "link": "https://arxiv.org/pdf/2512.15834",
      "code": null,
      "tags": [
        "llm inference",
        "speculative tool calls",
        "tool cache",
        "vLLM",
        "prefix-caching"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces system optimizations for language model agents that use external tools, specifically by speculating future tool calls and keeping sequences resident in the inference engine to reduce overhead. These methods lead to significant throughput improvements of hundreds of tokens per second. The authors also propose a new \"tool cache\" API to facilitate adoption of these optimizations.",
      "mindmap": ""
    },
    {
      "title": "Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference",
      "authors": "Jian Tian, Shuailong Li, Yang Cao, Wenbo Cui, Minghan Zhu, Wenkang Wu, Jianming Zhang, Yanpeng Wang, Zhiwen Xiao, Zhenyu Hou, Dou Shen",
      "institution": "Baidu Inc.",
      "link": "https://arxiv.org/pdf/2512.16134",
      "code": null,
      "tags": [
        "llm inference",
        "staggered batch scheduling",
        "load-aware global allocation",
        "DP+EP",
        "time-to-first-token",
        "throughput",
        "data parallelism",
        "expert parallelism"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes Staggered Batch Scheduling (SBS), a method that buffers requests to form optimal batches before dispatching them to a DP+EP inference cluster, eliminating internal queuing. It also introduces a Load-Aware Global Allocation strategy to balance computational load. The system reduces Time-to-First-Token by 30-40% and improves throughput by 15-20% compared to immediate scheduling baselines.",
      "mindmap": ""
    },
    {
      "title": "Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference",
      "authors": "Dhruv Deshmukh, Saurabh Goyal, Nipun Kwatra, Ramachandran Ramjee",
      "institution": "Microsoft Research India",
      "link": "https://arxiv.org/pdf/2512.16391",
      "code": null,
      "tags": [
        "llm inference",
        "sparse attention",
        "dynamic programming",
        "top-k selection",
        "anchor layers",
        "reuse layers",
        "FlashAttention-3"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes Kascade, a training-free sparse attention method that accelerates long-context LLM inference by computing exact Top-k indices in selected anchor layers and reusing them in intermediate layers, based on the stability of high-weight keys across layers. It uses a dynamic programming algorithm to select anchor layers and achieves significant speedups in both prefill and decode phases while maintaining accuracy close to dense attention on benchmarks.",
      "mindmap": ""
    },
    {
      "title": "AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research",
      "authors": "Ignacio Heredia, Álvaro López García, Germán Moltó, Amanda Calatrava, Valentin Kozlov, Alessandro Costantini, Viet Tran, Mario David, Daniel San Martín, Marcin Płóciennik, Marta Obregón Ruiz, Saúl Fernandez, Judith Sáinz-Pardo Díaz, Miguel Caballer, Caterina Alarcón Marín, Stefan Dlugolinsky, Martin Šeleng, Lisana Berberi, Khadijeh Alibabaei, Borja Esteban Sanchis, Pedro Castro, Giacinto Donvito, Diego Aguirre, Sergio Langarita, Vicente Rodriguez, Leonhard Duda, Andrés Heredia Canales, Susana Rebolledo Ruiz, João Machado, Giang Nguyen, Fernando Aguilar Gómez, Jaime Díez",
      "institution": "Instituto de Física de Cantabria (IFCA), Instituto de Instrumentación para Imagen Molecular (I3M), Institute of Informatics, Slovak Academy of Sciences (IISAS), Istituto Nazionale di Fisica Nucleare (INFN), Karlsruher Institut für Technologie, Poznańskie Centrum Superkomputerowo Sieciowe, Centro Nacional de Computação Avançada (CNCA)",
      "link": "https://arxiv.org/pdf/2512.16455",
      "code": null,
      "tags": [
        "cluster infrastructure",
        "federated cloud platform",
        "service catalogue",
        "interactive development environments",
        "GPU resources",
        "annotation tools",
        "experiment tracking",
        "federated learning",
        "model deployment",
        "traceability",
        "reproducibility"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper presents AI4EOSC, a federated cloud platform designed to support the full machine learning lifecycle for scientific research. The platform integrates distributed e-Infrastructures to provide consistent access to development environments, GPU training, annotation tools, and deployment options. Its main conclusion is that this integrated, customizable platform lowers adoption barriers and facilitates reproducible AI workflows for the scientific community.",
      "mindmap": ""
    },
    {
      "title": "Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint",
      "authors": "Endar Suprih Wihidayat, Sieteng Soh, Kwan-Wu Chin, Duc-son Pham",
      "institution": "Sebelas Maret University, Curtin University, University of Wollongong",
      "link": "https://arxiv.org/pdf/2512.16792",
      "code": null,
      "tags": [
        "edge computing",
        "mixed integer linear programming",
        "heuristic algorithm",
        "task offloading",
        "server deployment",
        "budget constraint"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a Multi-stage Edge Server Upgrade (M-ESU) framework, solved via an optimal Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). The main conclusion is that the heuristic solution performs close to optimal for small networks and significantly outperforms alternative strategies in large-scale networks, improving task satisfaction by up to 21.57% under budget and demand growth constraints.",
      "mindmap": ""
    },
    {
      "title": "Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning",
      "authors": "Bahman Abolhassani, Tugba Erpek, Kemal Davaslioglu, Yalin E. Sagduyu, Sastry Kompella",
      "institution": "Nexcepta",
      "link": "https://arxiv.org/pdf/2512.16813",
      "code": null,
      "tags": [
        "others",
        "multi-agent reinforcement learning",
        "QMIX",
        "reactive jamming",
        "channel hopping",
        "power control",
        "Upper Confidence Bound"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a multi-agent reinforcement learning framework based on the QMIX algorithm to coordinate anti-jamming strategies in swarm networks. The method enables agents to jointly select transmission channels and power levels to counter an adaptive reactive jammer. The results show that QMIX achieves near-optimal performance, higher throughput, and lower jamming incidence compared to baseline policies, demonstrating its effectiveness for securing swarm communications.",
      "mindmap": ""
    },
    {
      "title": "Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies",
      "authors": "Astrid Brull, Sara Aguti, Véronique Bolduc, Ying Hu, Daniel M. Jimenez-Gutierrez, Enrique Zuazua, Joaquin Del-Rio, Oleksii Sliusarenko, Haiyan Zhou, Francesco Muntoni, Carsten G. Bönnemann, Xabi Uribe-Etxebarria",
      "institution": "National Institute of Neurological Disorders and Stroke, National Institutes of Health; University College London; Sherpa.ai",
      "link": "https://arxiv.org/pdf/2512.16876",
      "code": null,
      "tags": [
        "others",
        "federated learning",
        "immunofluorescence microscopy",
        "collagen VI-related dystrophies",
        "rare disease diagnosis",
        "decentralized training"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper applies Federated Learning (FL) to train a diagnostic model for a rare disease using collagen VI immunofluorescence images from decentralized datasets across multiple institutions. This approach addresses data scarcity and privacy concerns by keeping patient data local. The resulting FL model outperformed single-institution models, demonstrating improved diagnostic accuracy and generalizability for classifying collagen VI-related dystrophies.",
      "mindmap": ""
    },
    {
      "title": "Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation",
      "authors": "Unai Laskurain, Aitor Aguirre-Ortuzar, Urko Zurutuza",
      "institution": "Mondragon Unibertsitatea",
      "link": "https://arxiv.org/pdf/2512.14767",
      "code": null,
      "tags": [
        "others",
        "Shapley-CMI",
        "Private Set Intersection",
        "Conditional Mutual Information",
        "Vertical Federated Learning",
        "data valuation"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a privacy-preserving method for evaluating feature contributions in Vertical Federated Learning (VFL) using Shapley-CMI and a Private Set Intersection (PSI) server to compute encrypted intersection sizes without sharing raw data. The system enables secure and fair data valuation before model training. Initial experiments confirm the approach's correctness and privacy, demonstrating its viability for secure feature contribution estimation in VFL.",
      "mindmap": ""
    },
    {
      "title": "LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs",
      "authors": "Erik Schultheis, Dan Alistarh",
      "institution": "IST Austria",
      "link": "https://arxiv.org/pdf/2512.15306",
      "code": null,
      "tags": [
        "llm training",
        "8-bit training",
        "activation checkpointing",
        "offloading",
        "copy-engine collectives",
        "dynamic tensor-level scaling",
        "ZeRO-1"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "LLMQ is an end-to-end CUDA/C++ framework that enables efficient 8-bit pretraining and fine-tuning of medium-sized language models on consumer GPUs by employing optimizations like activation checkpointing, offloading, and copy-engine based collectives to overcome memory and communication bottlenecks. It demonstrates that models up to 32B parameters can be trained on affordable hardware like a 4xRTX 4090 workstation while maintaining high FLOP utilization, rivaling the efficiency of production systems on more expensive cloud-grade GPUs.",
      "mindmap": ""
    },
    {
      "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX",
      "authors": "Xuting Liu, Daniel Alexander, Siva Kesava Reddy Kakarla, Behnaz Arzani, Vincent Liu",
      "institution": "University of Pennsylvania, Microsoft Research",
      "link": "https://arxiv.org/pdf/2512.15705",
      "code": null,
      "tags": [
        "llm inference",
        "early-exit",
        "dynamic rebatching",
        "copy-free buffer",
        "SLA-aware scheduler",
        "KV cache",
        "state-copying"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes Dynamic Rebatching and the DREX system to efficiently batch requests in Early-Exit LLMs, where tokens can exit at different layers. DREX dynamically reorganizes batches at exit points using a copy-free buffer and a predictive scheduler, improving throughput by 2-12% while eliminating involuntary exits and preserving output quality.",
      "mindmap": ""
    }
  ]
}