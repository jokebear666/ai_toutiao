{
  "label": "cs.MM",
  "slug": "csmm",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "SyncGait: Robust Long-Distance Authentication for Drone Delivery via Implicit Gait Behaviors",
      "authors": "Zijian Ling, Man Zhou, Hongda Zhai, Yating Huang, Lingchen Zhao, Qi Li, Chao Shen, Qian Wang",
      "institution": "Huazhong University of Science and Technology, Wuhan University, Tsinghua University, Xi'an Jiaotong University",
      "link": "https://arxiv.org/pdf/2512.23778",
      "code": null,
      "tags": [
        "biometric authentication",
        "gait authentication",
        "drone delivery",
        "mutual authentication",
        "arm swing",
        "spoofing attack"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aafeeb48e8be50aaee42b0a5fde6849716a124c81c8181642e777e2398cbf364_w640_q70.webp",
      "contributions": "1. Proposed SyncGait, a novel implicit gait-based mutual authentication system for drone delivery that leverages a user's unique arm swing during walking. 2. Demonstrated robust long-distance authentication performance (&gt;18m) with high accuracy (99.84%) through extensive experiments on 14 datasets from 31 subjects. 3. Showed strong resilience against various spoofing attacks, enhancing security for real-world drone delivery scenarios.",
      "summary": "This paper introduces SyncGait, a system for secure drone delivery that performs mutual authentication by implicitly analyzing a user's unique arm-swing gait as they walk towards the drone. It achieves high accuracy over long distances without requiring extra hardware or explicit user actions. The results show it is robust against spoofing attacks, offering a secure and user-friendly authentication solution.",
      "mindmap": "graph TB\n        Root[”SyncGait: 无人机配送的鲁棒远距离认证 / SyncGait: Robust Long-Distance Authentication for Drone Delivery”] --> Problem[”核心问题 / Problem: 现有无人机配送认证方案距离短、易受攻击 / Existing drone delivery authentication schemes have short range and are vulnerable to attacks.”]\n        Root --> Method[”主要方法 / Method: 利用行走时独特的摆臂行为进行隐式步态认证 / Implicit gait authentication using unique arm swing during walking.”]\n        Root --> Results[”关键结果 / Results: 在>18米距离实现99.84%准确率，抗欺骗攻击 / Achieves 99.84% accuracy at >18m, resilient to spoofing attacks.”]"
    },
    {
      "title": "AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives",
      "authors": "Yanxi Chen, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Xin Li, Peijie Qiu, Hao Wang, Xuanzhao Dong, Yujian Xiong, Anderson Schneider, Yuriy Nevmyvaka, Yalin Wang",
      "institution": "Arizona State University, Clemson University, Washington University in St. Louis, Rice University, Morgan Stanley",
      "link": "https://arxiv.org/pdf/2512.24052",
      "code": "https://github.com/LLM-VLM-GSL/AHA",
      "tags": [
        "multi-modal reasoning",
        "audio-language models",
        "hallucination mitigation",
        "counterfactual hard negatives",
        "preference alignment",
        "temporal reasoning"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/45abc0731e44f649614386415942c67de681cccc206f884d4ce3832844263cdf_w640_q70.webp",
      "contributions": "1. Proposed a taxonomy for audio grounding failures in LALMs, categorizing hallucinations into Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error. 2. Introduced the AHA (Audio Hallucination Alignment) framework, which uses counterfactual hard negative mining to construct a high-quality preference dataset for model alignment. 3. Established AHA-Eval, a diagnostic benchmark to rigorously evaluate fine-grained temporal reasoning capabilities in audio-language models.",
      "summary": "The paper addresses the problem of hallucinations in Large Audio-Language Models (LALMs), where models generate text not grounded in the audio input. To solve this, the authors propose the AHA framework, which uses counterfactual hard negative mining to create a preference dataset for aligning models to distinguish acoustic evidence from fabrications. The resulting aligned model, Qwen-Audio-AHA, shows significant improvements on both the diagnostic AHA-Eval benchmark and public benchmarks, demonstrating effective mitigation of grounding errors.",
      "mindmap": "graph TB\n        Root[AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[Large Audio-Language Models (LALMs) suffer from hallucinations / 大型音频语言模型存在幻觉问题]\n        Method[主要方法/Method] --> M1[Propose AHA framework with counterfactual hard negative mining / 提出AHA框架，使用反事实硬负例挖掘]\n        Method --> M2[Construct preference dataset for alignment / 构建用于对齐的偏好数据集]\n        Results[关键结果/Results] --> R1[13.7% improvement on AHA-Eval benchmark / 在AHA-Eval基准上提升13.7%]\n        Results --> R2[Gains on public benchmarks (MMAU-Test, MMAR) / 在公开基准(MMAU-Test, MMAR)上取得提升]"
    },
    {
      "title": "Neighbor-aware Instance Refining with Noisy Labels for Cross-Modal Retrieval",
      "authors": "Yizhi Liu, Ruitao Pu, Shilin Xu, Yingke Chen, Quan-Hui Liu, Yuan Sun",
      "institution": "Sichuan University, State Key Laboratory of AI Safety, Northumbria University",
      "link": "https://arxiv.org/pdf/2512.24064",
      "code": "https://github.com/perquisite/NIRNL",
      "tags": [
        "cross-modal retrieval",
        "noisy labels",
        "instance refining",
        "margin preserving",
        "neighborhood consensus"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/74b11b5fe9b39647355b1db0a9a978e8e0bd13a99b38678ec9490a28b9024434_w640_q70.webp",
      "contributions": "1. Proposes a novel robust cross-modal learning framework called Neighbor-aware Instance Refining with Noisy Labels (NIRNL). 2. Introduces Cross-modal Margin Preserving (CMP) to enhance discrimination between sample pairs by adjusting relative distances. 3. Designs Neighbor-aware Instance Refining (NIR) to identify and categorize instances into pure, hard, and noisy subsets for tailored optimization.",
      "summary": "This paper addresses the problem of noisy labels in cross-modal retrieval, which degrades model performance. The authors propose the NIRNL framework, which uses Cross-modal Margin Preserving and Neighbor-aware Instance Refining to better utilize all data and mitigate error propagation. Experiments show the method achieves state-of-the-art performance with strong robustness, especially under high noise rates.",
      "mindmap": "graph TB\n        Root[Neighbor-aware Instance Refining with Noisy Labels for Cross-Modal Retrieval] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Noisy labels degrade cross-modal retrieval performance] --> P1[标注噪声/Label Noise]\n        Method[主要方法/Method: NIRNL Framework] --> M1[Cross-modal Margin Preserving (CMP)]\n        Method --> M2[Neighbor-aware Instance Refining (NIR)]\n        M2 --> M2_1[识别子集/Identify Subsets: pure, hard, noisy]\n        Results[关键结果/Results: State-of-the-art performance] --> R1[高噪声下鲁棒/Robust under high noise]"
    },
    {
      "title": "Factorized Learning for Temporally Grounded Video-Language Models",
      "authors": "Wenzheng Zeng, Difei Gao, Mike Zheng Shou, Hwee Tou Ng",
      "institution": "National University of Singapore",
      "link": "https://arxiv.org/pdf/2512.24097",
      "code": "https://github.com/nusnlp/d2vlm",
      "tags": [
        "video-language models",
        "temporal grounding",
        "factorized learning",
        "preference optimization",
        "evidence tokens",
        "video understanding"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c7daa6b2b83cd5b8b5e9ed9cbeed8ecfc3d04fe90ac708e60dda996b6def5b97_w640_q70.webp",
      "contributions": "1. Proposes D2VLM, a framework that decouples the learning of temporal grounding and textual response using a \"grounding then answering with evidence referencing\" paradigm and introduces evidence tokens for explicit event-level visual semantic capture. 2. Introduces Factorized Preference Optimization (FPO), a novel algorithm that explicitly incorporates probabilistic temporal grounding modeling into the preference optimization objective for both grounding and response. 3. Constructs a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding.",
      "summary": "This paper addresses the challenge of accurate temporal grounding in video-language models by proposing a factorized learning approach. It introduces the D2VLM framework, which decouples grounding and response generation, and a novel Factorized Preference Optimization (FPO) algorithm for joint optimization. Experiments show the approach achieves clear advantages over existing methods on various tasks.",
      "mindmap": "graph TB\n        A[Factorized Learning for Temporally Grounded Video-Language Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Existing models struggle with accurate temporal grounding for event-level perception. 现有模型在事件级感知的精确时间定位上存在困难。]\n        C[主要方法/Method: Propose D2VLM framework and Factorized Preference Optimization (FPO). 提出D2VLM框架和因子化偏好优化算法。]\n        D[关键结果/Results: Demonstrates clear advantage on various tasks. 在多种任务上展现出明显优势。]"
    },
    {
      "title": "LiftProj: Space Lifting and Projection-Based Panorama Stitching",
      "authors": "Yuan Jia, Ruimin Wu, Rui Song, Jiaojiao Li, Bin Song",
      "institution": "The affiliations indicate the authors are members of IEEE. The specific institution is not explicitly stated on the first page, but the acknowledgment mentions support from the \"National Nature Science Foundation of China,\" suggesting a Chinese research institution.",
      "link": "https://arxiv.org/pdf/2512.24276",
      "code": null,
      "tags": [
        "image stitching",
        "3D lifting",
        "panoramic stitching",
        "point cloud fusion",
        "cylindrical projection",
        "hole filling"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aa42233e64d5c4ec1cce0b3482d738a3664cd44807fdde7db2fc77e9f5b1a58b_w640_q70.webp",
      "contributions": "1. Proposes a novel panoramic stitching framework that shifts from 2D warping to a 3D consistency paradigm by first lifting images into a dense 3D point representation. 2. Introduces a global cross-view fusion process in a unified 3D coordinate system, augmented by confidence metrics, followed by a unified projection to create a geometrically consistent 360° layout. 3. Designs the framework to be modular, allowing flexible integration of various 3D lifting and completion modules to address unknown regions via hole filling.",
      "summary": "This paper addresses the problem of ghosting and distortion in traditional 2D image stitching for complex 3D scenes with parallax. It proposes LiftProj, a method that lifts images to 3D point clouds for fusion and then projects them onto a panoramic manifold. The approach significantly reduces geometric artifacts and produces more natural panoramas in challenging scenarios.",
      "mindmap": "graph TB\n        A[LiftProj: Space Lifting and Projection-Based Panorama Stitching] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: 2D传统拼接在复杂3D场景中产生重影和扭曲/Ghosting & distortion in 3D scenes with parallax using 2D methods]\n        C[主要方法/Method: 3D空间提升与融合后投影/3D lifting, fusion, and cylindrical projection]\n        D[关键结果/Results: 减少几何失真，生成更自然的全景图/Reduces geometric artifacts, yields more natural panoramas]"
    },
    {
      "title": "HaineiFRDM: Explore Diffusion to Restore Defects in Fast-Movement Films",
      "authors": "Rongji Xun, Junjie Yuan, Zhongjie Wang",
      "institution": "Tongji University, Shanghai Film Restoration Laboratory",
      "link": "https://arxiv.org/pdf/2512.24946",
      "code": "https://anonymous.4open.science/r/HaineiFRDM",
      "tags": [
        "video restoration",
        "diffusion model",
        "film restoration",
        "high-resolution video",
        "patch-wise training",
        "global-local frequency module"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c49ddd01a0523d32c1f20822a4a1d270659f5454212011a90e04a39fe796d1ab_w640_q70.webp",
      "contributions": "1. Proposed HaineiFRDM, a film restoration framework leveraging diffusion models for content understanding to restore indistinguishable film defects. 2. Introduced a patch-wise training/testing strategy with a position-aware Global Prompt and Frame Fusion Module and a global-local frequency module to enable high-resolution restoration on a single 24GB GPU and ensure texture consistency. 3. Constructed a new film restoration dataset containing restored real-degraded films and realistic synthetic data.",
      "summary": "This paper proposes HaineiFRDM, a diffusion model-based framework for restoring high-resolution, real-world films. It addresses limitations of existing open-source methods by using a patch-wise strategy and novel modules to handle high-resolution videos efficiently and introduces a new dataset. Experiments show the model outperforms existing open-source methods in defect restoration.",
      "mindmap": "graph TB\n        A[HaineiFRDM: Explore Diffusion to Restore High-resolution Real-World Films] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[开源方法性能有限/Open-source methods have limited performance]\n        B --> B2[高分辨率电影未探索/High-resolution films unexplored]\n        C --> C1[基于扩散模型的修复框架/Diffusion-based restoration framework]\n        C --> C2[分块训练与测试策略/Patch-wise training & testing]\n        C --> C3[全局-局部频率模块/Global-local frequency module]\n        C --> C4[构建新数据集/Construct new dataset]\n        D --> D1[缺陷修复能力优越/Superior defect restoration ability]\n        D --> D2[代码与数据集将开源/Code & dataset to be released]"
    },
    {
      "title": "Generative Video Compression: Towards 0.01% Compression Rate for Video Transmission",
      "authors": "Xiangyu Chen, Jixiang Luo, Jingyu Xu, Fangqiu Yi, Chi Zhang, Xuelong Li",
      "institution": "Institute of Artificial Intelligence (TeleAI), China Telecom",
      "link": "https://arxiv.org/pdf/2512.24300",
      "code": null,
      "tags": [
        "communication & networking",
        "generative video compression",
        "task-oriented communication",
        "AI Flow",
        "compression-computation trade-off",
        "Level C Shannon-Weaver"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e0223eada26a73d6b029f6d32188e4919884b4dea1b6854eeb80d3b61f3d6ac_w640_q70.webp",
      "contributions": "1. Proposes Generative Video Compression (GVC), a novel framework leveraging generative video models to achieve extreme compression rates as low as 0.02%, 2. Introduces a paradigm shift by trading computation for bandwidth, shifting the reconstruction burden to the receiver using generative priors, 3. Presents a practical compression-computation trade-off strategy for fast inference on consumer-grade GPUs, enabling deployment within the AI Flow framework for constrained environments.",
      "summary": "This paper introduces Generative Video Compression (GVC), a framework that uses generative video models to achieve extreme compression rates by transmitting compact representations and reconstructing video at the receiver. It shifts the computational burden from transmission to inference and proposes a practical trade-off strategy for deployment. The work demonstrates a viable path for efficient video communication in bandwidth-constrained scenarios.",
      "mindmap": "graph TB\n        Root(”Generative Video Compression: Towards 0.01% Compression Rate for Video Transmission”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”能否实现0.01%极端压缩率?/Achieve 0.01% extreme compression rate?”)\n        Problem --> P2(”如何权衡计算与压缩?/Trade computation for compression?”)\n        Problem --> P3(”是否实用可部署?/Practical and deployable?”)\n        Method --> M1(”生成式视频压缩框架/GVC Framework”)\n        Method --> M2(”利用生成先验重建/Use generative priors for reconstruction”)\n        Method --> M3(”压缩-计算权衡策略/Compression-computation trade-off”)\n        Results --> R1(”实现~0.02%压缩率/Achieved ~0.02% compression rate”)\n        Results --> R2(”为AI Flow框架赋能/Enables AI Flow framework”)\n        Results --> R3(”开辟高效视频通信新范式/Opens new practical video communication paradigm”)"
    },
    {
      "title": "Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark",
      "authors": "Hieu Minh Nguyen, Tam Le-Thanh Dang, Kiet Van Nguyen",
      "institution": "University of Information Technology, Vietnam National University, Ho Chi Minh City",
      "link": "https://arxiv.org/pdf/2512.22218",
      "code": null,
      "tags": [
        "visual question answering",
        "signboard VQA",
        "OCR-integrated VQA",
        "multimodal dataset",
        "Vietnamese",
        "multi-agent framework"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d16c0f62d737eb6a3e9a6e55cda2d721775e9d3be82e08e0e0cd03f4d648a93_w640_q70.webp",
      "contributions": "1. Introduces ViSignVQA, the first large-scale Vietnamese dataset for signboard-oriented VQA, capturing diverse linguistic, cultural, and visual characteristics. 2. Benchmarks the task by adapting state-of-the-art VQA models with integrated Vietnamese OCR and language models, demonstrating significant performance gains from OCR-enhanced context. 3. Proposes a novel multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving high accuracy via majority voting.",
      "summary": "This paper addresses the under-explored task of understanding signboard text in natural scenes for Visual Question Answering (VQA), particularly in low-resource languages like Vietnamese. It introduces the ViSignVQA dataset and benchmarks adapted VQA models with OCR integration, showing substantial performance improvements, and proposes a multi-agent framework that achieves high accuracy. The work highlights the importance of domain-specific multimodal resources for enhancing text-based VQA in real-world applications.",
      "mindmap": "graph TB\n        A[”Towards Signboard-Oriented VQA: ViSignVQA Dataset, Method and Benchmark”] --> B[”核心问题/Problem: 理解自然场景中的招牌文本对于VQA的现实应用至关重要，但在低资源语言中仍未充分探索。”]\n        A --> C[”主要方法/Method: 1. 引入首个大规模越南语招牌VQA数据集ViSignVQA。 2. 通过集成越南语OCR和语言模型来适配SOTA VQA模型。 3. 提出结合感知与推理智能体及GPT-4的多智能体VQA框架。”]\n        A --> D[”关键结果/Results: 1. 添加OCR文本使F1分数提升高达209%。 2. 多智能体框架通过多数投票达到75.98%准确率。 3. 创建了首个捕获真实世界场景文本特征的大规模越南语多模态基准。”]"
    },
    {
      "title": "Mesquite MoCap: Democratizing Real-Time Motion Capture with Affordable, Bodyworn IoT Sensors and WebXR SLAM",
      "authors": "Poojan Vanani, Darsh Patel, Danyal Khorami, Siva Munaganuru, Pavan Reddy, Varun Reddy, Bhargav Raghunath, Ishrat Lallmamode, Romir Patel, Assegid Kidané, Tejaswi Gowda",
      "institution": "Arizona State University",
      "link": "https://arxiv.org/pdf/2512.22690",
      "code": null,
      "tags": [
        "motion capture",
        "wearable computing",
        "human-computer interaction",
        "IMU",
        "WebXR",
        "SLAM",
        "IoT",
        "edge computing"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/471e89e8fb0b1b1a76d2b789c6196559fd43adfdff5430691fa6cbaa29efc55b_w640_q70.webp",
      "contributions": "1. An open-source, low-cost inertial motion capture system using 15 body-worn IMU sensors and a smartphone for SLAM-based position tracking. 2. A fully browser-based application built on modern web technologies (WebGL, WebXR, WebSerial) for cross-platform, real-time visualization and recording. 3. Demonstrated performance comparable to commercial optical systems (2-5° joint-angle error) at approximately 5% of the cost, with low latency and high reliability.",
      "summary": "The paper presents Mesquite, an affordable motion capture system that uses wearable IMU sensors and a smartphone with WebXR for SLAM. It operates entirely in a web browser using modern web technologies for real-time tracking. The system achieves accuracy close to expensive commercial systems at a fraction of the cost, aiming to democratize motion capture technology.",
      "mindmap": "graph TB\n        Root[Mesquite MoCap: Democratizing Real-Time Motion Capture] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem: Motion capture is costly and complex, limiting accessibility] --> P1[昂贵且复杂/Expensive & Complex]\n        Problem --> P2[局限于专业实验室/Limited to Specialized Labs]\n        Method[主要方法/Method: Open-source, low-cost system using IoT sensors and web tech] --> M1[身体佩戴IMU传感器网络/Body-worn IMU Sensor Network]\n        Method --> M2[智能手机WebXR SLAM定位/Smartphone WebXR SLAM for Positioning]\n        Method --> M3[基于浏览器的应用/Web-Browser-Based Application]\n        Results[关键结果/Results: Affordable, accurate, and real-time performance] --> R1[成本约为商业系统的5%/~5% Cost of Commercial System]\n        Results --> R2[平均关节角度误差2-5度/Mean Joint-Angle Error 2-5°]\n        Results --> R3[实时低延迟/Real-Time with Low Latency]"
    },
    {
      "title": "Federated Multi-Task Clustering",
      "authors": "S. Dai, G. Sun, F. Li, X. Tang, Q. Wang, Y. Cong",
      "institution": "South China University of Technology, Dalian University of Technology, Xidian University",
      "link": "https://arxiv.org/pdf/2512.22897",
      "code": null,
      "tags": [
        "federated learning",
        "federated clustering",
        "spectral clustering",
        "multi-task learning",
        "tensor methods",
        "ADMM"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5e64c06637c228c53ac30a2069610927ac906eea1fc53a050d87f6f985e001ab_w640_q70.webp",
      "contributions": "1. Proposes a novel Federated Multi-Task Clustering (FMTC) framework that learns personalized models for heterogeneous clients while capturing shared knowledge in a privacy-preserving manner. 2. Introduces a client-side parameterized mapping model for robust out-of-sample inference, eliminating the need for unreliable pseudo-labels. 3. Develops a server-side tensorial correlation module using low-rank regularization on a unified model tensor to explicitly discover common subspace across clients, solved via a privacy-preserving ADMM-based distributed algorithm.",
      "summary": "This paper proposes FMTC, a federated multi-task clustering framework that addresses the limitations of centralized spectral clustering and unreliable pseudo-labels in federated settings. It combines client-side personalized clustering models with a server-side tensorial module to capture shared knowledge, using an ADMM-based algorithm for efficient, privacy-preserving optimization. Experiments show FMTC outperforms existing federated clustering methods.",
      "mindmap": "graph TB\n        A[Federated Multi-Task Clustering] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Centralized models inapplicable to decentralized environments/集中式模型不适用于去中心化环境]\n        B --> B2[Poor generalization due to unreliable pseudo-labels/伪标签不可靠导致泛化性能差]\n        B --> B3[Failure to capture latent client correlations/未能捕获客户端间的潜在关联]\n        C --> C1[Client-side personalized clustering module/客户端个性化聚类模块]\n        C --> C2[Server-side tensorial correlation module/服务器端张量关联模块]\n        C --> C3[ADMM-based distributed algorithm/基于ADMM的分布式算法]\n        D --> D1[Outperforms baselines and SOTA/性能优于基线和前沿方法]\n        D --> D2[Validated on real-world datasets/在真实数据集上得到验证]"
    },
    {
      "title": "Bridging Your Imagination with Audio-Video Generation via a Unified Director",
      "authors": "Jiaxu Zhang, Tianshu Hu, Yuan Zhang, Zenan Li, Linjie Luo, Guosheng Lin, Xin Chen",
      "institution": "ByteDance Intelligent Creation, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.23222",
      "code": "https://kebii.github.io/UniMAGE",
      "tags": [
        "video generation",
        "unified director model",
        "Mixture-of-Transformers",
        "interleaved concept learning",
        "disentangled expert learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c66a6f11305dbb66573f2efcf30ff0c1abc82467fe78ca1d1328a5beb5932ebd_w640_q70.webp",
      "contributions": "1. Proposes UniMAGE, a unified director model that integrates script drafting and key-shot design into a single framework. 2. Introduces a \"first interleaving, then disentangling\" training paradigm (Interleaved Concept Learning and Disentangled Expert Learning) to enhance narrative logic and keyframe consistency. 3. Employs a Mixture-of-Transformers architecture to unify text and image generation within one model.",
      "summary": "This paper addresses the disjoint treatment of script drafting and key-shot design in AI-driven video creation by proposing UniMAGE, a unified director model. It uses a Mixture-of-Transformers architecture and a novel \"first interleaving, then disentangling\" training paradigm to generate coherent scripts and consistent keyframes. Experiments show UniMAGE achieves state-of-the-art performance among open-source models for long-context, multi-shot film generation.",
      "mindmap": "graph TB\n        A[UniMAGE: Bridging Your Imagination with Audio-Video Generation via a Unified Director] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有系统将脚本起草与关键镜头设计视为独立任务/Existing systems treat script drafting and key-shot design as disjoint tasks]\n        C --> C1[提出统一导演模型UniMAGE/Propose unified director model UniMAGE]\n        C --> C2[采用混合Transformer架构/Employ Mixture-of-Transformers architecture]\n        C --> C3[引入”先交错，后解耦”训练范式/Introduce ”first interleaving, then disentangling” training paradigm]\n        D --> D1[在开源模型中达到SOTA性能/Achieves SOTA performance among open-source models]\n        D --> D2[生成逻辑连贯的脚本和视觉一致的图像/Generates logically coherent scripts and visually consistent keyframes]"
    },
    {
      "title": "Multi Agents Semantic Emotion Aligned Music to Image Generation with Music Derived Captions",
      "authors": "Junchang Shi, Gang Li",
      "institution": "University of Science and Technology Beijing",
      "link": "https://arxiv.org/pdf/2512.23320",
      "code": null,
      "tags": [
        "multi-modal generation",
        "music-to-image generation",
        "multi-agent system",
        "valence-arousal alignment",
        "diffusion models",
        "CLIP fine-tuning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cf4833583da2d40740efcb2a747ee2d1334ebcef0ce1d2e3d3717470dea449e2_w640_q70.webp",
      "contributions": "1. A multi-agent semantic-emotion aligned framework (MESA MIG) that uses cooperating agents to refine music-derived captions for image generation. 2. Integration of continuous Valence-Arousal regression from music and a CLIP-based visual emotion head to enforce emotional alignment between music and generated images. 3. Empirical demonstration that the framework outperforms caption-only and single-agent baselines in aesthetic quality, semantic consistency, and emotional alignment.",
      "summary": "This paper addresses the problem of generating images from music by externalizing the visual imagery evoked by listening. The proposed MESA MIG framework uses a multi-agent system to produce and refine structured music captions, and enforces emotional alignment using Valence-Arousal regression. Experiments show it outperforms baselines in semantic and emotional consistency.",
      "mindmap": "graph TB\n        Root[”Multi Agents Semantic Emotion Aligned Music to Image Generation<br>多智能体语义情感对齐的音乐到图像生成”] --> Problem[”Externalize visual imagery from music<br>将音乐引发的视觉意象外显化”]\n        Root --> Method[”Multi-agent caption refinement & VA alignment<br>多智能体描述优化与情感唤醒度对齐”]\n        Root --> Results[”Outperforms baselines in quality & alignment<br>在质量与对齐上超越基线”]"
    },
    {
      "title": "RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction",
      "authors": "Shuhong Liu, Chenyu Bao, Ziteng Cui, Yun Liu, Xuangeng Chu, Lin Gu, Marcos V. Conde, Ryo Umagami, Tomohiro Hashimoto, Zijian Hu, Tianhan Xu, Yuan Gan, Yusuke Kurose, Tatsuya Harada",
      "institution": "The University of Tokyo, NII, Tohoku University, University of Würzburg, RIKEN",
      "link": "https://arxiv.org/pdf/2512.23437",
      "code": null,
      "tags": [
        "3D reconstruction",
        "benchmark",
        "multi-view",
        "physical degradation",
        "neural radiance field",
        "Gaussian splatting"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/2ccf1373873a24f24accd40b8329f93e9af6b32bea07b7e7c4b639214553f8a0_w640_q70.webp",
      "contributions": "1. Introduces RealX3D, a real-capture benchmark for evaluating multi-view visual restoration and 3D reconstruction under diverse physical degradations. 2. Proposes a unified acquisition protocol that captures four families of corruptions (illumination, scattering, occlusion, blurring) at multiple severity levels, providing pixel-aligned low-quality and ground-truth views, RAW images, and dense laser scans. 3. Demonstrates through extensive benchmarking that current state-of-the-art optimization-based and feed-forward reconstruction methods suffer substantial quality degradation when faced with these real-world corruptions.",
      "summary": "This paper introduces RealX3D, a benchmark dataset for evaluating 3D reconstruction and novel view synthesis methods under real-world physical degradations like low light and blur. The benchmark provides aligned low-quality and high-quality multi-view data with ground-truth geometry. Experiments show current methods are fragile to these corruptions, highlighting a gap between lab performance and real-world deployment.",
      "mindmap": "graph TB\n        A[RealX3D: A Physically-Degraded 3D Benchmark] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[真实世界退化影响3D重建/Real-world degradations hinder 3D reconstruction]\n        C --> C1[构建真实捕获基准/Build real-capture benchmark]\n        C --> C2[四类退化, 多严重级别/Four corruption families, multiple severity levels]\n        C --> C3[提供对齐的LQ/GT视图, RAW数据, 激光扫描/Provide aligned LQ/GT views, RAW, laser scans]\n        D --> D1[当前方法质量显著下降/Current methods show substantial quality degradation]\n        D --> D2[突出现实挑战性/Underscores fragility in challenging real environments]"
    },
    {
      "title": "Unlocking WebRTC for End User Driven Innovation",
      "authors": "Kundan Singh",
      "institution": "Intencity Cloud Technologies",
      "link": "https://arxiv.org/pdf/2512.23688",
      "code": null,
      "tags": [
        "web systems",
        "WebRTC",
        "browser extension",
        "API interception",
        "real-time customization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54e58edaa93822049a8bb88a45bce19b8d615d6c0c4f64e6c5e2c16cc6a757d3_w640_q70.webp",
      "contributions": "1. A software architecture enabling end-user-driven customization of WebRTC applications via a browser extension. 2. A tool (RTC Helper) that intercepts and modifies WebRTC APIs in real-time to change web app behavior. 3. Support for rapid prototyping and over a hundred built-in examples across ten+ customization categories for novel communication use cases.",
      "summary": "The paper addresses the problem of vendor lock-in and limited innovation in WebRTC-based applications by proposing RTC Helper, a browser extension that intercepts WebRTC APIs to allow real-time customization by end-users and developers. This enables novel use cases and rapid prototyping without app redeployment. The conclusion is that this approach unlocks user-driven innovation in web multimedia communication.",
      "mindmap": "graph TB\n        A[Unlocking WebRTC for End User Driven Innovation] --> B[核心问题/Problem: Vendor lock-in hinders innovation in WebRTC apps]\n        A --> C[主要方法/Method: RTC Helper browser extension intercepts & modifies WebRTC APIs]\n        A --> D[关键结果/Results: Enables real-time customization & rapid prototyping with many examples]"
    },
    {
      "title": "SemCovert: Secure and Covert Video Transmission via Deep Semantic-Level Hiding",
      "authors": "Zhihan Cao, Xiao Yang, Gaolei Li, Jun Wu, Jianhua Li, Yuchen Liu",
      "institution": "Shanghai Jiao Tong University, North Carolina State University",
      "link": "https://arxiv.org/pdf/2512.22233",
      "code": null,
      "tags": [
        "semantic communication security",
        "semantic hiding",
        "randomized embedding",
        "secret semantic extractor"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c4b368b0e0e2b9ae1e6c7bf4629ba64b86e8e570fdf21089b033a631ae92af3c_w640_q70.webp",
      "contributions": "1. Proposes SemCovert, a deep semantic-level hiding framework for secure and covert video transmission, integrating co-designed hiding and extraction models into the semantic communication pipeline. 2. Introduces a randomized semantic hiding strategy to break embedding determinism and introduce unpredictable distribution patterns, improving resistance to analysis. 3. Demonstrates through experiments that the framework effectively mitigates eavesdropping/detection risks, reliably conceals secret videos with minor video quality degradation, preserving transmission fidelity.",
      "summary": "This paper addresses privacy leakage in video semantic communication by proposing SemCovert, a framework that hides secret information at the semantic level using co-designed models and a randomized hiding strategy. The method enables authorized recovery while remaining imperceptible to regular users and resistant to statistical analysis. Experimental results confirm its effectiveness for secure and covert transmission without significantly compromising video quality.",
      "mindmap": "graph TB\n        A[SemCovert: Secure and Covert Video Transmission] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[隐私泄露风险/Privacy Leakage Risk]\n        B --> B2[传统安全技术不适用/Traditional Security Inapplicable]\n        C --> C1[语义隐藏模型与提取器/Semantic Hiding Model & Extractor]\n        C --> C2[随机化语义隐藏策略/Randomized Semantic Hiding Strategy]\n        D --> D1[有效缓解窃听与检测风险/Effectively Mitigates Eavesdropping & Detection]\n        D --> D2[可靠隐藏秘密视频/Reliably Conceals Secret Videos]\n        D --> D3[视频质量轻微下降/Minor Video Quality Degradation]"
    },
    {
      "title": "Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding",
      "authors": "A V Uday Kiran Kandala",
      "institution": "Queen Mary University of London",
      "link": "https://arxiv.org/pdf/2512.21698",
      "code": null,
      "tags": [
        "steganography",
        "raster domain steganography",
        "glyph perturbation",
        "deterministic rasterization",
        "multimodal embedding",
        "text-based data hiding"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/86fd7815a5837b02c5d9e31511c3faca8253eee6c4c977836c3a42782decfdc2_w640_q70.webp",
      "contributions": "1. Proposes a unified Glyph Perturbation Cardinality (GPC) framework for embedding heterogeneous data (text, images, audio, video) directly into the pixel space of rendered text glyphs. 2. Operates exclusively in the raster domain after font rendering, modifying bitmap pixels with minimal, visually imperceptible intensity increments for covert communication. 3. Introduces a decoding method based on re-rasterizing cover text, subtracting canonical glyph rasters, and recovering payload via pixel count analysis, leveraging deterministic raster behavior.",
      "summary": "This paper introduces a raster domain steganography framework that embeds multimodal data into text by minimally perturbing the interior pixels of rendered glyphs. The method is visually imperceptible and computationally lightweight, enabling ordinary text to serve as a covert medium for secure data embedding. It generalizes beyond traditional linguistic steganography by operating directly on the deterministic bitmap output of text rendering pipelines.",
      "mindmap": "graph TB\n        Root(”Raster Domain Text Steganography: A Unified Framework for Multimodal Secure Embedding”) --> Problem(”核心问题/Problem: How to embed multimodal data covertly into ordinary text?”)\n        Root --> Method(”主要方法/Method: Glyph Perturbation Cardinality (GPC) Framework”)\n        Root --> Results(”关键结果/Results: Visually imperceptible, lightweight embedding in raster domain”)\n        Problem --> P1(”传统方法局限/Limitations of linguistic & structural methods”)\n        Method --> M1(”操作于栅格化后/Operates post-rasterization”)\n        Method --> M2(”扰动字形内部像素/Perturbs interior ink pixels”)\n        Method --> M3(”基于像素基数编码/Encodes via pixel cardinality”)\n        Results --> R1(”支持多模态数据/Supports multimodal data”)\n        Results --> R2(”解码稳定可靠/Stable & decodable signal”)"
    },
    {
      "title": "Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion",
      "authors": "Huatuan Sun, Yunshan Ma, Changguang Wu, Yanxin Zhang, Pengfei Wang, Xiaoyu Du",
      "institution": "Nanjing University of Science and Technology, Singapore Management University, University of Wisconsin-Madison, GienTech Technology Co., Ltd.",
      "link": "https://arxiv.org/pdf/2512.21863",
      "code": null,
      "tags": [
        "recommender systems",
        "frozen large video language models",
        "micro-video recommendation",
        "feature fusion",
        "intermediate hidden states",
        "dual feature fusion"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6963682bbae94363e4d55953d0681db3b9e674fbd82bcf7d3d2a179f4ea6f73e_w640_q70.webp",
      "contributions": "1. Conducted the first systematic empirical study on integrating frozen LVLMs into micro-video recommendation, evaluating feature extraction paradigms (captions vs. hidden states) and integration strategies (replacement vs. fusion) with ID embeddings. 2. Derived three key principles: intermediate hidden states outperform captions, ID embeddings are irreplaceable (fusion > replacement), and the effectiveness of hidden states varies across layers. 3. Proposed the Dual Feature Fusion (DFF) Framework, a lightweight plug-and-play method that adaptively fuses multi-layer LVLM representations with ID embeddings, achieving state-of-the-art performance.",
      "summary": "This paper systematically studies how to best integrate frozen Large Video-Language Models (LVLMs) as feature extractors for micro-video recommendation. It finds that using intermediate decoder hidden states and fusing them with item ID embeddings is superior to using generated captions or replacing IDs. Based on these insights, the authors propose the Dual Feature Fusion (DFF) framework, which achieves state-of-the-art results on benchmark datasets.",
      "mindmap": "graph TB\n        Root(”Frozen LVLMs for Micro-Video Recommendation”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”LVLM集成缺乏系统评估/Lack of systematic evaluation for LVLM integration”)\n        Method --> M1(”系统实证研究/Systematic empirical study”)\n        Method --> M2(”提出DFF框架/Propose DFF Framework”)\n        M1 --> M1a(”比较特征提取范式/Compare feature extraction paradigms”)\n        M1 --> M1b(”比较ID集成策略/Compare ID integration strategies”)\n        M2 --> M2a(”自适应融合多层特征/Adaptively fuse multi-layer features”)\n        M2 --> M2b(”轻量级即插即用/Lightweight plug-and-play”)\n        Results --> R1(”中间隐藏态优于描述/Intermediate hidden states > captions”)\n        Results --> R2(”ID嵌入不可替代/Fusion > replacement”)\n        Results --> R3(”DFF实现SOTA性能/DFF achieves SOTA performance”)"
    },
    {
      "title": "Data relativistic uncertainty framework for low-illumination anime scenery image enhancement",
      "authors": "Yiquan Gao, John See",
      "institution": "Heriot-Watt University",
      "link": "https://arxiv.org/pdf/2512.21944",
      "code": null,
      "tags": [
        "low-light image enhancement",
        "Data Relativistic Uncertainty",
        "Unsupervised Learning",
        "EnlightenGAN",
        "Anime Scenery",
        "Domain Gap"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/884273bf5357a2746f38f8b7d66727daf4d692ca1d5b3c247dfd4e89a209fdef_w640_q70.webp",
      "contributions": "1. Constructed an unpaired anime scenery dataset to address data scarcity for the underexplored task of low-illumination anime image enhancement. 2. Proposed a Data Relativistic Uncertainty (DRU) framework, inspired by Relativistic GAN, to interpretably define and quantify illumination uncertainty. 3. Demonstrated that dynamically adjusting objective functions based on data uncertainty leads to superior perceptual and aesthetic quality compared to state-of-the-art methods.",
      "summary": "This paper addresses the problem of enhancing low-illumination anime scenery images, a task with a domain gap from natural image enhancement. The authors propose a Data Relativistic Uncertainty (DRU) framework that quantifies illumination uncertainty to dynamically recalibrate model learning. Experiments show their method, applied to EnlightenGAN variants, outperforms existing methods in perceptual and aesthetic quality.",
      "mindmap": "graph TB\n        Root(”Data relativistic uncertainty framework for low-illumination anime scenery image enhancement”) --> Problem\n        Root --> Method\n        Root --> Results\n        Problem(”核心问题/Problem: Low-illumination quality degradation in anime scenery images, an underexplored task with a domain gap from natural images.”)\n        Method(”主要方法/Method: Propose a Data Relativistic Uncertainty (DRU) framework to quantify illumination uncertainty and dynamically adjust objective functions for model recalibration.”)\n        Results(”关键结果/Results: DRU framework yields superior perceptual and aesthetic qualities beyond state-of-the-art methods.”)"
    },
    {
      "title": "NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder",
      "authors": "Daichi Arai, Kyohei Unno, Yasuko Sugito, Yuichi Kusakabe",
      "institution": "Science & Technology Research Laboratories, NHK",
      "link": "https://arxiv.org/pdf/2512.20871",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "360-degree video",
        "implicit neural representations",
        "viewport decoding",
        "spatial-temporal affine transform",
        "video compression"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/77be849ee4606db0adeefbd5d6ebfff328fcdd9b300007aa2dc353aaea4af87e_w640_q70.webp",
      "contributions": "1. Proposes NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of the entire panoramic frame for 360-degree videos. 2. Introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. 3. Achieves significant reductions in memory consumption (7x) and increases in decoding speed (2.5x) compared to prior work HNeRV, while improving image quality.",
      "summary": "The paper addresses the high memory usage and slow decoding of applying implicit neural representations (NeRV) to high-resolution 360-degree videos. It proposes NeRV360, a framework that integrates viewport extraction directly into the decoding process using a conditional spatial-temporal affine transform module. Experiments show NeRV360 significantly reduces memory consumption and increases decoding speed while delivering better image quality compared to prior methods.",
      "mindmap": "graph LR\n    A[NeRV360] --> B[核心问题/Problem: High memory & slow decoding for 360° NeRV]\n    A --> C[主要方法/Method: Viewport decoder with spatial-temporal affine transform]\n    A --> D[关键结果/Results: 7x memory↓, 2.5x speed↑, better quality]"
    },
    {
      "title": "MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model",
      "authors": "Haoyu Wang, Yitong Wang, Jining Wang",
      "institution": "Fudan University",
      "link": "https://arxiv.org/pdf/2512.20916",
      "code": null,
      "tags": [
        "rag (retrieval-augmented generation)",
        "multimodal large language model",
        "sequential recommendation",
        "retrieval-augmented generation",
        "supervised fine-tuning",
        "multi-task learning"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/24628be848ba62fad6bf4a863ec676471dac0625d17315b1ad3a57018467d1d9_w640_q70.webp",
      "contributions": "1. Proposes a method to use MLLMs to adaptively summarize multimodal items into concise keywords via fine-tuning with a custom reward function. 2. Integrates collaborative signals into the recommendation process by transforming them into keywords and using them as supplementary context, inspired by RAG. 3. Aligns the MLLM for multimodal sequential recommendation through supervised fine-tuning with multi-task learning, balancing performance, interpretability, and computational cost.",
      "summary": "This paper proposes MMSRARec, a method that uses a Multimodal Large Language Model (MLLM) to summarize user behavior sequences and integrate collaborative signals for sequential recommendation. The approach fine-tunes the MLLM with adaptive summarization and retrieval-augmented context to improve efficiency and interpretability. Evaluations show it effectively understands user histories for accurate recommendations.",
      "mindmap": "graph LR\n    A[MMSRARec] --> B[核心问题/Problem]\n    A --> C[主要方法/Method]\n    A --> D[关键结果/Results]\n    B --> B1[现有MLLM推荐方法存在局限性/Existing MLLM Rec Methods Have Limitations]\n    C --> C1[自适应多模态摘要/Adaptive Multimodal Summarization]\n    C --> C2[检索增强协同信号/Retrieval-Augmented Collaborative Signals]\n    C --> C3[监督微调与多任务学习/Supervised Fine-Tuning & Multi-Task Learning]\n    D --> D1[高效且可解释的推荐/Effective & Interpretable Recommendation]"
    },
    {
      "title": "AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences",
      "authors": "Zhe Wang, Jinghang Li, Yifei Zhu",
      "institution": "Shanghai Jiao Tong University",
      "link": "https://arxiv.org/pdf/2512.20943",
      "code": null,
      "tags": [
        "communication & networking",
        "4D Gaussian Splatting",
        "video streaming",
        "integer linear programming",
        "pruning",
        "keyframe selection"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67fcf9423d5e160d4a5d4e949518213bf3a4f37a910a1c4fe209190f990922dc_w640_q70.webp",
      "contributions": "1. Proposes a streaming-optimized 4DGS framework that converts Gaussian streams into multi-channel 2D formats and uses intelligent keyframe identification to enhance reconstruction quality and reduce training time. 2. Models the 4DGS delivery problem as an integer linear programming problem and designs a lightweight pruning algorithm to adaptively prune Gaussian updates for bandwidth-efficient transmission. 3. Demonstrates significant improvements in quality stability (reducing PSNR deviation by >20%), training speed (6x acceleration), and transmission efficiency (50% size reduction) compared to state-of-the-art methods.",
      "summary": "The paper presents AirGS, a framework that optimizes the training and delivery pipeline for 4D Gaussian Splatting to enable real-time free-viewpoint video streaming. It addresses quality degradation and high bandwidth overhead by introducing a 2D representation format, keyframe selection, and an adaptive pruning algorithm for transmission. Experiments show AirGS significantly improves quality stability, accelerates training, and reduces transmission size.",
      "mindmap": "graph LR\n    A[AirGS: Real-Time 4D Gaussian Streaming] --> B[核心问题/Problem: 4DGS质量下降与高带宽开销/4DGS Quality Degradation & High Bandwidth Overhead]\n    A --> C[主要方法/Method: 流优化框架与自适应剪枝/Streaming-Optimized Framework & Adaptive Pruning]\n    A --> D[关键结果/Results: 质量稳定、训练加速、传输减小/Quality Stable, Training Faster, Transmission Smaller]"
    },
    {
      "title": "Blurb-Refined Inference from Crowdsourced Book Reviews using Hierarchical Genre Mining with Dual-Path Graph Convolutions",
      "authors": "Suraj Kumar, Utsav Kumar Nareti, Soumi Chattopadhyay, Chandranath Adak, Prolay Mallick",
      "institution": "Indian Institute of Technology Indore, Indian Institute of Technology Patna",
      "link": "https://arxiv.org/pdf/2512.21076",
      "code": null,
      "tags": [
        "text classification",
        "hierarchical genre classification",
        "zero-shot semantic alignment",
        "dual-path graph convolution",
        "label co-occurrence graph",
        "blurb-refined inference"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3afc3f4ebea9058118426cd2d72f37e4c09ae5bcc05f191e73c452f78fc501af_w640_q70.webp",
      "contributions": "1. Proposes a two-phase hierarchical genre mining framework (HiGeMine) that integrates noisy user reviews with authoritative blurbs using a zero-shot semantic alignment strategy to filter reviews. 2. Introduces a dual-path, two-level graph-based classification architecture that models inter-genre dependencies via a label co-occurrence graph for coarse and fine-grained prediction. 3. Curates a new hierarchical book genre dataset to facilitate systematic evaluation of the proposed method.",
      "summary": "This paper addresses the problem of noisy and unreliable book genre classification by proposing HiGeMine, a framework that filters user reviews using semantic alignment with book blurbs and then performs hierarchical classification using a dual-path graph-based model. The method outperforms baselines on a newly curated dataset, demonstrating an effective solution for leveraging structured and unstructured text in hierarchical genre analysis.",
      "mindmap": "graph LR\n    A[HiGeMine: Blurb-Refined Inference from Crowdsourced Book Reviews] --> B[核心问题/Problem: Noisy reviews & flat genre classification degrade reliability]\n    A --> C[主要方法/Method: Two-phase framework: 1. Zero-shot review filtering 2. Dual-path graph classification]\n    A --> D[关键结果/Results: Outperforms baselines on new hierarchical dataset]"
    },
    {
      "title": "DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion",
      "authors": "Ziyang Fan, Li Tao, Yi Wang, Jingwei Qu, Ying Wang, Fei Jiang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20059",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c40788fe60a93cff593d92f44508a7f4d8652aa38e707e2a9892801ec0179a3_w640_q70.webp",
      "contributions": "",
      "summary": "DS-HGCN: A Dual-Stream Hypergraph Convolutional Network for Predicting Student Engagement via Social Contagion",
      "mindmap": ""
    },
    {
      "title": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
      "authors": "Wenzheng Zeng, Mingyu Ouyang, Langyuan Cui, Hwee Tou Ng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20292",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4f80870c0718240101f019ec3db0f39be1afd3c26281e0c20366762d11e67351_w640_q70.webp",
      "contributions": "",
      "summary": "SlideTailor: Personalized Presentation Slide Generation for Scientific Papers",
      "mindmap": ""
    },
    {
      "title": "ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval",
      "authors": "Siyuan Fu, Xuchen Guo, Mingjun Liu, Hongxiang Li, Boyin Tan, Gongxi Zhu, Xianwei Zhuang, Jinghan Ru, Yuxin Xie, Yuguo Yin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19703",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac2c582d6058b0a98adaddd2fc52e7fcb4b2f111f05471b7984fd691dc97aed_w640_q70.webp",
      "contributions": "",
      "summary": "ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval",
      "mindmap": ""
    },
    {
      "title": "Neural Compression of 360-Degree Equirectangular Videos using Quality Parameter Adaptation",
      "authors": "Daichi Arai, Yuichi Kondo, Kyohei Unno, Yasuko Sugito, Yuichi Kusakabe",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20093",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/812f9579f0639bc36d83f19f057f2959cd40c919ca5879492cd5209f569425ef_w640_q70.webp",
      "contributions": "",
      "summary": "Neural Compression of 360-Degree Equirectangular Videos using Quality Parameter Adaptation",
      "mindmap": ""
    },
    {
      "title": "Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition",
      "authors": "Haiying Xia, Zhongyi Huang, Yumei Tan, Shuxiang Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17946",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ae2757814d2b16837d9b7cb3f26503eda8c49afc87dd1795f588ae949df6650_w640_q70.webp",
      "contributions": "",
      "summary": "Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition",
      "mindmap": ""
    },
    {
      "title": "Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown",
      "authors": "Changxu Duan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18115",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13ec88807faf2814dad94d8489b36894b8aa8c290f9026ca2108a6c17ac22ca6_w640_q70.webp",
      "contributions": "",
      "summary": "Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown",
      "mindmap": ""
    },
    {
      "title": "Accelerating End-to-End PDF to Markdown Conversion Through Assisted Generation",
      "authors": "Changxu Duan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18122",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/58612e72a18b314f91299b4a4f61d1fb3eaebfbccaa97bca668eb194568e1396_w640_q70.webp",
      "contributions": "",
      "summary": "Accelerating End-to-End PDF to Markdown Conversion Through Assisted Generation",
      "mindmap": ""
    },
    {
      "title": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
      "authors": "Eren Caglar, Amirkia Rafiei Oskooei, Mehmet Kutanoglu, Mustafa Keles, Mehmet S. Aktas",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18318",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ff046f84477e998c712a0f584e02cdfbe6c1bef89652e720bfe7cbb5bb7764ac_w640_q70.webp",
      "contributions": "",
      "summary": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
      "mindmap": ""
    },
    {
      "title": "PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval",
      "authors": "Pengxiang Ouyang, Qing Ma, Zheng Wang, Cong Bai",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18660",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/06fe8e37dc2d1d1d6a5d48298128004f86b1be4d6ecbeca32f5c72786b1a535c_w640_q70.webp",
      "contributions": "",
      "summary": "PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval",
      "mindmap": ""
    },
    {
      "title": "Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation",
      "authors": "Guangtao Lyu, Chenghao Xu, Qi Liu, Jiexi Yan, Muli Yang, Fen Fang, Cheng Deng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18804",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab2ab51309551324dcfdba087c2e5ec1e6dfdae10633047423e7233f30fdf84_w640_q70.webp",
      "contributions": "",
      "summary": "Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation",
      "mindmap": ""
    },
    {
      "title": "FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation",
      "authors": "Ziyuan Tao, Chuanzhi Xu, Sandaru Jayawardana, Wei Bao, Kanchana Thilakarathna, Teng Joon Lim",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18809",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/396672768b4960b9fefe0f861b938a8b78842c8181043ded9d12c7e8f28dbdfe_w640_q70.webp",
      "contributions": "",
      "summary": "FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation",
      "mindmap": ""
    },
    {
      "title": "Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification",
      "authors": "Alina Elena Baia, Andrea Cavallaro",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18864",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/54def5a8025f36fedd1db00274e2f13348c15e8cd933c948a5286d52b31223ae_w640_q70.webp",
      "contributions": "",
      "summary": "Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification",
      "mindmap": ""
    },
    {
      "title": "D$^\\{2\\}$Stream: Decoupled Dual-Stream Temporal-Speaker Interaction for Audio-Visual Speaker Detection",
      "authors": "Junhao Xiao, Shun Feng, Zhiyu Wu, Jianjun Li, Zhiyuan Ma, Yi Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19130",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/711fd748a4cf4a217aaf71023f458a85baa17b26b01b97cc24877d0ea5e48f6e_w640_q70.webp",
      "contributions": "",
      "summary": "D$^\\{2\\}$Stream: Decoupled Dual-Stream Temporal-Speaker Interaction for Audio-Visual Speaker Detection",
      "mindmap": ""
    },
    {
      "title": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
      "authors": "Xueming Yan, Boyan Xu, Yaochu Jin, Lixian Xiao, Wenlong Ye, Runyang Cai, Zeqi Zheng, Jingfa Liu, Aimin Yang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19379",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1619f2062f011937d18aa7e18ebf2c490f57bd39c04a7d06493e15df8a15ae19_w640_q70.webp",
      "contributions": "",
      "summary": "OmniMER: Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
      "mindmap": ""
    },
    {
      "title": "Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse",
      "authors": "Kunjal Panchal, Saayan Mitra, Somdeb Sarkhel, Haoliang Wang, Ishita Dasgupta, Gang Wu, Hui Guan",
      "institution": "University of Massachusetts Amherst, Adobe Research",
      "link": "https://arxiv.org/pdf/2512.17108",
      "code": null,
      "tags": [
        "multi-modal inference",
        "modular reuse",
        "on-device execution",
        "model decomposition",
        "parallel execution",
        "quantization"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ae01de3bb190d7134b2995b14582f1e46ed434d4588a9e6017b7c9282b01074_w640_q70.webp",
      "contributions": "",
      "summary": "The paper introduces Atom, an on-device system that decomposes large video-language models into reusable modules (e.g., visual encoder, language decoder) to eliminate redundant model loading and enable parallel execution across subtasks like captioning and retrieval. This modular reuse approach reduces end-to-end latency by 27–33% on commodity smartphones with only marginal performance drops, making it a practical solution for efficient video-language pipelines on edge devices.",
      "mindmap": ""
    },
    {
      "title": "A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs",
      "authors": "Yunkai Dang, Meiyi Zhu, Donghao Wang, Yizhuo Zhang, Jiacheng Yang, Qi Fan, Yuekun Yang, Wenbin Li, Feng Miao, Yang Gao",
      "institution": "Nanjing University",
      "link": "https://arxiv.org/pdf/2512.17319",
      "code": null,
      "tags": [
        "remote sensing",
        "multimodal evaluation",
        "RSHR-Bench",
        "adversarial filtering",
        "high-resolution imagery",
        "multimodal large language models",
        "visual question answering",
        "image captioning"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/283e413d1a1fd46323ee20a12df05789e8ef6dd69af3578d2d3e3e27ce803395_w640_q70.webp",
      "contributions": "",
      "summary": "This paper introduces RSHR-Bench, a super-high-resolution benchmark for evaluating multimodal large language models (MLLMs) in remote sensing. The benchmark uses adversarial filtering with strong LLMs and human verification to create tasks that reduce reliance on language priors and require genuine visual understanding. The evaluation reveals that existing models, including RS-specific ones, show significant performance gaps when handling ultra-high-resolution remote sensing imagery.",
      "mindmap": ""
    },
    {
      "title": "Region-Constraint In-Context Generation for Instructional Video Editing",
      "authors": "Zhongwei Zhang, Fuchen Long, Wei Li, Zhaofan Qiu, Wu Liu, Ting Yao, Tao Mei",
      "institution": "University of Science and Technology of China, HiDream.ai Inc.",
      "link": "https://arxiv.org/pdf/2512.17650",
      "code": null,
      "tags": [
        "diffusion training",
        "in-context generation",
        "video diffusion",
        "latent regularization",
        "attention regularization",
        "region-constraint",
        "joint denoising"
      ],
      "day": "2025-12-22",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a42bfbcfb80ccf85890223bbbea68fa4b5ab3ddd21035c9e061032429b289590_w640_q70.webp",
      "contributions": "",
      "summary": "The paper presents ReCo, a region-constraint in-context generation paradigm for instructional video editing. It introduces latent and attention regularization during joint denoising to precisely modify editing regions and mitigate interference. Experiments show the method's superiority across various video editing tasks.",
      "mindmap": ""
    },
    {
      "title": "Enhanced Web User Interface Design Via Cross-Device Responsiveness Assessment Using An Improved HCI-INTEGRATED DL Schemes",
      "authors": "Shrinivass Arunachalam Balasubramanian",
      "institution": "Independent Researcher",
      "link": "https://arxiv.org/pdf/2512.15775",
      "code": null,
      "tags": [
        "others",
        "Finite Exponential Continuous State Machine (FECSM)",
        "Quokka Nonlinear Difference Swarm Optimization Algorithm (QNDSOA)",
        "Bidirectional Gated Luong and Mish Recurrent Unit (BiGLMRU)",
        "HDBSCAN",
        "min-max normalization",
        "User Interface Change Prediction Index (UICPI)"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a dynamic web UI optimization method that uses a Finite Exponential Continuous State Machine for cross-device responsiveness assessment and a novel Quokka Nonlinear Difference Swarm Optimization Algorithm for design optimization. The core technique involves classifying user experience changes with a Bidirectional Gated Luong and Mish Recurrent Unit model. The main conclusion is that this integrated approach achieves an average fitness of 98.5632% for optimal UI design by incorporating cross-responsiveness assessment and user behavior patterns.",
      "mindmap": ""
    },
    {
      "title": "Secure AI-Driven Super-Resolution for Real-Time Mixed Reality Applications",
      "authors": "Mohammad Waquas Usmani, Sankalpa Timilsina, Michael Zink, Susmit Shannigrahi",
      "institution": "University of Massachusetts Amherst, Tennessee Technological University",
      "link": "https://arxiv.org/pdf/2512.15823",
      "code": null,
      "tags": [
        "multi-modal inference",
        "point cloud super-resolution",
        "attribute-based encryption",
        "downsampling",
        "upscaling"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a system to reduce latency in AR/VR streaming by downsampling and partially encrypting point cloud content at the server, then using a machine learning-based super-resolution model to reconstruct it at the client. The evaluation shows this approach effectively reduces bandwidth and encryption overhead while accurately reconstructing the original point clouds with minimal error.",
      "mindmap": ""
    },
    {
      "title": "Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models",
      "authors": "Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Pier Luigi Dovesi, Shaghayegh Roohi, Mark Granroth-Wilding, Rita Cucchiara",
      "institution": "University of Modena and Reggio Emilia, AMD Silo AI",
      "link": "https://arxiv.org/pdf/2512.15885",
      "code": null,
      "tags": [
        "multi-modal training",
        "self-supervised learning",
        "vision-language alignment",
        "I-JEPA",
        "JARVIS",
        "masked predictive loss",
        "frozen vision foundation models"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces JARVIS, a self-supervised framework that integrates the I-JEPA learning paradigm into multimodal large language model (MLLM) training to enhance visual understanding. It uses frozen vision models as encoders and trains an LLM-based predictor to learn visual regularities without relying solely on textual supervision. The method consistently improves performance on vision-centric benchmarks across different LLM families without degrading multimodal reasoning abilities.",
      "mindmap": ""
    },
    {
      "title": "A Tri-Dynamic Preprocessing Framework for UGC Video Compression",
      "authors": "Fei Zhao, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang, Xiaodong Xie",
      "institution": "Peking University, Bytedance",
      "link": "https://arxiv.org/pdf/2512.16101",
      "code": null,
      "tags": [
        "others",
        "Tri-Dynamic Preprocessing",
        "adaptive factor",
        "adaptive quantization level",
        "adaptive lambda tradeoff",
        "video compression",
        "UGC",
        "deep preprocessing"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a Tri-Dynamic Preprocessing (TDP) framework for UGC video compression, which adaptively adjusts preprocessing intensity, quantization level, and the rate-distortion tradeoff. This method addresses the high variability of UGC videos, where traditional deep preprocessing fails. Experiments show the framework achieves exceptional performance on large-scale test sets.",
      "mindmap": ""
    },
    {
      "title": "Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents",
      "authors": "Giulia Boato, Andrea Montibeller, Edward Delp, Luisa Verdoliva, Daniele Miorandi",
      "institution": "Truebees, University of Trento, Purdue University, University of Naples Federico II",
      "link": "https://arxiv.org/pdf/2512.16614",
      "code": null,
      "tags": [
        "multi-modal inference",
        "AI forensic agents",
        "uncertainty-aware assessments",
        "detector orchestration",
        "multimedia forensics",
        "authenticity verification"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a framework for AI forensic agents that autonomously orchestrate multiple forensic detectors to verify the authenticity of multimedia content. It argues that a holistic, uncertainty-calibrated approach is necessary to address the challenges posed by generative AI, moving beyond isolated, single-purpose detectors. The main conclusion is that such explainable, uncertainty-aware agents can improve the trustworthiness and interpretability of the forensic verification process.",
      "mindmap": ""
    },
    {
      "title": "LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation",
      "authors": "Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu",
      "institution": "Northeastern University, LinkedIn, University of Wisconsin–Madison",
      "link": "https://arxiv.org/pdf/2512.16891",
      "code": null,
      "tags": [
        "multi-modal inference",
        "cross-layer knowledge fusion MoE",
        "VLLM",
        "world-knowledge representation",
        "token extraction",
        "layer-wise fusion"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces LinkedOut, a method that extracts world-knowledge-aware tokens directly from video frames using Video Large Language Models (VLLMs) and fuses features across model layers via a Mixture of Experts (MoE) for recommendation. It achieves state-of-the-art results on benchmarks by enabling low-latency, interpretable video recommendation without handcrafted labels. The approach demonstrates that leveraging VLLM priors and visual reasoning through layer-wise fusion is effective for downstream vision tasks.",
      "mindmap": ""
    },
    {
      "title": "TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation",
      "authors": "Zhenzhi Wang, Jian Wang, Ke Ma, Dahua Lin, Bing Zhou",
      "institution": "The Chinese University of Hong Kong, Snap Inc.",
      "link": "https://arxiv.org/pdf/2512.14938",
      "code": null,
      "tags": [
        "multi-modal training",
        "diffusion transformer",
        "video VAE",
        "sliding window mechanism",
        "motion-frame context",
        "latent noise injection",
        "MLLM director"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces TalkVerse, a large-scale open dataset for audio-driven video generation, and a reproducible 5B Diffusion Transformer baseline model. The model uses a video VAE with a high downsampling ratio and a sliding window mechanism to enable minute-long video generation with low drift and lower inference cost. The main conclusion is that this open resource and efficient model democratize research in this field by enabling fair comparisons and reducing computational barriers.",
      "mindmap": ""
    },
    {
      "title": "A Preprocessing Framework for Video Machine Vision under Compression",
      "authors": "Fei Zhao, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang, Xiaodong Xie",
      "institution": "Peking University, Bytedance",
      "link": "https://arxiv.org/pdf/2512.15331",
      "code": null,
      "tags": [
        "others",
        "video compression",
        "neural preprocessor",
        "differentiable virtual codec",
        "rate-accuracy optimization"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a video preprocessing framework that uses a neural preprocessor and a differentiable virtual codec to optimize video compression for machine vision tasks. This method improves the rate-accuracy performance, saving over 15% of bitrate compared to standard codecs while maintaining task accuracy.",
      "mindmap": ""
    },
    {
      "title": "Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models",
      "authors": "Mikel Williams-Lekuona, Georgina Cosma",
      "institution": "Loughborough University",
      "link": "https://arxiv.org/pdf/2512.15372",
      "code": null,
      "tags": [
        "multi-modal inference",
        "adaptive computation",
        "early exit",
        "dual-path training",
        "image complexity classification",
        "ConvNeXt-IC"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes ICAR, an adaptive retrieval method that reduces computation for simple images in vision-language models by using early exits, while maintaining cross-modal alignment through dual-path training. It also introduces ConvNeXt-IC, a classifier for image complexity to decide the compute depth. The method achieves a 20% practical speedup with minimal performance loss, enabling more efficient scaling of vision-language systems.",
      "mindmap": ""
    },
    {
      "title": "VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics",
      "authors": "Opeyemi Bamigbade, Mark Scanlon, John Sheppard",
      "institution": "South East Technological University, University College Dublin",
      "link": "https://arxiv.org/pdf/2512.15512",
      "code": null,
      "tags": [
        "multi-modal inference",
        "Vision Transformers (ViT)",
        "SegFormer",
        "attention mechanisms",
        "anomaly scoring",
        "hybrid framework"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces VAAS, a hybrid framework for image manipulation detection that combines global anomaly estimation from Vision Transformers with patch-level self-consistency scoring from SegFormer embeddings. It provides continuous and interpretable anomaly scores to localize and quantify tampering. Evaluations show VAAS achieves competitive performance on benchmark datasets while enhancing visual explainability for digital forensics.",
      "mindmap": ""
    },
    {
      "title": "Generative Preprocessing for Image Compression with Pre-trained Diffusion Models",
      "authors": "Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang",
      "institution": "Bytedance Inc.",
      "link": "https://arxiv.org/pdf/2512.15270",
      "code": null,
      "tags": [
        "diffusion inference",
        "Consistent Score Identity Distillation (CiD)",
        "Rate-Perception (R-P) optimization",
        "Stable Diffusion 2.1",
        "parameter-efficient fine-tuning",
        "differentiable codec surrogate"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a two-stage generative preprocessing method for image compression using a pre-trained diffusion model. It first distills Stable Diffusion 2.1 into a one-step model and then fine-tunes it with a Rate-Perception loss. The method achieves significant perceptual quality improvements and integrates seamlessly with standard codecs.",
      "mindmap": ""
    }
  ]
}