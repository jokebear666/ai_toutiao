{
  "label": "cs.PF",
  "slug": "cspf",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs",
      "authors": "Ruifan Chu, Anbang Wang, Xiuxiu Bai, Shuai Liu, Xiaoshe Dong",
      "institution": "School of Software Engineering, Xi’an Jiaotong University",
      "link": "https://arxiv.org/pdf/2512.22147",
      "code": null,
      "tags": [
        "gpu kernels",
        "Minimal Executable Program (MEP)",
        "Automatic Error Repair",
        "Performance Pattern Inheritance",
        "iterative optimization",
        "cross-platform"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3fd593bd3569f30bdbf11d361054f51142863fb91e592b76bc4eb2f600850c5e_w640_q70.webp",
      "contributions": "1. Proposes an end-to-end LLM framework that optimizes GPU kernels by constructing Minimal Executable Programs (MEPs) to avoid expensive full application builds and executions. 2. Introduces Automatic Error Repair and Performance Pattern Inheritance to automatically fix faults and reuse effective optimization strategies, reducing search cost. 3. Demonstrates cross-platform portability and effectiveness on NVIDIA GPUs and the Haiguang DCU platform, achieving significant speedups over direct LLM optimization.",
      "summary": "The paper addresses the high cost of full builds for GPU kernel optimization in large HPC applications by proposing an LLM framework that uses Minimal Executable Programs (MEPs) for iterative optimization. The method integrates automatic error repair and performance pattern inheritance to maintain correctness and reuse strategies. It achieves substantial speedups across different hardware platforms without requiring full-source dependencies.",
      "mindmap": "graph TB\n        A[GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[Full builds & runs are expensive in large applications/大型应用中完整构建与运行成本高]\n        C --> C1[Construct Minimal Executable Program (MEP) for kernel/为内核构建最小可执行程序]\n        C --> C2[Multi-round iterative optimization with LLM feedback/基于LLM反馈的多轮迭代优化]\n        C --> C3[Integrate Automatic Error Repair & Performance Pattern Inheritance/集成自动错误修复与性能模式继承]\n        D --> D1[Achieves significant speedups (e.g., 5.05x, 7.77x)/获得显著加速比]\n        D --> D2[Cross-platform portability (NVIDIA, DCU)/跨平台可移植性]\n        D --> D3[Surpasses direct LLM optimization/超越直接LLM优化]"
    },
    {
      "title": "KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta",
      "authors": "Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik, Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang, Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner, Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Abdul Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu",
      "institution": "Meta Platforms",
      "link": "https://arxiv.org/pdf/2512.23236",
      "code": null,
      "tags": [
        "gpu kernels",
        "agentic kernel coding",
        "heterogeneous accelerators",
        "retrieval-augmented prompt synthesis",
        "graph-based search",
        "Triton/CuTe DSL"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/641ba327ba0d01461cd8fabad9a237e7b6667ce170be08aa3e89e6624ada0d38_w640_q70.webp",
      "contributions": "1. Proposes KernelEvolve, an agentic framework that automates kernel generation and optimization for DLRMs across heterogeneous hardware (NVIDIA/AMD GPUs, Meta accelerators) by operating at multiple programming abstractions. 2. Introduces a kernel optimization process modeled as a graph-based search with dynamic adaptation to runtime context via retrieval-augmented prompt synthesis and a persistent hardware knowledge base. 3. Demonstrates the system's effectiveness by achieving 100% correctness on benchmark suites and substantial performance speedups (up to 17x) in production, reducing development time from weeks to hours and lowering the programmability barrier for new AI hardware.",
      "summary": "This paper presents KernelEvolve, an agentic framework that automates the generation and optimization of compute kernels for deep learning recommendation models to address challenges posed by model, kernel, and hardware heterogeneity. The method uses a graph-based search process enhanced with retrieval-augmented prompts and operates across multiple programming abstractions. The system was validated on production models and benchmarks, showing significant performance improvements and reduced development time, effectively mitigating the programmability barrier for new AI accelerators.",
      "mindmap": "graph TB\n        A[KernelEvolve: Scaling Agentic Kernel Coding] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[DLRM训练/推理效率<br/>DLRM Training/Inference Efficiency]\n        B --> B2[模型、内核、硬件异构性<br/>Model, Kernel, Hardware Heterogeneity]\n        C --> C1[智能内核编码框架<br/>Agentic Kernel Coding Framework]\n        C --> C2[多抽象层: Triton, CuTe DSL<br/>Multi-Abstraction: Triton, CuTe DSL]\n        C --> C3[图搜索与检索增强提示<br/>Graph Search & Retrieval-Augmented Prompt]\n        D --> D1[100%正确率, 17倍加速<br/>100% Correctness, 17x Speedup]\n        D --> D2[开发时间: 数周->数小时<br/>Dev Time: Weeks->Hours]\n        D --> D3[降低新硬件编程壁垒<br/>Reduces New Hardware Programmability Barrier]"
    },
    {
      "title": "Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates",
      "authors": "Yongjie Guan",
      "institution": "Zhejiang University of Technology",
      "link": "https://arxiv.org/pdf/2512.23434",
      "code": null,
      "tags": [
        "distributed systems",
        "consistent hashing",
        "rendezvous hashing",
        "load balancing",
        "cache locality",
        "minimal churn"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c40c222a782553ce278b2cbc43564ea8beed18effebd850b7b92ac28f04bda05_w640_q70.webp",
      "contributions": "1. Introduces Local Rendezvous Hashing (LRH), which restricts HRW selection to a cache-local window of C distinct neighboring physical nodes on a ring. 2. Proposes next-distinct offsets to enforce bounded distinct candidate enumeration in exactly C ring steps. 3. Demonstrates that under fixed-candidate liveness failover, LRH achieves 0% excess churn while maintaining high throughput and good load balance.",
      "summary": "This paper addresses the trade-off between load balance and performance in consistent hashing for distributed systems. It proposes Local Rendezvous Hashing (LRH), a method that performs a Highest Random Weight selection within a small, cache-local window of nodes on a ring. LRH achieves near-optimal load balance with minimal key churn and significantly higher lookup throughput compared to multi-probe consistent hashing.",
      "mindmap": "graph TB\n        A[Local Rendezvous Hashing] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Ring-based consistent hashing has high load imbalance or scattered memory accesses.]\n        C --> C1[Restrict HRW selection to a cache-local window of C distinct nodes.]\n        D --> D1[Reduces Max/Avg load to 1.0947 and achieves 60.05 Mkeys/s throughput.]"
    },
    {
      "title": "Optimal Configuration of API Resources in Cloud Native Computing",
      "authors": "Eddy Truyen, Wouter Joosen",
      "institution": "DistriNet, KU Leuven",
      "link": "https://arxiv.org/pdf/2512.23494",
      "code": null,
      "tags": [
        "cloud computing",
        "Kubernetes",
        "resource optimization",
        "microservices",
        "DevOps",
        "Bayesian optimization"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0fd4cec4e268968c920e0f358d7cea15fb1e4dc177e4b11b53180a3f5172ef65_w640_q70.webp",
      "contributions": "1. Applies an existing black-box optimization framework to the largely unexplored problem of fine-tuning CPU and memory allocation during the DevOps Release phase, before deployment. 2. Empirically evaluates the framework using the TeaStore microservice application and provides a statistical comparison of different optimization algorithms, analyzing their trade-offs. 3. Provides practical guidance on when to use factor screening (for optimal configuration or algorithm comparison with a budget) versus pure Bayesian optimization (for finding a near-optimal configuration).",
      "summary": "This paper applies a black-box optimization framework to tune Kubernetes CPU and memory resource configurations for microservices during the DevOps Release phase, a problem often overlooked in favor of runtime autoscaling. The evaluation on the TeaStore application shows that factor screening is useful for finding the optimal configuration within a budget, but Bayesian optimization without screening is better for finding a near-optimal solution.",
      "mindmap": "graph TB\n        Root[”Optimal Configuration of API Resources in Cloud Native Computing<br/>云原生计算中API资源的最优配置”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br/>Untuned resource allocation before deployment<br/>部署前未调优的资源分配”] --> P1[”子问题/Sub-Problem<br/>Focus on Release phase, not Ops<br/>关注发布阶段，而非运维阶段”]\n        Method[”主要方法/Method<br/>Apply black-box optimization framework<br/>应用黑盒优化框架”] --> M1[”技术/Technique<br/>Factor screening & Bayesian optimization<br/>因子筛选与贝叶斯优化”]\n        Method --> M2[”评估/Evaluation<br/>Use TeaStore microservice app<br/>使用TeaStore微服务应用”]\n        Results[”关键结果/Results<br/>Guidance on screening vs. no screening<br/>关于是否使用筛选的指导”] --> R1[”结果1/Result 1<br/>Screening helps find optimal config with budget<br/>筛选有助于在预算内找到最优配置”]\n        Results --> R2[”结果2/Result 2<br/>Pure BO better for near-optimal config<br/>纯贝叶斯优化对寻找近似最优配置更好”]"
    },
    {
      "title": "DeepCQ: General-Purpose Deep-Surrogate Framework for Lossy Compression Quality Prediction",
      "authors": "Khondoker Mirazul Mumenin, Robert Underwood, Dong Dai, Jinzhen Wang, Sheng Di, Zarija Lukić, Franck Cappello",
      "institution": "University of North Carolina at Charlotte, Argonne National Laboratory, University of Delaware, Lawrence Berkeley National Laboratory",
      "link": "https://arxiv.org/pdf/2512.21433",
      "code": null,
      "tags": [
        "model compression (quantization/pruning)",
        "lossy compression",
        "quality prediction",
        "deep-surrogate",
        "mixture-of-experts",
        "feature-extraction"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d06e831f83754144a92a117a184fdcc51da0584d4163390cf94b34d4175b77a1_w640_q70.webp",
      "contributions": "1) A generalizable surrogate model for predicting compression quality across different compressors, quality metrics, and datasets. 2) A novel two-stage design that decouples expensive feature extraction from lightweight prediction for efficient training and modular inference. 3) A mixture-of-experts design to optimize performance and robustness for time-evolving scientific data.",
      "summary": "This paper proposes DeepCQ, a deep-surrogate framework to efficiently predict the quality of data after lossy compression, which is traditionally computationally expensive to assess. The method uses a two-stage and mixture-of-experts design for generalizability and robustness across different compressors, metrics, and time-evolving datasets. The framework achieves high predictive accuracy (errors under 10%) on real-world applications, enabling informed compression decisions and reducing I/O and computational overhead.",
      "mindmap": "graph TB\n        A[DeepCQ: 通用深度代理框架用于有损压缩质量预测] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[评估压缩后数据质量的计算成本高/Expensive to assess post-compression data quality]\n        C --> C1[两阶段设计: 特征提取 + 轻量预测/Two-stage design: feature extraction + lightweight prediction]\n        C --> C2[专家混合设计处理时变数据/Mixture-of-experts for time-evolving data]\n        D --> D1[预测误差普遍低于10%/Prediction errors generally under 10%]\n        D --> D2[显著优于现有方法/Significantly outperforms existing methods]"
    },
    {
      "title": "Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling",
      "authors": "Hannah Atmer, Yuan Yao, Thiemo Voigt, Stefanos Kaxiras",
      "institution": "Uppsala University",
      "link": "https://arxiv.org/pdf/2512.22066",
      "code": null,
      "tags": [
        "llm inference",
        "SRAM",
        "frequency scaling",
        "energy-delay product",
        "systolic array",
        "memory bandwidth"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/52c2db009cb44a92a388e1684ea0d1bdb9ae27c0c4a4e683651eb7bd091bbe93_w640_q70.webp",
      "contributions": "1. Quantified the distinct energy and performance impacts of SRAM size and operating frequency on the compute-bound prefill and memory-bound decode phases of LLM inference. 2. Demonstrated a counter-intuitive result: high compute frequency can reduce total energy by shortening execution time and reducing static energy more than the dynamic power increase. 3. Identified an optimal hardware configuration (high frequency, small SRAM buffer) that minimizes the energy-delay product, providing concrete design insights for energy-efficient accelerators.",
      "summary": "This paper investigates how on-chip SRAM size and operating frequency affect the energy efficiency of LLM inference. Using a simulation methodology combining OpenRAM, LLMCompass, and ScaleSIM, it finds that a high-frequency, small-SRAM configuration optimizes the energy-delay product, as memory bandwidth caps decode phase improvements. The analysis provides architectural guidance for designing efficient LLM accelerators.",
      "mindmap": "graph TB\n        Root[”Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>LLM推理能耗高，Prefill与Decode阶段瓶颈不同”] --> Problem_Sub1[”SRAM大小与频率如何影响能效？”]\n        Problem --> Problem_Sub2[”内存带宽如何限制性能？”]\n        Method[”主要方法/Method<br>结合OpenRAM, LLMCompass, ScaleSIM的模拟方法”] --> Method_Sub1[”能耗建模/Energy Modeling”]\n        Method --> Method_Sub2[”延迟模拟/Latency Simulation”]\n        Method --> Method_Sub3[”操作强度分析/Operational Intensity”]\n        Results[”关键结果/Results”] --> Results_Sub1[”总能耗主要由SRAM大小决定<br>大缓存增加静态能耗”]\n        Results --> Results_Sub2[”高频可降低总能耗<br>（减少静态能耗）”]\n        Results --> Results_Sub3[”最优配置：高频(1200-1400MHz) + 小缓存(32-64KB)”]"
    },
    {
      "title": "LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics",
      "authors": "Jiashuo Liu, Jiayun Wu, Chunjie Wu, Jingkai Liu, Zaiyuan Wang, Huan Zhou, Wenhao Huang, Hongseok Namkoong",
      "institution": "ByteDance Seed, Carnegie Mellon University, Columbia University",
      "link": "https://arxiv.org/pdf/2512.21010",
      "code": null,
      "tags": [
        "llm evaluation",
        "Competitive Swiss-System Dynamics",
        "Expected Win Score",
        "Failure Sensitivity Analysis",
        "Monte Carlo Simulation",
        "risk appetite"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c6a4e260d9e6100733ae95995348a1b30295905871b863cf4afa821492e22eb_w640_q70.webp",
      "contributions": "1. Introduces the Competitive Swiss-System Dynamics (CSD) framework, a novel sequential contest simulation for holistic LLM ranking across multiple benchmarks, 2. Proposes the Expected Win Score via Monte Carlo Simulation to provide a statistically robust ranking that reduces noise from random pairing, 3. Implements Failure Sensitivity Analysis to profile models by risk appetite, distinguishing between robust generalists and aggressive specialists.",
      "summary": "The paper addresses the limitations of static, fragmented LLM evaluation by proposing the Competitive Swiss-System Dynamics (CSD) framework, which simulates a multi-round sequential contest to aggregate performance across benchmarks dynamically. It uses Monte Carlo simulation to compute a robust Expected Win Score and includes a Failure Sensitivity Analysis to assess model risk profiles. The authors demonstrate that CSD provides a more nuanced and context-aware ranking than traditional methods, advancing risk-informed LLM evaluation.",
      "mindmap": "graph LR\n    A[LLM Swiss Round: Aggregating Multi-Benchmark Performance via Competitive Swiss-System Dynamics] --> B[核心问题/Problem: Fragmented benchmarks and static scoring fail to capture dynamic competitive fitness and risk]\n    A --> C[主要方法/Method: Competitive Swiss-System Dynamics (CSD) with Monte Carlo Simulation and Failure Sensitivity Analysis]\n    A --> D[关键结果/Results: More nuanced, context-aware ranking distinguishing robust generalists vs. aggressive specialists]"
    },
    {
      "title": "SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication",
      "authors": "Chen Zhuang, Lingqi Zhang, Benjamin Brock, Du Wu, Peng Chen, Toshio Endo, Satoshi Matsuoka, Mohamed Wahib",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20178",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/340dca9738b4b1930a1331103d9fe185151f34d58d7be73cc31d211665f20128_w640_q70.webp",
      "contributions": "",
      "summary": "SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication",
      "mindmap": ""
    },
    {
      "title": "Post-Quantum Cryptography in the 5G Core",
      "authors": "Thomas Attema, Bor de Kock, Sandesh Manganahalli Jayaprakash, Dimitrios Schoinianakis, Thom Sijpesteijn, Rintse van de Vlasakker",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20243",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/859dfd3ed6d1322ef0080bba0f59f3ae42dade36f0faa22a613bb943d0674177_w640_q70.webp",
      "contributions": "",
      "summary": "Post-Quantum Cryptography in the 5G Core",
      "mindmap": ""
    },
    {
      "title": "Performance Guarantees for Data Freshness in Resource-Constrained Adversarial IoT Systems",
      "authors": "Aresh Dadlani, Muthukrishnan Senthil Kumar, Omid Ardakanian, Ioanis Nikolaidis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18155",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d85be01fabe34bdb9a8cff81bbd4c5ee2a64d11886db3b1b67c1b63f6ae6ce3b_w640_q70.webp",
      "contributions": "",
      "summary": "Performance Guarantees for Data Freshness in Resource-Constrained Adversarial IoT Systems",
      "mindmap": ""
    },
    {
      "title": "Age of Information with Age-Dependent Server Selection",
      "authors": "Nail Akar, Ismail Cosandal, Sennur Ulukus",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18457",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e86459dce028a5878b3bc0d106b94e0a4f8147592507343a098f65b2532139a_w640_q70.webp",
      "contributions": "",
      "summary": "Age of Information with Age-Dependent Server Selection",
      "mindmap": ""
    },
    {
      "title": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference",
      "authors": "George Karfakis, Faraz Tahmasebi, Binglu Chen, Lime Yao, Saptarshi Mitra, Tianyue Pan, Hyoukjun Kwon, Puneet Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19606",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6ba70ec7306532ca1c35d62f262fda2524e63fa17cc3a261b1800c846a6c06b2_w640_q70.webp",
      "contributions": "",
      "summary": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference",
      "mindmap": ""
    },
    {
      "title": "GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping",
      "authors": "Yikang Yue, Yishu Yin, Xuehai Qian",
      "institution": "Tsinghua University, University of Illinois at Urbana-Champaign",
      "link": "https://arxiv.org/pdf/2512.17570",
      "code": null,
      "tags": [
        "llm training",
        "vertical scheduling",
        "optimizer step overlapping",
        "SSD-offloaded training",
        "gradient accumulation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces GreedySnake, a system that accelerates SSD-offloaded LLM training by using vertical scheduling to process all micro-batches per layer before moving to the next, and by overlapping the optimizer step with the next forward pass. This approach significantly reduces I/O bottlenecks and improves throughput compared to prior systems like ZeRO-Infinity, achieving up to 2.53x speedup for large models like GPT-175B.",
      "mindmap": ""
    },
    {
      "title": "AdaGradSelect: An adaptive gradient-guided layer selection method for efficient fine-tuning of SLMs",
      "authors": "Anshul Kumar, Gagan Raj Gupta, Manisha Chawla",
      "institution": "IIT Bhilai",
      "link": "https://arxiv.org/pdf/2512.15764",
      "code": null,
      "tags": [
        "llm training",
        "AdaGradSelect",
        "gradient-guided selection",
        "Dirichlet-based sampling",
        "epsilon-greedy exploration",
        "selective block update",
        "Parameter-Efficient Fine-Tuning (PEFT)"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces AdaGradSelect, an adaptive method for efficiently fine-tuning Small Language Models (SLMs) by selecting which transformer blocks to update based on gradient norms, using a combination of Dirichlet-based sampling and epsilon-greedy exploration. It achieves performance close to full fine-tuning while training about 12% faster and using 35% less GPU memory, outperforming methods like LoRA on benchmarks such as GSM8K.",
      "mindmap": ""
    },
    {
      "title": "LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models",
      "authors": "Yijie Zhi, Yayu Cao, Jianhua Dai, Xiaoyang Han, Jingwen Pu, Qingran Wu, Sheng Cheng, Ming Cai",
      "institution": "Zhejiang University, Zhejiang Institute of Administration, Beijing ShenZhou Aerospace Software Technology Ltd.",
      "link": "https://arxiv.org/pdf/2512.15766",
      "code": null,
      "tags": [
        "llm inference",
        "retrieval-augmented generation",
        "loop transformation",
        "static control part",
        "feedback-based iterative mechanism",
        "equivalence checking"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes LOOPRAG, a retrieval-augmented generation framework that guides Large Language Models (LLMs) to perform effective loop transformation optimization. It uses a loop-aware retrieval algorithm and a feedback-based iterative mechanism with compilation and testing to generate correct and efficient code. The evaluation shows LOOPRAG achieves significant speedups over both traditional compilers and base LLMs on standard benchmark suites.",
      "mindmap": ""
    },
    {
      "title": "Optimizing Agentic Language Model Inference via Speculative Tool Calls",
      "authors": "Daniel Nichols, Prajwal Singhania, Charles Jekel, Abhinav Bhatele, Harshitha Menon",
      "institution": "Lawrence Livermore National Laboratory, University of Maryland",
      "link": "https://arxiv.org/pdf/2512.15834",
      "code": null,
      "tags": [
        "llm inference",
        "speculative tool calls",
        "tool cache",
        "vLLM",
        "prefix-caching"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces system optimizations for language model agents that use external tools, specifically by speculating future tool calls and keeping sequences resident in the inference engine to reduce overhead. These methods lead to significant throughput improvements of hundreds of tokens per second. The authors also propose a new \"tool cache\" API to facilitate adoption of these optimizations.",
      "mindmap": ""
    },
    {
      "title": "XTC, A Research Platform for Optimizing AI Workload Operators",
      "authors": "Pompougnac Hugo, Guillon Christophe, Noiry Sylvain, Dutilleul Alban, Iooss Guillaume, Rastello Fabrice",
      "institution": "Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG",
      "link": "https://arxiv.org/pdf/2512.16512",
      "code": null,
      "tags": [
        "GPU kernels",
        "scheduling language",
        "code generation",
        "performance evaluation",
        "compiler",
        "autotuning"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces XTC, a research platform that provides a unified scheduling language and API to decouple optimization strategies from compiler-specific code generation and measurement. This enables portable experimentation and fair performance comparison across different compiler frameworks. The main conclusion is that XTC accelerates research on AI operator optimization by providing a common, reproducible framework for scheduling and evaluation.",
      "mindmap": ""
    }
  ]
}