{
  "label": "cs.PL",
  "slug": "cspl",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "Enforcing Temporal Constraints for LLM Agents",
      "authors": "Adharsh Kamath, Sishen Zhang, Calvin Xu, Shubham Ugare, Gagandeep Singh, Sasa Misailovic",
      "institution": "University of Illinois at Urbana-Champaign, Meta",
      "link": "https://arxiv.org/pdf/2512.23738",
      "code": "https://github.com/structuredllm/agent-c",
      "tags": [
        "agent system",
        "temporal constraints",
        "SMT solving",
        "constrained generation",
        "formal verification",
        "LLM agents"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/3a978adfab8b202d7b971f6b65f8d005235baabb93f5540985ad131638c67354_w640_q70.webp",
      "contributions": "1. A novel framework (Agent-C) providing runtime guarantees for LLM agents to adhere to formal temporal safety properties., 2. A domain-specific language for expressing temporal properties, which are translated to first-order logic and verified via SMT solving during token generation., 3. Demonstration of perfect safety (100% conformance) and improved task utility across real-world applications and multiple LLMs, outperforming state-of-the-art guardrails.",
      "summary": "The paper addresses the problem of LLM agents violating temporal safety policies, such as accessing data before authentication. It proposes Agent-C, a framework that uses a domain-specific language, formal logic translation, and SMT solving to enforce constraints during token generation, ensuring compliant actions. The evaluation shows Agent-C achieves 100% safety conformance and improves task utility compared to existing methods.",
      "mindmap": "graph TB\n        A[Enforcing Temporal Constraints for LLM Agents] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[现有护栏无法保证时间安全策略/Existing guardrails fail to enforce temporal safety policies]\n        C --> C1[提出Agent-C框架/Propose Agent-C framework]\n        C1 --> C2[使用DSL和SMT求解进行运行时验证/Use DSL & SMT solving for runtime verification]\n        C2 --> C3[采用约束生成确保合规/Achieve compliance via constrained generation]\n        D --> D1[100%安全性，0%危害/100% safety, 0% harm]\n        D --> D2[在真实应用中提高任务效用/Improve task utility in real-world applications]"
    },
    {
      "title": "Towards representation agnostic probabilistic programming",
      "authors": "Ole Fenske, Maximilian Popko, Sebastian Bader, Thomas Kirste",
      "institution": "Institute for Visual and Analytic Computing, University of Rostock",
      "link": "https://arxiv.org/pdf/2512.23740",
      "code": null,
      "tags": [
        "compiler & ir",
        "factor abstraction",
        "probabilistic programming",
        "hybrid models",
        "representation-agnostic",
        "factor graphs"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/97ac8ba28932b58d58cfedc5a8c2d53a706dd206b4f545e3d26852fb3ee19d75_w640_q70.webp",
      "contributions": "1. Introduces a factor abstraction with five fundamental operations as a universal interface for manipulating probabilistic factors. 2. Enables representation-agnostic probabilistic programming, allowing the mixing of different distribution representations (e.g., discrete tables, Gaussians, samples) within a single framework. 3. Facilitates practical inference in complex hybrid (mixed discrete-continuous) models that current toolkits cannot adequately handle.",
      "summary": "The paper addresses the tight coupling between model representations and inference algorithms in current probabilistic programming tools, which limits flexibility. It proposes a factor abstraction with a set of core operations to create a representation-agnostic interface. This allows users to mix various distribution representations, enabling inference in complex hybrid models previously difficult to express.",
      "mindmap": "graph TB\n        A[Towards representation agnostic probabilistic programming] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[PPLs耦合表示与推理算法/PPLs couple representations & inference]\n        B --> B2[阻碍混合模型实验/Prevents hybrid model experimentation]\n        C --> C1[引入因子抽象/Introduce factor abstraction]\n        C --> C2[定义五个基本操作/Define five fundamental operations]\n        C --> C3[创建通用接口/Create universal interface]\n        D --> D1[实现表示无关编程/Enable representation-agnostic programming]\n        D --> D2[支持混合表示/Support mixing representations]\n        D --> D3[处理复杂混合模型/Handle complex hybrid models]"
    },
    {
      "title": "VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python with Partitioning and Parallel Execution",
      "authors": "Abdulla M",
      "institution": "Could not be determined from the provided content.",
      "link": "https://arxiv.org/pdf/2512.23768",
      "code": "https://github.com/Abdullahlab-n/VGC-for-arxiv",
      "tags": [
        "memory management",
        "garbage collection",
        "concurrent mark-and-sweep",
        "memory fragmentation",
        "parallel execution",
        "zone-based architecture"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0edb6409b92db2ec79f95ff72421bf0148b259c4251b261594c32563a128cb85_w640_q70.webp",
      "contributions": "1. Introduces a dual-layer (Active and Passive) garbage collector architecture to separate compile-time and runtime memory management responsibilities., 2. Proposes a concurrent mark-and-sweep strategy for the Active VGC to reduce pause times in parallel workloads., 3. Employs predictive memory mapping and cache-aligned allocation in the Passive VGC to minimize fragmentation and reduce total memory usage.",
      "summary": "The paper proposes VGC, a novel garbage collector for Python designed to overcome performance bottlenecks like the GIL and fragmentation. It uses a dual-layer architecture with a runtime concurrent collector and a compile-time allocator to reduce pause times and memory usage. The results show significant improvements in performance and scalability for parallel applications.",
      "mindmap": "graph TB\n        Root[VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[Python GC Bottlenecks: GIL, Pauses, Fragmentation]\n        Method[主要方法/Method] --> M1[Dual-Layer Architecture / 双层架构]\n        M1 --> M1_1[Active VGC: Runtime Concurrent Mark-and-Sweep / 运行时并发标记清除]\n        M1 --> M1_2[Passive VGC: Compile-Time Predictive Allocation / 编译时预测性分配]\n        Results[关键结果/Results] --> R1[Reduced Pause Times (up to 30%) / 降低暂停时间]\n        Results --> R2[Reduced Memory Usage (up to 25%) / 降低内存使用]\n        Results --> R3[Improved Scalability for Parallel Apps / 提升并行应用可扩展性]"
    },
    {
      "title": "Yggdrasil: Bridging Dynamic Speculation and Static Runtime for Latency-Optimal Tree-Based LLM Decoding",
      "authors": "Yue Guan, Changming Yu, Shihan Fang, Weiming Hu, Zaifeng Pan, Zheng Wang, Zihan Liu, Yangjie Zhou, Yufei Ding, Minyi Guo, Jingwen Leng",
      "institution": "Shanghai Jiao Tong University, Shanghai Qizhi Institute, University of California, San Diego",
      "link": "https://arxiv.org/pdf/2512.23858",
      "code": null,
      "tags": [
        "llm inference",
        "speculative decoding",
        "tree-based decoding",
        "latency optimization",
        "compiler-friendly execution",
        "static runtime"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/13cf851f76b89c86dd0ecc981628839d1ca0ba1772a6b9b893ec9677720b6be0_w640_q70.webp",
      "contributions": "1. Introduces an equal-growth tree structure for speculative decoding that is compatible with static graph compilers. 2. Proposes a latency-aware optimization objective for draft selection, moving beyond simple average accepted length. 3. Designs a stage-based scheduling mechanism to reduce runtime overhead.",
      "summary": "The paper identifies a performance mismatch between dynamic speculative decoding algorithms and static runtime systems. It proposes Yggdrasil, a co-designed system that uses a context-aware tree drafting structure and compiler-friendly execution to achieve latency-optimal speculative decoding. The system supports unmodified LLMs and achieves up to 3.98x speedup over state-of-the-art baselines.",
      "mindmap": "graph TB\n        A[Yggdrasil: Bridging Dynamic Speculation and Static Runtime for Latency-Optimal Tree-Based LLM Decoding] --> B[核心问题/Problem: Mismatch between dynamic speculation and static runtime assumptions leads to suboptimal performance]\n        A --> C[主要方法/Method: Co-designed system with context-aware tree drafting and compiler-friendly execution]\n        A --> D[关键结果/Results: Up to 3.98x speedup over SOTA baselines, supports unmodified LLMs]"
    },
    {
      "title": "State Space Estimation for DPOR-based Model Checkers",
      "authors": "A. R. Balasubramanian, Mohammad Hossein Khoshechin Jorshari, Rupak Majumdar, Umang Mathur, Minjian Zhang",
      "institution": "Max Planck Institute for Software Systems (MPI-SWS), National University of Singapore, University of Illinois Urbana-Champaign",
      "link": "https://arxiv.org/pdf/2512.23996",
      "code": null,
      "tags": [
        "model checking",
        "DPOR",
        "Mazurkiewicz trace",
        "Monte Carlo estimation",
        "state space estimation",
        "stochastic enumeration"
      ],
      "day": "2026-01-01",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f877162e9490ceb5356ddbd67ad74f3fbb0e6eefb7872252a112f618f8723e92_w640_q70.webp",
      "contributions": "1. Proved the #P-hardness and inapproximability of counting Mazurkiewicz trace-equivalence classes for concurrent programs, establishing the theoretical difficulty of the problem. 2. Introduced a poly-time unbiased Monte Carlo estimator by converting an optimal DPOR algorithm into a bounded tree and applying Knuth's estimator with stochastic enumeration for variance control. 3. Implemented and evaluated the estimator in the JMC model checker, demonstrating its practical effectiveness in providing stable estimates for large state spaces (10^5–10^6 classes) with modest computational budgets.",
      "summary": "This paper addresses the problem of estimating the number of distinct behaviors (Mazurkiewicz trace-equivalence classes) in bounded concurrent programs to predict model checking runtime and progress. It proposes a Monte Carlo method that transforms a stateless optimal DPOR algorithm into an unbiased estimator, using Knuth's estimator and stochastic enumeration to control variance. The implemented estimator provides stable, practical estimates for large state spaces, offering the first provable poly-time unbiased solution for this important resource allocation problem.",
      "mindmap": "graph TB\n        A[State Space Estimation for DPOR-based Model Checkers] --> B(核心问题/Problem)\n        A --> C(主要方法/Method)\n        A --> D(关键结果/Results)\n        B --> B1[估计并发程序的状态空间大小/Estimate # of trace-equivalence classes]\n        B --> B2[预测模型检查成本与进度/Predict model-checking cost & progress]\n        C --> C1[将DPOR算法转为无偏估计器/Convert DPOR to unbiased estimator]\n        C --> C2[应用Knuth估计器与随机枚举/Apply Knuth's estimator & stochastic enumeration]\n        D --> D1[理论: 首个多项式时间无偏估计器/Theoretical: first poly-time unbiased estimator]\n        D --> D2[实践: 在JMC中实现, 估计稳定/ Practical: implemented in JMC, stable estimates]"
    },
    {
      "title": "TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures",
      "authors": "Wei Li, Zhenyu Bai, Heru Wang, Pranav Dangi, Zhiqiang Zhang, Cheng Tan, Huiying Lan, Weng-Fai Wong, Tulika Mitra",
      "institution": "National University of Singapore, Arizona State University, Google, Lumai Ltd.",
      "link": "https://arxiv.org/pdf/2512.22168",
      "code": null,
      "tags": [
        "compiler & ir",
        "spatial dataflow",
        "tile-based compilation",
        "MLIR",
        "on-chip network",
        "hardware representation"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/cce8d87d1dd357c986e9809985cc87b6430fd568820f39521500addb34e7eef7_w640_q70.webp",
      "contributions": "1. An end-to-end compiler framework (TL) that compiles tile-based programs (e.g., Triton kernels) onto spatial dataflow architectures, focusing on distributing tile instances across cores. 2. A novel hardware representation that captures interconnect topology, memory hierarchy, and compute capabilities to enable architecture-specific optimizations and support diverse targets. 3. A practical implementation built on the MLIR ecosystem, providing a generic entry point for different front-ends and an end point for different back-ends, demonstrated with performance gains over vendor libraries.",
      "summary": "This paper presents TL, an end-to-end compiler framework that tackles the limited programmability of spatial dataflow accelerators by automatically mapping tile-based workloads across distributed cores to optimize data reuse and reduce communications. TL introduces a hardware-aware representation and is built on MLIR to support diverse targets. Experiments show it can match or exceed the performance of hand-tuned vendor libraries on kernels like GEMM and FlashAttention.",
      "mindmap": "graph TB\n        Root[TL: Automatic End-to-End Compiler] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[Limited Programmability of Spatial Accelerators<br/>空间加速器的有限可编程性]\n        Problem --> P2[Poor Performance of Naive Mappings<br/>朴素映射性能差]\n        Method[主要方法/Method] --> M1[End-to-End Tile-Based Compiler Framework<br/>端到端基于分块的编译器框架]\n        Method --> M2[Hardware Representation for Topology & Memory<br/>用于拓扑和内存的硬件表示]\n        Method --> M3[Built on MLIR Ecosystem<br/>基于MLIR生态系统构建]\n        Results[关键结果/Results] --> R1[Performance on par with/vs Vendor Library (GEMM)<br/>性能与厂商库相当/超越(GEMM)]\n        Results --> R2[Significant Speedup for FlashAttention<br/>FlashAttention显著加速]"
    },
    {
      "title": "Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs",
      "authors": "Xinhao Cheng, Zhihao Zhang, Yu Zhou, Jianan Ji, Jinchen Jiang, Zepeng Zhao, Ziruo Xiao, Zihao Ye, Yingyi Huang, Ruihang Lai, Hongyi Jin, Bohan Hou, Mengdi Wu, Yixin Dong, Anthony Yip, Zihao Ye, Songting Wang, Wenqin Yang, Xupeng Miao, Tianqi Chen, Zhihao Jia",
      "institution": "Carnegie Mellon University, Tsinghua University, NVIDIA, University of Michigan, Purdue University",
      "link": "https://arxiv.org/pdf/2512.22219",
      "code": "https://github.com/mirage-project/mirage",
      "tags": [
        "llm inference",
        "megakernel",
        "kernel fusion",
        "SM-level graph",
        "software pipelining",
        "CUDA"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f886cd06ce6c0f773c062fa38aae0fa982d862cc66ef54da9fbbfd6cf62dd86a_w640_q70.webp",
      "contributions": "1. Introduces an SM-level graph representation for capturing fine-grained data dependencies across GPU streaming multiprocessors. 2. Develops a compiler and an in-kernel parallel runtime that automatically transforms multi-operator inference into a single, high-performance mega-kernel. 3. Enables previously infeasible GPU optimizations like cross-operator software pipelining and fine-grained kernel overlap, significantly reducing inference latency.",
      "summary": "The paper introduces Mirage Persistent Kernel (MPK), a compiler and runtime system that automatically fuses multiple GPU kernels for model inference into a single, optimized mega-kernel. It achieves this by using a novel SM-level graph representation and decentralized scheduling to enable fine-grained optimizations like software pipelining. Evaluation shows MPK reduces LLM inference latency by up to 1.7x, pushing performance close to hardware limits.",
      "mindmap": "graph TB\n        A[Mirage Persistent Kernel<br>幻影持久内核] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[Kernel-per-operator execution<br>limits GPU optimization<br>逐算子内核执行限制GPU优化]\n        C --> C1[SM-level graph &<br>mega-kernel runtime<br>SM级图与巨型内核运行时]\n        D --> D1[Reduces inference latency<br>by up to 1.7x<br>推理延迟降低高达1.7倍]"
    },
    {
      "title": "Symbolic Specification and Reasoning for Quantum Data and Operations",
      "authors": "Mingsheng Ying",
      "institution": "University of Technology Sydney",
      "link": "https://arxiv.org/pdf/2512.22383",
      "code": null,
      "tags": [
        "quantum programming languages & verification",
        "symbolic logic",
        "formal verification",
        "quantum computation",
        "automated reasoning",
        "SOL"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d170e9ed692953b589cb0ca609b55da71b661300061784405cc0fe9e73f4c680_w640_q70.webp",
      "contributions": "1. Proposes a novel logical framework called Symbolic Operator Logic (SOL) for symbolic specification of quantum data and operations., 2. Embeds classical first-order logic into SOL to enable reasoning about quantum properties modulo theories of classical data, leveraging existing classical verification tools., 3. Provides a conceptual foundation for formal verification and automated theorem proving of quantum computation in proof assistants like Lean and Coq.",
      "summary": "This paper addresses the lack of a formal theory for symbolic reasoning in quantum computing by introducing a general logical framework called Symbolic Operator Logic (SOL). The core method embeds classical first-order logic into a language of formal operators for quantum specifications, enabling automated reasoning by reusing classical verification tools. The authors conclude that SOL provides a foundational framework for the formal verification of quantum algorithms and programs.",
      "mindmap": "graph TB\n        Root[”Symbolic Specification and Reasoning for Quantum Data and Operations”] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem<br>缺乏量子数据与操作的形式化符号推理理论”] --> P1[”导致结果/Consequence<br>限制量子程序自动验证的实用性”]\n        Method[”主要方法/Method<br>提出符号算子逻辑(SOL)框架”] --> M1[”关键技术/Key Technique<br>将经典一阶逻辑嵌入形式算子语言”]\n        Method --> M2[”优势/Advantage<br>基于经典数据理论(如布尔代数)进行推理”]\n        Results[”关键结果/Results<br>为量子计算形式验证提供概念基础”] --> R1[”应用前景/Application<br>用于Lean, Coq等证明助手”]"
    },
    {
      "title": "Eliminate Branches by Melding IR Instructions",
      "authors": "Yuze Li, Srinivasan Ramachandra Sharma, Charitha Saumya, Ali R. Butt, Kirshanthan Sundararajah",
      "institution": "Virginia Tech, Intel Corporation",
      "link": "https://arxiv.org/pdf/2512.22390",
      "code": null,
      "tags": [
        "compiler & ir",
        "branch elimination",
        "if-conversion",
        "instruction melding",
        "sequence alignment",
        "LLVM pass"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/28b6ca42fd8300cb0b345dbbb72918ff7cecfcb6403ae9595873d4e5ffdc15ac_w640_q70.webp",
      "contributions": "1. Proposes MERIT, a novel compiler transformation that eliminates branches by aligning and melding similar operations from divergent paths at the IR level, differing from traditional if-conversion. 2. Adapts sequence alignment to discover merging opportunities and uses safe operand-level guarding to ensure correctness without hardware predication, overcoming limitations of speculation on architectures like x86. 3. Demonstrates effectiveness through an LLVM pass implementation, achieving a geometric mean speedup of 10.9% and up to 32x peak improvement over hardware branch prediction on 102 benchmark programs.",
      "summary": "This paper addresses the performance penalty from branch mispredictions by introducing MERIT, a compiler transformation that eliminates branches by aligning and melding similar instructions from divergent paths at the IR level. It uses sequence alignment to find merging opportunities and operand guarding for safety, avoiding the pitfalls of traditional if-conversion. Implemented in LLVM, MERIT achieves significant speedups, demonstrating its effectiveness in reducing branch overhead.",
      "mindmap": "graph TB\n        A[Eliminate Branches by Melding IR Instructions] --> B[核心问题/Problem: Branch mispredictions cause performance loss; traditional if-conversion has limitations]\n        A --> C[主要方法/Method: MERIT - melds similar IR instructions from divergent paths using sequence alignment & operand guarding]\n        A --> D[关键结果/Results: 10.9% mean speedup, up to 32x peak improvement, reduced instruction overhead]"
    },
    {
      "title": "A Bounded Game Semantics Checker for Precise Smart Contract Analysis",
      "authors": "Vasileios Koutavas, Yu-Yang Lin, Nikos Tzevelekos",
      "institution": "Trinity College Dublin, Lero - the Science Foundation Ireland Research Centre for Software, Queen Mary University of London",
      "link": "https://arxiv.org/pdf/2512.22417",
      "code": null,
      "tags": [
        "smart contract security",
        "game semantics",
        "bounded model checking",
        "Yul",
        "reentrancy",
        "trace enumeration"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9d265d01c22405ca441e39696a7690570d06e75e375e5f9138e9d8f0d3b02e5c_w640_q70.webp",
      "contributions": "1. Introduces a precise, bounded-complete vulnerability detection method for smart contracts based on game semantics, which models contract-environment interactions to reduce reasoning about external contracts to trace enumeration., 2. Implements the approach in YulToolkit, a tool for the Yul intermediate language that avoids over-approximation by exploring only feasible interactions and supports instrumentation for tractable analysis., 3. Demonstrates the tool's effectiveness by successfully detecting known vulnerabilities (e.g., reentrancy) in real-world incidents like The DAO and benchmark contracts, with no false positives within the analysis bounds.",
      "summary": "This paper presents a new approach for precise smart contract vulnerability detection using bounded game semantics. The method, implemented in the YulToolkit, models computation as contract-environment interactions to explore all feasible traces within bounds, avoiding false positives. Evaluation on real-world incidents shows it effectively detects complex vulnerabilities like reentrancy.",
      "mindmap": "graph TB\n        Root[”A Bounded Game Semantics Checker for Precise Smart Contract Analysis”] --> Problem[”核心问题/Problem: Smart contract vulnerabilities lead to significant financial losses; existing tools suffer from false positives, false negatives, or scalability issues.”]\n        Root --> Method[”主要方法/Method: Bounded game semantics modeling; YulToolkit implementation for Yul; Feasible trace enumeration with instrumentation.”]\n        Root --> Results[”关键结果/Results: Detects known vulnerabilities (e.g., in The DAO) precisely; No false positives within bounds; Effective for hard-to-detect bugs like reentrancy.”]"
    },
    {
      "title": "Compiling Gradual Types with Evidence",
      "authors": "José Luis Romero, Cristóbal Isla, Matías Toro, Éric Tanter",
      "institution": "PLEIAD Lab, Computer Science Department (DCC), University of Chile",
      "link": "https://arxiv.org/pdf/2512.22684",
      "code": null,
      "tags": [
        "gradual typing",
        "evidence-based semantics",
        "monotonic references",
        "structural types"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/fb574126eb37250ba1fbd02c5000fce4a412197f9e248e6204ab7c46a65c386d_w640_q70.webp",
      "contributions": "1. Designed and implemented GrEv, an evidence-based compiler for gradual typing, demonstrating its viability for efficient implementation. 2. Bridged the gap between the formal Abstracting Gradual Typing (AGT) semantics and a practical compiler, identifying novel monotonic semantics. 3. Showed through empirical evaluation that the evidence-based approach can be competitive with and sometimes faster than coercion-based approaches, while offering more stable performance across the static-dynamic spectrum.",
      "summary": "This paper addresses the challenge of efficiently implementing sound gradual typing for languages with structural types. It proposes GrEv, a compiler based on the evidence-based semantics from the Abstracting Gradual Typing methodology, and demonstrates that this approach can achieve performance competitive with or better than existing coercion-based compilers.",
      "mindmap": "graph TB\n        Root[”Compiling Gradual Types with Evidence”] --> Problem[”核心问题/Problem<br>Is evidence-based semantics viable for efficient gradual typing implementation?”]\n        Root --> Method[”主要方法/Method<br>Design & implement GrEv compiler using evidence-based semantics”]\n        Root --> Results[”关键结果/Results<br>GrEv is competitive/faster than coercion-based, more stable performance”]"
    },
    {
      "title": "Anka: A Domain-Specific Language for Reliable LLM Code Generation",
      "authors": "Saif Khalfan Saif Al Mazrouei",
      "institution": "University of Wisconsin-Madison",
      "link": "https://arxiv.org/pdf/2512.23214",
      "code": null,
      "tags": [
        "llm inference",
        "Domain-Specific Language",
        "Constrained Syntax",
        "Code Generation",
        "Data Transformation Pipeline",
        "In-Context Learning"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6e450f3b6354a05c4c0dfa0c22c4f8b8dfc33c08282380080deb2d2f3a335d4_w640_q70.webp",
      "contributions": "1. Introduced Anka, a domain-specific language (DSL) with explicit, constrained syntax designed to reduce ambiguity in LLM code generation. 2. Demonstrated that LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy without prior training. 3. Showed that purposefully designed DSLs can outperform general-purpose languages (e.g., Python) on complex multi-step tasks, significantly reducing errors in operation sequencing and state management.",
      "summary": "This paper hypothesizes that the flexibility of general-purpose languages leads to systematic errors in LLM code generation for complex tasks. To test this, it introduces Anka, a constrained DSL for data transformation pipelines. The results show that LLMs can learn Anka from prompts and achieve significantly higher accuracy on multi-step tasks compared to Python, demonstrating the advantage of constrained syntax for reliable code generation.",
      "mindmap": "graph TB\n        A[Anka: A Domain-Specific Language for Reliable LLM Code Generation] --> B[核心问题/Problem: LLMs make systematic errors in complex multi-step code generation]\n        A --> C[主要方法/Method: Design Anka, a constrained DSL for data transformation pipelines]\n        A --> D[关键结果/Results: High parse success & task accuracy; Anka outperforms Python on multi-step tasks]"
    },
    {
      "title": "Verifying Asynchronous Hyperproperties in Reactive Systems",
      "authors": "Raven Beutner, Bernd Finkbeiner",
      "institution": "CISPA Helmholtz Center for Information Security",
      "link": "https://arxiv.org/pdf/2512.23344",
      "code": null,
      "tags": [
        "formal verification",
        "asynchronous hyperproperties",
        "HyperLTL",
        "model checking",
        "game semantics",
        "observational determinism"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0e321908ac7277c8c91e3b8e509efae3be66dbe7c391e613f19a6ca343db4f16_w640_q70.webp",
      "contributions": "1. Proposes a novel game-based approach for verifying arbitrary ∀∗∃∗ formulas in Asynchronous HyperLTL (A-HLTL) in reactive systems. 2. Interprets verification as a game between a verifier and refuter, where a winning strategy provides witnesses for traces and asynchronous alignments for stutterings. 3. Identifies fragments for which the game-based interpretation is complete, providing a finite-state decision procedure, and contributes a prototype implementation with encouraging experimental results.",
      "summary": "This paper addresses the challenge of model-checking asynchronous hyperproperties in reactive systems, which require comparing execution traces across different timesteps. It proposes a novel game-based verification method for a logic called Asynchronous HyperLTL (A-HLTL), interpreting the problem as a two-player game to find suitable trace stutterings. The approach provides a decision procedure for certain formula fragments and is supported by a prototype implementation.",
      "mindmap": "graph TB\n        Root[Verifying Asynchronous Hyperproperties in Reactive Systems] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem] --> P1[同步HyperLTL无法表达异步超属性/Synchronous HyperLTL cannot express asynchronous hyperproperties]\n        Problem --> P2[现有方法限制于受限片段或终止系统/Existing methods limited to restricted fragments or terminating systems]\n        Method[主要方法/Method] --> M1[提出基于游戏的验证方法/Propose a game-based verification approach]\n        Method --> M2[验证者与反驳者的双人游戏/Two-player game between verifier and refuter]\n        Method --> M3[获胜策略对应存在量化的证据/Winning strategy corresponds to witnesses for existential quantification]\n        Results[关键结果/Results] --> R1[为∀∗∃∗ A-HLTL公式提供方法/Provides method for arbitrary ∀∗∃∗ A-HLTL formulas]\n        Results --> R2[识别完全性的片段/Identifies fragments for which the interpretation is complete]\n        Results --> R3[原型实现与实验结果/Prototype implementation and experimental results]"
    },
    {
      "title": "Adaptable TeaStore: A Choreographic Approach",
      "authors": "Giuseppe De Palma, Saverio Giallorenzo, Ivan Lanese, Gianluigi Zavattaro",
      "institution": "Università di Bologna, INRIA",
      "link": "https://arxiv.org/pdf/2512.23497",
      "code": null,
      "tags": [
        "choreographic programming",
        "adaptable microservices",
        "choreographic programming",
        "AIOCJ",
        "runtime adaptation",
        "communication correctness"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/185975480d365934f5e03c65e64353b019eae6a649d47d0cfc956b5c90524a0a_w640_q70.webp",
      "contributions": "1. Presents an implementation of the Adaptable TeaStore reference model using the AIOCJ choreographic language. 2. Demonstrates that AIOCJ ensures by-construction correctness of communications (e.g., deadlock freedom) before, during, and after runtime adaptation. 3. Provides an analysis of the strengths and current limitations of the choreographic approach for adaptable cloud architectures, suggesting future refinements.",
      "summary": "This paper models the Adaptable TeaStore, a reference model for adaptable microservice architectures, using the AIOCJ choreographic programming language. The approach ensures communication correctness by construction and supports dynamic runtime adaptation. The work showcases the paradigm's strengths, identifies its limitations, and suggests future directions to better align it with real-world cloud systems.",
      "mindmap": "graph TB\n        Root(”Adaptable TeaStore: A Choreographic Approach”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”需要可适应的微服务架构/Need for adaptable microservice architectures”)\n        Method --> M1(”使用AIOCJ编排语言/Use AIOCJ choreographic language”)\n        Method --> M2(”确保通信正确性/Ensure communication correctness”)\n        Results --> R1(”展示方法的优势与局限/Showcase strengths and limitations”)\n        Results --> R2(”提出未来改进方向/Propose future refinements”)"
    },
    {
      "title": "Fancy Some Chips for Your TeaStore? Modeling the Control of an Adaptable Discrete System",
      "authors": "Anna Gallone, Simon Bliudze, Sophie Cerf, Olga Kouchnarenko",
      "institution": "Université Marie et Louis Pasteur (FEMTO-ST), Univ. Lille (Inria, CNRS, CRIStAL)",
      "link": "https://arxiv.org/pdf/2512.23496",
      "code": "https://github.com/NwaitDev/Chips_Public,",
      "tags": [
        "modeling languages",
        "control theory",
        "distributed systems",
        "Chips",
        "control theory",
        "component-based modeling",
        "Adaptable TeaStore",
        "BIP"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/80812c1a02bcd560c40919556c5ed13e3adfb056050e40a68f66bb940765d6f7_w640_q70.webp",
      "contributions": "1. Introduces Chips, a novel language for designing models of complex, intertwined systems by mixing control theory with general-purpose programming concepts. 2. Enables systematic design, modeling, and analysis of adaptable systems through functional block descriptions. 3. Demonstrates the language's application and utility using a variation of the Adaptable TeaStore as a concrete running example.",
      "summary": "This paper introduces Chips, a modeling language that combines control theory with programming concepts to facilitate the design and analysis of robust, component-based systems. The method is demonstrated on an Adaptable TeaStore application, showing how Chips can be used to systematically model complex, interacting entities like software, hardware, and services. The main conclusion is that Chips aids in ensuring system robustness and quality of service for web applications and cyber-physical systems.",
      "mindmap": "graph TB\n        Root[Fancy Some Chips for Your TeaStore?<br/>Modeling the Control of an Adaptable Discrete System] --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[核心问题/Problem<br/>Web应用需管理复杂、相互依赖的资源以确保鲁棒性] --> Problem_Detail[系统复杂/Complex System<br/>软件、硬件、网络、微服务交织]\n        Method[主要方法/Method<br/>提出Chips建模语言] --> Method_Detail1[混合概念/Mixed Concepts<br/>控制理论 + 通用编程语言]\n        Method --> Method_Detail2[功能块描述/Functional Blocks<br/>生成鲁棒的组件模型]\n        Results[关键结果/Results<br/>系统化设计、建模与分析] --> Results_Detail[案例演示/Case Study<br/>使用Adaptable TeaStore变体验证]"
    },
    {
      "title": "Beyond Per-Thread Lock Sets: Multi-Thread Critical Sections and Dynamic Deadlock Prediction",
      "authors": "Martin Sulzmann",
      "institution": "Karlsruhe University of Applied Sciences",
      "link": "https://arxiv.org/pdf/2512.23552",
      "code": null,
      "tags": [
        "dynamic deadlock prediction",
        "lock sets",
        "critical sections",
        "partial order relations",
        "false positives",
        "false negatives"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/67d58fc411df804f4dd88c8339e7b2a1b64be70d9eca8844a0dbb324c049cdea_w640_q70.webp",
      "contributions": "1. Introduces a novel trace-based characterization of critical sections that can span multiple threads, correcting the standard per-thread model., 2. Proposes a sound approximation of the multi-thread critical section concept using partial order relations, enabling an improved lock set construction., 3. Integrates the improved lock set construction into an extended SPDOffline deadlock predictor, reducing both false positives and false negatives without impacting performance.",
      "summary": "The paper identifies that standard per-thread lock set analysis for deadlock prediction is flawed because it ignores locks acquired across thread boundaries, leading to inaccurate results. To solve this, the authors propose a new model of multi-thread critical sections and a sound approximation method using partial order relations to construct more precise lock sets. This approach, integrated into an extended predictor, reduces false positives and false negatives while maintaining performance.",
      "mindmap": "graph TB\n        A[Beyond Per-Thread Lock Sets: Multi-Thread Critical Sections and Dynamic Deadlock Prediction] --> B[核心问题/Problem]\n        A --> C[主要方法/Method]\n        A --> D[关键结果/Results]\n        B --> B1[标准每线程锁集分析忽略跨线程锁/Standard per-thread lock sets ignore cross-thread locks]\n        B1 --> B2[导致假阳性和假阴性/Leads to false positives and false negatives]\n        C --> C1[提出基于轨迹的多线程临界区概念/Propose trace-based multi-thread critical sections]\n        C1 --> C2[使用偏序关系进行可靠近似/Use partial order relations for sound approximation]\n        C2 --> C3[改进锁集构造/Improved lock set construction]\n        D --> D1[减少假阳性和假阴性/Reduces false positives and false negatives]\n        D1 --> D2[性能不受影响/Performance not affected]"
    },
    {
      "title": "Automating the Analysis of Parsing Algorithms (and other Dynamic Programs)",
      "authors": "Tim Vieira, Ryan Cotterell, Jason Eisner",
      "institution": "Johns Hopkins University, ETH Zürich",
      "link": "https://arxiv.org/pdf/2512.23665",
      "code": "https://github.com/timvieira/dyna-pi",
      "tags": [
        "program analysis",
        "declarative programming",
        "dynamic programming",
        "static analysis",
        "complexity analysis",
        "Dyna"
      ],
      "day": "2025-12-30",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a554685bfd3ae13d5c3add707df8b528c44420979de28bdc7b5ff913d70b027_w640_q70.webp",
      "contributions": "1. Developed a system for the automated static analysis of declarative programs, specifically those written in the Dyna language for dynamic programming. 2. Successfully applied the system to infer types, identify dead/redundant code, and derive parametric runtime and space complexity bounds for NLP algorithms. 3. Demonstrated the system's utility by correctly analyzing and tightening the known complexity bounds of existing NLP parsing algorithms (e.g., from O(n⁷) to O(n⁶)).",
      "summary": "This paper develops an automated system for analyzing declarative programs, particularly dynamic programs used in NLP. The system helps programmers by statically inferring types, complexity bounds, and identifying code issues. The authors demonstrate its effectiveness by applying it to several NLP algorithms, where it successfully infers correct properties and even improves upon previously published complexity analyses.",
      "mindmap": "graph TB\n        Root(”Automating the Analysis of Parsing Algorithms”) --> Problem\n        Root --> Method\n        Root --> Results\n        Problem[”核心问题/Problem: Manual analysis of dynamic programs is tedious and error-prone.”]\n        Method[”主要方法/Method: Develop a system for automated static analysis of declarative (Dyna) programs.”]\n        Results[”关键结果/Results: System infers types, complexity bounds, and finds dead code; validates on NLP algorithms.”]"
    },
    {
      "title": "AInsteinBench: Benchmarking Coding Agents on Scientific Repositories",
      "authors": "Titouan Duston, Shuo Xin, Yang Sun, Daoguang Zan, Aoyan Li, Shulin Xin, Kai Shen, Yixiao Chen, Qiming Sun, Ge Zhang, Jiashuo Liu, Huan Zhou, Jingkai Liu, Zhichen Pu, Yuanheng Wang, Bo-Xuan Ge, Xin Tong, Fei Ye, Zhi-Chao Zhao, Wen-Biao Han, Zhoujian Cao, Yueran Zhao, Weiluo Ren, Qingshen Long, Yuxiao Liu, Anni Huang, Yidi Du, Yuanyuan Rong, Jiahao Peng",
      "institution": "ByteDance Seed, Princeton University",
      "link": "https://arxiv.org/pdf/2512.21373",
      "code": "https://github.com/ByteDance-Seed/AInsteinBench",
      "tags": [
        "software engineering",
        "benchmark",
        "scientific computing",
        "code generation",
        "pull requests",
        "test-driven verification"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/aadf07b453d8d5a061a247b4c4e5e4fc27a43f5b1ffca131e81738bd3728f348_w640_q70.webp",
      "contributions": "1. Introduces a novel benchmark (AInsteinBench) for evaluating LLM agents in end-to-end scientific development using real-world, production-grade codebases. 2. Curates tasks from maintainer-authored pull requests across six diverse scientific domains, ensuring scientific challenge and calibrated difficulty. 3. Employs executable environments and test-driven verification to measure core competencies beyond surface-level code generation.",
      "summary": "The paper introduces AInsteinBench, a benchmark designed to evaluate LLM agents' ability to function as scientific computing developers by solving tasks derived from real pull requests in scientific repositories. It uses executable environments and test-driven verification to assess deeper competencies. The benchmark provides a new standard for measuring AI's role in computational scientific research.",
      "mindmap": "graph TB\n        A[AInsteinBench: Benchmarking Coding Agents on Scientific Repositories] --> B[核心问题/Problem: Can LLM agents operate as scientific computing development agents?]\n        A --> C[主要方法/Method: End-to-end evaluation using tasks from real scientific pull requests]\n        A --> D[关键结果/Results: Measures ability beyond surface-level code generation]"
    },
    {
      "title": "Quantitative Verification of Omega-regular Properties in Probabilistic Programming",
      "authors": "Peixin Wang, Jianhao Bai, Min Zhang, C.-H. Luke Ong",
      "institution": "East China Normal University, Nanyang Technological University",
      "link": "https://arxiv.org/pdf/2512.21596",
      "code": null,
      "tags": [
        "probabilistic programming and verification",
        "temporal posterior inference",
        "omega-regular properties",
        "stochastic barrier certificates",
        "Rabin automata",
        "quantitative verification"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c111a01f9d5e96a85d9b5c62645dae0f5bb40053d723e34cb57dc7f31554dcda_w640_q70.webp",
      "contributions": "1. Introduces Temporal Posterior Inference (TPI), a new framework unifying probabilistic programming with temporal logic to compute posterior distributions over execution traces satisfying omega-regular properties. 2. Develops a novel method for computing rigorous upper and lower bounds on satisfaction probabilities by decomposing Rabin acceptance conditions and constructing sound stochastic barrier certificates. 3. Implements the approach in a prototype tool named TPInfer and demonstrates its effectiveness and efficiency on a suite of benchmarks.",
      "summary": "This paper addresses the limitation of standard probabilistic program inference, which fails to capture temporal behavior, by proposing Temporal Posterior Inference (TPI). TPI computes posterior distributions over program traces that satisfy omega-regular temporal specifications, using a method based on stochastic barrier certificates to provide quantitative verification bounds. The approach is implemented in the TPInfer tool and shown to be effective for inference over rich temporal properties.",
      "mindmap": "graph TB\n        Root(”Quantitative Verification of Omega-regular Properties in Probabilistic Programming”) --> Problem(”核心问题/Problem”)\n        Root --> Method(”主要方法/Method”)\n        Root --> Results(”关键结果/Results”)\n        Problem --> P1(”标准后验推断的局限/Limitation of Standard Posterior Inference”)\n        P1 --> P2(”无法捕捉程序执行的时间演化/Fails to capture temporal evolution”)\n        Method --> M1(”提出时间后验推断框架/Propose Temporal Posterior Inference (TPI)”)\n        M1 --> M2(”统一概率编程与时序逻辑/Unifies Probabilistic Programming & Temporal Logic”)\n        M2 --> M3(”基于随机屏障证书的定量验证方法/Quantitative Verification via Stochastic Barrier Certificates”)\n        Results --> R1(”实现原型工具 TPInfer/Implement Prototype Tool TPInfer”)\n        Results --> R2(”在基准测试中展示有效性与效率/Demonstrates Effectiveness & Efficiency on Benchmarks”)"
    },
    {
      "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking",
      "authors": "Tobias von Arx, Niels Mündler, Mark Vero, Maximilian Baader, Martin Vechev",
      "institution": "ETH Zurich, Snyk, INSAIT (Sofia University \"St. Kliment Ohridski\")",
      "link": "https://arxiv.org/pdf/2512.21132",
      "code": "https://github.com/eth-sri/autobaxbuilder",
      "tags": [
        "code security evaluation",
        "automated benchmarking",
        "LLM-generated code",
        "security vulnerabilities",
        "exploit generation",
        "plausibility checks"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/385abd6729afb970eba2217cc3d408efe70ab80f9d7aa0cbe7c2e0254f48b74c_w640_q70.webp",
      "contributions": "1. Introduces AutoBaxBuilder, a framework for generating code security benchmarking tasks and tests from scratch, addressing the limitations of manual benchmarks. 2. Proposes a robust pipeline with fine-grained plausibility checks that leverages LLMs to construct functionality tests and end-to-end security exploits. 3. Releases AutoBaxBench, a new benchmark of generated tasks, and demonstrates the framework's efficiency (under 2 hours and $10 per task) and quality through comparison with human-crafted tasks.",
      "summary": "The paper presents AutoBaxBuilder, a framework that automatically generates tasks and tests for benchmarking the security of code produced by large language models (LLMs). It uses an LLM-powered pipeline to create functional tests and security exploits, ensuring benchmark quality and scalability. The authors show the method is efficient and release a new benchmark, AutoBaxBench, to evaluate LLM security capabilities.",
      "mindmap": "graph LR\n    A[AutoBaxBuilder] --> B[核心问题/Problem: Manual security benchmarks are insufficient]\n    A --> C[主要方法/Method: Auto-generate tasks & tests with LLM pipeline]\n    A --> D[关键结果/Results: New benchmark, low cost, under 2 hours/task]"
    },
    {
      "title": "A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows",
      "authors": "Ivan Daunis",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19769",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/39e158baf642d33624c0967b1dcd509fbc3876a4bc52a539d4b6e7c800995b42_w640_q70.webp",
      "contributions": "",
      "summary": "A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows",
      "mindmap": ""
    },
    {
      "title": "Error Localization, Certificates, and Hints for Probabilistic Program Verification via Slicing (Extended Version)",
      "authors": "Philipp Schröer, Darion Haase, Joost-Pieter Katoen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20214",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/69d3cbfad240346e5e410cac1825291c7eadf08769c78985ee6d4b0712429906_w640_q70.webp",
      "contributions": "",
      "summary": "Error Localization, Certificates, and Hints for Probabilistic Program Verification via Slicing (Extended Version)",
      "mindmap": ""
    },
    {
      "title": "Symmaries: Automatic Inference of Formal Security Summaries for Java Programs",
      "authors": "Narges Khakpour, Nicolas Berthier",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20396",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a6bdfa1cdb9c42587497d38e68d6ea55a15a3372a5c197a241c9e2cf0c52caf1_w640_q70.webp",
      "contributions": "",
      "summary": "Symmaries: Automatic Inference of Formal Security Summaries for Java Programs",
      "mindmap": ""
    },
    {
      "title": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs",
      "authors": "Rupanshu Soi, Rohan Yadav, Fredrik Kjolstad, Alex Aiken, Maryam Mehri Dehnavi, Michael Garland, Michael Bauer",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18134",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/0cfd081b538bd7f4d1c91e06ba3fae36d2a230ad65cf3fc2e5160c3bdd9d98b3_w640_q70.webp",
      "contributions": "",
      "summary": "Optimal Software Pipelining and Warp Specialization for Tensor Core GPUs",
      "mindmap": ""
    },
    {
      "title": "DafnyMPI: A Dafny Library for Verifying Message-Passing Concurrent Programs",
      "authors": "Aleksandr Fedchin, Antero Mejr, Hari Sundar, Jeffrey S. Foster",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18842",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/865d9d3085c35dc9683acc6a3733dcc32fd4a99cf1bb506d84b85aa30412cf3e_w640_q70.webp",
      "contributions": "",
      "summary": "DafnyMPI: A Dafny Library for Verifying Message-Passing Concurrent Programs",
      "mindmap": ""
    },
    {
      "title": "Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems",
      "authors": "Prathamesh Devadiga",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19250",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9992d696f5e075764c08a2b42f4505f7b8d093d1a3975265c2c2a7716a8fbcbc_w640_q70.webp",
      "contributions": "",
      "summary": "Small Language Models as Compiler Experts: Auto-Parallelization for Heterogeneous Systems",
      "mindmap": ""
    },
    {
      "title": "LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models",
      "authors": "Yijie Zhi, Yayu Cao, Jianhua Dai, Xiaoyang Han, Jingwen Pu, Qingran Wu, Sheng Cheng, Ming Cai",
      "institution": "Zhejiang University, Zhejiang Institute of Administration, Beijing ShenZhou Aerospace Software Technology Ltd.",
      "link": "https://arxiv.org/pdf/2512.15766",
      "code": null,
      "tags": [
        "llm inference",
        "retrieval-augmented generation",
        "loop transformation",
        "static control part",
        "feedback-based iterative mechanism",
        "equivalence checking"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes LOOPRAG, a retrieval-augmented generation framework that guides Large Language Models (LLMs) to perform effective loop transformation optimization. It uses a loop-aware retrieval algorithm and a feedback-based iterative mechanism with compilation and testing to generate correct and efficient code. The evaluation shows LOOPRAG achieves significant speedups over both traditional compilers and base LLMs on standard benchmark suites.",
      "mindmap": ""
    },
    {
      "title": "A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning",
      "authors": "Daragh King, Vasileios Koutavas, Laura Kovacs",
      "institution": "Trinity College Dublin, Lero, TU Wien",
      "link": "https://arxiv.org/pdf/2512.15816",
      "code": null,
      "tags": [
        "others",
        "Hoare logic",
        "weakest precondition reasoning",
        "neurosymbolic AI",
        "OpenJML",
        "counterexample-guided repair"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper presents NeuroInv, a neurosymbolic method for generating loop invariants that combines a neural module using LLMs and Hoare logic for backward-chaining weakest precondition reasoning with a symbolic module that iteratively repairs invariants using counterexamples from OpenJML. It achieves a 99.5% success rate on a benchmark of 150 Java programs and demonstrates scalability on complex multi-loop programs, substantially outperforming other approaches.",
      "mindmap": ""
    },
    {
      "title": "Optimizing Agentic Language Model Inference via Speculative Tool Calls",
      "authors": "Daniel Nichols, Prajwal Singhania, Charles Jekel, Abhinav Bhatele, Harshitha Menon",
      "institution": "Lawrence Livermore National Laboratory, University of Maryland",
      "link": "https://arxiv.org/pdf/2512.15834",
      "code": null,
      "tags": [
        "llm inference",
        "speculative tool calls",
        "tool cache",
        "vLLM",
        "prefix-caching"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces system optimizations for language model agents that use external tools, specifically by speculating future tool calls and keeping sequences resident in the inference engine to reduce overhead. These methods lead to significant throughput improvements of hundreds of tokens per second. The authors also propose a new \"tool cache\" API to facilitate adoption of these optimizations.",
      "mindmap": ""
    },
    {
      "title": "Sharing State Between Prompts and Programs",
      "authors": "Ellie Y. Cheng, Logan Weber, Tian Jin, Michael Carbin",
      "institution": "MIT CSAIL",
      "link": "https://arxiv.org/pdf/2512.14805",
      "code": null,
      "tags": [
        "llm inference",
        "natural language programming",
        "shared program state",
        "natural function interface",
        "interoperability",
        "Nightjar"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces a programming abstraction called \"shared program state\" to enable seamless interoperability between natural language code (prompts) and formal program code (e.g., Python). It implements this abstraction in the Nightjar system, allowing natural code to directly read and write program variables. The results show that Nightjar programs can achieve higher task accuracy (+4-19%) and reduce lines of code by 39.6% on average, though with a runtime overhead of 0.4-4.3x compared to manual implementations.",
      "mindmap": ""
    }
  ]
}