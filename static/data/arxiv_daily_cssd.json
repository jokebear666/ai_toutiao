{
  "label": "cs.SD",
  "slug": "cssd",
  "week": "20251229-20260104",
  "items": [
    {
      "title": "Semantic Codebooks as Effective Priors for Neural Speech Compression",
      "authors": "Liuyang Bai, Weiyi Lu, Li Guo",
      "institution": "NYU Shanghai",
      "link": "https://arxiv.org/pdf/2512.21653",
      "code": null,
      "tags": [
        "speech compression",
        "semantic codebooks",
        "residual vector quantization (RVQ)",
        "HuBERT",
        "FiLM-conditioned decoder",
        "neural audio codec"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8a14c953c6c23139c4473b8d6e59b36c7615f79f4316119e168deb19de30eced_w640_q70.webp",
      "contributions": "1. Proposes SemDAC, a semantic-aware neural audio codec that uses semantic codebooks as priors for compression., 2. Introduces a design where the first RVQ quantizer is distilled from HuBERT to capture phonetic content, and a FiLM-conditioned decoder uses these semantic tokens., 3. Demonstrates superior performance over baseline DAC in perceptual metrics and ASR (Whisper) WER at significantly lower bitrates.",
      "summary": "The paper proposes SemDAC, a neural speech codec that uses semantic codebooks distilled from HuBERT as priors within an RVQ framework to separate phonetic from acoustic information. This method achieves better perceptual quality and lower word error rates for speech recognition at much lower bitrates compared to traditional neural codecs. The results show that semantic priors provide an effective inductive bias for efficient, recognition-friendly speech compression.",
      "mindmap": "graph TB\n        Root[”Semantic Codebooks as Effective Priors for Neural Speech Compression”] --> Problem[”核心问题/Problem: Traditional codecs inefficiently allocate bits for acoustic detail, neglecting linguistic structure.”]\n        Root --> Method[”主要方法/Method: Propose SemDAC, using HuBERT-distilled semantic codebooks in RVQ and a FiLM-conditioned decoder.”]\n        Root --> Results[”关键结果/Results: Outperforms DAC in perceptual metrics & ASR WER at lower bitrates (e.g., 0.95 vs 2.5 kbps).”]"
    },
    {
      "title": "Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning",
      "authors": "Most. Sharmin Sultana Samu, Md. Rakibul Islam, Md. Zahid Hossain, Md. Kamrozzaman Bhuiyan, Farhad Uz Zaman",
      "institution": "Not explicitly stated in the provided content.",
      "link": "https://arxiv.org/pdf/2512.21702",
      "code": null,
      "tags": [
        "audio deepfake detection",
        "transfer learning",
        "zero-shot inference",
        "fine-tuning",
        "Bengali audio",
        "BanglaFake dataset"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/66eab5318a9b87f6facd46827f1723103def2a66467b77d3a3f1b6ea7a41d92f_w640_q70.webp",
      "contributions": "1. Conducts the first systematic benchmark for Bengali deepfake audio detection using the BanglaFake dataset. 2. Evaluates and demonstrates the limited performance of multiple pre-trained models in a zero-shot setting for this task. 3. Shows that fine-tuning deep learning models (e.g., ResNet18) significantly improves detection performance, establishing an effective approach for low-resource languages.",
      "summary": "This paper addresses the underexplored problem of Bengali deepfake audio detection. It first evaluates several pre-trained models using zero-shot inference, finding limited performance, and then fine-tunes various architectures, with ResNet18 achieving the best results. The study concludes that fine-tuning is crucial for effective deepfake detection in low-resource languages like Bengali.",
      "mindmap": "graph TB\n        A[Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning] --> B(核心问题/Problem: Bengali Deepfake Audio Detection is unexplored)\n        A --> C(主要方法/Method: Zero-shot inference & Fine-tuning of pre-trained models)\n        A --> D(关键结果/Results: Fine-tuned ResNet18 achieves best performance (79.17% accuracy))"
    },
    {
      "title": "Rare Word Recognition and Translation Without Fine-Tuning via Task Vector in Speech Models",
      "authors": "Ruihao Jing, Cheng Gong, Yu Jiang, Boyu Zhu, Shansong Liu, Chi Zhang, Xiao-Lei Zhang, Xuelong Li",
      "institution": "Institute of Artificial Intelligence (TeleAI), China Telecom",
      "link": "https://arxiv.org/pdf/2512.21894",
      "code": null,
      "tags": [
        "speech recognition & translation",
        "task vector",
        "rare word recognition",
        "catastrophic forgetting",
        "speech-to-text",
        "parameter arithmetic"
      ],
      "day": "2025-12-29",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/4dd10590bdc1608f8eae90820d97325d5b60e598c9d06e1275430238b0313b56_w640_q70.webp",
      "contributions": "1. Proposes a training-free paradigm for rare word handling using task vectors, eliminating the need for fine-tuning. 2. Introduces word-level task vector arithmetic for flexible composition and reuse of rare-word capabilities. 3. Demonstrates that the method matches or surpasses fine-tuning on target words, improves general performance (~5 BLEU), and mitigates catastrophic forgetting.",
      "summary": "The paper addresses the bottleneck of rare word recognition in speech-to-text systems. It proposes a training-free method based on task vector arithmetic to compose rare-word capabilities, which avoids the costs and forgetting issues of fine-tuning. Experiments show the method performs comparably to fine-tuning on target words while improving overall translation quality and reducing catastrophic forgetting.",
      "mindmap": "graph TB\n        A[论文标题: Rare Word Recognition and Translation Without Fine-Tuning via Task Vector in Speech Models] --> B\n        A --> C\n        A --> D\n        B[核心问题/Problem: Rare words are a bottleneck for speech-to-text systems. Fine-tuning is costly and causes forgetting.]\n        C[主要方法/Method: Training-free paradigm using task vector arithmetic for flexible composition of rare-word capabilities.]\n        D[关键结果/Results: Matches/surpasses fine-tuning on target words, improves general performance (~5 BLEU), mitigates forgetting.]"
    },
    {
      "title": "SACodec: Asymmetric Quantization with Semantic Anchoring for Low-Bitrate High-Fidelity Neural Speech Codecs",
      "authors": "Zhongren Dong, Bin Wang, Jing Han, Haotian Guo, Xiaojun Mo, Yimin Cao, Zixing Zhang",
      "institution": "Hunan University, Beijing Xiaomi Mobile Software Co., Ltd",
      "link": "https://arxiv.org/pdf/2512.20944",
      "code": "https://github.com/SmileHnu/SACodec",
      "tags": [
        "model compression (quantization/pruning)",
        "neural speech codec",
        "asymmetric quantization",
        "semantic anchoring",
        "residual vector quantization",
        "low-bitrate"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/384253f52eb69a47b5425625dd427efd5329874714378c79c8064748111f7059_w640_q70.webp",
      "contributions": "1. Proposes SACodec, a novel neural speech codec based on an asymmetric dual-quantizer design that decouples semantic and acoustic detail quantization. 2. Introduces a Semantic Anchoring mechanism using a lightweight projector to align features with a frozen mHuBERT codebook, injecting linguistic priors and ensuring full codebook utilization. 3. Employs a residual activation module with SimVQ in the acoustic path, enabling a single-layer quantizer to recover fine-grained information at a low bitrate of 1.5 kbps.",
      "summary": "The paper addresses the trade-off between acoustic fidelity and semantic richness in low-bitrate neural speech codecs. It proposes SACodec, which uses an asymmetric dual-quantizer with semantic anchoring to decouple and efficiently quantize semantic and acoustic information. At 1.5 kbps, SACodec achieves state-of-the-art performance, delivering high-fidelity audio and semantically rich tokens for downstream tasks.",
      "mindmap": "graph LR\n        A[SACodec] --> B[核心问题/Problem: 低比特率下保真度与语义丰富度的权衡<br/>Trade-off between fidelity & semantics at low bitrate]\n        A --> C[主要方法/Method: 非对称双量化器与语义锚定<br/>Asymmetric dual-quantizer & Semantic Anchoring]\n        A --> D[关键结果/Results: 1.5 kbps SOTA，高保真与语义丰富<br/>1.5 kbps SOTA, high-fidelity & semantic richness]"
    },
    {
      "title": "Foundation Model-based Evaluation of Neuropsychiatric Disorders: A Lifespan-Inclusive, Multi-Modal, and Multi-Lingual Study",
      "authors": "Zhongren Dong, Haotian Guo, Weixiang Xu, Huan Zhao, Zixing Zhang",
      "institution": "Hunan University",
      "link": "https://arxiv.org/pdf/2512.20948",
      "code": null,
      "tags": [
        "multi-modal learning",
        "foundation models",
        "multi-modal fusion",
        "cross-corpus evaluation",
        "neuropsychiatric disorders",
        "multi-lingual datasets"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/18e74bbfe865915192b7a6c5c53058f33d0688ed82ef83023913d356622a3899_w640_q70.webp",
      "contributions": "1. Proposed FEND, a comprehensive multi-modal framework using foundation models for evaluating neuropsychiatric disorders across the lifespan. 2. Conducted a systematic evaluation using 13 multi-lingual datasets, identifying strengths and limitations of multi-modal fusion for different disorders. 3. Provided extensive benchmarks and analysis of performance-influencing factors (e.g., modality imbalance, dataset heterogeneity) to advance reproducible research in the field.",
      "summary": "The paper proposes FEND, a foundation model-based multi-modal framework for detecting neuropsychiatric disorders like Alzheimer's, depression, and autism from speech and text. It evaluates the framework on 13 multi-lingual datasets, finding that multi-modal fusion works well for Alzheimer's and depression but underperforms for autism due to dataset heterogeneity, and identifies modality imbalance as a key challenge.",
      "mindmap": "graph LR\n    A[Foundation Model-based Evaluation of Neuropsychiatric Disorders] --> B(核心问题/Problem: Multi-lingual generalization & lack of unified framework)\n    A --> C(主要方法/Method: FEND multi-modal framework using speech & text)\n    A --> D(关键结果/Results: Multi-modal fusion excels for AD/depression, underperforms for ASD)"
    },
    {
      "title": "Towards Practical Automatic Piano Reduction using BERT with Semi-supervised Learning",
      "authors": "Wan Ki Wong, Ka Ho To, Chuck-jee Chau, Lucas Wong, Kevin Y. Yip, Irwin King",
      "institution": "The Chinese University of Hong Kong",
      "link": "https://arxiv.org/pdf/2512.21324",
      "code": null,
      "tags": [
        "music information retrieval",
        "semi-supervised learning",
        "BERT",
        "MidiBERT",
        "piano reduction",
        "music simplification"
      ],
      "day": "2025-12-25",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9fa6dffcdfac263a4e2d329afe0244496c97160392fba638eb7885e17b6f2cd0_w640_q70.webp",
      "contributions": "1. Proposes a novel semi-supervised learning approach for automatic piano reduction to overcome the scarcity of labeled data. 2. Formulates the task as a two-step process of music simplification followed by harmonization. 3. Adapts and implements the MidiBERT framework to demonstrate practical and realistic piano reduction outputs.",
      "summary": "This paper addresses the challenge of automatic piano reduction, which is difficult due to a lack of labeled training data. The authors propose a semi-supervised learning method using a two-step simplification and harmonization approach based on the MidiBERT framework. They demonstrate that their method can produce practical piano reductions requiring only minor post-processing adjustments.",
      "mindmap": "graph LR\n    A[Towards Practical Automatic Piano Reduction using BERT with Semi-supervised Learning] --> B(核心问题/Problem: Lack of labeled data for piano reduction);\n    A --> C(主要方法/Method: Semi-supervised learning with two-step simplification & harmonization using MidiBERT);\n    A --> D(关键结果/Results: Outputs practical, realistic samples needing small post-processing);"
    },
    {
      "title": "OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting",
      "authors": "Soumen Garai, Suman Samui",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19739",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9778cadae68709b008dcf5db962164fda68a414cd3d9f0a2847361f564c06bad_w640_q70.webp",
      "contributions": "",
      "summary": "OASI: Objective-Aware Surrogate Initialization for Multi-Objective Bayesian Optimization in TinyML Keyword Spotting",
      "mindmap": ""
    },
    {
      "title": "DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation",
      "authors": "Jingqi Tian, Yiheng Du, Haoji Zhang, Yuji Wang, Isaac Ning Lee, Xulong Bai, Tianrui Zhu, Jingxuan Niu, Yansong Tang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20117",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5a290a5343ed508cf106c79abdb63608b4b4b69f2e0d42fc218565b45b8eb64f_w640_q70.webp",
      "contributions": "",
      "summary": "DDAVS: Disentangled Audio Semantics and Delayed Bidirectional Alignment for Audio-Visual Segmentation",
      "mindmap": ""
    },
    {
      "title": "Fun-Audio-Chat Technical Report",
      "authors": "Qian Chen, Luyao Cheng, Chong Deng, Xiangang Li, Jiaqing Liu, Chao-Hong Tan, Wen Wang, Junhao Xu, Jieping Ye, Qinglin Zhang, Qiquan Zhang, Jingren Zhou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20156",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/eedb25d29b9c63de7f75bd47632c06e734814fe19fe3f517a37a4d3d42f693c7_w640_q70.webp",
      "contributions": "",
      "summary": "Fun-Audio-Chat Technical Report",
      "mindmap": ""
    },
    {
      "title": "Spectral or spatial? Leveraging both for speaker extraction in challenging data conditions",
      "authors": "Aviad Eisenberg, Sharon Gannot, Shlomo E. Chazan",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20165",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a2e6c293f3a92cf0f0415d5fec939a5c5f428a3e0c209c365ab28c3e809efdf2_w640_q70.webp",
      "contributions": "",
      "summary": "Spectral or spatial? Leveraging both for speaker extraction in challenging data conditions",
      "mindmap": ""
    },
    {
      "title": "Aliasing-Free Neural Audio Synthesis",
      "authors": "Yicheng Gu, Junan Zhang, Chaoren Wang, Jerry Li, Zhizheng Wu, Lauri Juvela",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20211",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/a549aa0c2de9d6cdc7bb5318340c3e483cb7107e9edceb93cca1996336055d9b_w640_q70.webp",
      "contributions": "",
      "summary": "Aliasing-Free Neural Audio Synthesis",
      "mindmap": ""
    },
    {
      "title": "SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision",
      "authors": "Maxime Poli, Mahi Luthra, Youssef Benchekroun, Yosuke Higuchi, Martin Gleize, Jiayi Shen, Robin Algayres, Yu-An Chung, Mido Assran, Juan Pino, Emmanuel Dupoux",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20308",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d6351d1c5218be825df7f7572c20ff77011eb6baf9648cff6ea5fd370a23fda1_w640_q70.webp",
      "contributions": "",
      "summary": "SpidR: Learning Fast and Stable Linguistic Units for Spoken Language Models Without Supervision",
      "mindmap": ""
    },
    {
      "title": "MMEDIT: A Unified Framework for Multi-Type Audio Editing via Audio Language Model",
      "authors": "Ye Tao, Xuenan Xu, Wen Wu, Shuai Wang, Mengyue Wu, Chao Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20339",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/d9f6587dd18333bec8a3c1ef09e4b0d88ae24a552834119a986773e4cab8d287_w640_q70.webp",
      "contributions": "",
      "summary": "MMEDIT: A Unified Framework for Multi-Type Audio Editing via Audio Language Model",
      "mindmap": ""
    },
    {
      "title": "EnvSSLAM-FFN: Lightweight Layer-Fused System for ESDD 2026 Challenge",
      "authors": "Xiaoxuan Guo, Hengyan Huang, Jiayi Zhou, Renhe Sun, Jian Liu, Haonan Cheng, Long Ye, Qin Zhang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20369",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/8c97fcd2fb08810e50b437e65adda92aaad995c7c6a2fe1b0d5e18fe64a60d23_w640_q70.webp",
      "contributions": "",
      "summary": "EnvSSLAM-FFN: Lightweight Layer-Fused System for ESDD 2026 Challenge",
      "mindmap": ""
    },
    {
      "title": "AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition",
      "authors": "Rajdeep Chatterjee, Sudip Chakrabarty, Trishaani Acharjee, Deepanjali Mishra",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20407",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e762dea30a9edc887d84deefc367d35dd7c953e636f54a824f1e7f853c9d370f_w640_q70.webp",
      "contributions": "",
      "summary": "AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition",
      "mindmap": ""
    },
    {
      "title": "ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval",
      "authors": "Siyuan Fu, Xuchen Guo, Mingjun Liu, Hongxiang Li, Boyin Tan, Gongxi Zhu, Xianwei Zhuang, Jinghan Ru, Yuxin Xie, Yuguo Yin",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19703",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1ac2c582d6058b0a98adaddd2fc52e7fcb4b2f111f05471b7984fd691dc97aed_w640_q70.webp",
      "contributions": "",
      "summary": "ASK: Adaptive Self-improving Knowledge Framework for Audio Text Retrieval",
      "mindmap": ""
    },
    {
      "title": "QuarkAudio Technical Report",
      "authors": "Chengwei Liu, Haoyin Yan, Shaofei Xue, Xiaotao Liang, Xiaofu Chen, Bin Gong, Zheng Xue, Gang Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.20151",
      "code": null,
      "tags": [],
      "day": "2025-12-24",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7e02363a4a98352b6c8c9414798dd67bf2f94507522a80052452e339cb11e583_w640_q70.webp",
      "contributions": "",
      "summary": "QuarkAudio Technical Report",
      "mindmap": ""
    },
    {
      "title": "chatter: a Python library for applying information theory and AI/ML models to animal communication",
      "authors": "Mason Youngblood",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17935",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/03a1f96f640c75358ed76ff8b7bb2631f43b52386c0c7724516992109d54aa7a_w640_q70.webp",
      "contributions": "",
      "summary": "chatter: a Python library for applying information theory and AI/ML models to animal communication",
      "mindmap": ""
    },
    {
      "title": "Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition",
      "authors": "Haiying Xia, Zhongyi Huang, Yumei Tan, Shuxiang Song",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17946",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9ae2757814d2b16837d9b7cb3f26503eda8c49afc87dd1795f588ae949df6650_w640_q70.webp",
      "contributions": "",
      "summary": "Let the Model Learn to Feel: Mode-Guided Tonality Injection for Symbolic Music Emotion Recognition",
      "mindmap": ""
    },
    {
      "title": "Influence of string register locations on vibratos among violoncellists",
      "authors": "Steven Hu, Sophia H. Kim, Helena H. Kim, Hugo Mackay, Eric J. Heller",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18162",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/1976f9763c20928a682e746b3da0df875eff504fde311658b1b374a93a86593e_w640_q70.webp",
      "contributions": "",
      "summary": "Influence of string register locations on vibratos among violoncellists",
      "mindmap": ""
    },
    {
      "title": "A Data-Centric Approach to Generalizable Speech Deepfake Detection",
      "authors": "Wen Huang, Yuchen Mao, Yanmin Qian",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18210",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/11c5c62108382427b9c65030ed5660aed9d27504a01d24a882c770942db88020_w640_q70.webp",
      "contributions": "",
      "summary": "A Data-Centric Approach to Generalizable Speech Deepfake Detection",
      "mindmap": ""
    },
    {
      "title": "AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation",
      "authors": "Stephen Ni-Hahn, Rico Zhu, Jerry Yin, Yue Jiang, Cynthia Rudin, Simon Mak",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18232",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/ea758b7b2303802fc5c9547c032b22177a2accdeac3b36caed4230425fda6efa_w640_q70.webp",
      "contributions": "",
      "summary": "AutoSchA: Automatic Hierarchical Music Representations via Multi-Relational Node Isolation",
      "mindmap": ""
    },
    {
      "title": "Explainable Transformer-CNN Fusion for Noise-Robust Speech Emotion Recognition",
      "authors": "Sudip Chakrabarty, Pappu Bishwas, Rajdeep Chatterjee",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18298",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/c2d1fa5f1ea12d3598f0bd43290aa418e4ebda7629f53494201d317e1a493a95_w640_q70.webp",
      "contributions": "",
      "summary": "Explainable Transformer-CNN Fusion for Noise-Robust Speech Emotion Recognition",
      "mindmap": ""
    },
    {
      "title": "Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis",
      "authors": "Pengchao Feng, Yao Xiao, Ziyang Ma, Zhikang Niu, Shuai Fan, Yao Li, Sheng Wang, Xie Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18699",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6001c759cfdab0e5242e7ed2d8eff42f898cd26dbf0e8845df8e26a3c8936e15_w640_q70.webp",
      "contributions": "",
      "summary": "Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis",
      "mindmap": ""
    },
    {
      "title": "X-Talk: On the Underestimated Potential of Modular Speech-to-Speech Dialogue System",
      "authors": "Zhanxun Liu, Yifan Duan, Mengmeng Wang, Pengchao Feng, Haotian Zhang, Xiaoyu Xing, Yijia Shan, Haina Zhu, Yuhang Dai, Chaochao Lu, Xipeng Qiu, Lei Xie, Lan Wang, Nan Yan, Zilong Zheng, Ziyang Ma, Kai Yu, Xie Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18706",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/91aa95b8cdf1da17e389c0ae53a6896f970612bb3bc4d4b3762866d213442d93_w640_q70.webp",
      "contributions": "",
      "summary": "X-Talk: On the Underestimated Potential of Modular Speech-to-Speech Dialogue System",
      "mindmap": ""
    },
    {
      "title": "Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs",
      "authors": "Lisan Al Amin, Vandana P. Janeja",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18797",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/e389f385848f8816c83b5a1ff23be8e5adce564472d3ca92ed4e1c1107846a61_w640_q70.webp",
      "contributions": "",
      "summary": "Reliable Audio Deepfake Detection in Variable Conditions via Quantum-Kernel SVMs",
      "mindmap": ""
    },
    {
      "title": "Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform",
      "authors": "Yichuan Zhang, Chengxin Li, Yujie Gu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18791",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/828d54530ed9add4098a79bb9dd1f4047ed230dfaa399d57cade241c18713658_w640_q70.webp",
      "contributions": "",
      "summary": "Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform",
      "mindmap": ""
    },
    {
      "title": "Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation",
      "authors": "Guangtao Lyu, Chenghao Xu, Qi Liu, Jiexi Yan, Muli Yang, Fen Fang, Cheng Deng",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18804",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/dab2ab51309551324dcfdba087c2e5ec1e6dfdae10633047423e7233f30fdf84_w640_q70.webp",
      "contributions": "",
      "summary": "Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation",
      "mindmap": ""
    },
    {
      "title": "Speaker Recognition -- Wavelet Packet Based Multiresolution Feature Extraction Approach",
      "authors": "Saurabh Bhardwaj, Smriti Srivastava, Abhishek Bhandari, Krit Gupta, Hitesh Bahl, J.R.P. Gupta",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18902",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/6caea0e80d84c68574b410a0a5b25554cd842375056a2d48aeaecfbde8e7de29_w640_q70.webp",
      "contributions": "",
      "summary": "Speaker Recognition -- Wavelet Packet Based Multiresolution Feature Extraction Approach",
      "mindmap": ""
    },
    {
      "title": "JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis",
      "authors": "Fan Yu, Tao Wang, You Wu, Lin Zhu, Wei Deng, Weisheng Han, Wenchao Wang, Lin Hu, Xiangyu Liang, Xiaodong He, Yankun Huang, Yu Gu, Yuan Liu, Yuxuan Wang, Zhangyu Xiao, Ziteng Wang, Boya Dong, Feng Dang, Jinming Chen, Jingdong Li, Jun Wang, Yechen Jin, Yuan Zhang, Zhengyan Sheng, Xin Wang",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19090",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/f755dde99b4324e798ea9a6068c6ff699cdf819d3ac6581fe6856fcbfb048957_w640_q70.webp",
      "contributions": "",
      "summary": "JoyVoice: Long-Context Conditioning for Anthropomorphic Multi-Speaker Conversational Synthesis",
      "mindmap": ""
    },
    {
      "title": "DeepGESI: A Non-Intrusive Objective Evaluation Model for Predicting Speech Intelligibility in Hearing-Impaired Listeners",
      "authors": "Wenyu Luo, Jinhui Chen",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19374",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/5c0a852bcb45255507a3386c23b0bf527a541804415204b268f6689a7f82bed8_w640_q70.webp",
      "contributions": "",
      "summary": "DeepGESI: A Non-Intrusive Objective Evaluation Model for Predicting Speech Intelligibility in Hearing-Impaired Listeners",
      "mindmap": ""
    },
    {
      "title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
      "authors": "Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao, Julius Richter, Sanyuan Chen, Matt Le, Piotr Dollár, Christoph Feichtenhofer, Ann Lee, Wei-Ning Hsu",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19687",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/de3d411e351c730f5a4f6bcbb2b3a9ec59b568b77417127cf865aadf04270b18_w640_q70.webp",
      "contributions": "",
      "summary": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning",
      "mindmap": ""
    },
    {
      "title": "LIWhiz: A Non-Intrusive Lyric Intelligibility Prediction System for the Cadenza Challenge",
      "authors": "Ram C. M. C. Shekar, Iván López-Espejo",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17937",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/233c21744d1a90963372126b2d4d2d816073bc137a142aacddff67d2c28bb772_w640_q70.webp",
      "contributions": "",
      "summary": "LIWhiz: A Non-Intrusive Lyric Intelligibility Prediction System for the Cadenza Challenge",
      "mindmap": ""
    },
    {
      "title": "MEGState: Phoneme Decoding from Magnetoencephalography Signals",
      "authors": "Shuntaro Suzuki, Chia-Chun Dan Hsu, Yu Tsao, Komei Sugiura",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17978",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/82b8623a222c1e415238f07bfd00f186b0fd97345fe3a78288d0e4e33530c69c_w640_q70.webp",
      "contributions": "",
      "summary": "MEGState: Phoneme Decoding from Magnetoencephalography Signals",
      "mindmap": ""
    },
    {
      "title": "Continual Learning for Acoustic Event Classification",
      "authors": "Yang Xiao",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.17932",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/7ab087a2804d86d0265d9e36f2c4b37428d7f82d8a8dbb1c750ee9c6811ecc4a_w640_q70.webp",
      "contributions": "",
      "summary": "Continual Learning for Acoustic Event Classification",
      "mindmap": ""
    },
    {
      "title": "Phoneme-based speech recognition driven by large language models and sampling marginalization",
      "authors": "Te Ma, Nanjie Li, Hao Huang, Zhijian Ou",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.18371",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/b5833ba374008a7d74b1de29306f41fcd84ac79c51363d046bb5612486395c04_w640_q70.webp",
      "contributions": "",
      "summary": "Phoneme-based speech recognition driven by large language models and sampling marginalization",
      "mindmap": ""
    },
    {
      "title": "Real-Time Streamable Generative Speech Restoration with Flow Matching",
      "authors": "Simon Welker, Bunlong Lay, Maris Hillemann, Tal Peer, Timo Gerkmann",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19442",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/50f384e81e75842e398563da395c9a42cafd7dfeb79c90bc7c4489b17d033102_w640_q70.webp",
      "contributions": "",
      "summary": "Real-Time Streamable Generative Speech Restoration with Flow Matching",
      "mindmap": ""
    },
    {
      "title": "Sonified Quantum Seizures. Sonification of time series in epileptic seizures and simulation of seizures via quantum modelling",
      "authors": "Maria Mannone, Paulo Vitor Itaborai, Omar Costa Hamido, Miriam Goldack, Norbert Marwan, Peppino Fazio, Patrizia Ribino",
      "institution": "TBD",
      "link": "https://arxiv.org/pdf/2512.19272",
      "code": null,
      "tags": [],
      "day": "2025-12-23",
      "thumbnail": "https://pub-9ba4a5dae3bf4fc7b7d26411a74e92db.r2.dev/thumbnails/9688c851d6f89a1c52df7b967d23351ceaa8d6d34b065ab9a2ccaf7e65fd9a17_w640_q70.webp",
      "contributions": "",
      "summary": "Sonified Quantum Seizures. Sonification of time series in epileptic seizures and simulation of seizures via quantum modelling",
      "mindmap": ""
    },
    {
      "title": "Do Foundational Audio Encoders Understand Music Structure?",
      "authors": "Keisuke Toyama, Zhi Zhong, Akira Takahashi, Shusuke Takahashi, Yuki Mitsufuji",
      "institution": "Sony Group Corporation, Sony AI",
      "link": "https://arxiv.org/pdf/2512.17209",
      "code": null,
      "tags": [
        "music information retrieval",
        "music structure analysis",
        "foundational audio encoders",
        "self-supervised learning",
        "masked language modeling",
        "boundary detection",
        "function prediction"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper investigates the use of pretrained foundational audio encoders (FAEs) for music structure analysis (MSA). Through comprehensive experiments on 11 FAE types, it finds that models using self-supervised learning with masked language modeling on music data are particularly effective for MSA tasks like boundary detection and function prediction.",
      "mindmap": ""
    },
    {
      "title": "LibriVAD: A Scalable Open Dataset with Deep Learning Benchmarks for Voice Activity Detection",
      "authors": "Ioannis Stylianou, Achintya kr. Sarkar, Nauman Dawalatabad, James Glass, Zheng-Hua Tan",
      "institution": "Aalborg University, Pioneer Centre for AI, IIIT SriCity, Zoom Communications Inc., Massachusetts Institute of Technology",
      "link": "https://arxiv.org/pdf/2512.17281",
      "code": null,
      "tags": [
        "speech processing",
        "voice activity detection",
        "vision transformer",
        "MFCC",
        "Gammatone filter bank cepstral coefficients",
        "dataset augmentation",
        "out-of-distribution evaluation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces LibriVAD, a scalable open dataset for voice activity detection (VAD) created by augmenting LibriSpeech with diverse noise sources. It benchmarks several feature-model combinations and proposes using a Vision Transformer (ViT) architecture for VAD. The main conclusion is that ViT with MFCC features outperforms established models across various conditions, and scaling the dataset size and balancing its silence-to-speech ratio consistently improves out-of-distribution generalization.",
      "mindmap": ""
    },
    {
      "title": "Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026 TTS Track",
      "authors": "June Young Yi, Hyeongju Kim, Juheon Lee",
      "institution": "Supertone Inc.",
      "link": "https://arxiv.org/pdf/2512.17293",
      "code": null,
      "tags": [
        "diffusion training",
        "Self-Purifying Flow Matching (SPFM)",
        "flow matching",
        "text-to-speech (TTS)",
        "Supertonic",
        "fine-tuning",
        "in-the-wild speech",
        "label noise mitigation"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper presents a lightweight TTS system that fine-tunes the Supertonic model using Self-Purifying Flow Matching (SPFM) to robustly adapt to noisy, in-the-wild speech data. SPFM handles label noise by comparing conditional and unconditional flow matching losses, routing suspicious samples for unconditional training while still using their acoustic information. The resulting model achieved the best word error rate in the WildSpoof 2026 challenge, demonstrating that open-weight architectures can be effectively adapted to real-world conditions with explicit noise-handling mechanisms.",
      "mindmap": ""
    },
    {
      "title": "When De-noising Hurts: A Systematic Study of Speech Enhancement Effects on Modern Medical ASR Systems",
      "authors": "Sujal Chondhekar, Vasanth Murukuri, Rushabh Vasani, Sanika Goyal, Rajshree Badami, Anushree Rana, Sanjana SN, Karthik Pandia, Sulabh Katiyar, Neha Jagadeesh, Sankalp Gulati",
      "institution": "EkaCare (Orbi Health Private Limited)",
      "link": "https://arxiv.org/pdf/2512.17562",
      "code": null,
      "tags": [
        "others",
        "MetricGAN-plus-voicebank",
        "semantic WER",
        "noise robustness",
        "speech enhancement"
      ],
      "day": "2025-12-22",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper systematically evaluates the effect of MetricGAN-plus-voicebank speech enhancement on four modern ASR systems using medical speech under nine noise conditions. The study finds that denoising preprocessing consistently degrades ASR performance across all models and conditions, suggesting modern ASR models are inherently noise-robust and enhancement may remove critical acoustic features.",
      "mindmap": ""
    },
    {
      "title": "Domain-Agnostic Causal-Aware Audio Transformer for Infant Cry Classification",
      "authors": "Geofrey Owino, Bernard Shibwabo Kasamani, Ahmed M. Abdelmoniem, Edem Wornyo",
      "institution": "Strathmore University, Google Research, Queen Mary University of London",
      "link": "https://arxiv.org/pdf/2512.16271",
      "code": null,
      "tags": [
        "multi-modal training",
        "transformer",
        "causal attention",
        "hierarchical representation",
        "multi-task learning",
        "domain-adversarial training",
        "controlled perturbation training",
        "counterfactual simulation"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes DACH-TIC, a domain-agnostic causal-aware hierarchical audio transformer that integrates causal attention, multi-task learning, and adversarial domain generalization for robust infant cry classification. It outperforms state-of-the-art baselines in accuracy and macro-F1 score and demonstrates strong generalization to unseen acoustic environments with minimal performance degradation.",
      "mindmap": ""
    },
    {
      "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
      "authors": "Sara Papi, Javier Garcia Gilabert, Zachary Hopton, Vilém Zouhar, Carlos Escolano, Gerard I. Gállego, Jorge Iranzo-Sánchez, Ahrii Kim, Dominik Macháček, Patricia Schmidtova, Maike Züfle",
      "institution": "Fondazione Bruno Kessler, Barcelona Supercomputing Center, University of Zurich, ETH Zurich, Universitat Politècnica de Catalunya, Universitat Politècnica de València, AI-Bio Convergence Research Institute, Charles University, KIT",
      "link": "https://arxiv.org/pdf/2512.16378",
      "code": null,
      "tags": [
        "multi-modal inference",
        "SpeechLLMs",
        "cascaded systems",
        "speech foundation models",
        "speech-to-text translation",
        "benchmarking",
        "Whisper",
        "SeamlessM4T",
        "Gemma",
        "Tower+"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces the \"Hearing to Translate\" test suite to benchmark SpeechLLMs against cascaded and direct speech-to-text translation systems. It finds that cascaded systems, which combine speech recognition with LLM-based translation, remain the most reliable overall, while current SpeechLLMs only match them in specific scenarios. The study concludes that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.",
      "mindmap": ""
    },
    {
      "title": "Pseudo-Cepstrum: Pitch Modification for Mel-Based Neural Vocoders",
      "authors": "Nikolaos Ellinas, Alexandra Vioni, Panos Kakoulidis, Georgios Vamvoukakis, Myrsini Christidou, Konstantinos Markopoulos, Junkwang Oh, Gunu Jho, Inchul Hwang, Aimilios Chalamandaris, Pirros Tsiakoulis",
      "institution": "Innoetics, Samsung Electronics, Samsung Electronics Mobile eXperience Business",
      "link": "https://arxiv.org/pdf/2512.16519",
      "code": null,
      "tags": [
        "speech synthesis",
        "cepstrum",
        "pitch shifting",
        "mel-spectrogram",
        "DCT",
        "pseudo-inverse mel transform",
        "neural vocoder"
      ],
      "day": "2025-12-19",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces a cepstrum-based pitch modification method that directly manipulates the cepstral feature space to shift the harmonic structure in a mel-spectrogram, making it compatible with any mel-based neural vocoder without retraining. The method is validated through objective and subjective evaluations, showing its effectiveness compared to traditional pitch modification techniques.",
      "mindmap": ""
    },
    {
      "title": "Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms",
      "authors": "Lucas Cesar Ferreira Domingos, Russell Brinkworth, Paulo Eduardo Santos, Karl Sammut",
      "institution": "Flinders University, PrioriAnalytica",
      "link": "https://arxiv.org/pdf/2512.14714",
      "code": null,
      "tags": [
        "others",
        "learnable Gabor filters",
        "ResNeXt",
        "squeeze-and-excitation attention",
        "spectrograms",
        "deep learning"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes GSE ResNeXt, a deep learning model that integrates learnable Gabor filter convolutions with a ResNeXt backbone and squeeze-and-excitation attention mechanisms for underwater acoustic target classification. The model demonstrates improved classification accuracy and a 28% reduction in training time compared to baseline models, highlighting the effectiveness of combining adaptive signal processing with attention for better generalization in data-limited scenarios.",
      "mindmap": ""
    },
    {
      "title": "Audio MultiChallenge: A Multi-Turn Evaluation of Spoken Dialogue Systems on Natural Human Interaction",
      "authors": "Advait Gosai, Tyler Vuong, Utkarsh Tyagi, Steven Li, Wenjia You, Miheer Bavare, Arda Uçar, Zhongwang Fang, Brian Jang, Bing Liu, Yunzhong He",
      "institution": "Scale AI",
      "link": "https://arxiv.org/pdf/2512.14865",
      "code": null,
      "tags": [
        "multi-modal inference",
        "end-to-end spoken dialogue systems",
        "audio language models",
        "multi-turn evaluation",
        "speech-to-speech",
        "audio-native benchmark",
        "inference memory",
        "instruction retention",
        "self coherence",
        "voice editing"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces Audio MultiChallenge, a benchmark for evaluating end-to-end spoken dialogue systems on natural, multi-turn conversations. It extends a text-based framework with a new \"Voice Editing\" axis and audio-specific augmentations, using a hybrid pipeline to curate conversations with natural disfluencies. The evaluation shows even top models like Gemini 3 Pro Preview struggle, highlighting difficulties in tracking audio edits, cues, and long context in spoken dialogue.",
      "mindmap": ""
    },
    {
      "title": "TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation",
      "authors": "Zhenzhi Wang, Jian Wang, Ke Ma, Dahua Lin, Bing Zhou",
      "institution": "The Chinese University of Hong Kong, Snap Inc.",
      "link": "https://arxiv.org/pdf/2512.14938",
      "code": null,
      "tags": [
        "multi-modal training",
        "diffusion transformer",
        "video VAE",
        "sliding window mechanism",
        "motion-frame context",
        "latent noise injection",
        "MLLM director"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper introduces TalkVerse, a large-scale open dataset for audio-driven video generation, and a reproducible 5B Diffusion Transformer baseline model. The model uses a video VAE with a high downsampling ratio and a sliding window mechanism to enable minute-long video generation with low drift and lower inference cost. The main conclusion is that this open resource and efficient model democratize research in this field by enabling fair comparisons and reducing computational barriers.",
      "mindmap": ""
    },
    {
      "title": "Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities",
      "authors": "Aref Farhadipour, Teodora Vukovic, Volker Dellwo, Petr Motlicek, Srikanth Madikeri",
      "institution": "University of Zurich, Idiap Research Institute",
      "link": "https://arxiv.org/pdf/2512.14961",
      "code": null,
      "tags": [
        "multi-modal inference",
        "multi-task learning",
        "cross-attention",
        "gated fusion",
        "confidence-weighted fusion",
        "adaptive fusion"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes a robust trimodal person recognition framework that integrates voice, face, and gesture data using multi-task learning, cross-attention, and a gated, confidence-weighted fusion strategy to handle missing or degraded modalities. It achieves high accuracy on the CANDOR and VoxCeleb1 datasets and maintains performance even when modalities are unavailable, demonstrating robustness for real-world applications.",
      "mindmap": ""
    },
    {
      "title": "BEAT2AASIST model with layer fusion for ESDD 2026 Challenge",
      "authors": "Sanghyeok Chung, Eujin Kim, Donggun Kim, Gaeun Heo, Jeongbin You, Nahyun Lee, Sunmook Choi, Soyul Han, Seungsang Oh, Il-Youp Kwak",
      "institution": "Korea University, Chung-Ang University, Cornell University, Hannam University",
      "link": "https://arxiv.org/pdf/2512.15180",
      "code": null,
      "tags": [
        "audio deepfake detection",
        "BEATs",
        "AASIST",
        "transformer layer fusion",
        "vocoder-based data augmentation",
        "dual-branch architecture"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "The paper proposes BEAT2AASIST, a model for environmental sound deepfake detection that extends BEATs-AASIST by splitting features into dual AASIST branches and incorporating transformer layer fusion strategies. It also uses vocoder-based data augmentation for robustness. The approach demonstrates competitive performance on the ESDD 2026 Challenge test sets.",
      "mindmap": ""
    },
    {
      "title": "O-EENC-SD: Efficient Online End-to-End Neural Clustering for Speaker Diarization",
      "authors": "Elio Gruttadauria, Mathieu Fontaine, Jonathan Le Roux, Slim Essid",
      "institution": "Télécom Paris, Institut polytechnique de Paris, Mitsubishi Electric Research Laboratories (MERL)",
      "link": "https://arxiv.org/pdf/2512.15229",
      "code": null,
      "tags": [
        "others",
        "EEND-EDA",
        "RNN-based stitching",
        "centroid refinement decoder",
        "permutation-invariant training (PIT)",
        "online speaker diarization"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper introduces O-EENC-SD, an efficient online end-to-end neural clustering system for speaker diarization. It is based on EEND-EDA and features a novel RNN-based stitching mechanism and a centroid refinement decoder for online prediction. The system is shown to be competitive with state-of-the-art methods on the CallHome dataset while offering a better trade-off between diarization error rate and computational complexity.",
      "mindmap": ""
    },
    {
      "title": "Time-Varying Audio Effect Modeling by End-to-End Adversarial Training",
      "authors": "Yann Bourdin, Pierrick Legrand, Fanny Roche",
      "institution": "Arturia, Inria, IMS, University of Bordeaux, Bordeaux INP",
      "link": "https://arxiv.org/pdf/2512.15313",
      "code": null,
      "tags": [
        "others",
        "Generative Adversarial Network (GAN)",
        "convolutional-recurrent architecture",
        "adversarial training",
        "State Prediction Network (SPN)",
        "chirp-train signals"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes a two-stage GAN framework for black-box modeling of time-varying audio effects, using only input-output recordings without needing control signals. The method involves an adversarial training phase followed by supervised fine-tuning with a State Prediction Network for synchronization. Experiments on a vintage phaser demonstrate the approach's effectiveness in capturing time-varying dynamics.",
      "mindmap": ""
    },
    {
      "title": "A Conditioned UNet for Music Source Separation",
      "authors": "Ken O'Hanlon, Basil Woods, Lin Wang, Mark Sandler",
      "institution": "Queen Mary University of London, AudioStrip Ltd.",
      "link": "https://arxiv.org/pdf/2512.15532",
      "code": null,
      "tags": [
        "others",
        "conditioned UNet",
        "music source separation",
        "QSCNet",
        "Sparse Compressed Network",
        "Bandsplit RNN",
        "Banquet",
        "MoisesDb"
      ],
      "day": "2025-12-18",
      "thumbnail": null,
      "contributions": "",
      "summary": "This paper proposes QSCNet, a novel conditioned UNet architecture for music source separation that uses an audio query to specify the target stem, eliminating the need for a predefined instrument vocabulary. The method integrates conditioning elements into a Sparse Compressed Network and is shown to outperform the prior Banquet model by over 1dB SNR on certain tasks while using fewer than half the parameters.",
      "mindmap": ""
    }
  ]
}